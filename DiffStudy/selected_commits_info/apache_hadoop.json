[
  {
    "oid": "c7a5f2906fcfd073a402b6981b091bd6d9b80294",
    "message": "HADOOP-13766. Fix a typo in the comments of RPC.getProtocolVersion. Contributed by Ethan Li.",
    "date": "2016-11-24T01:08:38Z",
    "url": "https://github.com/apache/hadoop/commit/c7a5f2906fcfd073a402b6981b091bd6d9b80294",
    "details": {
      "sha": "e16a8f5e6321f83b6e0edff3e84ac176005768f3",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/RPC.java",
      "status": "modified",
      "additions": 3,
      "deletions": 2,
      "changes": 5,
      "blob_url": "https://github.com/apache/hadoop/blob/c7a5f2906fcfd073a402b6981b091bd6d9b80294/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FRPC.java",
      "raw_url": "https://github.com/apache/hadoop/raw/c7a5f2906fcfd073a402b6981b091bd6d9b80294/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FRPC.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FRPC.java?ref=c7a5f2906fcfd073a402b6981b091bd6d9b80294",
      "patch": "@@ -158,8 +158,9 @@ static public String getProtocolName(Class<?> protocol) {\n   \n   /**\n    * Get the protocol version from protocol class.\n-   * If the protocol class has a ProtocolAnnotation, then get the protocol\n-   * name from the annotation; otherwise the class name is the protocol name.\n+   * If the protocol class has a ProtocolAnnotation,\n+   * then get the protocol version from the annotation;\n+   * otherwise get it from the versionID field of the protocol class.\n    */\n   static public long getProtocolVersion(Class<?> protocol) {\n     if (protocol == null) {",
      "parent_sha": "860d49aa6ae7790d970d7f2322fed890b0e5cda2"
    }
  },
  {
    "oid": "d6c233fce67104c1c4da802eb695526e60058536",
    "message": "HDDS-1377. OM failed to start with incorrect hostname set as ip address in CSR. (#683)",
    "date": "2019-04-03T18:53:51Z",
    "url": "https://github.com/apache/hadoop/commit/d6c233fce67104c1c4da802eb695526e60058536",
    "details": {
      "sha": "b8da717a043996c80f0242562262a3d4ecac87bf",
      "filename": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java",
      "status": "modified",
      "additions": 4,
      "deletions": 3,
      "changes": 7,
      "blob_url": "https://github.com/apache/hadoop/blob/d6c233fce67104c1c4da802eb695526e60058536/hadoop-ozone%2Fozone-manager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fom%2FOzoneManager.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d6c233fce67104c1c4da802eb695526e60058536/hadoop-ozone%2Fozone-manager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fom%2FOzoneManager.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone%2Fozone-manager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fom%2FOzoneManager.java?ref=d6c233fce67104c1c4da802eb695526e60058536",
      "patch": "@@ -1382,6 +1382,7 @@ private static void getSCMSignedCert(CertificateClient client,\n     }\n     // Get host name.\n     String hostname = omRpcAdd.getAddress().getHostName();\n+    String ip = omRpcAdd.getAddress().getHostAddress();\n \n     String subject = UserGroupInformation.getCurrentUser()\n         .getShortUserName() + \"@\" + hostname;\n@@ -1392,16 +1393,16 @@ private static void getSCMSignedCert(CertificateClient client,\n         .setScmID(omStore.getScmId())\n         .setClusterID(omStore.getClusterID())\n         .setSubject(subject)\n-        .addIpAddress(hostname);\n+        .addIpAddress(ip);\n \n     LOG.info(\"Creating csr for OM->dns:{},ip:{},scmId:{},clusterId:{},\" +\n-            \"subject:{}\", hostname, omRpcAdd.getAddress().getHostAddress(),\n+            \"subject:{}\", hostname, ip,\n         omStore.getScmId(), omStore.getClusterID(), subject);\n \n     HddsProtos.OzoneManagerDetailsProto.Builder omDetailsProtoBuilder =\n         HddsProtos.OzoneManagerDetailsProto.newBuilder()\n             .setHostName(omRpcAdd.getHostName())\n-            .setIpAddress(hostname)\n+            .setIpAddress(ip)\n             .setUuid(omStore.getOmId())\n             .addPorts(HddsProtos.Port.newBuilder()\n                 .setName(RPC_PORT)",
      "parent_sha": "be488b6070a124234c77f16193ee925d32ca9a20"
    }
  },
  {
    "oid": "99ab511cbac570bea9d31a55898b95590a8e3159",
    "message": "HADOOP-15293. TestLogLevel fails on Java 9\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
    "date": "2018-03-09T18:20:35Z",
    "url": "https://github.com/apache/hadoop/commit/99ab511cbac570bea9d31a55898b95590a8e3159",
    "details": {
      "sha": "fd30b50141f2882a1a013cd2c03ec8299e2880b9",
      "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/log/TestLogLevel.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/hadoop/blob/99ab511cbac570bea9d31a55898b95590a8e3159/hadoop-common-project%2Fhadoop-common%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Flog%2FTestLogLevel.java",
      "raw_url": "https://github.com/apache/hadoop/raw/99ab511cbac570bea9d31a55898b95590a8e3159/hadoop-common-project%2Fhadoop-common%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Flog%2FTestLogLevel.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Flog%2FTestLogLevel.java?ref=99ab511cbac570bea9d31a55898b95590a8e3159",
      "patch": "@@ -358,7 +358,7 @@ public void testLogLevelByHttp() throws Exception {\n     } catch (SSLException e) {\n       GenericTestUtils.assertExceptionContains(\"Error while authenticating \"\n           + \"with endpoint\", e);\n-      GenericTestUtils.assertExceptionContains(\"Unrecognized SSL message\", e\n+      GenericTestUtils.assertExceptionContains(\"recognized SSL message\", e\n           .getCause());\n     }\n   }\n@@ -379,7 +379,7 @@ public void testLogLevelByHttpWithSpnego() throws Exception {\n     } catch (SSLException e) {\n       GenericTestUtils.assertExceptionContains(\"Error while authenticating \"\n           + \"with endpoint\", e);\n-      GenericTestUtils.assertExceptionContains(\"Unrecognized SSL message\", e\n+      GenericTestUtils.assertExceptionContains(\"recognized SSL message\", e\n           .getCause());\n     }\n   }",
      "parent_sha": "4f395063bbae1636d4c59bc962916d78694b50d3"
    }
  },
  {
    "oid": "a1f6564d31c16582ea949e4448091f1cc56c7dfe",
    "message": "HADOOP-13458. LoadBalancingKMSClientProvider#doOp should log IOException stacktrace. Contributed by Chen Liang.",
    "date": "2016-08-03T21:19:47Z",
    "url": "https://github.com/apache/hadoop/commit/a1f6564d31c16582ea949e4448091f1cc56c7dfe",
    "details": {
      "sha": "e6ff0798212de95464d3b68dc2c1b465620a02e7",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java",
      "status": "modified",
      "additions": 3,
      "deletions": 2,
      "changes": 5,
      "blob_url": "https://github.com/apache/hadoop/blob/a1f6564d31c16582ea949e4448091f1cc56c7dfe/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2FLoadBalancingKMSClientProvider.java",
      "raw_url": "https://github.com/apache/hadoop/raw/a1f6564d31c16582ea949e4448091f1cc56c7dfe/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2FLoadBalancingKMSClientProvider.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2FLoadBalancingKMSClientProvider.java?ref=a1f6564d31c16582ea949e4448091f1cc56c7dfe",
      "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension;\n import org.apache.hadoop.security.Credentials;\n import org.apache.hadoop.security.token.Token;\n+import org.apache.hadoop.util.StringUtils;\n import org.apache.hadoop.util.Time;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -93,8 +94,8 @@ private <T> T doOp(ProviderCallable<T> op, int currPos)\n       try {\n         return op.call(provider);\n       } catch (IOException ioe) {\n-        LOG.warn(\"KMS provider at [{}] threw an IOException [{}]!!\",\n-            provider.getKMSUrl(), ioe.getMessage());\n+        LOG.warn(\"KMS provider at [{}] threw an IOException!! {}\",\n+            provider.getKMSUrl(), StringUtils.stringifyException(ioe));\n         ex = ioe;\n       } catch (Exception e) {\n         if (e instanceof RuntimeException) {",
      "parent_sha": "6f63566694f8cec64a469448a8fa00ce921ce367"
    }
  },
  {
    "oid": "3470dd9a2f6687d2d015bdfb34ca309ccf776a4b",
    "message": "HDFS-6614. Addendum patch to shorten TestPread run time with smaller retry timeout setting. Contributed by Liang Xie.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1608846 13f79535-47bb-0310-9956-ffa450edef68",
    "date": "2014-07-08T16:43:16Z",
    "url": "https://github.com/apache/hadoop/commit/3470dd9a2f6687d2d015bdfb34ca309ccf776a4b",
    "details": {
      "sha": "209cd7b44c3a748abc0252de2d82eba07b641ef6",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPread.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/hadoop/blob/3470dd9a2f6687d2d015bdfb34ca309ccf776a4b/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FTestPread.java",
      "raw_url": "https://github.com/apache/hadoop/raw/3470dd9a2f6687d2d015bdfb34ca309ccf776a4b/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FTestPread.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FTestPread.java?ref=3470dd9a2f6687d2d015bdfb34ca309ccf776a4b",
      "patch": "@@ -284,6 +284,7 @@ public void testHedgedReadLoopTooManyTimes() throws IOException {\n         numHedgedReadPoolThreads);\n     conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS,\n         hedgedReadTimeoutMillis);\n+    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE, 0);\n     // Set up the InjectionHandler\n     DFSClientFaultInjector.instance = Mockito\n         .mock(DFSClientFaultInjector.class);\n@@ -329,11 +330,11 @@ public Void answer(InvocationOnMock invocation) throws Throwable {\n     } catch (BlockMissingException e) {\n       assertTrue(false);\n     } finally {\n+      Mockito.reset(injector);\n       IOUtils.cleanup(null, input);\n       IOUtils.cleanup(null, output);\n       fileSys.close();\n       cluster.shutdown();\n-      Mockito.reset(injector);\n     }\n   }\n ",
      "parent_sha": "cddd6184ad4f7ffbfa6aef84c7a095cfa54e4692"
    }
  },
  {
    "oid": "21176a8ba7aa8acc4b1fe04c013b8716c2b25aa7",
    "message": "HADOOP-15282. HADOOP-15235 broke TestHttpFSServerWebServer\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
    "date": "2018-03-04T15:33:47Z",
    "url": "https://github.com/apache/hadoop/commit/21176a8ba7aa8acc4b1fe04c013b8716c2b25aa7",
    "details": {
      "sha": "a05876620e582bb613739c43f05dd6bd6b0addf1",
      "filename": "hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServerWebServer.java",
      "status": "modified",
      "additions": 8,
      "deletions": 2,
      "changes": 10,
      "blob_url": "https://github.com/apache/hadoop/blob/21176a8ba7aa8acc4b1fe04c013b8716c2b25aa7/hadoop-hdfs-project%2Fhadoop-hdfs-httpfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fhttp%2Fserver%2FTestHttpFSServerWebServer.java",
      "raw_url": "https://github.com/apache/hadoop/raw/21176a8ba7aa8acc4b1fe04c013b8716c2b25aa7/hadoop-hdfs-project%2Fhadoop-hdfs-httpfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fhttp%2Fserver%2FTestHttpFSServerWebServer.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs-httpfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fhttp%2Fserver%2FTestHttpFSServerWebServer.java?ref=21176a8ba7aa8acc4b1fe04c013b8716c2b25aa7",
      "patch": "@@ -22,9 +22,12 @@\n import java.io.InputStreamReader;\n import java.net.HttpURLConnection;\n import java.net.URL;\n+import java.nio.charset.Charset;\n import java.text.MessageFormat;\n \n+import org.apache.commons.io.FileUtils;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.security.authentication.server.AuthenticationFilter;\n import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.test.HadoopUsersConfTestHelper;\n import org.junit.Assert;\n@@ -57,14 +60,17 @@ public static void beforeClass() throws Exception {\n     System.setProperty(\"httpfs.home.dir\", homeDir.getAbsolutePath());\n     System.setProperty(\"httpfs.log.dir\", logsDir.getAbsolutePath());\n     System.setProperty(\"httpfs.config.dir\", confDir.getAbsolutePath());\n-    new File(confDir, \"httpfs-signature.secret\").createNewFile();\n+    FileUtils.writeStringToFile(new File(confDir, \"httpfs-signature.secret\"),\n+        \"foo\", Charset.forName(\"UTF-8\"));\n   }\n \n   @Before\n   public void setUp() throws Exception {\n     Configuration conf = new Configuration();\n     conf.set(HttpFSServerWebServer.HTTP_HOSTNAME_KEY, \"localhost\");\n     conf.setInt(HttpFSServerWebServer.HTTP_PORT_KEY, 0);\n+    conf.set(AuthenticationFilter.SIGNATURE_SECRET_FILE,\n+        \"httpfs-signature.secret\");\n     Configuration sslConf = new Configuration();\n     webServer = new HttpFSServerWebServer(conf, sslConf);\n   }\n@@ -76,7 +82,7 @@ public void testStartStop() throws Exception {\n     URL url = new URL(webServer.getUrl(), MessageFormat.format(\n         \"/webhdfs/v1/?user.name={0}&op=liststatus\", user));\n     HttpURLConnection conn = (HttpURLConnection) url.openConnection();\n-    Assert.assertEquals(conn.getResponseCode(), HttpURLConnection.HTTP_OK);\n+    Assert.assertEquals(HttpURLConnection.HTTP_OK, conn.getResponseCode());\n     BufferedReader reader = new BufferedReader(\n         new InputStreamReader(conn.getInputStream()));\n     reader.readLine();",
      "parent_sha": "3dc30bc24e50343efe1f514b923d27a0786d3ac1"
    }
  },
  {
    "oid": "79f6118dccf809aed74dd54f46f5d77416e14aa1",
    "message": "MAPREDUCE-6521. MiniMRYarnCluster should not create /tmp/hadoop-yarn/staging on local filesystem in unit test.\n\nContributed by Masatake Iwasaki.\n\nChange-Id: Id74b90eb7cfb9d676188fc5aa47249ee8904a3d5",
    "date": "2019-07-18T12:15:18Z",
    "url": "https://github.com/apache/hadoop/commit/79f6118dccf809aed74dd54f46f5d77416e14aa1",
    "details": {
      "sha": "299383d76bf8ad68a6055061e813fa5cf45f5296",
      "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/MiniMRYarnCluster.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/hadoop/blob/79f6118dccf809aed74dd54f46f5d77416e14aa1/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fv2%2FMiniMRYarnCluster.java",
      "raw_url": "https://github.com/apache/hadoop/raw/79f6118dccf809aed74dd54f46f5d77416e14aa1/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fv2%2FMiniMRYarnCluster.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fv2%2FMiniMRYarnCluster.java?ref=79f6118dccf809aed74dd54f46f5d77416e14aa1",
      "patch": "@@ -119,7 +119,9 @@ public static String getResolvedMRHistoryWebAppURLWithoutScheme(\n   @Override\n   public void serviceInit(Configuration conf) throws Exception {\n     conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\n-    if (conf.get(MRJobConfig.MR_AM_STAGING_DIR) == null) {\n+    String stagingDir = conf.get(MRJobConfig.MR_AM_STAGING_DIR);\n+    if (stagingDir == null ||\n+        stagingDir.equals(MRJobConfig.DEFAULT_MR_AM_STAGING_DIR)) {\n       conf.set(MRJobConfig.MR_AM_STAGING_DIR, new File(getTestWorkDir(),\n           \"apps_staging_dir/\").getAbsolutePath());\n     }",
      "parent_sha": "20cf50c6d054324503225c772256f0f91678599a"
    }
  },
  {
    "oid": "d54f5598f4ccd1031e8295a215a3183f3647031a",
    "message": "HDDS-661. When a volume fails in datanode, VersionEndpointTask#call ends up in dead lock. Contributed by Hanisha Koneru.",
    "date": "2018-10-17T13:14:05Z",
    "url": "https://github.com/apache/hadoop/commit/d54f5598f4ccd1031e8295a215a3183f3647031a",
    "details": {
      "sha": "79fa1746d1c4d411494111e9b1613b2f0c39d838",
      "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/hadoop/blob/d54f5598f4ccd1031e8295a215a3183f3647031a/hadoop-hdds%2Fcontainer-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Fstates%2Fendpoint%2FVersionEndpointTask.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d54f5598f4ccd1031e8295a215a3183f3647031a/hadoop-hdds%2Fcontainer-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Fstates%2Fendpoint%2FVersionEndpointTask.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds%2Fcontainer-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Fstates%2Fendpoint%2FVersionEndpointTask.java?ref=d54f5598f4ccd1031e8295a215a3183f3647031a",
      "patch": "@@ -75,7 +75,7 @@ public EndpointStateMachine.EndPointStates call() throws Exception {\n \n       // Check volumes\n       VolumeSet volumeSet = ozoneContainer.getVolumeSet();\n-      volumeSet.readLock();\n+      volumeSet.writeLock();\n       try {\n         Map<String, HddsVolume> volumeMap = volumeSet.getVolumeMap();\n \n@@ -94,12 +94,12 @@ public EndpointStateMachine.EndPointStates call() throws Exception {\n           }\n         }\n         if (volumeSet.getVolumesList().size() == 0) {\n-          // All volumes are inconsistent state\n+          // All volumes are in inconsistent state\n           throw new DiskOutOfSpaceException(\"All configured Volumes are in \" +\n               \"Inconsistent State\");\n         }\n       } finally {\n-        volumeSet.readUnlock();\n+        volumeSet.writeUnlock();\n       }\n \n       ozoneContainer.getDispatcher().setScmId(scmId);",
      "parent_sha": "50715c0699b2603622223c40ef0729c83ac26cf0"
    }
  },
  {
    "oid": "cb3f3cca01d6ab9f4befb1bcd31f384c37c0231a",
    "message": "HADOOP-16868. ipc.Server readAndProcess threw NullPointerException. Contributed by Tsz-wo Sze.",
    "date": "2020-02-19T05:53:08Z",
    "url": "https://github.com/apache/hadoop/commit/cb3f3cca01d6ab9f4befb1bcd31f384c37c0231a",
    "details": {
      "sha": "4448164f4b13760e3d2a5bd16c685782c4550cd1",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/hadoop/blob/cb3f3cca01d6ab9f4befb1bcd31f384c37c0231a/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FServer.java",
      "raw_url": "https://github.com/apache/hadoop/raw/cb3f3cca01d6ab9f4befb1bcd31f384c37c0231a/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FServer.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FServer.java?ref=cb3f3cca01d6ab9f4befb1bcd31f384c37c0231a",
      "patch": "@@ -1794,7 +1794,7 @@ public class Connection {\n \n     private SocketChannel channel;\n     private ByteBuffer data;\n-    private ByteBuffer dataLengthBuffer;\n+    private final ByteBuffer dataLengthBuffer;\n     private LinkedList<RpcCall> responseQueue;\n     // number of outstanding rpcs\n     private AtomicInteger rpcCount = new AtomicInteger();\n@@ -2838,7 +2838,6 @@ public void setServiceClass(int serviceClass) {\n     private synchronized void close() {\n       disposeSasl();\n       data = null;\n-      dataLengthBuffer = null;\n       if (!channel.isOpen())\n         return;\n       try {socket.shutdownOutput();} catch(Exception e) {",
      "parent_sha": "6526f95bd281fc011f8776d21ff933087c5924de"
    }
  },
  {
    "oid": "1942364ef14396e9bd94a87c0d901ff9abe1d42a",
    "message": "HADOOP-11105. MetricsSystemImpl could leak memory in registered callbacks. Contributed by Chuan Liu.",
    "date": "2014-09-18T22:36:43Z",
    "url": "https://github.com/apache/hadoop/commit/1942364ef14396e9bd94a87c0d901ff9abe1d42a",
    "details": {
      "sha": "2107e68895b52dd5d9424cbb07a3b0c131493bd4",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java",
      "status": "modified",
      "additions": 28,
      "deletions": 7,
      "changes": 35,
      "blob_url": "https://github.com/apache/hadoop/blob/1942364ef14396e9bd94a87c0d901ff9abe1d42a/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics2%2Fimpl%2FMetricsSystemImpl.java",
      "raw_url": "https://github.com/apache/hadoop/raw/1942364ef14396e9bd94a87c0d901ff9abe1d42a/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics2%2Fimpl%2FMetricsSystemImpl.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics2%2Fimpl%2FMetricsSystemImpl.java?ref=1942364ef14396e9bd94a87c0d901ff9abe1d42a",
      "patch": "@@ -83,7 +83,12 @@ enum InitMode { NORMAL, STANDBY }\n   private final Map<String, MetricsSource> allSources;\n   private final Map<String, MetricsSinkAdapter> sinks;\n   private final Map<String, MetricsSink> allSinks;\n+\n+  // The callback list is used by register(Callback callback), while\n+  // the callback map is used by register(String name, String desc, T sink)\n   private final List<Callback> callbacks;\n+  private final Map<String, Callback> namedCallbacks;\n+\n   private final MetricsCollectorImpl collector;\n   private final MetricsRegistry registry = new MetricsRegistry(MS_NAME);\n   @Metric({\"Snapshot\", \"Snapshot stats\"}) MutableStat snapshotStat;\n@@ -119,6 +124,7 @@ public MetricsSystemImpl(String prefix) {\n     sourceConfigs = Maps.newHashMap();\n     sinkConfigs = Maps.newHashMap();\n     callbacks = Lists.newArrayList();\n+    namedCallbacks = Maps.newHashMap();\n     injectedTags = Lists.newArrayList();\n     collector = new MetricsCollectorImpl();\n     if (prefix != null) {\n@@ -178,11 +184,13 @@ public synchronized void start() {\n       return;\n     }\n     for (Callback cb : callbacks) cb.preStart();\n+    for (Callback cb : namedCallbacks.values()) cb.preStart();\n     configure(prefix);\n     startTimer();\n     monitoring = true;\n     LOG.info(prefix +\" metrics system started\");\n     for (Callback cb : callbacks) cb.postStart();\n+    for (Callback cb : namedCallbacks.values()) cb.postStart();\n   }\n \n   @Override\n@@ -198,6 +206,7 @@ public synchronized void stop() {\n       return;\n     }\n     for (Callback cb : callbacks) cb.preStop();\n+    for (Callback cb : namedCallbacks.values()) cb.preStop();\n     LOG.info(\"Stopping \"+ prefix +\" metrics system...\");\n     stopTimer();\n     stopSources();\n@@ -206,6 +215,7 @@ public synchronized void stop() {\n     monitoring = false;\n     LOG.info(prefix +\" metrics system stopped.\");\n     for (Callback cb : callbacks) cb.postStop();\n+    for (Callback cb : namedCallbacks.values()) cb.postStop();\n   }\n \n   @Override public synchronized <T>\n@@ -224,7 +234,7 @@ T register(String name, String desc, T source) {\n     }\n     // We want to re-register the source to pick up new config when the\n     // metrics system restarts.\n-    register(new AbstractCallback() {\n+    register(name, new AbstractCallback() {\n       @Override public void postStart() {\n         registerSource(finalName, finalDesc, s);\n       }\n@@ -241,6 +251,9 @@ void unregisterSource(String name) {\n     if (allSources.containsKey(name)) {\n       allSources.remove(name);\n     }\n+    if (namedCallbacks.containsKey(name)) {\n+      namedCallbacks.remove(name);\n+    }\n   }\n \n   synchronized\n@@ -268,7 +281,7 @@ T register(final String name, final String description, final T sink) {\n     }\n     // We want to re-register the sink to pick up new config\n     // when the metrics system restarts.\n-    register(new AbstractCallback() {\n+    register(name, new AbstractCallback() {\n       @Override public void postStart() {\n         register(name, description, sink);\n       }\n@@ -289,21 +302,28 @@ synchronized void registerSink(String name, String desc, MetricsSink sink) {\n \n   @Override\n   public synchronized void register(final Callback callback) {\n-    callbacks.add((Callback) Proxy.newProxyInstance(\n-        callback.getClass().getClassLoader(), new Class<?>[] { Callback.class },\n-        new InvocationHandler() {\n+    callbacks.add((Callback) getProxyForCallback(callback));\n+  }\n+\n+  private synchronized void register(String name, final Callback callback) {\n+    namedCallbacks.put(name, (Callback) getProxyForCallback(callback));\n+  }\n+\n+  private Object getProxyForCallback(final Callback callback) {\n+    return Proxy.newProxyInstance(callback.getClass().getClassLoader(),\n+        new Class<?>[] { Callback.class }, new InvocationHandler() {\n           @Override\n           public Object invoke(Object proxy, Method method, Object[] args)\n               throws Throwable {\n             try {\n               return method.invoke(callback, args);\n             } catch (Exception e) {\n               // These are not considered fatal.\n-              LOG.warn(\"Caught exception in callback \"+ method.getName(), e);\n+              LOG.warn(\"Caught exception in callback \" + method.getName(), e);\n             }\n             return null;\n           }\n-        }));\n+        });\n   }\n \n   @Override\n@@ -572,6 +592,7 @@ public synchronized boolean shutdown() {\n     allSources.clear();\n     allSinks.clear();\n     callbacks.clear();\n+    namedCallbacks.clear();\n     if (mbeanName != null) {\n       MBeans.unregister(mbeanName);\n       mbeanName = null;",
      "parent_sha": "2c3da25fd718b3a9c1ed67f05b577975ae613f4e"
    }
  },
  {
    "oid": "b4f1ba14133568f663da080adf644149253b5b05",
    "message": "YARN-7963.  Updated MockServiceAM unit test to prevent test hang.\n             Contributed by Chandni Singh",
    "date": "2018-02-26T23:49:01Z",
    "url": "https://github.com/apache/hadoop/commit/b4f1ba14133568f663da080adf644149253b5b05",
    "details": {
      "sha": "4373893d47e159f1ff7be61ab56f078e9a9a79a7",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockServiceAM.java",
      "status": "modified",
      "additions": 9,
      "deletions": 1,
      "changes": 10,
      "blob_url": "https://github.com/apache/hadoop/blob/b4f1ba14133568f663da080adf644149253b5b05/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-services%2Fhadoop-yarn-services-core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fservice%2FMockServiceAM.java",
      "raw_url": "https://github.com/apache/hadoop/raw/b4f1ba14133568f663da080adf644149253b5b05/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-services%2Fhadoop-yarn-services-core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fservice%2FMockServiceAM.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-services%2Fhadoop-yarn-services-core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fservice%2FMockServiceAM.java?ref=b4f1ba14133568f663da080adf644149253b5b05",
      "patch": "@@ -196,7 +196,15 @@ protected AMRMClientAsync<AMRMClient.ContainerRequest> createAMRMClient() {\n \n           @Override\n           public RegisterApplicationMasterResponse registerApplicationMaster(\n-              String appHostName, int appHostPort, String appTrackingUrl) {\n+              String appHostName, int appHostPort, String appTrackingUrl,\n+              Map placementConstraintsMap) throws YarnException, IOException {\n+            return this.registerApplicationMaster(appHostName, appHostPort,\n+                appTrackingUrl);\n+          }\n+\n+          @Override\n+            public RegisterApplicationMasterResponse registerApplicationMaster(\n+                String appHostName, int appHostPort, String appTrackingUrl) {\n             RegisterApplicationMasterResponse response = mock(\n                 RegisterApplicationMasterResponse.class);\n             when(response.getResourceTypes()).thenReturn(",
      "parent_sha": "6ce9f79cc9b2107e5953a39d05b22966aff0b7ff"
    }
  },
  {
    "oid": "f121d0b036fe031dd24f2f549ae5729304bfa59c",
    "message": "YARN-5875. TestTokenClientRMService#testTokenRenewalWrongUser fails. Contributed by Gergely Nov\u00e1k.",
    "date": "2016-11-15T21:58:11Z",
    "url": "https://github.com/apache/hadoop/commit/f121d0b036fe031dd24f2f549ae5729304bfa59c",
    "details": {
      "sha": "2a4e49d910e8cd402e565126ccb931a910e363b7",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestTokenClientRMService.java",
      "status": "modified",
      "additions": 3,
      "deletions": 2,
      "changes": 5,
      "blob_url": "https://github.com/apache/hadoop/blob/f121d0b036fe031dd24f2f549ae5729304bfa59c/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FTestTokenClientRMService.java",
      "raw_url": "https://github.com/apache/hadoop/raw/f121d0b036fe031dd24f2f549ae5729304bfa59c/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FTestTokenClientRMService.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FTestTokenClientRMService.java?ref=f121d0b036fe031dd24f2f549ae5729304bfa59c",
      "patch": "@@ -123,8 +123,9 @@ public Void run() throws Exception {\n             return null;\n           } catch (YarnException ex) {\n             Assert.assertTrue(ex.getMessage().contains(\n-                owner.getUserName() + \" tries to renew a token with renewer \"\n-                    + other.getUserName()));\n+                owner.getUserName() + \" tries to renew a token\"));\n+            Assert.assertTrue(ex.getMessage().contains(\n+                \"with non-matching renewer \" + other.getUserName()));\n             throw ex;\n           }\n         }",
      "parent_sha": "264ddb13ff7455282fb640b6ff6c565adddea44e"
    }
  },
  {
    "oid": "532ced18fcec73d691a5600de22a1ab182af79bd",
    "message": "HDFS-12280. Ozone: TestOzoneContainer#testCreateOzoneContainer fails. Contributed by Lokesh Jain.",
    "date": "2018-04-26T12:36:04Z",
    "url": "https://github.com/apache/hadoop/commit/532ced18fcec73d691a5600de22a1ab182af79bd",
    "details": {
      "sha": "ae58ade9578d5d01bd1bf1abdc5c3b9d88d54311",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/container/ozoneimpl/TestOzoneContainer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/532ced18fcec73d691a5600de22a1ab182af79bd/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fozoneimpl%2FTestOzoneContainer.java",
      "raw_url": "https://github.com/apache/hadoop/raw/532ced18fcec73d691a5600de22a1ab182af79bd/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fozoneimpl%2FTestOzoneContainer.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fozoneimpl%2FTestOzoneContainer.java?ref=532ced18fcec73d691a5600de22a1ab182af79bd",
      "patch": "@@ -59,14 +59,14 @@ public void testCreateOzoneContainer() throws Exception {\n     MiniOzoneCluster cluster = null;\n     try {\n       cluster = new MiniOzoneCluster.Builder(conf)\n-          .setRandomContainerPort(false)\n           .setHandlerType(OzoneConsts.OZONE_HANDLER_DISTRIBUTED).build();\n       // We don't start Ozone Container via data node, we will do it\n       // independently in our test path.\n       Pipeline pipeline = ContainerTestHelper.createSingleNodePipeline(\n           containerName);\n       conf.setInt(OzoneConfigKeys.DFS_CONTAINER_IPC_PORT,\n           pipeline.getLeader().getContainerPort());\n+      conf.setBoolean(OzoneConfigKeys.DFS_CONTAINER_IPC_RANDOM_PORT, false);\n       container = new OzoneContainer(DFSTestUtil.getLocalDatanodeID(1),\n           conf);\n       container.start();",
      "parent_sha": "4d3b8d332df81075f54569f38d198cecba872b59"
    }
  },
  {
    "oid": "5c7e40f910986decdcbbe16ecc2a910f91ffbbf8",
    "message": "HADOOP-19111. Removing redundant debug message about client info (#6666). Contributed by Zhongkun Wu.\n\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
    "date": "2024-03-25T03:44:33Z",
    "url": "https://github.com/apache/hadoop/commit/5c7e40f910986decdcbbe16ecc2a910f91ffbbf8",
    "details": {
      "sha": "65fe89b30fc7b3534f25bbd303b11e5dea4cd902",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/hadoop/blob/5c7e40f910986decdcbbe16ecc2a910f91ffbbf8/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FClient.java",
      "raw_url": "https://github.com/apache/hadoop/raw/5c7e40f910986decdcbbe16ecc2a910f91ffbbf8/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FClient.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FClient.java?ref=5c7e40f910986decdcbbe16ecc2a910f91ffbbf8",
      "patch": "@@ -1128,8 +1128,7 @@ public void run() {\n             synchronized (ipcStreams.out) {\n               if (LOG.isDebugEnabled()) {\n                 Call call = pair.getLeft();\n-                LOG.debug(getName() + \"{} sending #{} {}\", getName(), call.id,\n-                    call.rpcRequest);\n+                LOG.debug(\"{} sending #{} {}\", getName(), call.id, call.rpcRequest);\n               }\n               // RpcRequestHeader + RpcRequest\n               ipcStreams.sendRequest(buf.toByteArray());",
      "parent_sha": "55dca911cc67d498be5e0c58afb22e9d808c72d4"
    }
  },
  {
    "oid": "7cac7655fd84ac394250705b31e3927fe548e34a",
    "message": "HDFS-10668. Fix intermittently failing UT TestDataNodeMXBean#testDataNodeMXBeanBlockCount. Contributed by Mingliang Liu.",
    "date": "2016-07-26T05:24:24Z",
    "url": "https://github.com/apache/hadoop/commit/7cac7655fd84ac394250705b31e3927fe548e34a",
    "details": {
      "sha": "e7b2c7cf5d3415d18b7b5de48034c5b542f2a05b",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMXBean.java",
      "status": "modified",
      "additions": 15,
      "deletions": 5,
      "changes": 20,
      "blob_url": "https://github.com/apache/hadoop/blob/7cac7655fd84ac394250705b31e3927fe548e34a/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FTestDataNodeMXBean.java",
      "raw_url": "https://github.com/apache/hadoop/raw/7cac7655fd84ac394250705b31e3927fe548e34a/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FTestDataNodeMXBean.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FTestDataNodeMXBean.java?ref=7cac7655fd84ac394250705b31e3927fe548e34a",
      "patch": "@@ -25,11 +25,13 @@\n import javax.management.MBeanServer;\n import javax.management.ObjectName;\n \n+import com.google.common.base.Supplier;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSTestUtil;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.junit.Assert;\n import org.junit.Test;\n import org.mortbay.util.ajax.JSON;\n@@ -119,10 +121,18 @@ public void testDataNodeMXBeanBlockCount() throws Exception {\n       cluster.waitActive();\n       assertEquals(\"After restart DN\", 5, getTotalNumBlocks(mbs, mxbeanName));\n       fs.delete(new Path(\"/tmp.txt1\"), true);\n-      // Wait till replica gets deleted on disk.\n-      Thread.sleep(5000);\n-      assertEquals(\"After delete one file\", 4,\n-              getTotalNumBlocks(mbs, mxbeanName));\n+      // The total numBlocks should be updated after one file is deleted\n+      GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+        @Override\n+        public Boolean get() {\n+          try {\n+            return getTotalNumBlocks(mbs, mxbeanName) == 4;\n+          } catch (Exception e) {\n+            e.printStackTrace();\n+            return false;\n+          }\n+        }\n+      }, 100, 30000);\n     } finally {\n       if (cluster != null) {\n         cluster.shutdown();\n@@ -131,7 +141,7 @@ public void testDataNodeMXBeanBlockCount() throws Exception {\n   }\n \n   @SuppressWarnings(\"unchecked\")\n-  int getTotalNumBlocks(MBeanServer mbs, ObjectName mxbeanName)\n+  private int getTotalNumBlocks(MBeanServer mbs, ObjectName mxbeanName)\n           throws Exception {\n     int totalBlocks = 0;\n     String volumeInfo = (String) mbs.getAttribute(mxbeanName, \"VolumeInfo\");",
      "parent_sha": "85a20508bd04851d47c24b7562ec2927d5403446"
    }
  },
  {
    "oid": "a89ca56a1b0eb949f56e7c6c5c25fdf87914a02f",
    "message": "HDFS-15600 Fix TestRouterQuota#testStorageTypeQuota (#2347)",
    "date": "2020-09-30T02:00:55Z",
    "url": "https://github.com/apache/hadoop/commit/a89ca56a1b0eb949f56e7c6c5c25fdf87914a02f",
    "details": {
      "sha": "551ae8a8e061238781dee830850b482139c836e3",
      "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterQuota.java",
      "status": "modified",
      "additions": 6,
      "deletions": 6,
      "changes": 12,
      "blob_url": "https://github.com/apache/hadoop/blob/a89ca56a1b0eb949f56e7c6c5c25fdf87914a02f/hadoop-hdfs-project%2Fhadoop-hdfs-rbf%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Ffederation%2Frouter%2FTestRouterQuota.java",
      "raw_url": "https://github.com/apache/hadoop/raw/a89ca56a1b0eb949f56e7c6c5c25fdf87914a02f/hadoop-hdfs-project%2Fhadoop-hdfs-rbf%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Ffederation%2Frouter%2FTestRouterQuota.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs-rbf%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Ffederation%2Frouter%2FTestRouterQuota.java?ref=a89ca56a1b0eb949f56e7c6c5c25fdf87914a02f",
      "patch": "@@ -414,13 +414,13 @@ public void testStorageTypeQuota() throws Exception {\n     QuotaUsage usage = client.getQuotaUsage(\"/type0\");\n     assertEquals(HdfsConstants.QUOTA_RESET, usage.getQuota());\n     assertEquals(HdfsConstants.QUOTA_RESET, usage.getSpaceQuota());\n-    verifyTypeQuotaAndConsume(new long[] {-1, -1, ssQuota * 2, -1, -1}, null,\n+    verifyTypeQuotaAndConsume(new long[] {-1, -1, -1, ssQuota * 2, -1, -1}, null,\n         usage);\n     // Verify /type1 quota on NN1.\n     usage = client.getQuotaUsage(\"/type1\");\n     assertEquals(HdfsConstants.QUOTA_RESET, usage.getQuota());\n     assertEquals(HdfsConstants.QUOTA_RESET, usage.getSpaceQuota());\n-    verifyTypeQuotaAndConsume(new long[] {-1, -1, ssQuota, -1, -1}, null,\n+    verifyTypeQuotaAndConsume(new long[] {-1, -1, -1, ssQuota, -1, -1}, null,\n         usage);\n \n     FileSystem routerFs = routerContext.getFileSystem();\n@@ -431,15 +431,15 @@ public void testStorageTypeQuota() throws Exception {\n     assertEquals(2, u1.getFileAndDirectoryCount());\n     assertEquals(HdfsConstants.QUOTA_RESET, u1.getSpaceQuota());\n     assertEquals(fileSize * 3, u1.getSpaceConsumed());\n-    verifyTypeQuotaAndConsume(new long[] {-1, -1, ssQuota, -1, -1},\n-        new long[] {0, 0, fileSize * 3, 0, 0}, u1);\n+    verifyTypeQuotaAndConsume(new long[] {-1, -1, -1, ssQuota, -1, -1},\n+        new long[] {0, 0, 0, fileSize * 3, 0, 0}, u1);\n     // Verify /type0 storage type quota usage on Router.\n     assertEquals(HdfsConstants.QUOTA_RESET, u0.getQuota());\n     assertEquals(4, u0.getFileAndDirectoryCount());\n     assertEquals(HdfsConstants.QUOTA_RESET, u0.getSpaceQuota());\n     assertEquals(fileSize * 3 * 2, u0.getSpaceConsumed());\n-    verifyTypeQuotaAndConsume(new long[] {-1, -1, ssQuota * 2, -1, -1},\n-        new long[] {0, 0, fileSize * 3 * 2, 0, 0}, u0);\n+    verifyTypeQuotaAndConsume(new long[] {-1, -1, -1, ssQuota * 2, -1, -1},\n+        new long[] {0, 0, 0, fileSize * 3 * 2, 0, 0}, u0);\n   }\n \n   @Test",
      "parent_sha": "a7a1f1541afe8ae9f98817d45bae2f30f78c1271"
    }
  },
  {
    "oid": "1639071b054d120d8b99f34b4deed837d3afa11f",
    "message": "HDDS-1307. Test ScmChillMode testChillModeOperations failed. (#622)\n\nHDDS-1307. Test ScmChillMode testChillModeOperations failed. Contributed by Bharat Viswanadham. (#622)",
    "date": "2019-03-19T20:57:02Z",
    "url": "https://github.com/apache/hadoop/commit/1639071b054d120d8b99f34b4deed837d3afa11f",
    "details": {
      "sha": "e66532f43083ffd34285ed196bf2a87b98d16297",
      "filename": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestScmChillMode.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/hadoop/blob/1639071b054d120d8b99f34b4deed837d3afa11f/hadoop-ozone%2Fintegration-test%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fom%2FTestScmChillMode.java",
      "raw_url": "https://github.com/apache/hadoop/raw/1639071b054d120d8b99f34b4deed837d3afa11f/hadoop-ozone%2Fintegration-test%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fom%2FTestScmChillMode.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone%2Fintegration-test%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fom%2FTestScmChillMode.java?ref=1639071b054d120d8b99f34b4deed837d3afa11f",
      "patch": "@@ -41,7 +41,6 @@\n import org.apache.hadoop.ozone.MiniOzoneClusterImpl;\n import org.apache.hadoop.ozone.OzoneConfigKeys;\n import org.apache.hadoop.ozone.TestStorageContainerManagerHelper;\n-import org.apache.hadoop.ozone.om.exceptions.OMException;\n import org.apache.hadoop.ozone.om.helpers.OmBucketInfo;\n import org.apache.hadoop.ozone.om.helpers.OmKeyArgs;\n import org.apache.hadoop.ozone.om.helpers.OmKeyInfo;\n@@ -178,7 +177,7 @@ public void testChillModeOperations() throws Exception {\n \n     om = miniCluster.get().getOzoneManager();\n \n-    LambdaTestUtils.intercept(OMException.class,\n+    LambdaTestUtils.intercept(IOException.class,\n         \"ChillModePrecheck failed for allocateBlock\",\n         () -> om.openKey(keyArgs));\n   }",
      "parent_sha": "5d8bd0e5cbf1142dbdf6a5f8cf1a448da88b9e9a"
    }
  },
  {
    "oid": "5e0acee75e259c4e241c89b8227efb85f6ea953a",
    "message": "Addendum for YARN-6064. Support fromId for flowRuns and flow/flowRun apps REST API's",
    "date": "2017-08-30T05:59:52Z",
    "url": "https://github.com/apache/hadoop/commit/5e0acee75e259c4e241c89b8227efb85f6ea953a",
    "details": {
      "sha": "4e8286dce4a22975310f3b73dc387ae97e2eb744",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-hbase/src/main/java/org/apache/hadoop/yarn/server/timelineservice/storage/reader/ApplicationEntityReader.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/hadoop/blob/5e0acee75e259c4e241c89b8227efb85f6ea953a/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-timelineservice-hbase%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Ftimelineservice%2Fstorage%2Freader%2FApplicationEntityReader.java",
      "raw_url": "https://github.com/apache/hadoop/raw/5e0acee75e259c4e241c89b8227efb85f6ea953a/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-timelineservice-hbase%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Ftimelineservice%2Fstorage%2Freader%2FApplicationEntityReader.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-timelineservice-hbase%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Ftimelineservice%2Fstorage%2Freader%2FApplicationEntityReader.java?ref=5e0acee75e259c4e241c89b8227efb85f6ea953a",
      "patch": "@@ -375,9 +375,9 @@ protected ResultScanner getResults(Configuration hbaseConf,\n       Long flowRunId = context.getFlowRunId();\n       if (flowRunId == null) {\n         AppToFlowRowKey appToFlowRowKey = new AppToFlowRowKey(\n-            context.getClusterId(), getFilters().getFromId());\n-        FlowContext flowContext =\n-            lookupFlowContext(appToFlowRowKey, hbaseConf, conn);\n+            getFilters().getFromId());\n+        FlowContext flowContext = lookupFlowContext(appToFlowRowKey,\n+            context.getClusterId(), hbaseConf, conn);\n         flowRunId = flowContext.getFlowRunId();\n       }\n ",
      "parent_sha": "6f65cf27bb5bfdc03adf9db6c8a72f80d0aee0bd"
    }
  },
  {
    "oid": "2950c5405ba95ddd53f02cd095184eeb73923bf9",
    "message": "HADOOP-16674. Fix when TestDNS.testRDNS can fail with ServiceUnavailableException (#4802). Contributed by Ashutosh Gupta.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
    "date": "2022-09-20T17:39:59Z",
    "url": "https://github.com/apache/hadoop/commit/2950c5405ba95ddd53f02cd095184eeb73923bf9",
    "details": {
      "sha": "d33545ab6fe0ddbc559316bb7b7f5f58fcbe8490",
      "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/net/TestDNS.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/apache/hadoop/blob/2950c5405ba95ddd53f02cd095184eeb73923bf9/hadoop-common-project%2Fhadoop-common%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FTestDNS.java",
      "raw_url": "https://github.com/apache/hadoop/raw/2950c5405ba95ddd53f02cd095184eeb73923bf9/hadoop-common-project%2Fhadoop-common%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FTestDNS.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FTestDNS.java?ref=2950c5405ba95ddd53f02cd095184eeb73923bf9",
      "patch": "@@ -25,10 +25,12 @@\n \n import javax.naming.CommunicationException;\n import javax.naming.NameNotFoundException;\n+import javax.naming.ServiceUnavailableException;\n \n import org.apache.hadoop.util.Time;\n \n import org.assertj.core.api.Assertions;\n+import org.junit.Assume;\n import org.junit.Test;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -167,7 +169,7 @@ public void testRDNS() throws Exception {\n     try {\n       String s = DNS.reverseDns(localhost, null);\n       LOG.info(\"Local reverse DNS hostname is \" + s);\n-    } catch (NameNotFoundException | CommunicationException e) {\n+    } catch (NameNotFoundException | CommunicationException | ServiceUnavailableException e) {\n       if (!localhost.isLinkLocalAddress() || localhost.isLoopbackAddress()) {\n         //these addresses probably won't work with rDNS anyway, unless someone\n         //has unusual entries in their DNS server mapping 1.0.0.127 to localhost\n@@ -176,6 +178,7 @@ public void testRDNS() throws Exception {\n                 + \" Loopback=\" + localhost.isLoopbackAddress()\n                 + \" Linklocal=\" + localhost.isLinkLocalAddress());\n       }\n+      Assume.assumeNoException(e);\n     }\n   }\n ",
      "parent_sha": "fd687bb4c4edc18c560c9a3dfe8d1f061031288d"
    }
  },
  {
    "oid": "2499435d9d8eacb6142de6a7a2378bdfcb80100b",
    "message": "HADOOP-16014. Fix test, checkstyle and javadoc issues in TestKerberosAuthenticationHandler. Contributed by Dinesh Chitlangia.",
    "date": "2018-12-21T21:30:48Z",
    "url": "https://github.com/apache/hadoop/commit/2499435d9d8eacb6142de6a7a2378bdfcb80100b",
    "details": {
      "sha": "d0709bf4cacc6b4c1c57fed3258ccc2b2d74dbf1",
      "filename": "hadoop-common-project/hadoop-auth/src/test/java/org/apache/hadoop/security/authentication/server/TestKerberosAuthenticationHandler.java",
      "status": "modified",
      "additions": 63,
      "deletions": 35,
      "changes": 98,
      "blob_url": "https://github.com/apache/hadoop/blob/2499435d9d8eacb6142de6a7a2378bdfcb80100b/hadoop-common-project%2Fhadoop-auth%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthentication%2Fserver%2FTestKerberosAuthenticationHandler.java",
      "raw_url": "https://github.com/apache/hadoop/raw/2499435d9d8eacb6142de6a7a2378bdfcb80100b/hadoop-common-project%2Fhadoop-auth%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthentication%2Fserver%2FTestKerberosAuthenticationHandler.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-auth%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthentication%2Fserver%2FTestKerberosAuthenticationHandler.java?ref=2499435d9d8eacb6142de6a7a2378bdfcb80100b",
      "patch": "@@ -26,27 +26,32 @@\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n+import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.Timeout;\n import org.mockito.Mockito;\n import org.ietf.jgss.Oid;\n \n-import javax.security.auth.Subject;\n import javax.security.auth.kerberos.KerberosPrincipal;\n import javax.servlet.ServletException;\n import javax.servlet.http.HttpServletRequest;\n import javax.servlet.http.HttpServletResponse;\n \n import java.io.File;\n import java.security.Principal;\n-import java.util.Arrays;\n-import java.util.List;\n import java.util.Properties;\n import java.util.Set;\n import java.util.concurrent.Callable;\n \n+/**\n+ * Tests for Kerberos Authentication Handler.\n+ */\n public class TestKerberosAuthenticationHandler\n     extends KerberosSecurityTestcase {\n \n+  @Rule\n+  public Timeout globalTimeout = Timeout.millis(60000);\n+\n   protected KerberosAuthenticationHandler handler;\n \n   protected KerberosAuthenticationHandler getNewAuthenticationHandler() {\n@@ -74,8 +79,10 @@ public void setup() throws Exception {\n     File keytabFile = new File(KerberosTestUtils.getKeytabFile());\n     String clientPrincipal = KerberosTestUtils.getClientPrincipal();\n     String serverPrincipal = KerberosTestUtils.getServerPrincipal();\n-    clientPrincipal = clientPrincipal.substring(0, clientPrincipal.lastIndexOf(\"@\"));\n-    serverPrincipal = serverPrincipal.substring(0, serverPrincipal.lastIndexOf(\"@\"));\n+    clientPrincipal = clientPrincipal.substring(0,\n+        clientPrincipal.lastIndexOf(\"@\"));\n+    serverPrincipal = serverPrincipal.substring(0,\n+        serverPrincipal.lastIndexOf(\"@\"));\n     getKdc().createPrincipal(keytabFile, clientPrincipal, serverPrincipal);\n     // handler\n     handler = getNewAuthenticationHandler();\n@@ -88,7 +95,7 @@ public void setup() throws Exception {\n     }\n   }\n \n-  @Test(timeout=60000)\n+  @Test\n   public void testNameRules() throws Exception {\n     KerberosName kn = new KerberosName(KerberosTestUtils.getServerPrincipal());\n     Assert.assertEquals(KerberosTestUtils.getRealm(), kn.getRealm());\n@@ -100,7 +107,8 @@ public void testNameRules() throws Exception {\n     \n     handler = getNewAuthenticationHandler();\n     Properties props = getDefaultProperties();\n-    props.setProperty(KerberosAuthenticationHandler.NAME_RULES, \"RULE:[1:$1@$0](.*@BAR)s/@.*//\\nDEFAULT\");\n+    props.setProperty(KerberosAuthenticationHandler.NAME_RULES,\n+        \"RULE:[1:$1@$0](.*@BAR)s/@.*//\\nDEFAULT\");\n     try {\n       handler.init(props);\n     } catch (Exception ex) {\n@@ -111,13 +119,12 @@ public void testNameRules() throws Exception {\n     try {\n       kn.getShortName();\n       Assert.fail();\n-    }\n-    catch (Exception ex) {      \n+    } catch (Exception ex) {\n     }\n   }\n \n-  @Test(timeout=60000)\n-  public void testInit() throws Exception {\n+  @Test\n+  public void testInit() {\n     Assert.assertEquals(KerberosTestUtils.getKeytabFile(), handler.getKeytab());\n     Set<KerberosPrincipal> principals = handler.getPrincipals();\n     Principal expectedPrincipal =\n@@ -126,8 +133,11 @@ public void testInit() throws Exception {\n     Assert.assertEquals(1, principals.size());\n   }\n \n-  // dynamic configuration of HTTP principals\n-  @Test(timeout=60000)\n+  /**\n+   * Tests dynamic configuration of HTTP principals.\n+    * @throws Exception\n+   */\n+  @Test\n   public void testDynamicPrincipalDiscovery() throws Exception {\n     String[] keytabUsers = new String[]{\n         \"HTTP/host1\", \"HTTP/host2\", \"HTTP2/host1\", \"XHTTP/host\"\n@@ -143,7 +153,8 @@ public void testDynamicPrincipalDiscovery() throws Exception {\n     handler = getNewAuthenticationHandler();\n     handler.init(props);\n \n-    Assert.assertEquals(KerberosTestUtils.getKeytabFile(), handler.getKeytab());    \n+    Assert.assertEquals(KerberosTestUtils.getKeytabFile(),\n+        handler.getKeytab());\n     \n     Set<KerberosPrincipal> loginPrincipals = handler.getPrincipals();\n     for (String user : keytabUsers) {\n@@ -155,9 +166,13 @@ public void testDynamicPrincipalDiscovery() throws Exception {\n     }\n   }\n \n-  // dynamic configuration of HTTP principals\n-  @Test(timeout=60000)\n-  public void testDynamicPrincipalDiscoveryMissingPrincipals() throws Exception {\n+  /**\n+   * Tests dynamic principal discovery for missing principals.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDynamicPrincipalDiscoveryMissingPrincipals()\n+      throws Exception {\n     String[] keytabUsers = new String[]{\"hdfs/localhost\"};\n     String keytab = KerberosTestUtils.getKeytabFile();\n     getKdc().createPrincipal(new File(keytab), keytabUsers);\n@@ -179,32 +194,37 @@ public void testDynamicPrincipalDiscoveryMissingPrincipals() throws Exception {\n     }\n   }\n \n-  @Test(timeout=60000)\n-  public void testType() throws Exception {\n+  @Test\n+  public void testType() {\n     Assert.assertEquals(getExpectedType(), handler.getType());\n   }\n \n+  @Test\n   public void testRequestWithoutAuthorization() throws Exception {\n     HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n     HttpServletResponse response = Mockito.mock(HttpServletResponse.class);\n \n     Assert.assertNull(handler.authenticate(request, response));\n-    Mockito.verify(response).setHeader(KerberosAuthenticator.WWW_AUTHENTICATE, KerberosAuthenticator.NEGOTIATE);\n+    Mockito.verify(response).setHeader(KerberosAuthenticator.WWW_AUTHENTICATE,\n+        KerberosAuthenticator.NEGOTIATE);\n     Mockito.verify(response).setStatus(HttpServletResponse.SC_UNAUTHORIZED);\n   }\n \n+  @Test\n   public void testRequestWithInvalidAuthorization() throws Exception {\n     HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n     HttpServletResponse response = Mockito.mock(HttpServletResponse.class);\n \n-    Mockito.when(request.getHeader(KerberosAuthenticator.AUTHORIZATION)).thenReturn(\"invalid\");\n+    Mockito.when(request.getHeader(KerberosAuthenticator.AUTHORIZATION))\n+        .thenReturn(\"invalid\");\n     Assert.assertNull(handler.authenticate(request, response));\n-    Mockito.verify(response).setHeader(KerberosAuthenticator.WWW_AUTHENTICATE, KerberosAuthenticator.NEGOTIATE);\n+    Mockito.verify(response).setHeader(KerberosAuthenticator.WWW_AUTHENTICATE,\n+        KerberosAuthenticator.NEGOTIATE);\n     Mockito.verify(response).setStatus(HttpServletResponse.SC_UNAUTHORIZED);\n   }\n \n-  @Test(timeout=60000)\n-  public void testRequestWithIncompleteAuthorization() throws Exception {\n+  @Test\n+  public void testRequestWithIncompleteAuthorization() {\n     HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n     HttpServletResponse response = Mockito.mock(HttpServletResponse.class);\n \n@@ -220,6 +240,7 @@ public void testRequestWithIncompleteAuthorization() throws Exception {\n     }\n   }\n \n+  @Test\n   public void testRequestWithAuthorization() throws Exception {\n     String token = KerberosTestUtils.doAsClient(new Callable<String>() {\n       @Override\n@@ -228,7 +249,8 @@ public String call() throws Exception {\n         GSSContext gssContext = null;\n         try {\n           String servicePrincipal = KerberosTestUtils.getServerPrincipal();\n-          Oid oid = KerberosUtil.getOidInstance(\"NT_GSS_KRB5_PRINCIPAL\");\n+          Oid oid =\n+              KerberosUtil.getOidInstance(\"NT_GSS_KRB5_PRINCIPAL\");\n           GSSName serviceName = gssManager.createName(servicePrincipal,\n               oid);\n           oid = KerberosUtil.getOidInstance(\"GSS_KRB5_MECH_OID\");\n@@ -238,7 +260,8 @@ public String call() throws Exception {\n           gssContext.requestMutualAuth(true);\n \n           byte[] inToken = new byte[0];\n-          byte[] outToken = gssContext.initSecContext(inToken, 0, inToken.length);\n+          byte[] outToken =\n+              gssContext.initSecContext(inToken, 0, inToken.length);\n           Base64 base64 = new Base64(0);\n           return base64.encodeToString(outToken);\n \n@@ -260,29 +283,34 @@ public String call() throws Exception {\n     AuthenticationToken authToken = handler.authenticate(request, response);\n \n     if (authToken != null) {\n-      Mockito.verify(response).setHeader(Mockito.eq(KerberosAuthenticator.WWW_AUTHENTICATE),\n-                                         Mockito.matches(KerberosAuthenticator.NEGOTIATE + \" .*\"));\n+      Mockito.verify(response)\n+          .setHeader(Mockito.eq(KerberosAuthenticator.WWW_AUTHENTICATE),\n+              Mockito.matches(KerberosAuthenticator.NEGOTIATE + \" .*\"));\n       Mockito.verify(response).setStatus(HttpServletResponse.SC_OK);\n \n-      Assert.assertEquals(KerberosTestUtils.getClientPrincipal(), authToken.getName());\n-      Assert.assertTrue(KerberosTestUtils.getClientPrincipal().startsWith(authToken.getUserName()));\n+      Assert.assertEquals(KerberosTestUtils.getClientPrincipal(),\n+          authToken.getName());\n+      Assert.assertTrue(KerberosTestUtils.getClientPrincipal()\n+          .startsWith(authToken.getUserName()));\n       Assert.assertEquals(getExpectedType(), authToken.getType());\n     } else {\n-      Mockito.verify(response).setHeader(Mockito.eq(KerberosAuthenticator.WWW_AUTHENTICATE),\n-                                         Mockito.matches(KerberosAuthenticator.NEGOTIATE + \" .*\"));\n+      Mockito.verify(response).setHeader(\n+          Mockito.eq(KerberosAuthenticator.WWW_AUTHENTICATE),\n+          Mockito.matches(KerberosAuthenticator.NEGOTIATE + \" .*\"));\n       Mockito.verify(response).setStatus(HttpServletResponse.SC_UNAUTHORIZED);\n     }\n   }\n \n-  public void testRequestWithInvalidKerberosAuthorization() throws Exception {\n+  @Test\n+  public void testRequestWithInvalidKerberosAuthorization() {\n \n     String token = new Base64(0).encodeToString(new byte[]{0, 1, 2});\n \n     HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n     HttpServletResponse response = Mockito.mock(HttpServletResponse.class);\n \n-    Mockito.when(request.getHeader(KerberosAuthenticator.AUTHORIZATION)).thenReturn(\n-      KerberosAuthenticator.NEGOTIATE + token);\n+    Mockito.when(request.getHeader(KerberosAuthenticator.AUTHORIZATION))\n+        .thenReturn(KerberosAuthenticator.NEGOTIATE + token);\n \n     try {\n       handler.authenticate(request, response);",
      "parent_sha": "ea724181d66ebe3c2cc7ed071948e9bc463bf223"
    }
  },
  {
    "oid": "1f004b3367c57de9e8a67040a57efc31c9ba8ee2",
    "message": "HDFS-9871. \"Bytes Being Moved\" -ve(-1 B) when cluster was already balanced. (Contributed by Brahma Reddy Battulla)",
    "date": "2016-03-29T03:25:41Z",
    "url": "https://github.com/apache/hadoop/commit/1f004b3367c57de9e8a67040a57efc31c9ba8ee2",
    "details": {
      "sha": "a7600e009133785ad04a68a64d92a8a47c7809a8",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/1f004b3367c57de9e8a67040a57efc31c9ba8ee2/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fbalancer%2FBalancer.java",
      "raw_url": "https://github.com/apache/hadoop/raw/1f004b3367c57de9e8a67040a57efc31c9ba8ee2/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fbalancer%2FBalancer.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fbalancer%2FBalancer.java?ref=1f004b3367c57de9e8a67040a57efc31c9ba8ee2",
      "patch": "@@ -583,7 +583,7 @@ Result runOneIteration() {\n       final long bytesLeftToMove = init(reports);\n       if (bytesLeftToMove == 0) {\n         System.out.println(\"The cluster is balanced. Exiting...\");\n-        return newResult(ExitStatus.SUCCESS, bytesLeftToMove, -1);\n+        return newResult(ExitStatus.SUCCESS, bytesLeftToMove, 0);\n       } else {\n         LOG.info( \"Need to move \"+ StringUtils.byteDesc(bytesLeftToMove)\n             + \" to make the cluster balanced.\" );",
      "parent_sha": "0ef8bbfd8791899c7cfed3dd9c1670182fd87575"
    }
  },
  {
    "oid": "d78b300ed4c04100614542d3c86edfc40f3a9aa1",
    "message": "YARN-10841. Fix token reset synchronization for UAM response token. (#3194) \n\nYARN-10841. Fix token reset synchronization for UAM response token.  Contributed by Minni Mittal",
    "date": "2021-07-29T09:25:39Z",
    "url": "https://github.com/apache/hadoop/commit/d78b300ed4c04100614542d3c86edfc40f3a9aa1",
    "details": {
      "sha": "c32afee7e9d6f7e0a71e4b1100b0c9903655cce2",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/amrmproxy/FederationInterceptor.java",
      "status": "modified",
      "additions": 11,
      "deletions": 4,
      "changes": 15,
      "blob_url": "https://github.com/apache/hadoop/blob/d78b300ed4c04100614542d3c86edfc40f3a9aa1/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-nodemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fnodemanager%2Famrmproxy%2FFederationInterceptor.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d78b300ed4c04100614542d3c86edfc40f3a9aa1/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-nodemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fnodemanager%2Famrmproxy%2FFederationInterceptor.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-nodemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fnodemanager%2Famrmproxy%2FFederationInterceptor.java?ref=d78b300ed4c04100614542d3c86edfc40f3a9aa1",
      "patch": "@@ -1413,8 +1413,8 @@ protected void mergeAllocateResponse(AllocateResponse homeResponse,\n       if (otherRMAddress.equals(this.homeSubClusterId)) {\n         homeResponse.setAMRMToken(otherResponse.getAMRMToken());\n       } else {\n-        throw new YarnRuntimeException(\n-            \"amrmToken from UAM \" + otherRMAddress + \" should be null here\");\n+        LOG.warn(\"amrmToken from UAM {} not null, it should be null here\",\n+            otherRMAddress);\n       }\n     }\n \n@@ -1691,6 +1691,8 @@ private class HeartbeatCallBack implements AsyncCallback<AllocateResponse> {\n \n     @Override\n     public void callback(AllocateResponse response) {\n+      org.apache.hadoop.yarn.api.records.Token amrmToken =\n+          response.getAMRMToken();\n       synchronized (asyncResponseSink) {\n         List<AllocateResponse> responses = null;\n         if (asyncResponseSink.containsKey(subClusterId)) {\n@@ -1700,6 +1702,11 @@ public void callback(AllocateResponse response) {\n           asyncResponseSink.put(subClusterId, responses);\n         }\n         responses.add(response);\n+\n+        if (this.isUAM) {\n+          // Do not further propagate the new amrmToken for UAM\n+          response.setAMRMToken(null);\n+        }\n         // Notify main thread about the response arrival\n         asyncResponseSink.notifyAll();\n       }\n@@ -1716,9 +1723,9 @@ public void callback(AllocateResponse response) {\n \n       // Save the new AMRMToken for the UAM if present\n       // Do this last because it can be slow...\n-      if (this.isUAM && response.getAMRMToken() != null) {\n+      if (this.isUAM && amrmToken != null) {\n         Token<AMRMTokenIdentifier> newToken = ConverterUtils\n-            .convertFromYarn(response.getAMRMToken(), (Text) null);\n+            .convertFromYarn(amrmToken, (Text) null);\n         // Do not further propagate the new amrmToken for UAM\n         response.setAMRMToken(null);\n ",
      "parent_sha": "6f730fd25c9845fea09b98c64b5d59bee5576db2"
    }
  },
  {
    "oid": "43bc9848914cee5e03a9965eb2165f42efb2c35d",
    "message": "HADOOP-15793. ABFS: Skip unsupported test cases when non namespace enabled in ITestAzureBlobFileSystemAuthorization\nContributed by Yuan Gao.",
    "date": "2018-10-02T10:37:28Z",
    "url": "https://github.com/apache/hadoop/commit/43bc9848914cee5e03a9965eb2165f42efb2c35d",
    "details": {
      "sha": "17055169773b226fbdc15056f5c551c6ea90b219",
      "filename": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemAuthorization.java",
      "status": "modified",
      "additions": 17,
      "deletions": 1,
      "changes": 18,
      "blob_url": "https://github.com/apache/hadoop/blob/43bc9848914cee5e03a9965eb2165f42efb2c35d/hadoop-tools%2Fhadoop-azure%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fazurebfs%2FITestAzureBlobFileSystemAuthorization.java",
      "raw_url": "https://github.com/apache/hadoop/raw/43bc9848914cee5e03a9965eb2165f42efb2c35d/hadoop-tools%2Fhadoop-azure%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fazurebfs%2FITestAzureBlobFileSystemAuthorization.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools%2Fhadoop-azure%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fazurebfs%2FITestAzureBlobFileSystemAuthorization.java?ref=43bc9848914cee5e03a9965eb2165f42efb2c35d",
      "patch": "@@ -35,6 +35,7 @@\n import static org.apache.hadoop.fs.permission.AclEntryScope.ACCESS;\n import static org.apache.hadoop.fs.permission.AclEntryType.GROUP;\n import static org.apache.hadoop.test.LambdaTestUtils.intercept;\n+import static org.junit.Assume.assumeTrue;\n \n /**\n  * Test Perform Authorization Check operation\n@@ -202,13 +203,15 @@ public void testGetFileStatusUnauthorized() throws Exception {\n   @Test\n   public void testSetOwnerAuthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_ONLY_FILE_PATH_0).close();\n     fs.setOwner(TEST_WRITE_ONLY_FILE_PATH_0, \"testUser\", \"testGroup\");\n   }\n \n   @Test\n   public void testSetOwnerUnauthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_THEN_READ_ONLY_PATH).close();\n     intercept(AbfsAuthorizationException.class,\n         ()-> {\n@@ -219,13 +222,15 @@ public void testSetOwnerUnauthorized() throws Exception {\n   @Test\n   public void testSetPermissionAuthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_ONLY_FILE_PATH_0).close();\n     fs.setPermission(TEST_WRITE_ONLY_FILE_PATH_0, new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE));\n   }\n \n   @Test\n   public void testSetPermissionUnauthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_THEN_READ_ONLY_PATH).close();\n     intercept(AbfsAuthorizationException.class,\n         ()-> {\n@@ -236,6 +241,7 @@ public void testSetPermissionUnauthorized() throws Exception {\n   @Test\n   public void testModifyAclEntriesAuthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_ONLY_FILE_PATH_0).close();\n     List<AclEntry> aclSpec = Arrays.asList(aclEntry(ACCESS, GROUP, \"bar\", FsAction.ALL));\n     fs.modifyAclEntries(TEST_WRITE_ONLY_FILE_PATH_0, aclSpec);\n@@ -244,6 +250,7 @@ public void testModifyAclEntriesAuthorized() throws Exception {\n   @Test\n   public void testModifyAclEntriesUnauthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_THEN_READ_ONLY_PATH).close();\n     List<AclEntry> aclSpec = Arrays.asList(aclEntry(ACCESS, GROUP, \"bar\", FsAction.ALL));\n     intercept(AbfsAuthorizationException.class,\n@@ -255,15 +262,16 @@ public void testModifyAclEntriesUnauthorized() throws Exception {\n   @Test\n   public void testRemoveAclEntriesAuthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_ONLY_FILE_PATH_0).close();\n     List<AclEntry> aclSpec = Arrays.asList(aclEntry(ACCESS, GROUP, \"bar\", FsAction.ALL));\n-    //fs.modifyAclEntries(TEST_WRITE_ONLY_FILE_PATH_0, aclSpec);\n     fs.removeAclEntries(TEST_WRITE_ONLY_FILE_PATH_0, aclSpec);\n   }\n \n   @Test\n   public void testRemoveAclEntriesUnauthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_THEN_READ_ONLY_PATH).close();\n     List<AclEntry> aclSpec = Arrays.asList(aclEntry(ACCESS, GROUP, \"bar\", FsAction.ALL));\n     intercept(AbfsAuthorizationException.class,\n@@ -275,13 +283,15 @@ public void testRemoveAclEntriesUnauthorized() throws Exception {\n   @Test\n   public void testRemoveDefaultAclAuthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_ONLY_FILE_PATH_0).close();\n     fs.removeDefaultAcl(TEST_WRITE_ONLY_FILE_PATH_0);\n   }\n \n   @Test\n   public void testRemoveDefaultAclUnauthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_THEN_READ_ONLY_PATH).close();\n     intercept(AbfsAuthorizationException.class,\n         ()-> {\n@@ -292,13 +302,15 @@ public void testRemoveDefaultAclUnauthorized() throws Exception {\n   @Test\n   public void testRemoveAclAuthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_ONLY_FILE_PATH_0).close();\n     fs.removeAcl(TEST_WRITE_ONLY_FILE_PATH_0);\n   }\n \n   @Test\n   public void testRemoveAclUnauthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_THEN_READ_ONLY_PATH).close();\n     intercept(AbfsAuthorizationException.class,\n         ()-> {\n@@ -309,6 +321,7 @@ public void testRemoveAclUnauthorized() throws Exception {\n   @Test\n   public void testSetAclAuthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_ONLY_FILE_PATH_0).close();\n     List<AclEntry> aclSpec = Arrays.asList(aclEntry(ACCESS, GROUP, \"bar\", FsAction.ALL));\n     fs.setAcl(TEST_WRITE_ONLY_FILE_PATH_0, aclSpec);\n@@ -317,6 +330,7 @@ public void testSetAclAuthorized() throws Exception {\n   @Test\n   public void testSetAclUnauthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_THEN_READ_ONLY_PATH).close();\n     List<AclEntry> aclSpec = Arrays.asList(aclEntry(ACCESS, GROUP, \"bar\", FsAction.ALL));\n     intercept(AbfsAuthorizationException.class,\n@@ -328,6 +342,7 @@ public void testSetAclUnauthorized() throws Exception {\n   @Test\n   public void testGetAclStatusAuthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_THEN_READ_ONLY_PATH).close();\n     List<AclEntry> aclSpec = Arrays.asList(aclEntry(ACCESS, GROUP, \"bar\", FsAction.ALL));\n     fs.getAclStatus(TEST_WRITE_THEN_READ_ONLY_PATH);\n@@ -336,6 +351,7 @@ public void testGetAclStatusAuthorized() throws Exception {\n   @Test\n   public void testGetAclStatusUnauthorized() throws Exception {\n     final AzureBlobFileSystem fs = getFileSystem();\n+    assumeTrue(\"This test case only runs when namespace is enabled\", fs.getIsNamespaceEnabeld());\n     fs.create(TEST_WRITE_ONLY_FILE_PATH_0).close();\n     List<AclEntry> aclSpec = Arrays.asList(aclEntry(ACCESS, GROUP, \"bar\", FsAction.ALL));\n     intercept(AbfsAuthorizationException.class,",
      "parent_sha": "5689355783de005ebc604f4403dc5129a286bfca"
    }
  },
  {
    "oid": "04a5f5a6dc88769cca8b1a15057a0756712b5013",
    "message": "HADOOP-14156. Fix grammar error in ConfTest.java.\n\nThis closes #187\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
    "date": "2017-03-13T07:15:53Z",
    "url": "https://github.com/apache/hadoop/commit/04a5f5a6dc88769cca8b1a15057a0756712b5013",
    "details": {
      "sha": "1915e791d21eba8155e4d7e12ceb11316dc5a955",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ConfTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/04a5f5a6dc88769cca8b1a15057a0756712b5013/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Futil%2FConfTest.java",
      "raw_url": "https://github.com/apache/hadoop/raw/04a5f5a6dc88769cca8b1a15057a0756712b5013/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Futil%2FConfTest.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Futil%2FConfTest.java?ref=04a5f5a6dc88769cca8b1a15057a0756712b5013",
      "patch": "@@ -269,7 +269,7 @@ public static void main(String[] args) throws IOException {\n     } else {\n       String confDirName = System.getenv(HADOOP_CONF_DIR);\n       if (confDirName == null) {\n-        terminate(1, HADOOP_CONF_DIR + \" does not defined\");\n+        terminate(1, HADOOP_CONF_DIR + \" is not defined\");\n       }\n       File confDir = new File(confDirName);\n       if (!confDir.isDirectory()) {",
      "parent_sha": "4db9cc70d0178703fb28f451eb84d97f2bf63af8"
    }
  },
  {
    "oid": "fd1c09be3e7c67c188a1dd7e4fccb3d92dcc5b5b",
    "message": "HDFS-9521. TransferFsImage.receiveFile should account and log separate times for image download and fsync to disk. Contributed by Wellington Chevreuil",
    "date": "2016-03-07T11:44:51Z",
    "url": "https://github.com/apache/hadoop/commit/fd1c09be3e7c67c188a1dd7e4fccb3d92dcc5b5b",
    "details": {
      "sha": "0186d8b3bd0e4e1e5fd7d51773088bffcd0e91a5",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/TransferFsImage.java",
      "status": "modified",
      "additions": 27,
      "deletions": 6,
      "changes": 33,
      "blob_url": "https://github.com/apache/hadoop/blob/fd1c09be3e7c67c188a1dd7e4fccb3d92dcc5b5b/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FTransferFsImage.java",
      "raw_url": "https://github.com/apache/hadoop/raw/fd1c09be3e7c67c188a1dd7e4fccb3d92dcc5b5b/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FTransferFsImage.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FTransferFsImage.java?ref=fd1c09be3e7c67c188a1dd7e4fccb3d92dcc5b5b",
      "patch": "@@ -30,6 +30,7 @@\n import java.security.DigestInputStream;\n import java.security.MessageDigest;\n import java.util.ArrayList;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.Map.Entry;\n@@ -481,6 +482,9 @@ private static MD5Hash receiveFile(String url, List<File> localPaths,\n       MD5Hash advertisedDigest, String fsImageName, InputStream stream,\n       DataTransferThrottler throttler) throws IOException {\n     long startTime = Time.monotonicNow();\n+    Map<FileOutputStream, File> streamPathMap = new HashMap<>();\n+    StringBuilder xferStats = new StringBuilder();\n+    double xferCombined = 0;\n     if (localPaths != null) {\n       // If the local paths refer to directories, use the server-provided header\n       // as the filename within that directory\n@@ -517,7 +521,9 @@ private static MD5Hash receiveFile(String url, List<File> localPaths,\n               LOG.warn(\"Overwriting existing file \" + f\n                   + \" with file downloaded from \" + url);\n             }\n-            outputStreams.add(new FileOutputStream(f));\n+            FileOutputStream fos = new FileOutputStream(f);\n+            outputStreams.add(fos);\n+            streamPathMap.put(fos, f);\n           } catch (IOException ioe) {\n             LOG.warn(\"Unable to download file \" + f, ioe);\n             // This will be null if we're downloading the fsimage to a file\n@@ -550,11 +556,26 @@ private static MD5Hash receiveFile(String url, List<File> localPaths,\n         }\n       }\n       finishedReceiving = true;\n+      double xferSec = Math.max(\n+                 ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n+      long xferKb = received / 1024;\n+      xferCombined += xferSec;\n+      xferStats.append(\n+          String.format(\" The fsimage download took %.2fs at %.2f KB/s.\",\n+              xferSec, xferKb / xferSec));\n     } finally {\n       stream.close();\n       for (FileOutputStream fos : outputStreams) {\n+        long flushStartTime = Time.monotonicNow();\n         fos.getChannel().force(true);\n         fos.close();\n+        double writeSec = Math.max(((float)\n+               (flushStartTime - Time.monotonicNow())) / 1000.0, 0.001);\n+        xferCombined += writeSec;\n+        xferStats.append(String\n+                .format(\" Synchronous (fsync) write to disk of \" +\n+                 streamPathMap.get(fos).getAbsolutePath() +\n+                \" took %.2fs.\", writeSec));\n       }\n \n       // Something went wrong and did not finish reading.\n@@ -573,11 +594,11 @@ private static MD5Hash receiveFile(String url, List<File> localPaths,\n                               advertisedSize);\n       }\n     }\n-    double xferSec = Math.max(\n-        ((float)(Time.monotonicNow() - startTime)) / 1000.0, 0.001);\n-    long xferKb = received / 1024;\n-    LOG.info(String.format(\"Transfer took %.2fs at %.2f KB/s\",\n-        xferSec, xferKb / xferSec));\n+    xferStats.insert(\n+        0, String.format(\n+            \"Combined time for fsimage download and fsync \" +\n+            \"to all disks took %.2fs.\", xferCombined));\n+    LOG.info(xferStats.toString());\n \n     if (digester != null) {\n       MD5Hash computedDigest = new MD5Hash(digester.digest());",
      "parent_sha": "8ed2e060e80c0def3fcb7604e0bd27c1c24d291e"
    }
  },
  {
    "oid": "e4b538bbda6dc25d7f45bffd6a4ce49f3f84acdc",
    "message": "YARN-9723. ApplicationPlacementContext is not required for terminated jobs during recovery. Contributed by Prabhu Joseph",
    "date": "2019-08-12T13:15:43Z",
    "url": "https://github.com/apache/hadoop/commit/e4b538bbda6dc25d7f45bffd6a4ce49f3f84acdc",
    "details": {
      "sha": "3cf3dd1d969313c79cc0e7cd0ebaf0df6f91d849",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
      "status": "modified",
      "additions": 11,
      "deletions": 6,
      "changes": 17,
      "blob_url": "https://github.com/apache/hadoop/blob/e4b538bbda6dc25d7f45bffd6a4ce49f3f84acdc/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FRMAppManager.java",
      "raw_url": "https://github.com/apache/hadoop/raw/e4b538bbda6dc25d7f45bffd6a4ce49f3f84acdc/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FRMAppManager.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FRMAppManager.java?ref=e4b538bbda6dc25d7f45bffd6a4ce49f3f84acdc",
      "patch": "@@ -64,6 +64,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRecoverEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue;\n@@ -371,7 +372,7 @@ protected void submitApplication(\n     // Passing start time as -1. It will be eventually set in RMAppImpl\n     // constructor.\n     RMAppImpl application = createAndPopulateNewRMApp(\n-        submissionContext, submitTime, user, false, -1);\n+        submissionContext, submitTime, user, false, -1, null);\n     try {\n       if (UserGroupInformation.isSecurityEnabled()) {\n         this.rmContext.getDelegationTokenRenewer()\n@@ -408,18 +409,22 @@ protected void recoverApplication(ApplicationStateData appState,\n     // create and recover app.\n     RMAppImpl application =\n         createAndPopulateNewRMApp(appContext, appState.getSubmitTime(),\n-            appState.getUser(), true, appState.getStartTime());\n+            appState.getUser(), true, appState.getStartTime(),\n+            appState.getState());\n \n     application.handle(new RMAppRecoverEvent(appId, rmState));\n   }\n \n   private RMAppImpl createAndPopulateNewRMApp(\n       ApplicationSubmissionContext submissionContext, long submitTime,\n-      String user, boolean isRecovery, long startTime) throws YarnException {\n+      String user, boolean isRecovery, long startTime,\n+      RMAppState recoveredFinalState) throws YarnException {\n \n-    ApplicationPlacementContext placementContext =\n-        placeApplication(rmContext.getQueuePlacementManager(),\n-            submissionContext, user, isRecovery);\n+    ApplicationPlacementContext placementContext = null;\n+    if (recoveredFinalState == null) {\n+      placementContext = placeApplication(rmContext.getQueuePlacementManager(),\n+          submissionContext, user, isRecovery);\n+    }\n \n     // We only replace the queue when it's a new application\n     if (!isRecovery) {",
      "parent_sha": "b91099efd6e1fdcb31ec4ca7142439443c9ae536"
    }
  },
  {
    "oid": "c8b3dfa6250cd74fb3e449748595117b244089da",
    "message": "HDDS-860. Fix TestDataValidate unit tests. Contributed by Shashikant Banerjee.",
    "date": "2018-11-21T05:58:20Z",
    "url": "https://github.com/apache/hadoop/commit/c8b3dfa6250cd74fb3e449748595117b244089da",
    "details": {
      "sha": "d85b829e43b470bc73c4619021d65e50285cc5ab",
      "filename": "hadoop-ozone/tools/src/main/java/org/apache/hadoop/ozone/freon/RandomKeyGenerator.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/c8b3dfa6250cd74fb3e449748595117b244089da/hadoop-ozone%2Ftools%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Ffreon%2FRandomKeyGenerator.java",
      "raw_url": "https://github.com/apache/hadoop/raw/c8b3dfa6250cd74fb3e449748595117b244089da/hadoop-ozone%2Ftools%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Ffreon%2FRandomKeyGenerator.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone%2Ftools%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Ffreon%2FRandomKeyGenerator.java?ref=c8b3dfa6250cd74fb3e449748595117b244089da",
      "patch": "@@ -269,7 +269,7 @@ public Void call() throws Exception {\n \n     processor.shutdown();\n     processor.awaitTermination(Integer.MAX_VALUE, TimeUnit.MILLISECONDS);\n-\n+    completed = true;\n     progressbar.shutdown();\n \n     if (validateWrites) {",
      "parent_sha": "14e1a0a3d6cf0566ba696a73699aa7ce6ed1f94f"
    }
  },
  {
    "oid": "fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a",
    "message": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.",
    "date": "2018-08-17T09:52:55Z",
    "url": "https://github.com/apache/hadoop/commit/fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a",
    "details": {
      "sha": "fe5499366f5af84ced16a2712cea8a70f6454cd3",
      "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
      "status": "modified",
      "additions": 158,
      "deletions": 1202,
      "changes": 1360,
      "blob_url": "https://github.com/apache/hadoop/blob/fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a/hadoop-hdfs-project%2Fhadoop-hdfs-rbf%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Ffederation%2Frouter%2FRouterRpcServer.java",
      "raw_url": "https://github.com/apache/hadoop/raw/fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a/hadoop-hdfs-project%2Fhadoop-hdfs-rbf%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Ffederation%2Frouter%2FRouterRpcServer.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs-rbf%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Ffederation%2Frouter%2FRouterRpcServer.java?ref=fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a",
      "patch": "@@ -33,16 +33,12 @@\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.EnumSet;\n-import java.util.HashMap;\n-import java.util.Iterator;\n import java.util.LinkedHashMap;\n import java.util.LinkedHashSet;\n-import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n import java.util.Map.Entry;\n import java.util.Set;\n-import java.util.TreeMap;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.crypto.CryptoProtocolVersion;\n@@ -54,7 +50,6 @@\n import org.apache.hadoop.fs.FileAlreadyExistsException;\n import org.apache.hadoop.fs.FsServerDefaults;\n import org.apache.hadoop.fs.Options;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.QuotaUsage;\n import org.apache.hadoop.fs.StorageType;\n import org.apache.hadoop.fs.XAttr;\n@@ -64,7 +59,6 @@\n import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.hdfs.AddBlockFlag;\n-import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.DFSUtil;\n import org.apache.hadoop.hdfs.inotify.EventBatchList;\n import org.apache.hadoop.hdfs.protocol.AddErasureCodingPolicyResponse;\n@@ -93,16 +87,15 @@\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n import org.apache.hadoop.hdfs.protocol.LocatedBlocks;\n import org.apache.hadoop.hdfs.protocol.OpenFileEntry;\n-import org.apache.hadoop.hdfs.protocol.OpenFilesIterator;\n import org.apache.hadoop.hdfs.protocol.OpenFilesIterator.OpenFilesType;\n import org.apache.hadoop.hdfs.protocol.ReplicatedBlockStats;\n import org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo;\n import org.apache.hadoop.hdfs.protocol.SnapshotDiffReport;\n import org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing;\n import org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus;\n import org.apache.hadoop.hdfs.protocol.ZoneReencryptionStatus;\n-import org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.ClientNamenodeProtocol;\n import org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos.NamenodeProtocolService;\n+import org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.ClientNamenodeProtocol;\n import org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB;\n import org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB;\n import org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolPB;\n@@ -167,11 +160,6 @@ public class RouterRpcServer extends AbstractService\n   /** Configuration for the RPC server. */\n   private Configuration conf;\n \n-  /** Identifier for the super user. */\n-  private final String superUser;\n-  /** Identifier for the super group. */\n-  private final String superGroup;\n-\n   /** Router using this RPC server. */\n   private final Router router;\n \n@@ -199,11 +187,10 @@ public class RouterRpcServer extends AbstractService\n   // Modules implementing groups of RPC calls\n   /** Router Quota calls. */\n   private final Quota quotaCall;\n-  /** Erasure coding calls. */\n-  private final ErasureCoding erasureCoding;\n   /** NamenodeProtocol calls. */\n   private final RouterNamenodeProtocol nnProto;\n-\n+  /** ClientProtocol calls. */\n+  private final RouterClientProtocol clientProto;\n \n   /**\n    * Construct a router RPC server.\n@@ -223,12 +210,6 @@ public RouterRpcServer(Configuration configuration, Router router,\n     this.namenodeResolver = nnResolver;\n     this.subclusterResolver = fileResolver;\n \n-    // User and group for reporting\n-    this.superUser = System.getProperty(\"user.name\");\n-    this.superGroup = this.conf.get(\n-        DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY,\n-        DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_DEFAULT);\n-\n     // RPC server settings\n     int handlerCount = this.conf.getInt(DFS_ROUTER_HANDLER_COUNT_KEY,\n         DFS_ROUTER_HANDLER_COUNT_DEFAULT);\n@@ -315,8 +296,8 @@ public RouterRpcServer(Configuration configuration, Router router,\n \n     // Initialize modules\n     this.quotaCall = new Quota(this.router, this);\n-    this.erasureCoding = new ErasureCoding(this);\n     this.nnProto = new RouterNamenodeProtocol(this);\n+    this.clientProto = new RouterClientProtocol(conf, this);\n   }\n \n   @Override\n@@ -370,6 +351,13 @@ public FileSubclusterResolver getSubclusterResolver() {\n     return subclusterResolver;\n   }\n \n+  /**\n+   * Get the active namenode resolver\n+   */\n+  public ActiveNamenodeResolver getNamenodeResolver() {\n+    return namenodeResolver;\n+  }\n+\n   /**\n    * Get the RPC monitor and metrics.\n    *\n@@ -411,7 +399,7 @@ public InetSocketAddress getRpcAddress() {\n    *                           client requests.\n    * @throws UnsupportedOperationException If the operation is not supported.\n    */\n-  protected void checkOperation(OperationCategory op, boolean supported)\n+  void checkOperation(OperationCategory op, boolean supported)\n       throws StandbyException, UnsupportedOperationException {\n     checkOperation(op);\n \n@@ -433,7 +421,7 @@ protected void checkOperation(OperationCategory op, boolean supported)\n    * @throws SafeModeException If the Router is in safe mode and cannot serve\n    *                           client requests.\n    */\n-  protected void checkOperation(OperationCategory op)\n+  void checkOperation(OperationCategory op)\n       throws StandbyException {\n     // Log the function we are currently calling.\n     if (rpcMonitor != null) {\n@@ -464,58 +452,44 @@ protected void checkOperation(OperationCategory op)\n     }\n   }\n \n+  /**\n+   * Get the name of the method that is calling this function.\n+   *\n+   * @return Name of the method calling this function.\n+   */\n+  static String getMethodName() {\n+    final StackTraceElement[] stack = Thread.currentThread().getStackTrace();\n+    String methodName = stack[3].getMethodName();\n+    return methodName;\n+  }\n+\n   @Override // ClientProtocol\n   public Token<DelegationTokenIdentifier> getDelegationToken(Text renewer)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n-    return null;\n-  }\n-\n-  /**\n-   * The the delegation token from each name service.\n-   * @param renewer\n-   * @return Name service -> Token.\n-   * @throws IOException\n-   */\n-  public Map<FederationNamespaceInfo, Token<DelegationTokenIdentifier>>\n-      getDelegationTokens(Text renewer) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n-    return null;\n+    return clientProto.getDelegationToken(renewer);\n   }\n \n   @Override // ClientProtocol\n   public long renewDelegationToken(Token<DelegationTokenIdentifier> token)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n-    return 0;\n+    return clientProto.renewDelegationToken(token);\n   }\n \n   @Override // ClientProtocol\n   public void cancelDelegationToken(Token<DelegationTokenIdentifier> token)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.cancelDelegationToken(token);\n   }\n \n   @Override // ClientProtocol\n   public LocatedBlocks getBlockLocations(String src, final long offset,\n       final long length) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod remoteMethod = new RemoteMethod(\"getBlockLocations\",\n-        new Class<?>[] {String.class, long.class, long.class},\n-        new RemoteParam(), offset, length);\n-    return (LocatedBlocks) rpcClient.invokeSequential(locations, remoteMethod,\n-        LocatedBlocks.class, null);\n+    return clientProto.getBlockLocations(src, offset, length);\n   }\n \n   @Override // ClientProtocol\n   public FsServerDefaults getServerDefaults() throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    RemoteMethod method = new RemoteMethod(\"getServerDefaults\");\n-    String ns = subclusterResolver.getDefaultNamespace();\n-    return (FsServerDefaults) rpcClient.invokeSingle(ns, method);\n+    return clientProto.getServerDefaults();\n   }\n \n   @Override // ClientProtocol\n@@ -524,44 +498,8 @@ public HdfsFileStatus create(String src, FsPermission masked,\n       boolean createParent, short replication, long blockSize,\n       CryptoProtocolVersion[] supportedVersions, String ecPolicyName)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    if (createParent && isPathAll(src)) {\n-      int index = src.lastIndexOf(Path.SEPARATOR);\n-      String parent = src.substring(0, index);\n-      LOG.debug(\"Creating {} requires creating parent {}\", src, parent);\n-      FsPermission parentPermissions = getParentPermission(masked);\n-      boolean success = mkdirs(parent, parentPermissions, createParent);\n-      if (!success) {\n-        // This shouldn't happen as mkdirs returns true or exception\n-        LOG.error(\"Couldn't create parents for {}\", src);\n-      }\n-    }\n-\n-    RemoteLocation createLocation = getCreateLocation(src);\n-    RemoteMethod method = new RemoteMethod(\"create\",\n-        new Class<?>[] {String.class, FsPermission.class, String.class,\n-                        EnumSetWritable.class, boolean.class, short.class,\n-                        long.class, CryptoProtocolVersion[].class,\n-                        String.class},\n-        createLocation.getDest(), masked, clientName, flag, createParent,\n+    return clientProto.create(src, masked, clientName, flag, createParent,\n         replication, blockSize, supportedVersions, ecPolicyName);\n-    return (HdfsFileStatus) rpcClient.invokeSingle(createLocation, method);\n-  }\n-\n-  /**\n-   * Get the permissions for the parent of a child with given permissions.\n-   * Add implicit u+wx permission for parent. This is based on\n-   * @{FSDirMkdirOp#addImplicitUwx}.\n-   * @param mask The permission mask of the child.\n-   * @return The permission mask of the parent.\n-   */\n-  private static FsPermission getParentPermission(final FsPermission mask) {\n-    FsPermission ret = new FsPermission(\n-        mask.getUserAction().or(FsAction.WRITE_EXECUTE),\n-        mask.getGroupAction(),\n-        mask.getOtherAction());\n-    return ret;\n   }\n \n   /**\n@@ -572,7 +510,7 @@ private static FsPermission getParentPermission(final FsPermission mask) {\n    * @return The remote location for this file.\n    * @throws IOException If the file has no creation location.\n    */\n-  protected RemoteLocation getCreateLocation(final String src)\n+  RemoteLocation getCreateLocation(final String src)\n       throws IOException {\n \n     final List<RemoteLocation> locations = getLocationsForPath(src, true);\n@@ -613,100 +551,45 @@ protected RemoteLocation getCreateLocation(final String src)\n     return createLocation;\n   }\n \n-  // Medium\n   @Override // ClientProtocol\n   public LastBlockWithStatus append(String src, final String clientName,\n       final EnumSetWritable<CreateFlag> flag) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"append\",\n-        new Class<?>[] {String.class, String.class, EnumSetWritable.class},\n-        new RemoteParam(), clientName, flag);\n-    return rpcClient.invokeSequential(\n-        locations, method, LastBlockWithStatus.class, null);\n+    return clientProto.append(src, clientName, flag);\n   }\n \n-  // Low\n   @Override // ClientProtocol\n   public boolean recoverLease(String src, String clientName)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"recoverLease\",\n-        new Class<?>[] {String.class, String.class}, new RemoteParam(),\n-        clientName);\n-    Object result = rpcClient.invokeSequential(\n-        locations, method, Boolean.class, Boolean.TRUE);\n-    return (boolean) result;\n+    return clientProto.recoverLease(src, clientName);\n   }\n \n   @Override // ClientProtocol\n   public boolean setReplication(String src, short replication)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"setReplication\",\n-        new Class<?>[] {String.class, short.class}, new RemoteParam(),\n-        replication);\n-    Object result = rpcClient.invokeSequential(\n-        locations, method, Boolean.class, Boolean.TRUE);\n-    return (boolean) result;\n+    return clientProto.setReplication(src, replication);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void setStoragePolicy(String src, String policyName)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"setStoragePolicy\",\n-        new Class<?>[] {String.class, String.class},\n-        new RemoteParam(), policyName);\n-    rpcClient.invokeSequential(locations, method, null, null);\n+    clientProto.setStoragePolicy(src, policyName);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BlockStoragePolicy[] getStoragePolicies() throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    RemoteMethod method = new RemoteMethod(\"getStoragePolicies\");\n-    String ns = subclusterResolver.getDefaultNamespace();\n-    return (BlockStoragePolicy[]) rpcClient.invokeSingle(ns, method);\n+    return clientProto.getStoragePolicies();\n   }\n \n   @Override // ClientProtocol\n   public void setPermission(String src, FsPermission permissions)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"setPermission\",\n-        new Class<?>[] {String.class, FsPermission.class},\n-        new RemoteParam(), permissions);\n-    if (isPathAll(src)) {\n-      rpcClient.invokeConcurrent(locations, method);\n-    } else {\n-      rpcClient.invokeSequential(locations, method);\n-    }\n+    clientProto.setPermission(src, permissions);\n   }\n \n   @Override // ClientProtocol\n   public void setOwner(String src, String username, String groupname)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"setOwner\",\n-        new Class<?>[] {String.class, String.class, String.class},\n-        new RemoteParam(), username, groupname);\n-    if (isPathAll(src)) {\n-      rpcClient.invokeConcurrent(locations, method);\n-    } else {\n-      rpcClient.invokeSequential(locations, method);\n-    }\n+    clientProto.setOwner(src, username, groupname);\n   }\n \n   /**\n@@ -718,18 +601,8 @@ public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes, EnumSet<AddBlockFlag> addBlockFlags)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"addBlock\",\n-        new Class<?>[] {String.class, String.class, ExtendedBlock.class,\n-                        DatanodeInfo[].class, long.class, String[].class,\n-                        EnumSet.class},\n-        new RemoteParam(), clientName, previous, excludedNodes, fileId,\n-        favoredNodes, addBlockFlags);\n-    // TODO verify the excludedNodes and favoredNodes are acceptable to this NN\n-    return (LocatedBlock) rpcClient.invokeSequential(\n-        locations, method, LocatedBlock.class, null);\n+    return clientProto.addBlock(src, clientName, previous, excludedNodes,\n+        fileId, favoredNodes, addBlockFlags);\n   }\n \n   /**\n@@ -742,55 +615,26 @@ public LocatedBlock getAdditionalDatanode(final String src, final long fileId,\n       final String[] existingStorageIDs, final DatanodeInfo[] excludes,\n       final int numAdditionalNodes, final String clientName)\n           throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod method = new RemoteMethod(\"getAdditionalDatanode\",\n-        new Class<?>[] {String.class, long.class, ExtendedBlock.class,\n-                        DatanodeInfo[].class, String[].class,\n-                        DatanodeInfo[].class, int.class, String.class},\n-        new RemoteParam(), fileId, blk, existings, existingStorageIDs, excludes,\n-        numAdditionalNodes, clientName);\n-    return (LocatedBlock) rpcClient.invokeSequential(\n-        locations, method, LocatedBlock.class, null);\n+    return clientProto.getAdditionalDatanode(src, fileId, blk, existings,\n+        existingStorageIDs, excludes, numAdditionalNodes, clientName);\n   }\n \n   @Override // ClientProtocol\n   public void abandonBlock(ExtendedBlock b, long fileId, String src,\n       String holder) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    RemoteMethod method = new RemoteMethod(\"abandonBlock\",\n-        new Class<?>[] {ExtendedBlock.class, long.class, String.class,\n-                        String.class},\n-        b, fileId, new RemoteParam(), holder);\n-    rpcClient.invokeSingle(b, method);\n+    clientProto.abandonBlock(b, fileId, src, holder);\n   }\n \n   @Override // ClientProtocol\n   public boolean complete(String src, String clientName, ExtendedBlock last,\n       long fileId) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"complete\",\n-        new Class<?>[] {String.class, String.class, ExtendedBlock.class,\n-                        long.class},\n-        new RemoteParam(), clientName, last, fileId);\n-    // Complete can return true/false, so don't expect a result\n-    return ((Boolean) rpcClient.invokeSequential(\n-        locations, method, Boolean.class, null)).booleanValue();\n+    return clientProto.complete(src, clientName, last, fileId);\n   }\n \n   @Override // ClientProtocol\n   public LocatedBlock updateBlockForPipeline(\n       ExtendedBlock block, String clientName) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    RemoteMethod method = new RemoteMethod(\"updateBlockForPipeline\",\n-        new Class<?>[] {ExtendedBlock.class, String.class},\n-        block, clientName);\n-    return (LocatedBlock) rpcClient.invokeSingle(block, method);\n+    return clientProto.updateBlockForPipeline(block, clientName);\n   }\n \n   /**\n@@ -801,462 +645,91 @@ public LocatedBlock updateBlockForPipeline(\n   public void updatePipeline(String clientName, ExtendedBlock oldBlock,\n       ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs)\n           throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    RemoteMethod method = new RemoteMethod(\"updatePipeline\",\n-        new Class<?>[] {String.class, ExtendedBlock.class, ExtendedBlock.class,\n-                        DatanodeID[].class, String[].class},\n-        clientName, oldBlock, newBlock, newNodes, newStorageIDs);\n-    rpcClient.invokeSingle(oldBlock, method);\n+    clientProto.updatePipeline(clientName, oldBlock, newBlock, newNodes,\n+        newStorageIDs);\n   }\n \n   @Override // ClientProtocol\n   public long getPreferredBlockSize(String src) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"getPreferredBlockSize\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-    return ((Long) rpcClient.invokeSequential(\n-        locations, method, Long.class, null)).longValue();\n-  }\n-\n-  /**\n-   * Determines combinations of eligible src/dst locations for a rename. A\n-   * rename cannot change the namespace. Renames are only allowed if there is an\n-   * eligible dst location in the same namespace as the source.\n-   *\n-   * @param srcLocations List of all potential source destinations where the\n-   *          path may be located. On return this list is trimmed to include\n-   *          only the paths that have corresponding destinations in the same\n-   *          namespace.\n-   * @param dst The destination path\n-   * @return A map of all eligible source namespaces and their corresponding\n-   *         replacement value.\n-   * @throws IOException If the dst paths could not be determined.\n-   */\n-  private RemoteParam getRenameDestinations(\n-      final List<RemoteLocation> srcLocations, final String dst)\n-          throws IOException {\n-\n-    final List<RemoteLocation> dstLocations = getLocationsForPath(dst, true);\n-    final Map<RemoteLocation, String> dstMap = new HashMap<>();\n-\n-    Iterator<RemoteLocation> iterator = srcLocations.iterator();\n-    while (iterator.hasNext()) {\n-      RemoteLocation srcLocation = iterator.next();\n-      RemoteLocation eligibleDst =\n-          getFirstMatchingLocation(srcLocation, dstLocations);\n-      if (eligibleDst != null) {\n-        // Use this dst for this source location\n-        dstMap.put(srcLocation, eligibleDst.getDest());\n-      } else {\n-        // This src destination is not valid, remove from the source list\n-        iterator.remove();\n-      }\n-    }\n-    return new RemoteParam(dstMap);\n-  }\n-\n-  /**\n-   * Get first matching location.\n-   *\n-   * @param location Location we are looking for.\n-   * @param locations List of locations.\n-   * @return The first matchin location in the list.\n-   */\n-  private RemoteLocation getFirstMatchingLocation(RemoteLocation location,\n-      List<RemoteLocation> locations) {\n-    for (RemoteLocation loc : locations) {\n-      if (loc.getNameserviceId().equals(location.getNameserviceId())) {\n-        // Return first matching location\n-        return loc;\n-      }\n-    }\n-    return null;\n+    return clientProto.getPreferredBlockSize(src);\n   }\n \n   @Deprecated\n   @Override // ClientProtocol\n   public boolean rename(final String src, final String dst)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> srcLocations =\n-        getLocationsForPath(src, true, false);\n-    // srcLocations may be trimmed by getRenameDestinations()\n-    final List<RemoteLocation> locs = new LinkedList<>(srcLocations);\n-    RemoteParam dstParam = getRenameDestinations(locs, dst);\n-    if (locs.isEmpty()) {\n-      throw new IOException(\n-          \"Rename of \" + src + \" to \" + dst + \" is not allowed,\" +\n-          \" no eligible destination in the same namespace was found.\");\n-    }\n-    RemoteMethod method = new RemoteMethod(\"rename\",\n-        new Class<?>[] {String.class, String.class},\n-        new RemoteParam(), dstParam);\n-    return ((Boolean) rpcClient.invokeSequential(\n-        locs, method, Boolean.class, Boolean.TRUE)).booleanValue();\n+    return clientProto.rename(src, dst);\n   }\n \n   @Override // ClientProtocol\n   public void rename2(final String src, final String dst,\n       final Options.Rename... options) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> srcLocations =\n-        getLocationsForPath(src, true, false);\n-    // srcLocations may be trimmed by getRenameDestinations()\n-    final List<RemoteLocation> locs = new LinkedList<>(srcLocations);\n-    RemoteParam dstParam = getRenameDestinations(locs, dst);\n-    if (locs.isEmpty()) {\n-      throw new IOException(\n-          \"Rename of \" + src + \" to \" + dst + \" is not allowed,\" +\n-          \" no eligible destination in the same namespace was found.\");\n-    }\n-    RemoteMethod method = new RemoteMethod(\"rename2\",\n-        new Class<?>[] {String.class, String.class, options.getClass()},\n-        new RemoteParam(), dstParam, options);\n-    rpcClient.invokeSequential(locs, method, null, null);\n+    clientProto.rename2(src, dst, options);\n   }\n \n   @Override // ClientProtocol\n   public void concat(String trg, String[] src) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // See if the src and target files are all in the same namespace\n-    LocatedBlocks targetBlocks = getBlockLocations(trg, 0, 1);\n-    if (targetBlocks == null) {\n-      throw new IOException(\"Cannot locate blocks for target file - \" + trg);\n-    }\n-    LocatedBlock lastLocatedBlock = targetBlocks.getLastLocatedBlock();\n-    String targetBlockPoolId = lastLocatedBlock.getBlock().getBlockPoolId();\n-    for (String source : src) {\n-      LocatedBlocks sourceBlocks = getBlockLocations(source, 0, 1);\n-      if (sourceBlocks == null) {\n-        throw new IOException(\n-            \"Cannot located blocks for source file \" + source);\n-      }\n-      String sourceBlockPoolId =\n-          sourceBlocks.getLastLocatedBlock().getBlock().getBlockPoolId();\n-      if (!sourceBlockPoolId.equals(targetBlockPoolId)) {\n-        throw new IOException(\"Cannot concatenate source file \" + source\n-            + \" because it is located in a different namespace\"\n-            + \" with block pool id \" + sourceBlockPoolId\n-            + \" from the target file with block pool id \"\n-            + targetBlockPoolId);\n-      }\n-    }\n-\n-    // Find locations in the matching namespace.\n-    final RemoteLocation targetDestination =\n-        getLocationForPath(trg, true, targetBlockPoolId);\n-    String[] sourceDestinations = new String[src.length];\n-    for (int i = 0; i < src.length; i++) {\n-      String sourceFile = src[i];\n-      RemoteLocation location =\n-          getLocationForPath(sourceFile, true, targetBlockPoolId);\n-      sourceDestinations[i] = location.getDest();\n-    }\n-    // Invoke\n-    RemoteMethod method = new RemoteMethod(\"concat\",\n-        new Class<?>[] {String.class, String[].class},\n-        targetDestination.getDest(), sourceDestinations);\n-    rpcClient.invokeSingle(targetDestination, method);\n+    clientProto.concat(trg, src);\n   }\n \n   @Override // ClientProtocol\n   public boolean truncate(String src, long newLength, String clientName)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"truncate\",\n-        new Class<?>[] {String.class, long.class, String.class},\n-        new RemoteParam(), newLength, clientName);\n-    return ((Boolean) rpcClient.invokeSequential(locations, method,\n-        Boolean.class, Boolean.TRUE)).booleanValue();\n+    return clientProto.truncate(src, newLength, clientName);\n   }\n \n   @Override // ClientProtocol\n   public boolean delete(String src, boolean recursive) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations =\n-        getLocationsForPath(src, true, false);\n-    RemoteMethod method = new RemoteMethod(\"delete\",\n-        new Class<?>[] {String.class, boolean.class}, new RemoteParam(),\n-        recursive);\n-    if (isPathAll(src)) {\n-      return rpcClient.invokeAll(locations, method);\n-    } else {\n-      return rpcClient.invokeSequential(locations, method,\n-          Boolean.class, Boolean.TRUE).booleanValue();\n-    }\n+    return clientProto.delete(src, recursive);\n   }\n \n   @Override // ClientProtocol\n   public boolean mkdirs(String src, FsPermission masked, boolean createParent)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"mkdirs\",\n-        new Class<?>[] {String.class, FsPermission.class, boolean.class},\n-        new RemoteParam(), masked, createParent);\n-\n-    // Create in all locations\n-    if (isPathAll(src)) {\n-      return rpcClient.invokeAll(locations, method);\n-    }\n-\n-    if (locations.size() > 1) {\n-      // Check if this directory already exists\n-      try {\n-        HdfsFileStatus fileStatus = getFileInfo(src);\n-        if (fileStatus != null) {\n-          // When existing, the NN doesn't return an exception; return true\n-          return true;\n-        }\n-      } catch (IOException ioe) {\n-        // Can't query if this file exists or not.\n-        LOG.error(\"Error requesting file info for path {} while proxing mkdirs\",\n-            src, ioe);\n-      }\n-    }\n-\n-    RemoteLocation firstLocation = locations.get(0);\n-    return ((Boolean) rpcClient.invokeSingle(firstLocation, method))\n-        .booleanValue();\n+    return clientProto.mkdirs(src, masked, createParent);\n   }\n \n   @Override // ClientProtocol\n   public void renewLease(String clientName) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    RemoteMethod method = new RemoteMethod(\"renewLease\",\n-        new Class<?>[] {String.class}, clientName);\n-    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    rpcClient.invokeConcurrent(nss, method, false, false);\n+    clientProto.renewLease(clientName);\n   }\n \n   @Override // ClientProtocol\n   public DirectoryListing getListing(String src, byte[] startAfter,\n       boolean needLocation) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    // Locate the dir and fetch the listing\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"getListing\",\n-        new Class<?>[] {String.class, startAfter.getClass(), boolean.class},\n-        new RemoteParam(), startAfter, needLocation);\n-    Map<RemoteLocation, DirectoryListing> listings =\n-        rpcClient.invokeConcurrent(\n-            locations, method, false, false, DirectoryListing.class);\n-\n-    Map<String, HdfsFileStatus> nnListing = new TreeMap<>();\n-    int totalRemainingEntries = 0;\n-    int remainingEntries = 0;\n-    boolean namenodeListingExists = false;\n-    if (listings != null) {\n-      // Check the subcluster listing with the smallest name\n-      String lastName = null;\n-      for (Entry<RemoteLocation, DirectoryListing> entry :\n-          listings.entrySet()) {\n-        RemoteLocation location = entry.getKey();\n-        DirectoryListing listing = entry.getValue();\n-        if (listing == null) {\n-          LOG.debug(\"Cannot get listing from {}\", location);\n-        } else {\n-          totalRemainingEntries += listing.getRemainingEntries();\n-          HdfsFileStatus[] partialListing = listing.getPartialListing();\n-          int length = partialListing.length;\n-          if (length > 0) {\n-            HdfsFileStatus lastLocalEntry = partialListing[length-1];\n-            String lastLocalName = lastLocalEntry.getLocalName();\n-            if (lastName == null || lastName.compareTo(lastLocalName) > 0) {\n-              lastName = lastLocalName;\n-            }\n-          }\n-        }\n-      }\n-\n-      // Add existing entries\n-      for (Object value : listings.values()) {\n-        DirectoryListing listing = (DirectoryListing) value;\n-        if (listing != null) {\n-          namenodeListingExists = true;\n-          for (HdfsFileStatus file : listing.getPartialListing()) {\n-            String filename = file.getLocalName();\n-            if (totalRemainingEntries > 0 && filename.compareTo(lastName) > 0) {\n-              // Discarding entries further than the lastName\n-              remainingEntries++;\n-            } else {\n-              nnListing.put(filename, file);\n-            }\n-          }\n-          remainingEntries += listing.getRemainingEntries();\n-        }\n-      }\n-    }\n-\n-    // Add mount points at this level in the tree\n-    final List<String> children = subclusterResolver.getMountPoints(src);\n-    if (children != null) {\n-      // Get the dates for each mount point\n-      Map<String, Long> dates = getMountPointDates(src);\n-\n-      // Create virtual folder with the mount name\n-      for (String child : children) {\n-        long date = 0;\n-        if (dates != null && dates.containsKey(child)) {\n-          date = dates.get(child);\n-        }\n-        // TODO add number of children\n-        HdfsFileStatus dirStatus = getMountPointStatus(child, 0, date);\n-\n-        // This may overwrite existing listing entries with the mount point\n-        // TODO don't add if already there?\n-        nnListing.put(child, dirStatus);\n-      }\n-    }\n-\n-    if (!namenodeListingExists && nnListing.size() == 0) {\n-      // NN returns a null object if the directory cannot be found and has no\n-      // listing. If we didn't retrieve any NN listing data, and there are no\n-      // mount points here, return null.\n-      return null;\n-    }\n-\n-    // Generate combined listing\n-    HdfsFileStatus[] combinedData = new HdfsFileStatus[nnListing.size()];\n-    combinedData = nnListing.values().toArray(combinedData);\n-    return new DirectoryListing(combinedData, remainingEntries);\n+    return clientProto.getListing(src, startAfter, needLocation);\n   }\n \n   @Override // ClientProtocol\n   public HdfsFileStatus getFileInfo(String src) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod method = new RemoteMethod(\"getFileInfo\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-\n-    HdfsFileStatus ret = null;\n-    // If it's a directory, we check in all locations\n-    if (isPathAll(src)) {\n-      ret = getFileInfoAll(locations, method);\n-    } else {\n-      // Check for file information sequentially\n-      ret = (HdfsFileStatus) rpcClient.invokeSequential(\n-          locations, method, HdfsFileStatus.class, null);\n-    }\n-\n-    // If there is no real path, check mount points\n-    if (ret == null) {\n-      List<String> children = subclusterResolver.getMountPoints(src);\n-      if (children != null && !children.isEmpty()) {\n-        Map<String, Long> dates = getMountPointDates(src);\n-        long date = 0;\n-        if (dates != null && dates.containsKey(src)) {\n-          date = dates.get(src);\n-        }\n-        ret = getMountPointStatus(src, children.size(), date);\n-      }\n-    }\n-\n-    return ret;\n-  }\n-\n-  /**\n-   * Get the file info from all the locations.\n-   *\n-   * @param locations Locations to check.\n-   * @param method The file information method to run.\n-   * @return The first file info if it's a file, the directory if it's\n-   *         everywhere.\n-   * @throws IOException If all the locations throw an exception.\n-   */\n-  private HdfsFileStatus getFileInfoAll(final List<RemoteLocation> locations,\n-      final RemoteMethod method) throws IOException {\n-\n-    // Get the file info from everybody\n-    Map<RemoteLocation, HdfsFileStatus> results =\n-        rpcClient.invokeConcurrent(locations, method, HdfsFileStatus.class);\n-\n-    // We return the first file\n-    HdfsFileStatus dirStatus = null;\n-    for (RemoteLocation loc : locations) {\n-      HdfsFileStatus fileStatus = results.get(loc);\n-      if (fileStatus != null) {\n-        if (!fileStatus.isDirectory()) {\n-          return fileStatus;\n-        } else if (dirStatus == null) {\n-          dirStatus = fileStatus;\n-        }\n-      }\n-    }\n-    return dirStatus;\n+    return clientProto.getFileInfo(src);\n   }\n \n   @Override // ClientProtocol\n   public boolean isFileClosed(String src) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod method = new RemoteMethod(\"isFileClosed\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-    return ((Boolean) rpcClient.invokeSequential(\n-        locations, method, Boolean.class, Boolean.TRUE)).booleanValue();\n+    return clientProto.isFileClosed(src);\n   }\n \n   @Override // ClientProtocol\n   public HdfsFileStatus getFileLinkInfo(String src) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod method = new RemoteMethod(\"getFileLinkInfo\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-    return (HdfsFileStatus) rpcClient.invokeSequential(\n-        locations, method, HdfsFileStatus.class, null);\n+    return clientProto.getFileLinkInfo(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public HdfsLocatedFileStatus getLocatedFileInfo(String src,\n       boolean needBlockToken) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-    final List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod method = new RemoteMethod(\"getLocatedFileInfo\",\n-        new Class<?>[] {String.class, boolean.class}, new RemoteParam(),\n-        Boolean.valueOf(needBlockToken));\n-    return (HdfsLocatedFileStatus) rpcClient.invokeSequential(\n-        locations, method, HdfsFileStatus.class, null);\n+    return clientProto.getLocatedFileInfo(src, needBlockToken);\n   }\n \n   @Override // ClientProtocol\n   public long[] getStats() throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    RemoteMethod method = new RemoteMethod(\"getStats\");\n-    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    Map<FederationNamespaceInfo, long[]> results =\n-        rpcClient.invokeConcurrent(nss, method, true, false, long[].class);\n-    long[] combinedData = new long[STATS_ARRAY_LENGTH];\n-    for (long[] data : results.values()) {\n-      for (int i = 0; i < combinedData.length && i < data.length; i++) {\n-        if (data[i] >= 0) {\n-          combinedData[i] += data[i];\n-        }\n-      }\n-    }\n-    return combinedData;\n+    return clientProto.getStats();\n   }\n \n   @Override // ClientProtocol\n   public DatanodeInfo[] getDatanodeReport(DatanodeReportType type)\n       throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-    return getDatanodeReport(type, true, 0);\n+    return clientProto.getDatanodeReport(type);\n   }\n \n   /**\n@@ -1305,29 +778,7 @@ public DatanodeInfo[] getDatanodeReport(\n   @Override // ClientProtocol\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       DatanodeReportType type) throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    Map<String, DatanodeStorageReport[]> dnSubcluster =\n-        getDatanodeStorageReportMap(type);\n-\n-    // Avoid repeating machines in multiple subclusters\n-    Map<String, DatanodeStorageReport> datanodesMap = new LinkedHashMap<>();\n-    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n-      for (DatanodeStorageReport dn : dns) {\n-        DatanodeInfo dnInfo = dn.getDatanodeInfo();\n-        String nodeId = dnInfo.getXferAddr();\n-        if (!datanodesMap.containsKey(nodeId)) {\n-          datanodesMap.put(nodeId, dn);\n-        }\n-        // TODO merge somehow, right now it just takes the first one\n-      }\n-    }\n-\n-    Collection<DatanodeStorageReport> datanodes = datanodesMap.values();\n-    DatanodeStorageReport[] combinedData =\n-        new DatanodeStorageReport[datanodes.size()];\n-    combinedData = datanodes.toArray(combinedData);\n-    return combinedData;\n+    return clientProto.getDatanodeStorageReport(type);\n   }\n \n   /**\n@@ -1360,740 +811,388 @@ public Map<String, DatanodeStorageReport[]> getDatanodeStorageReportMap(\n   @Override // ClientProtocol\n   public boolean setSafeMode(SafeModeAction action, boolean isChecked)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // Set safe mode in all the name spaces\n-    RemoteMethod method = new RemoteMethod(\"setSafeMode\",\n-        new Class<?>[] {SafeModeAction.class, boolean.class},\n-        action, isChecked);\n-    Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    Map<FederationNamespaceInfo, Boolean> results =\n-        rpcClient.invokeConcurrent(\n-            nss, method, true, !isChecked, Boolean.class);\n-\n-    // We only report true if all the name space are in safe mode\n-    int numSafemode = 0;\n-    for (boolean safemode : results.values()) {\n-      if (safemode) {\n-        numSafemode++;\n-      }\n-    }\n-    return numSafemode == results.size();\n+    return clientProto.setSafeMode(action, isChecked);\n   }\n \n   @Override // ClientProtocol\n   public boolean restoreFailedStorage(String arg) throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    RemoteMethod method = new RemoteMethod(\"restoreFailedStorage\",\n-        new Class<?>[] {String.class}, arg);\n-    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    Map<FederationNamespaceInfo, Boolean> ret =\n-        rpcClient.invokeConcurrent(nss, method, true, false, Boolean.class);\n-\n-    boolean success = true;\n-    for (boolean s : ret.values()) {\n-      if (!s) {\n-        success = false;\n-        break;\n-      }\n-    }\n-    return success;\n+    return clientProto.restoreFailedStorage(arg);\n   }\n \n   @Override // ClientProtocol\n   public boolean saveNamespace(long timeWindow, long txGap) throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    RemoteMethod method = new RemoteMethod(\"saveNamespace\",\n-        new Class<?>[] {Long.class, Long.class}, timeWindow, txGap);\n-    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    Map<FederationNamespaceInfo, Boolean> ret =\n-        rpcClient.invokeConcurrent(nss, method, true, false, boolean.class);\n-\n-    boolean success = true;\n-    for (boolean s : ret.values()) {\n-      if (!s) {\n-        success = false;\n-        break;\n-      }\n-    }\n-    return success;\n+    return clientProto.saveNamespace(timeWindow, txGap);\n   }\n \n   @Override // ClientProtocol\n   public long rollEdits() throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    RemoteMethod method = new RemoteMethod(\"rollEdits\", new Class<?>[] {});\n-    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    Map<FederationNamespaceInfo, Long> ret =\n-        rpcClient.invokeConcurrent(nss, method, true, false, long.class);\n-\n-    // Return the maximum txid\n-    long txid = 0;\n-    for (long t : ret.values()) {\n-      if (t > txid) {\n-        txid = t;\n-      }\n-    }\n-    return txid;\n+    return clientProto.rollEdits();\n   }\n \n   @Override // ClientProtocol\n   public void refreshNodes() throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    RemoteMethod method = new RemoteMethod(\"refreshNodes\", new Class<?>[] {});\n-    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    rpcClient.invokeConcurrent(nss, method, true, true);\n+    clientProto.refreshNodes();\n   }\n \n   @Override // ClientProtocol\n   public void finalizeUpgrade() throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    RemoteMethod method = new RemoteMethod(\"finalizeUpgrade\",\n-        new Class<?>[] {});\n-    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    rpcClient.invokeConcurrent(nss, method, true, false);\n+    clientProto.finalizeUpgrade();\n   }\n \n   @Override // ClientProtocol\n   public boolean upgradeStatus() throws IOException {\n-    String methodName = getMethodName();\n-    throw new UnsupportedOperationException(\n-        \"Operation \\\"\" + methodName + \"\\\" is not supported\");\n+    return clientProto.upgradeStatus();\n   }\n \n   @Override // ClientProtocol\n   public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action)\n       throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    RemoteMethod method = new RemoteMethod(\"rollingUpgrade\",\n-        new Class<?>[] {RollingUpgradeAction.class}, action);\n-    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    Map<FederationNamespaceInfo, RollingUpgradeInfo> ret =\n-        rpcClient.invokeConcurrent(\n-            nss, method, true, false, RollingUpgradeInfo.class);\n-\n-    // Return the first rolling upgrade info\n-    RollingUpgradeInfo info = null;\n-    for (RollingUpgradeInfo infoNs : ret.values()) {\n-      if (info == null && infoNs != null) {\n-        info = infoNs;\n-      }\n-    }\n-    return info;\n+    return clientProto.rollingUpgrade(action);\n   }\n \n   @Override // ClientProtocol\n   public void metaSave(String filename) throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    RemoteMethod method = new RemoteMethod(\"metaSave\",\n-        new Class<?>[] {String.class}, filename);\n-    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    rpcClient.invokeConcurrent(nss, method, true, false);\n+    clientProto.metaSave(filename);\n   }\n \n   @Override // ClientProtocol\n   public CorruptFileBlocks listCorruptFileBlocks(String path, String cookie)\n       throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(path, false);\n-    RemoteMethod method = new RemoteMethod(\"listCorruptFileBlocks\",\n-        new Class<?>[] {String.class, String.class},\n-        new RemoteParam(), cookie);\n-    return (CorruptFileBlocks) rpcClient.invokeSequential(\n-        locations, method, CorruptFileBlocks.class, null);\n+    return clientProto.listCorruptFileBlocks(path, cookie);\n   }\n \n   @Override // ClientProtocol\n   public void setBalancerBandwidth(long bandwidth) throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    RemoteMethod method = new RemoteMethod(\"setBalancerBandwidth\",\n-        new Class<?>[] {Long.class}, bandwidth);\n-    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    rpcClient.invokeConcurrent(nss, method, true, false);\n+    clientProto.setBalancerBandwidth(bandwidth);\n   }\n \n   @Override // ClientProtocol\n   public ContentSummary getContentSummary(String path) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    // Get the summaries from regular files\n-    Collection<ContentSummary> summaries = new LinkedList<>();\n-    FileNotFoundException notFoundException = null;\n-    try {\n-      final List<RemoteLocation> locations = getLocationsForPath(path, false);\n-      RemoteMethod method = new RemoteMethod(\"getContentSummary\",\n-          new Class<?>[] {String.class}, new RemoteParam());\n-      Map<RemoteLocation, ContentSummary> results =\n-          rpcClient.invokeConcurrent(\n-              locations, method, false, false, ContentSummary.class);\n-      summaries.addAll(results.values());\n-    } catch (FileNotFoundException e) {\n-      notFoundException = e;\n-    }\n-\n-    // Add mount points at this level in the tree\n-    final List<String> children = subclusterResolver.getMountPoints(path);\n-    if (children != null) {\n-      for (String child : children) {\n-        Path childPath = new Path(path, child);\n-        try {\n-          ContentSummary mountSummary = getContentSummary(childPath.toString());\n-          if (mountSummary != null) {\n-            summaries.add(mountSummary);\n-          }\n-        } catch (Exception e) {\n-          LOG.error(\"Cannot get content summary for mount {}: {}\",\n-              childPath, e.getMessage());\n-        }\n-      }\n-    }\n-\n-    // Throw original exception if no original nor mount points\n-    if (summaries.isEmpty() && notFoundException != null) {\n-      throw notFoundException;\n-    }\n-\n-    return aggregateContentSummary(summaries);\n-  }\n-\n-  /**\n-   * Aggregate content summaries for each subcluster.\n-   *\n-   * @param summaries Collection of individual summaries.\n-   * @return Aggregated content summary.\n-   */\n-  private ContentSummary aggregateContentSummary(\n-      Collection<ContentSummary> summaries) {\n-    if (summaries.size() == 1) {\n-      return summaries.iterator().next();\n-    }\n-\n-    long length = 0;\n-    long fileCount = 0;\n-    long directoryCount = 0;\n-    long quota = 0;\n-    long spaceConsumed = 0;\n-    long spaceQuota = 0;\n-\n-    for (ContentSummary summary : summaries) {\n-      length += summary.getLength();\n-      fileCount += summary.getFileCount();\n-      directoryCount += summary.getDirectoryCount();\n-      quota += summary.getQuota();\n-      spaceConsumed += summary.getSpaceConsumed();\n-      spaceQuota += summary.getSpaceQuota();\n-    }\n-\n-    ContentSummary ret = new ContentSummary.Builder()\n-        .length(length)\n-        .fileCount(fileCount)\n-        .directoryCount(directoryCount)\n-        .quota(quota)\n-        .spaceConsumed(spaceConsumed)\n-        .spaceQuota(spaceQuota)\n-        .build();\n-    return ret;\n+    return clientProto.getContentSummary(path);\n   }\n \n   @Override // ClientProtocol\n   public void fsync(String src, long fileId, String clientName,\n       long lastBlockLength) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"fsync\",\n-        new Class<?>[] {String.class, long.class, String.class, long.class },\n-        new RemoteParam(), fileId, clientName, lastBlockLength);\n-    rpcClient.invokeSequential(locations, method);\n+    clientProto.fsync(src, fileId, clientName, lastBlockLength);\n   }\n \n   @Override // ClientProtocol\n   public void setTimes(String src, long mtime, long atime) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"setTimes\",\n-        new Class<?>[] {String.class, long.class, long.class},\n-        new RemoteParam(), mtime, atime);\n-    rpcClient.invokeSequential(locations, method);\n+    clientProto.setTimes(src, mtime, atime);\n   }\n \n   @Override // ClientProtocol\n   public void createSymlink(String target, String link, FsPermission dirPerms,\n       boolean createParent) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // TODO Verify that the link location is in the same NS as the targets\n-    final List<RemoteLocation> targetLocations =\n-        getLocationsForPath(target, true);\n-    final List<RemoteLocation> linkLocations =\n-        getLocationsForPath(link, true);\n-    RemoteLocation linkLocation = linkLocations.get(0);\n-    RemoteMethod method = new RemoteMethod(\"createSymlink\",\n-        new Class<?>[] {String.class, String.class, FsPermission.class,\n-                        boolean.class},\n-        new RemoteParam(), linkLocation.getDest(), dirPerms, createParent);\n-    rpcClient.invokeSequential(targetLocations, method);\n+    clientProto.createSymlink(target, link, dirPerms, createParent);\n   }\n \n   @Override // ClientProtocol\n   public String getLinkTarget(String path) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    final List<RemoteLocation> locations = getLocationsForPath(path, true);\n-    RemoteMethod method = new RemoteMethod(\"getLinkTarget\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-    return (String) rpcClient.invokeSequential(\n-        locations, method, String.class, null);\n+    return clientProto.getLinkTarget(path);\n   }\n \n   @Override // Client Protocol\n   public void allowSnapshot(String snapshotRoot) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.allowSnapshot(snapshotRoot);\n   }\n \n   @Override // Client Protocol\n   public void disallowSnapshot(String snapshot) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.disallowSnapshot(snapshot);\n   }\n \n   @Override // ClientProtocol\n   public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n       String snapshotNewName) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.renameSnapshot(snapshotRoot, snapshotOldName, snapshotNewName);\n   }\n \n   @Override // Client Protocol\n   public SnapshottableDirectoryStatus[] getSnapshottableDirListing()\n       throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.getSnapshottableDirListing();\n   }\n \n   @Override // ClientProtocol\n   public SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot,\n       String earlierSnapshotName, String laterSnapshotName) throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.getSnapshotDiffReport(\n+        snapshotRoot, earlierSnapshotName, laterSnapshotName);\n   }\n \n   @Override // ClientProtocol\n   public SnapshotDiffReportListing getSnapshotDiffReportListing(\n       String snapshotRoot, String earlierSnapshotName, String laterSnapshotName,\n       byte[] startPath, int index) throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.getSnapshotDiffReportListing(snapshotRoot,\n+        earlierSnapshotName, laterSnapshotName, startPath, index);\n   }\n \n   @Override // ClientProtocol\n   public long addCacheDirective(CacheDirectiveInfo path,\n       EnumSet<CacheFlag> flags) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n-    return 0;\n+    return clientProto.addCacheDirective(path, flags);\n   }\n \n   @Override // ClientProtocol\n   public void modifyCacheDirective(CacheDirectiveInfo directive,\n       EnumSet<CacheFlag> flags) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.modifyCacheDirective(directive, flags);\n   }\n \n   @Override // ClientProtocol\n   public void removeCacheDirective(long id) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.removeCacheDirective(id);\n   }\n \n   @Override // ClientProtocol\n   public BatchedEntries<CacheDirectiveEntry> listCacheDirectives(\n       long prevId, CacheDirectiveInfo filter) throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.listCacheDirectives(prevId, filter);\n   }\n \n   @Override // ClientProtocol\n   public void addCachePool(CachePoolInfo info) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.addCachePool(info);\n   }\n \n   @Override // ClientProtocol\n   public void modifyCachePool(CachePoolInfo info) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.modifyCachePool(info);\n   }\n \n   @Override // ClientProtocol\n   public void removeCachePool(String cachePoolName) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.removeCachePool(cachePoolName);\n   }\n \n   @Override // ClientProtocol\n   public BatchedEntries<CachePoolEntry> listCachePools(String prevKey)\n       throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.listCachePools(prevKey);\n   }\n \n   @Override // ClientProtocol\n   public void modifyAclEntries(String src, List<AclEntry> aclSpec)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"modifyAclEntries\",\n-        new Class<?>[] {String.class, List.class},\n-        new RemoteParam(), aclSpec);\n-    rpcClient.invokeSequential(locations, method, null, null);\n+    clientProto.modifyAclEntries(src, aclSpec);\n   }\n \n   @Override // ClienProtocol\n   public void removeAclEntries(String src, List<AclEntry> aclSpec)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"removeAclEntries\",\n-        new Class<?>[] {String.class, List.class},\n-        new RemoteParam(), aclSpec);\n-    rpcClient.invokeSequential(locations, method, null, null);\n+    clientProto.removeAclEntries(src, aclSpec);\n   }\n \n   @Override // ClientProtocol\n   public void removeDefaultAcl(String src) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"removeDefaultAcl\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-    rpcClient.invokeSequential(locations, method);\n+    clientProto.removeDefaultAcl(src);\n   }\n \n   @Override // ClientProtocol\n   public void removeAcl(String src) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"removeAcl\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-    rpcClient.invokeSequential(locations, method);\n+    clientProto.removeAcl(src);\n   }\n \n   @Override // ClientProtocol\n   public void setAcl(String src, List<AclEntry> aclSpec) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\n-        \"setAcl\", new Class<?>[] {String.class, List.class},\n-        new RemoteParam(), aclSpec);\n-    rpcClient.invokeSequential(locations, method);\n+    clientProto.setAcl(src, aclSpec);\n   }\n \n   @Override // ClientProtocol\n   public AclStatus getAclStatus(String src) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod method = new RemoteMethod(\"getAclStatus\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-    return (AclStatus) rpcClient.invokeSequential(\n-        locations, method, AclStatus.class, null);\n+    return clientProto.getAclStatus(src);\n   }\n \n   @Override // ClientProtocol\n   public void createEncryptionZone(String src, String keyName)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"createEncryptionZone\",\n-        new Class<?>[] {String.class, String.class},\n-        new RemoteParam(), keyName);\n-    rpcClient.invokeSequential(locations, method);\n+    clientProto.createEncryptionZone(src, keyName);\n   }\n \n   @Override // ClientProtocol\n   public EncryptionZone getEZForPath(String src) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod method = new RemoteMethod(\"getEZForPath\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-    return (EncryptionZone) rpcClient.invokeSequential(\n-        locations, method, EncryptionZone.class, null);\n+    return clientProto.getEZForPath(src);\n   }\n \n   @Override // ClientProtocol\n   public BatchedEntries<EncryptionZone> listEncryptionZones(long prevId)\n       throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.listEncryptionZones(prevId);\n   }\n \n   @Override // ClientProtocol\n   public void reencryptEncryptionZone(String zone, ReencryptAction action)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.reencryptEncryptionZone(zone, action);\n   }\n \n   @Override // ClientProtocol\n   public BatchedEntries<ZoneReencryptionStatus> listReencryptionStatus(\n       long prevId) throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.listReencryptionStatus(prevId);\n   }\n \n   @Override // ClientProtocol\n   public void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"setXAttr\",\n-        new Class<?>[] {String.class, XAttr.class, EnumSet.class},\n-        new RemoteParam(), xAttr, flag);\n-    rpcClient.invokeSequential(locations, method);\n+    clientProto.setXAttr(src, xAttr, flag);\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   @Override // ClientProtocol\n   public List<XAttr> getXAttrs(String src, List<XAttr> xAttrs)\n       throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod method = new RemoteMethod(\"getXAttrs\",\n-        new Class<?>[] {String.class, List.class}, new RemoteParam(), xAttrs);\n-    return (List<XAttr>) rpcClient.invokeSequential(\n-        locations, method, List.class, null);\n+    return clientProto.getXAttrs(src, xAttrs);\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   @Override // ClientProtocol\n   public List<XAttr> listXAttrs(String src) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, false);\n-    RemoteMethod method = new RemoteMethod(\"listXAttrs\",\n-        new Class<?>[] {String.class}, new RemoteParam());\n-    return (List<XAttr>) rpcClient.invokeSequential(\n-        locations, method, List.class, null);\n+    return clientProto.listXAttrs(src);\n   }\n \n   @Override // ClientProtocol\n   public void removeXAttr(String src, XAttr xAttr) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(src, true);\n-    RemoteMethod method = new RemoteMethod(\"removeXAttr\",\n-        new Class<?>[] {String.class, XAttr.class}, new RemoteParam(), xAttr);\n-    rpcClient.invokeSequential(locations, method);\n+    clientProto.removeXAttr(src, xAttr);\n   }\n \n   @Override // ClientProtocol\n   public void checkAccess(String path, FsAction mode) throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    // TODO handle virtual directories\n-    final List<RemoteLocation> locations = getLocationsForPath(path, true);\n-    RemoteMethod method = new RemoteMethod(\"checkAccess\",\n-        new Class<?>[] {String.class, FsAction.class},\n-        new RemoteParam(), mode);\n-    rpcClient.invokeSequential(locations, method);\n+    clientProto.checkAccess(path, mode);\n   }\n \n   @Override // ClientProtocol\n   public long getCurrentEditLogTxid() throws IOException {\n-    checkOperation(OperationCategory.READ);\n-\n-    RemoteMethod method = new RemoteMethod(\n-        \"getCurrentEditLogTxid\", new Class<?>[] {});\n-    final Set<FederationNamespaceInfo> nss = namenodeResolver.getNamespaces();\n-    Map<FederationNamespaceInfo, Long> ret =\n-        rpcClient.invokeConcurrent(nss, method, true, false, long.class);\n-\n-    // Return the maximum txid\n-    long txid = 0;\n-    for (long t : ret.values()) {\n-      if (t > txid) {\n-        txid = t;\n-      }\n-    }\n-    return txid;\n+    return clientProto.getCurrentEditLogTxid();\n   }\n \n   @Override // ClientProtocol\n   public EventBatchList getEditsFromTxid(long txid) throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.getEditsFromTxid(txid);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public DataEncryptionKey getDataEncryptionKey() throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.getDataEncryptionKey();\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public String createSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-    return null;\n+    return clientProto.createSnapshot(snapshotRoot, snapshotName);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void deleteSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.deleteSnapshot(snapshotRoot, snapshotName);\n   }\n \n   @Override // ClientProtocol\n   public void setQuota(String path, long namespaceQuota, long storagespaceQuota,\n       StorageType type) throws IOException {\n-    this.quotaCall.setQuota(path, namespaceQuota, storagespaceQuota, type);\n+    clientProto.setQuota(path, namespaceQuota, storagespaceQuota, type);\n   }\n \n   @Override // ClientProtocol\n   public QuotaUsage getQuotaUsage(String path) throws IOException {\n-    return this.quotaCall.getQuotaUsage(path);\n+    return clientProto.getQuotaUsage(path);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void reportBadBlocks(LocatedBlock[] blocks) throws IOException {\n-    checkOperation(OperationCategory.WRITE);\n-\n-    // Block pool id -> blocks\n-    Map<String, List<LocatedBlock>> blockLocations = new HashMap<>();\n-    for (LocatedBlock block : blocks) {\n-      String bpId = block.getBlock().getBlockPoolId();\n-      List<LocatedBlock> bpBlocks = blockLocations.get(bpId);\n-      if (bpBlocks == null) {\n-        bpBlocks = new LinkedList<>();\n-        blockLocations.put(bpId, bpBlocks);\n-      }\n-      bpBlocks.add(block);\n-    }\n-\n-    // Invoke each block pool\n-    for (Entry<String, List<LocatedBlock>> entry : blockLocations.entrySet()) {\n-      String bpId = entry.getKey();\n-      List<LocatedBlock> bpBlocks = entry.getValue();\n-\n-      LocatedBlock[] bpBlocksArray =\n-          bpBlocks.toArray(new LocatedBlock[bpBlocks.size()]);\n-      RemoteMethod method = new RemoteMethod(\"reportBadBlocks\",\n-          new Class<?>[] {LocatedBlock[].class},\n-          new Object[] {bpBlocksArray});\n-      rpcClient.invokeSingleBlockPool(bpId, method);\n-    }\n+    clientProto.reportBadBlocks(blocks);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void unsetStoragePolicy(String src) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n+    clientProto.unsetStoragePolicy(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BlockStoragePolicy getStoragePolicy(String path) throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.getStoragePolicy(path);\n   }\n \n   @Override // ClientProtocol\n   public ErasureCodingPolicyInfo[] getErasureCodingPolicies()\n       throws IOException {\n-    return erasureCoding.getErasureCodingPolicies();\n+    return clientProto.getErasureCodingPolicies();\n   }\n \n   @Override // ClientProtocol\n   public Map<String, String> getErasureCodingCodecs() throws IOException {\n-    return erasureCoding.getErasureCodingCodecs();\n+    return clientProto.getErasureCodingCodecs();\n   }\n \n   @Override // ClientProtocol\n   public AddErasureCodingPolicyResponse[] addErasureCodingPolicies(\n       ErasureCodingPolicy[] policies) throws IOException {\n-    return erasureCoding.addErasureCodingPolicies(policies);\n+    return clientProto.addErasureCodingPolicies(policies);\n   }\n \n   @Override // ClientProtocol\n   public void removeErasureCodingPolicy(String ecPolicyName)\n       throws IOException {\n-    erasureCoding.removeErasureCodingPolicy(ecPolicyName);\n+    clientProto.removeErasureCodingPolicy(ecPolicyName);\n   }\n \n   @Override // ClientProtocol\n   public void disableErasureCodingPolicy(String ecPolicyName)\n       throws IOException {\n-    erasureCoding.disableErasureCodingPolicy(ecPolicyName);\n+    clientProto.disableErasureCodingPolicy(ecPolicyName);\n   }\n \n   @Override // ClientProtocol\n   public void enableErasureCodingPolicy(String ecPolicyName)\n       throws IOException {\n-    erasureCoding.enableErasureCodingPolicy(ecPolicyName);\n+    clientProto.enableErasureCodingPolicy(ecPolicyName);\n   }\n \n   @Override // ClientProtocol\n   public ErasureCodingPolicy getErasureCodingPolicy(String src)\n       throws IOException {\n-    return erasureCoding.getErasureCodingPolicy(src);\n+    return clientProto.getErasureCodingPolicy(src);\n   }\n \n   @Override // ClientProtocol\n   public void setErasureCodingPolicy(String src, String ecPolicyName)\n       throws IOException {\n-    erasureCoding.setErasureCodingPolicy(src, ecPolicyName);\n+    clientProto.setErasureCodingPolicy(src, ecPolicyName);\n   }\n \n   @Override // ClientProtocol\n   public void unsetErasureCodingPolicy(String src) throws IOException {\n-    erasureCoding.unsetErasureCodingPolicy(src);\n+    clientProto.unsetErasureCodingPolicy(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public ECBlockGroupStats getECBlockGroupStats() throws IOException {\n-    return erasureCoding.getECBlockGroupStats();\n+    return clientProto.getECBlockGroupStats();\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public ReplicatedBlockStats getReplicatedBlockStats() throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.getReplicatedBlockStats();\n   }\n \n   @Deprecated\n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<OpenFileEntry> listOpenFiles(long prevId)\n       throws IOException {\n-    return listOpenFiles(prevId, EnumSet.of(OpenFilesType.ALL_OPEN_FILES),\n-        OpenFilesIterator.FILTER_PATH_DEFAULT);\n+    return clientProto.listOpenFiles(prevId);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<OpenFileEntry> listOpenFiles(long prevId,\n       EnumSet<OpenFilesType> openFilesTypes, String path) throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    return null;\n+    return clientProto.listOpenFiles(prevId, openFilesTypes, path);\n+  }\n+\n+  @Override // ClientProtocol\n+  public void satisfyStoragePolicy(String path) throws IOException {\n+    clientProto.satisfyStoragePolicy(path);\n   }\n \n   @Override // NamenodeProtocol\n@@ -2167,6 +1266,11 @@ public boolean isRollingUpgrade() throws IOException {\n     return nnProto.isRollingUpgrade();\n   }\n \n+  @Override // NamenodeProtocol\n+  public Long getNextSPSPath() throws IOException {\n+    return nnProto.getNextSPSPath();\n+  }\n+\n   /**\n    * Locate the location with the matching block pool id.\n    *\n@@ -2176,7 +1280,7 @@ public boolean isRollingUpgrade() throws IOException {\n    * @return Prioritized list of locations in the federated cluster.\n    * @throws IOException if the location for this path cannot be determined.\n    */\n-  private RemoteLocation getLocationForPath(\n+  protected RemoteLocation getLocationForPath(\n       String path, boolean failIfLocked, String blockPoolId)\n           throws IOException {\n \n@@ -2275,27 +1379,6 @@ protected List<RemoteLocation> getLocationsForPath(String path,\n     }\n   }\n \n-  /**\n-   * Check if a path should be in all subclusters.\n-   *\n-   * @param path Path to check.\n-   * @return If a path should be in all subclusters.\n-   */\n-  private boolean isPathAll(final String path) {\n-    if (subclusterResolver instanceof MountTableResolver) {\n-      try {\n-        MountTableResolver mountTable = (MountTableResolver)subclusterResolver;\n-        MountTable entry = mountTable.getMountPoint(path);\n-        if (entry != null) {\n-          return entry.isAll();\n-        }\n-      } catch (IOException e) {\n-        LOG.error(\"Cannot get mount point\", e);\n-      }\n-    }\n-    return false;\n-  }\n-\n   /**\n    * Check if a path is in a read only mount point.\n    *\n@@ -2317,121 +1400,6 @@ private boolean isPathReadOnly(final String path) {\n     return false;\n   }\n \n-  /**\n-   * Get the modification dates for mount points.\n-   *\n-   * @param path Name of the path to start checking dates from.\n-   * @return Map with the modification dates for all sub-entries.\n-   */\n-  private Map<String, Long> getMountPointDates(String path) {\n-    Map<String, Long> ret = new TreeMap<>();\n-    if (subclusterResolver instanceof MountTableResolver) {\n-      try {\n-        final List<String> children = subclusterResolver.getMountPoints(path);\n-        for (String child : children) {\n-          Long modTime = getModifiedTime(ret, path, child);\n-          ret.put(child, modTime);\n-        }\n-      } catch (IOException e) {\n-        LOG.error(\"Cannot get mount point\", e);\n-      }\n-    }\n-    return ret;\n-  }\n-\n-  /**\n-   * Get modified time for child. If the child is present in mount table it\n-   * will return the modified time. If the child is not present but subdirs of\n-   * this child are present then it will return latest modified subdir's time\n-   * as modified time of the requested child.\n-   * @param ret contains children and modified times.\n-   * @param mountTable.\n-   * @param path Name of the path to start checking dates from.\n-   * @param child child of the requested path.\n-   * @return modified time.\n-   */\n-  private long getModifiedTime(Map<String, Long> ret, String path,\n-      String child) {\n-    MountTableResolver mountTable = (MountTableResolver)subclusterResolver;\n-    String srcPath;\n-    if (path.equals(Path.SEPARATOR)) {\n-      srcPath = Path.SEPARATOR + child;\n-    } else {\n-      srcPath = path + Path.SEPARATOR + child;\n-    }\n-    Long modTime = 0L;\n-    try {\n-      // Get mount table entry for the srcPath\n-      MountTable entry = mountTable.getMountPoint(srcPath);\n-      // if srcPath is not in mount table but its subdirs are in mount\n-      // table we will display latest modified subdir date/time.\n-      if (entry == null) {\n-        List<MountTable> entries = mountTable.getMounts(srcPath);\n-        for (MountTable eachEntry : entries) {\n-          // Get the latest date\n-          if (ret.get(child) == null ||\n-              ret.get(child) < eachEntry.getDateModified()) {\n-            modTime = eachEntry.getDateModified();\n-          }\n-        }\n-      } else {\n-        modTime = entry.getDateModified();\n-      }\n-    } catch (IOException e) {\n-      LOG.error(\"Cannot get mount point\", e);\n-    }\n-    return modTime;\n-  }\n-\n-  /**\n-   * Create a new file status for a mount point.\n-   *\n-   * @param name Name of the mount point.\n-   * @param childrenNum Number of children.\n-   * @param date Map with the dates.\n-   * @return New HDFS file status representing a mount point.\n-   */\n-  private HdfsFileStatus getMountPointStatus(\n-      String name, int childrenNum, long date) {\n-    long modTime = date;\n-    long accessTime = date;\n-    FsPermission permission = FsPermission.getDirDefault();\n-    String owner = this.superUser;\n-    String group = this.superGroup;\n-    try {\n-      // TODO support users, it should be the user for the pointed folder\n-      UserGroupInformation ugi = getRemoteUser();\n-      owner = ugi.getUserName();\n-      group = ugi.getPrimaryGroupName();\n-    } catch (IOException e) {\n-      LOG.error(\"Cannot get the remote user: {}\", e.getMessage());\n-    }\n-    long inodeId = 0;\n-    return new HdfsFileStatus.Builder()\n-      .isdir(true)\n-      .mtime(modTime)\n-      .atime(accessTime)\n-      .perm(permission)\n-      .owner(owner)\n-      .group(group)\n-      .symlink(new byte[0])\n-      .path(DFSUtil.string2Bytes(name))\n-      .fileId(inodeId)\n-      .children(childrenNum)\n-      .build();\n-  }\n-\n-  /**\n-   * Get the name of the method that is calling this function.\n-   *\n-   * @return Name of the method calling this function.\n-   */\n-  private static String getMethodName() {\n-    final StackTraceElement[] stack = Thread.currentThread().getStackTrace();\n-    String methodName = stack[3].getMethodName();\n-    return methodName;\n-  }\n-\n   /**\n    * Get the user that is invoking this operation.\n    *\n@@ -2490,16 +1458,4 @@ public Quota getQuotaModule() {\n   public FederationRPCMetrics getRPCMetrics() {\n     return this.rpcMonitor.getRPCMetrics();\n   }\n-\n-  @Override\n-  public void satisfyStoragePolicy(String path) throws IOException {\n-    checkOperation(OperationCategory.WRITE, false);\n-  }\n-\n-  @Override\n-  public Long getNextSPSPath() throws IOException {\n-    checkOperation(OperationCategory.READ, false);\n-    // not supported\n-    return null;\n-  }\n }",
      "parent_sha": "77b015000a48545209928e31630adaaf6960b4c5"
    }
  },
  {
    "oid": "d75cbc5749808491d2b06f80506d95b6fb1b9e9c",
    "message": "HADOOP-13693. Remove the message about HTTP OPTIONS in SPNEGO initialization message from kms audit log.",
    "date": "2016-10-19T01:24:59Z",
    "url": "https://github.com/apache/hadoop/commit/d75cbc5749808491d2b06f80506d95b6fb1b9e9c",
    "details": {
      "sha": "928a8aa48f7dedde5f1fae3943e976872a181268",
      "filename": "hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSAuthenticationFilter.java",
      "status": "modified",
      "additions": 7,
      "deletions": 3,
      "changes": 10,
      "blob_url": "https://github.com/apache/hadoop/blob/d75cbc5749808491d2b06f80506d95b6fb1b9e9c/hadoop-common-project%2Fhadoop-kms%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2Fserver%2FKMSAuthenticationFilter.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d75cbc5749808491d2b06f80506d95b6fb1b9e9c/hadoop-common-project%2Fhadoop-kms%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2Fserver%2FKMSAuthenticationFilter.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-kms%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2Fserver%2FKMSAuthenticationFilter.java?ref=d75cbc5749808491d2b06f80506d95b6fb1b9e9c",
      "patch": "@@ -145,9 +145,13 @@ public void doFilter(ServletRequest request, ServletResponse response,\n         requestURL.append(\"?\").append(queryString);\n       }\n \n-      KMSWebApp.getKMSAudit().unauthenticated(\n-          request.getRemoteHost(), method, requestURL.toString(),\n-          kmsResponse.msg);\n+      if (!method.equals(\"OPTIONS\")) {\n+        // an HTTP OPTIONS request is made as part of the SPNEGO authentication\n+        // sequence. We do not need to audit log it, since it doesn't belong\n+        // to KMS context. KMS server doesn't handle OPTIONS either.\n+        KMSWebApp.getKMSAudit().unauthenticated(request.getRemoteHost(), method,\n+            requestURL.toString(), kmsResponse.msg);\n+      }\n     }\n   }\n ",
      "parent_sha": "efdf810cf9f72d78e97e860576c64a382ece437c"
    }
  },
  {
    "oid": "32cc2b8f1a3561fd922f6a7e662c1617a1871fb3",
    "message": "HDFS-11509. Ozone: Fix TestEndpoint test regression. Contributed by Anu Engineer.",
    "date": "2018-04-25T22:54:56Z",
    "url": "https://github.com/apache/hadoop/commit/32cc2b8f1a3561fd922f6a7e662c1617a1871fb3",
    "details": {
      "sha": "d6f30a13283e7518fc15af08545816d4ae501a5e",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/ozone/container/common/TestEndPoint.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/32cc2b8f1a3561fd922f6a7e662c1617a1871fb3/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2FTestEndPoint.java",
      "raw_url": "https://github.com/apache/hadoop/raw/32cc2b8f1a3561fd922f6a7e662c1617a1871fb3/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2FTestEndPoint.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2FTestEndPoint.java?ref=32cc2b8f1a3561fd922f6a7e662c1617a1871fb3",
      "patch": "@@ -92,7 +92,7 @@ public static void setUp() throws Exception {\n         scmServerImpl, serverAddress, 10);\n     testDir = PathUtils.getTestDir(TestEndPoint.class);\n     defaultReportState = StorageContainerDatanodeProtocolProtos.ReportState.\n-        newBuilder().setState(noContainerReports).build();\n+        newBuilder().setState(noContainerReports).setCount(0).build();\n   }\n \n   @Test",
      "parent_sha": "9c57a61f68ab59e74210a77b2ba18ee028533058"
    }
  },
  {
    "oid": "81f635f47f0737eb551bef1aa55afdf7b268253d",
    "message": "HADOOP-15817. Reuse Object Mapper in KMSJSONReader. Contributed by Jonathan Eagles.",
    "date": "2018-10-04T02:30:30Z",
    "url": "https://github.com/apache/hadoop/commit/81f635f47f0737eb551bef1aa55afdf7b268253d",
    "details": {
      "sha": "af781f5277850985a740826d18127c01b0cd6ca9",
      "filename": "hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSJSONReader.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/hadoop/blob/81f635f47f0737eb551bef1aa55afdf7b268253d/hadoop-common-project%2Fhadoop-kms%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2Fserver%2FKMSJSONReader.java",
      "raw_url": "https://github.com/apache/hadoop/raw/81f635f47f0737eb551bef1aa55afdf7b268253d/hadoop-common-project%2Fhadoop-kms%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2Fserver%2FKMSJSONReader.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-kms%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2Fserver%2FKMSJSONReader.java?ref=81f635f47f0737eb551bef1aa55afdf7b268253d",
      "patch": "@@ -38,6 +38,7 @@\n @Consumes(MediaType.APPLICATION_JSON)\n @InterfaceAudience.Private\n public class KMSJSONReader implements MessageBodyReader<Object> {\n+  private static final ObjectMapper MAPPER = new ObjectMapper();\n \n   @Override\n   public boolean isReadable(Class<?> type, Type genericType,\n@@ -51,7 +52,6 @@ public Object readFrom(Class<Object> type, Type genericType,\n       Annotation[] annotations, MediaType mediaType,\n       MultivaluedMap<String, String> httpHeaders, InputStream entityStream)\n       throws IOException, WebApplicationException {\n-    ObjectMapper mapper = new ObjectMapper();\n-    return mapper.readValue(entityStream, type);\n+    return MAPPER.readValue(entityStream, type);\n   }\n }",
      "parent_sha": "39b35036ba47064149003046a7b59feb01575d1e"
    }
  },
  {
    "oid": "10fc865d3ce34288dc030a35a3eb0ec70dd72bdd",
    "message": "MAPREDUCE-7387. Fix TestJHSSecurity#testDelegationToken AssertionError due to HDFS-16563 (#4428). Contributed by fanshilun.\n\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
    "date": "2022-06-20T06:44:04Z",
    "url": "https://github.com/apache/hadoop/commit/10fc865d3ce34288dc030a35a3eb0ec70dd72bdd",
    "details": {
      "sha": "9e58d460d17835ac17e3c6783386997f74e4ff9e",
      "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/security/TestJHSSecurity.java",
      "status": "modified",
      "additions": 9,
      "deletions": 11,
      "changes": 20,
      "blob_url": "https://github.com/apache/hadoop/blob/10fc865d3ce34288dc030a35a3eb0ec70dd72bdd/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fsecurity%2FTestJHSSecurity.java",
      "raw_url": "https://github.com/apache/hadoop/raw/10fc865d3ce34288dc030a35a3eb0ec70dd72bdd/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fsecurity%2FTestJHSSecurity.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fsecurity%2FTestJHSSecurity.java?ref=10fc865d3ce34288dc030a35a3eb0ec70dd72bdd",
      "patch": "@@ -18,14 +18,15 @@\n \n package org.apache.hadoop.mapreduce.security;\n \n-import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n import java.io.IOException;\n import java.net.InetSocketAddress;\n import java.security.PrivilegedAction;\n import java.security.PrivilegedExceptionAction;\n \n+import org.apache.hadoop.security.token.SecretManager;\n+import org.apache.hadoop.test.LambdaTestUtils;\n import org.junit.Assert;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -61,7 +62,7 @@ public class TestJHSSecurity {\n       LoggerFactory.getLogger(TestJHSSecurity.class);\n   \n   @Test\n-  public void testDelegationToken() throws IOException, InterruptedException {\n+  public void testDelegationToken() throws Exception {\n \n     org.apache.log4j.Logger rootLogger = LogManager.getRootLogger();\n     rootLogger.setLevel(Level.DEBUG);\n@@ -80,7 +81,7 @@ public void testDelegationToken() throws IOException, InterruptedException {\n     final long renewInterval = 10000l;\n \n     JobHistoryServer jobHistoryServer = null;\n-    MRClientProtocol clientUsingDT = null;\n+    MRClientProtocol clientUsingDT;\n     long tokenFetchTime;\n     try {\n       jobHistoryServer = new JobHistoryServer() {\n@@ -155,14 +156,11 @@ protected JHSDelegationTokenSecretManager createJHSSecretManager(\n       }\n       Thread.sleep(50l);\n       LOG.info(\"At time: \" + System.currentTimeMillis() + \", token should be invalid\");\n-      // Token should have expired.      \n-      try {\n-        clientUsingDT.getJobReport(jobReportRequest);\n-        fail(\"Should not have succeeded with an expired token\");\n-      } catch (IOException e) {\n-        assertTrue(e.getCause().getMessage().contains(\"is expired\"));\n-      }\n-      \n+      // Token should have expired.\n+      final MRClientProtocol finalClientUsingDT = clientUsingDT;\n+      LambdaTestUtils.intercept(SecretManager.InvalidToken.class, \"has expired\",\n+          () -> finalClientUsingDT.getJobReport(jobReportRequest));\n+\n       // Test cancellation\n       // Stop the existing proxy, start another.\n       if (clientUsingDT != null) {",
      "parent_sha": "477b67a335897bc8c7f9e6935c64e66da386797b"
    }
  },
  {
    "oid": "3a20debddeac69596ceb5b36f8413529ea8570e6",
    "message": "MAPREDUCE-6958. Shuffle audit logger should log size of shuffle transfer. Contributed by Jason Lowe",
    "date": "2017-09-19T14:13:17Z",
    "url": "https://github.com/apache/hadoop/commit/3a20debddeac69596ceb5b36f8413529ea8570e6",
    "details": {
      "sha": "b7f2c6dac8e7ddcef73e7a1065ec8addf7ec102b",
      "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java",
      "status": "modified",
      "additions": 11,
      "deletions": 7,
      "changes": 18,
      "blob_url": "https://github.com/apache/hadoop/blob/3a20debddeac69596ceb5b36f8413529ea8570e6/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-shuffle%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FShuffleHandler.java",
      "raw_url": "https://github.com/apache/hadoop/raw/3a20debddeac69596ceb5b36f8413529ea8570e6/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-shuffle%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FShuffleHandler.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-shuffle%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FShuffleHandler.java?ref=3a20debddeac69596ceb5b36f8413529ea8570e6",
      "patch": "@@ -992,13 +992,6 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt)\n         return;\n       }\n \n-      // this audit log is disabled by default,\n-      // to turn it on please enable this audit log\n-      // on log4j.properties by uncommenting the setting\n-      if (AUDITLOG.isDebugEnabled()) {\n-        AUDITLOG.debug(\"shuffle for \" + jobQ.get(0) + \" mappers: \" + mapIds +\n-                         \" reducer \" + reduceQ.get(0));\n-      }\n       int reduceId;\n       String jobId;\n       try {\n@@ -1183,6 +1176,17 @@ protected void populateHeaders(List<String> mapIds, String jobId,\n \n       // Now set the response headers.\n       setResponseHeaders(response, keepAliveParam, contentLength);\n+\n+      // this audit log is disabled by default,\n+      // to turn it on please enable this audit log\n+      // on log4j.properties by uncommenting the setting\n+      if (AUDITLOG.isDebugEnabled()) {\n+        StringBuilder sb = new StringBuilder(\"shuffle for \");\n+        sb.append(jobId).append(\" reducer \").append(reduce);\n+        sb.append(\" length \").append(contentLength);\n+        sb.append(\" mappers: \").append(mapIds);\n+        AUDITLOG.debug(sb.toString());\n+      }\n     }\n \n     protected void setResponseHeaders(HttpResponse response,",
      "parent_sha": "ea845ba58c585647c4be8d30d9b814f098e34a12"
    }
  },
  {
    "oid": "bdce75d737bc7d207c777bb0a9e5fc4c9a78cc0a",
    "message": "HDFS-15371. Nonstandard characters exist in NameNode.java (#2032)\n\n\nContributed by zhaoyim",
    "date": "2020-07-14T13:42:12Z",
    "url": "https://github.com/apache/hadoop/commit/bdce75d737bc7d207c777bb0a9e5fc4c9a78cc0a",
    "details": {
      "sha": "9bffaaabebeafd0f77dfa2e877348cd77564be27",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/bdce75d737bc7d207c777bb0a9e5fc4c9a78cc0a/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FNameNode.java",
      "raw_url": "https://github.com/apache/hadoop/raw/bdce75d737bc7d207c777bb0a9e5fc4c9a78cc0a/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FNameNode.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FNameNode.java?ref=bdce75d737bc7d207c777bb0a9e5fc4c9a78cc0a",
      "patch": "@@ -301,7 +301,7 @@ public enum OperationCategory {\n     DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY,\n     DFS_NAMENODE_KERBEROS_INTERNAL_SPNEGO_PRINCIPAL_KEY,\n     DFS_HA_FENCE_METHODS_KEY,\n-    DFS_HA_ZKFC_PORT_KEY,\n+    DFS_HA_ZKFC_PORT_KEY\n   };\n   \n   /**",
      "parent_sha": "380e0f4506a818d6337271ae6d996927f70b601b"
    }
  },
  {
    "oid": "655c3df0508ca4f3a89b57eddc61454bf2d92ccf",
    "message": "HDFS-17150. EC: Fix the bug of failed lease recovery. (#5937). Contributed by Shuyan Zhang.\n\nReviewed-by: hfutatzhanghb <1036798979@qq.com>\r\nReviewed-by: Haiyang Hu <haiyang.hu@shopee.com>\r\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
    "date": "2023-08-15T11:54:15Z",
    "url": "https://github.com/apache/hadoop/commit/655c3df0508ca4f3a89b57eddc61454bf2d92ccf",
    "details": {
      "sha": "b41120aebfec2c9540a344ca3267f18ca807ae6d",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "status": "modified",
      "additions": 19,
      "deletions": 3,
      "changes": 22,
      "blob_url": "https://github.com/apache/hadoop/blob/655c3df0508ca4f3a89b57eddc61454bf2d92ccf/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FFSNamesystem.java",
      "raw_url": "https://github.com/apache/hadoop/raw/655c3df0508ca4f3a89b57eddc61454bf2d92ccf/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FFSNamesystem.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FFSNamesystem.java?ref=655c3df0508ca4f3a89b57eddc61454bf2d92ccf",
      "patch": "@@ -111,6 +111,7 @@\n \n import org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo;\n \n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped;\n import org.apache.hadoop.thirdparty.com.google.common.collect.Maps;\n import org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDeletionGc;\n import org.apache.hadoop.thirdparty.protobuf.ByteString;\n@@ -3802,16 +3803,31 @@ boolean internalReleaseLease(Lease lease, String src, INodesInPath iip,\n             lastBlock.getBlockType());\n       }\n \n-      if (uc.getNumExpectedLocations() == 0 && lastBlock.getNumBytes() == 0) {\n+      int minLocationsNum = 1;\n+      if (lastBlock.isStriped()) {\n+        minLocationsNum = ((BlockInfoStriped) lastBlock).getRealDataBlockNum();\n+      }\n+      if (uc.getNumExpectedLocations() < minLocationsNum &&\n+          lastBlock.getNumBytes() == 0) {\n         // There is no datanode reported to this block.\n         // may be client have crashed before writing data to pipeline.\n         // This blocks doesn't need any recovery.\n         // We can remove this block and close the file.\n         pendingFile.removeLastBlock(lastBlock);\n         finalizeINodeFileUnderConstruction(src, pendingFile,\n             iip.getLatestSnapshotId(), false);\n-        NameNode.stateChangeLog.warn(\"BLOCK* internalReleaseLease: \"\n-            + \"Removed empty last block and closed file \" + src);\n+        if (uc.getNumExpectedLocations() == 0) {\n+          // If uc.getNumExpectedLocations() is 0, regardless of whether it\n+          // is a striped block or not, we should consider it as an empty block.\n+          NameNode.stateChangeLog.warn(\"BLOCK* internalReleaseLease: \"\n+              + \"Removed empty last block and closed file \" + src);\n+        } else {\n+          // If uc.getNumExpectedLocations() is greater than 0, it means that\n+          // minLocationsNum must be greater than 1, so this must be a striped\n+          // block.\n+          NameNode.stateChangeLog.warn(\"BLOCK* internalReleaseLease: \"\n+              + \"Removed last unrecoverable block group and closed file \" + src);\n+        }\n         return true;\n       }\n       // Start recovery of the last block for this file",
      "parent_sha": "8d95c588d2df0048b0d3eb711d74bf34bf4ae3c4"
    }
  },
  {
    "oid": "5b85af87f0edeaded5f91c1b3317ddbf67d11d36",
    "message": "YARN-11278. Fixed Ambiguous error message in mutation API. Contributed by Ashutosh Gupta.",
    "date": "2022-09-09T12:38:41Z",
    "url": "https://github.com/apache/hadoop/commit/5b85af87f0edeaded5f91c1b3317ddbf67d11d36",
    "details": {
      "sha": "6a6a2e1de1e817405e98d397a1bedf51dceb0ddb",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
      "status": "modified",
      "additions": 10,
      "deletions": 10,
      "changes": 20,
      "blob_url": "https://github.com/apache/hadoop/blob/5b85af87f0edeaded5f91c1b3317ddbf67d11d36/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fwebapp%2FRMWebServices.java",
      "raw_url": "https://github.com/apache/hadoop/raw/5b85af87f0edeaded5f91c1b3317ddbf67d11d36/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fwebapp%2FRMWebServices.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fwebapp%2FRMWebServices.java?ref=5b85af87f0edeaded5f91c1b3317ddbf67d11d36",
      "patch": "@@ -2748,26 +2748,26 @@ public synchronized Response updateSchedulerConfiguration(SchedConfUpdateInfo\n     initForWritableEndpoints(callerUGI, true);\n \n     ResourceScheduler scheduler = rm.getResourceScheduler();\n-    if (isConfigurationMutable(scheduler)) {\n+    if (!(scheduler instanceof MutableConfScheduler)) {\n+      return Response.status(Status.BAD_REQUEST)\n+          .entity(\"Configuration change only supported by MutableConfScheduler.\").build();\n+    } else if (!((MutableConfScheduler) scheduler).isConfigurationMutable()) {\n+      return Response.status(Status.BAD_REQUEST)\n+          .entity(\"Configuration change only supported by mutable configuration store.\").build();\n+    } else {\n       try {\n         callerUGI.doAs((PrivilegedExceptionAction<Void>) () -> {\n-          MutableConfigurationProvider provider = ((MutableConfScheduler)\n-              scheduler).getMutableConfProvider();\n+          MutableConfigurationProvider provider =\n+              ((MutableConfScheduler) scheduler).getMutableConfProvider();\n           LogMutation logMutation = applyMutation(provider, callerUGI, mutationInfo);\n           return refreshQueues(provider, logMutation);\n         });\n       } catch (IOException e) {\n         LOG.error(\"Exception thrown when modifying configuration.\", e);\n-        return Response.status(Status.BAD_REQUEST).entity(e.getMessage())\n-            .build();\n+        return Response.status(Status.BAD_REQUEST).entity(e.getMessage()).build();\n       }\n       return Response.status(Status.OK).entity(\"Configuration change successfully applied.\")\n           .build();\n-    } else {\n-      return Response.status(Status.BAD_REQUEST)\n-          .entity(String.format(\"Configuration change only supported by \" +\n-              \"%s.\", MutableConfScheduler.class.getSimpleName()))\n-          .build();\n     }\n   }\n ",
      "parent_sha": "56387cce575960f76a78dd7c4b164fca1c0a3b69"
    }
  },
  {
    "oid": "ce93595d7a18bd0561ae2c1983b51f6aba887179",
    "message": "MAPREDUCE-6729. Accurately compute the test execute time in DFSIO. Contributed by mingleizhang.\n\nThis closes #112",
    "date": "2016-07-30T13:13:19Z",
    "url": "https://github.com/apache/hadoop/commit/ce93595d7a18bd0561ae2c1983b51f6aba887179",
    "details": {
      "sha": "e7aa66b62f0f960b2c13d360a21daec79f677ba8",
      "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/fs/TestDFSIO.java",
      "status": "modified",
      "additions": 27,
      "deletions": 27,
      "changes": 54,
      "blob_url": "https://github.com/apache/hadoop/blob/ce93595d7a18bd0561ae2c1983b51f6aba887179/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FTestDFSIO.java",
      "raw_url": "https://github.com/apache/hadoop/raw/ce93595d7a18bd0561ae2c1983b51f6aba887179/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FTestDFSIO.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FTestDFSIO.java?ref=ce93595d7a18bd0561ae2c1983b51f6aba887179",
      "patch": "@@ -228,67 +228,53 @@ public static void afterClass() throws Exception {\n \n   public static void testWrite() throws Exception {\n     FileSystem fs = cluster.getFileSystem();\n-    long tStart = System.currentTimeMillis();\n-    bench.writeTest(fs);\n-    long execTime = System.currentTimeMillis() - tStart;\n+    long execTime = bench.writeTest(fs);\n     bench.analyzeResult(fs, TestType.TEST_TYPE_WRITE, execTime);\n   }\n \n   @Test (timeout = 10000)\n   public void testRead() throws Exception {\n     FileSystem fs = cluster.getFileSystem();\n-    long tStart = System.currentTimeMillis();\n-    bench.readTest(fs);\n-    long execTime = System.currentTimeMillis() - tStart;\n+    long execTime = bench.readTest(fs);\n     bench.analyzeResult(fs, TestType.TEST_TYPE_READ, execTime);\n   }\n \n   @Test (timeout = 10000)\n   public void testReadRandom() throws Exception {\n     FileSystem fs = cluster.getFileSystem();\n-    long tStart = System.currentTimeMillis();\n     bench.getConf().setLong(\"test.io.skip.size\", 0);\n-    bench.randomReadTest(fs);\n-    long execTime = System.currentTimeMillis() - tStart;\n+    long execTime = bench.randomReadTest(fs);\n     bench.analyzeResult(fs, TestType.TEST_TYPE_READ_RANDOM, execTime);\n   }\n \n   @Test (timeout = 10000)\n   public void testReadBackward() throws Exception {\n     FileSystem fs = cluster.getFileSystem();\n-    long tStart = System.currentTimeMillis();\n     bench.getConf().setLong(\"test.io.skip.size\", -DEFAULT_BUFFER_SIZE);\n-    bench.randomReadTest(fs);\n-    long execTime = System.currentTimeMillis() - tStart;\n+    long execTime = bench.randomReadTest(fs);\n     bench.analyzeResult(fs, TestType.TEST_TYPE_READ_BACKWARD, execTime);\n   }\n \n   @Test (timeout = 10000)\n   public void testReadSkip() throws Exception {\n     FileSystem fs = cluster.getFileSystem();\n-    long tStart = System.currentTimeMillis();\n     bench.getConf().setLong(\"test.io.skip.size\", 1);\n-    bench.randomReadTest(fs);\n-    long execTime = System.currentTimeMillis() - tStart;\n+    long execTime = bench.randomReadTest(fs);\n     bench.analyzeResult(fs, TestType.TEST_TYPE_READ_SKIP, execTime);\n   }\n \n   @Test (timeout = 10000)\n   public void testAppend() throws Exception {\n     FileSystem fs = cluster.getFileSystem();\n-    long tStart = System.currentTimeMillis();\n-    bench.appendTest(fs);\n-    long execTime = System.currentTimeMillis() - tStart;\n+    long execTime = bench.appendTest(fs);\n     bench.analyzeResult(fs, TestType.TEST_TYPE_APPEND, execTime);\n   }\n \n   @Test (timeout = 60000)\n   public void testTruncate() throws Exception {\n     FileSystem fs = cluster.getFileSystem();\n     bench.createControlFile(fs, DEFAULT_NR_BYTES / 2, DEFAULT_NR_FILES);\n-    long tStart = System.currentTimeMillis();\n-    bench.truncateTest(fs);\n-    long execTime = System.currentTimeMillis() - tStart;\n+    long execTime = bench.truncateTest(fs);\n     bench.analyzeResult(fs, TestType.TEST_TYPE_TRUNCATE, execTime);\n   }\n \n@@ -430,12 +416,14 @@ public Long doIO(Reporter reporter,\n     }\n   }\n \n-  private void writeTest(FileSystem fs) throws IOException {\n+  private long writeTest(FileSystem fs) throws IOException {\n     Path writeDir = getWriteDir(config);\n     fs.delete(getDataDir(config), true);\n     fs.delete(writeDir, true);\n-    \n+    long tStart = System.currentTimeMillis();\n     runIOTest(WriteMapper.class, writeDir);\n+    long execTime = System.currentTimeMillis() - tStart;\n+    return execTime;\n   }\n   \n   private void runIOTest(\n@@ -496,10 +484,13 @@ public Long doIO(Reporter reporter,\n     }\n   }\n \n-  private void appendTest(FileSystem fs) throws IOException {\n+  private long appendTest(FileSystem fs) throws IOException {\n     Path appendDir = getAppendDir(config);\n     fs.delete(appendDir, true);\n+    long tStart = System.currentTimeMillis();\n     runIOTest(AppendMapper.class, appendDir);\n+    long execTime = System.currentTimeMillis() - tStart;\n+    return execTime;\n   }\n \n   /**\n@@ -539,10 +530,13 @@ public Long doIO(Reporter reporter,\n     }\n   }\n \n-  private void readTest(FileSystem fs) throws IOException {\n+  private long readTest(FileSystem fs) throws IOException {\n     Path readDir = getReadDir(config);\n     fs.delete(readDir, true);\n+    long tStart = System.currentTimeMillis();\n     runIOTest(ReadMapper.class, readDir);\n+    long execTime = System.currentTimeMillis() - tStart;\n+    return execTime;\n   }\n \n   /**\n@@ -620,10 +614,13 @@ private long nextOffset(long current) {\n     }\n   }\n \n-  private void randomReadTest(FileSystem fs) throws IOException {\n+  private long randomReadTest(FileSystem fs) throws IOException {\n     Path readDir = getRandomReadDir(config);\n     fs.delete(readDir, true);\n+    long tStart = System.currentTimeMillis();\n     runIOTest(RandomReadMapper.class, readDir);\n+    long execTime = System.currentTimeMillis() - tStart;\n+    return execTime;\n   }\n \n   /**\n@@ -665,10 +662,13 @@ public Long doIO(Reporter reporter,\n     }\n   }\n \n-  private void truncateTest(FileSystem fs) throws IOException {\n+  private long truncateTest(FileSystem fs) throws IOException {\n     Path TruncateDir = getTruncateDir(config);\n     fs.delete(TruncateDir, true);\n+    long tStart = System.currentTimeMillis();\n     runIOTest(TruncateMapper.class, TruncateDir);\n+    long execTime = System.currentTimeMillis() - tStart;\n+    return execTime;\n   }\n \n   private void sequentialTest(FileSystem fs, ",
      "parent_sha": "8d32bd884d53948728b77de14531b3be196f4bc7"
    }
  },
  {
    "oid": "47c41e7ac7e6b905a58550f8899f629c1cf8b138",
    "message": "YARN-5048. DelegationTokenRenewer#skipTokenRenewal may throw NPE (Jian He via Yongjun Zhang)",
    "date": "2016-05-07T04:50:09Z",
    "url": "https://github.com/apache/hadoop/commit/47c41e7ac7e6b905a58550f8899f629c1cf8b138",
    "details": {
      "sha": "fd12f11c78999a7ba9a95e9e164562c13138a457",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
      "status": "modified",
      "additions": 7,
      "deletions": 2,
      "changes": 9,
      "blob_url": "https://github.com/apache/hadoop/blob/47c41e7ac7e6b905a58550f8899f629c1cf8b138/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fsecurity%2FDelegationTokenRenewer.java",
      "raw_url": "https://github.com/apache/hadoop/raw/47c41e7ac7e6b905a58550f8899f629c1cf8b138/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fsecurity%2FDelegationTokenRenewer.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fsecurity%2FDelegationTokenRenewer.java?ref=47c41e7ac7e6b905a58550f8899f629c1cf8b138",
      "patch": "@@ -539,9 +539,14 @@ public boolean cancel() {\n    */\n   private boolean skipTokenRenewal(Token<?> token)\n       throws IOException {\n+\n     @SuppressWarnings(\"unchecked\")\n-    Text renewer = ((Token<AbstractDelegationTokenIdentifier>)token).\n-        decodeIdentifier().getRenewer();\n+    AbstractDelegationTokenIdentifier identifier =\n+        ((Token<AbstractDelegationTokenIdentifier>) token).decodeIdentifier();\n+    if (identifier == null) {\n+      return false;\n+    }\n+    Text renewer = identifier.getRenewer();\n     return (renewer != null && renewer.toString().equals(\"\"));\n   }\n ",
      "parent_sha": "6957e4569996734b1b176e04df5a03d000bed5b7"
    }
  },
  {
    "oid": "5e12dc51f0681cc8c94e06692fad36faf604a9ec",
    "message": "YARN-10419. Javadoc error in hadoop-yarn-server-common module. (#2271)",
    "date": "2020-09-03T07:17:52Z",
    "url": "https://github.com/apache/hadoop/commit/5e12dc51f0681cc8c94e06692fad36faf604a9ec",
    "details": {
      "sha": "d4c4f2387c2cf6d5c73f1c434af593415668aae4",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/webapp/dao/RemoteLogPathEntry.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/5e12dc51f0681cc8c94e06692fad36faf604a9ec/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fwebapp%2Fdao%2FRemoteLogPathEntry.java",
      "raw_url": "https://github.com/apache/hadoop/raw/5e12dc51f0681cc8c94e06692fad36faf604a9ec/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fwebapp%2Fdao%2FRemoteLogPathEntry.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fwebapp%2Fdao%2FRemoteLogPathEntry.java?ref=5e12dc51f0681cc8c94e06692fad36faf604a9ec",
      "patch": "@@ -20,7 +20,7 @@\n /**\n  * A remote log path for a log aggregation file controller.\n  * <pre>\n- *   <ROOT_PATH>/%USER/<SUFFIX>\n+ *   {@code <ROOT_PATH>/%USER/<SUFFIX>}\n  * </pre>\n  */\n public class RemoteLogPathEntry {",
      "parent_sha": "0207f5cf4610664b9fbd7d2b5fbe528f16ff8e44"
    }
  },
  {
    "oid": "a2d083f2c546ef9e0a543ea287c2435c6440d9aa",
    "message": "HDDS-1413. Attempt to fix TestCloseContainerCommandHandler by adjusting timeouts\n\nSigned-off-by: Anu Engineer <aengineer@apache.org>",
    "date": "2019-08-31T01:17:08Z",
    "url": "https://github.com/apache/hadoop/commit/a2d083f2c546ef9e0a543ea287c2435c6440d9aa",
    "details": {
      "sha": "84a1e5d89861e72f12a6ef2fec482a58a0d4144a",
      "filename": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/TestCloseContainerCommandHandler.java",
      "status": "modified",
      "additions": 143,
      "deletions": 267,
      "changes": 410,
      "blob_url": "https://github.com/apache/hadoop/blob/a2d083f2c546ef9e0a543ea287c2435c6440d9aa/hadoop-hdds%2Fcontainer-service%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Fstatemachine%2Fcommandhandler%2FTestCloseContainerCommandHandler.java",
      "raw_url": "https://github.com/apache/hadoop/raw/a2d083f2c546ef9e0a543ea287c2435c6440d9aa/hadoop-hdds%2Fcontainer-service%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Fstatemachine%2Fcommandhandler%2FTestCloseContainerCommandHandler.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds%2Fcontainer-service%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Fstatemachine%2Fcommandhandler%2FTestCloseContainerCommandHandler.java?ref=a2d083f2c546ef9e0a543ea287c2435c6440d9aa",
      "patch": "@@ -16,306 +16,187 @@\n  */\n package org.apache.hadoop.ozone.container.common.statemachine.commandhandler;\n \n-import org.apache.commons.io.FileUtils;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hdds.HddsConfigKeys;\n import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n-import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n-import org.apache.hadoop.hdds.scm.container.ContainerID;\n import org.apache.hadoop.hdds.scm.pipeline.PipelineID;\n-import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n import org.apache.hadoop.ozone.container.common.interfaces.Container;\n+import org.apache.hadoop.ozone.container.common.interfaces.Handler;\n import org.apache.hadoop.ozone.container.common.statemachine\n     .DatanodeStateMachine;\n import org.apache.hadoop.ozone.container.common.statemachine.StateContext;\n+import org.apache.hadoop.ozone.container.common.transport.server.XceiverServerSpi;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.ozoneimpl.ContainerController;\n import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n import org.apache.hadoop.ozone.protocol.commands.CloseContainerCommand;\n-import org.apache.hadoop.test.GenericTestUtils;\n-import org.apache.hadoop.hdds.ratis.RatisHelper;\n-import org.apache.ratis.client.RaftClient;\n-import org.apache.ratis.protocol.RaftGroup;\n-import org.apache.ratis.protocol.RaftGroupId;\n-import org.apache.ratis.protocol.RaftPeer;\n-import org.apache.ratis.retry.RetryPolicy;\n-import org.apache.ratis.rpc.SupportedRpcType;\n-import org.apache.ratis.util.TimeDuration;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n+import org.junit.Before;\n import org.junit.Test;\n-import org.mockito.Mockito;\n \n-import java.io.File;\n import java.io.IOException;\n-import java.util.Collections;\n-import java.util.Random;\n import java.util.UUID;\n-import java.util.concurrent.TimeUnit;\n+\n+import static java.util.Collections.singletonMap;\n+import static org.apache.hadoop.ozone.OzoneConsts.GB;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.eq;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n \n /**\n  * Test cases to verify CloseContainerCommandHandler in datanode.\n  */\n public class TestCloseContainerCommandHandler {\n \n-  private final StateContext context = Mockito.mock(StateContext.class);\n-  private final Random random = new Random();\n-  private static File testDir;\n+  private static final long CONTAINER_ID = 123L;\n+\n+  private OzoneContainer ozoneContainer;\n+  private StateContext context;\n+  private XceiverServerSpi writeChannel;\n+  private Container container;\n+  private Handler containerHandler;\n+  private PipelineID pipelineID;\n+  private PipelineID nonExistentPipelineID = PipelineID.randomId();\n+\n+  private CloseContainerCommandHandler subject =\n+      new CloseContainerCommandHandler();\n+\n+  @Before\n+  public void before() throws Exception {\n+    context = mock(StateContext.class);\n+    DatanodeStateMachine dnStateMachine = mock(DatanodeStateMachine.class);\n+    when(dnStateMachine.getDatanodeDetails())\n+        .thenReturn(randomDatanodeDetails());\n+    when(context.getParent()).thenReturn(dnStateMachine);\n+\n+    pipelineID = PipelineID.randomId();\n+\n+    KeyValueContainerData data = new KeyValueContainerData(CONTAINER_ID, GB,\n+        pipelineID.getId().toString(), null);\n+\n+    container = new KeyValueContainer(data, new OzoneConfiguration());\n+    ContainerSet containerSet = new ContainerSet();\n+    containerSet.addContainer(container);\n+\n+    containerHandler = mock(Handler.class);\n+    ContainerController controller = new ContainerController(containerSet,\n+        singletonMap(ContainerProtos.ContainerType.KeyValueContainer,\n+            containerHandler));\n+\n+    writeChannel = mock(XceiverServerSpi.class);\n+    ozoneContainer = mock(OzoneContainer.class);\n+    when(ozoneContainer.getController()).thenReturn(controller);\n+    when(ozoneContainer.getContainerSet()).thenReturn(containerSet);\n+    when(ozoneContainer.getWriteChannel()).thenReturn(writeChannel);\n+    when(writeChannel.isExist(pipelineID.getProtobuf())).thenReturn(true);\n+    when(writeChannel.isExist(nonExistentPipelineID.getProtobuf()))\n+        .thenReturn(false);\n+  }\n \n   @Test\n-  public void testCloseContainerViaRatis()\n-      throws Exception {\n-    final OzoneConfiguration conf = new OzoneConfiguration();\n-    final DatanodeDetails datanodeDetails = randomDatanodeDetails();\n-    final OzoneContainer ozoneContainer =\n-        getOzoneContainer(conf, datanodeDetails);\n-    ozoneContainer.start(UUID.randomUUID().toString());\n-    try {\n-      final Container container =\n-          createContainer(conf, datanodeDetails, ozoneContainer);\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(1)).triggerHeartbeat();\n-      final long containerId = container.getContainerData().getContainerID();\n-      final PipelineID pipelineId = PipelineID.valueOf(UUID.fromString(\n-          container.getContainerData().getOriginPipelineId()));\n-\n-      // We have created a container via ratis.\n-      // Now close the container on ratis.\n-      final CloseContainerCommandHandler closeHandler =\n-          new CloseContainerCommandHandler();\n-      final CloseContainerCommand command = new CloseContainerCommand(\n-          containerId, pipelineId);\n-\n-      closeHandler.handle(command, ozoneContainer, context, null);\n-\n-      Assert.assertEquals(ContainerProtos.ContainerDataProto.State.CLOSED,\n-          ozoneContainer.getContainerSet().getContainer(containerId)\n-              .getContainerState());\n-\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(3)).triggerHeartbeat();\n-    } finally {\n-      ozoneContainer.stop();\n-    }\n+  public void closeContainerWithPipeline() throws Exception {\n+    // close a container that's associated with an existing pipeline\n+    subject.handle(closeWithKnownPipeline(), ozoneContainer, context, null);\n+\n+    verify(containerHandler)\n+        .markContainerForClose(container);\n+    verify(writeChannel)\n+        .submitRequest(any(), eq(pipelineID.getProtobuf()));\n+    verify(containerHandler, never())\n+        .quasiCloseContainer(container);\n   }\n \n   @Test\n-  public void testCloseContainerViaStandalone()\n-      throws Exception {\n-    final OzoneConfiguration conf = new OzoneConfiguration();\n-    final DatanodeDetails datanodeDetails = randomDatanodeDetails();\n-    final OzoneContainer ozoneContainer =\n-        getOzoneContainer(conf, datanodeDetails);\n-    ozoneContainer.start(UUID.randomUUID().toString());\n-    try {\n-      final Container container =\n-          createContainer(conf, datanodeDetails, ozoneContainer);\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(1)).triggerHeartbeat();\n-      final long containerId = container.getContainerData().getContainerID();\n-      // To quasi close specify a pipeline which doesn't exist in the datanode.\n-      final PipelineID pipelineId = PipelineID.randomId();\n-\n-      // We have created a container via ratis. Now quasi close it.\n-      final CloseContainerCommandHandler closeHandler =\n-          new CloseContainerCommandHandler();\n-      final CloseContainerCommand command = new CloseContainerCommand(\n-          containerId, pipelineId);\n-\n-      closeHandler.handle(command, ozoneContainer, context, null);\n-\n-      Assert.assertEquals(ContainerProtos.ContainerDataProto.State.QUASI_CLOSED,\n-          ozoneContainer.getContainerSet().getContainer(containerId)\n-              .getContainerState());\n-\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(3)).triggerHeartbeat();\n-    } finally {\n-      ozoneContainer.stop();\n-    }\n+  public void closeContainerWithoutPipeline() throws IOException {\n+    // close a container that's NOT associated with an open pipeline\n+    subject.handle(closeWithUnknownPipeline(), ozoneContainer, context, null);\n+\n+    verify(containerHandler)\n+        .markContainerForClose(container);\n+    verify(writeChannel, never())\n+        .submitRequest(any(), any());\n+    verify(containerHandler)\n+        .quasiCloseContainer(container);\n+  }\n+\n+  @Test\n+  public void forceCloseQuasiClosedContainer() throws Exception {\n+    // force-close a container that's already quasi closed\n+    container.getContainerData()\n+        .setState(ContainerProtos.ContainerDataProto.State.QUASI_CLOSED);\n+\n+    subject.handle(forceCloseWithoutPipeline(), ozoneContainer, context, null);\n+\n+    verify(writeChannel, never())\n+        .submitRequest(any(), any());\n+    verify(containerHandler)\n+        .closeContainer(container);\n   }\n \n   @Test\n-  public void testQuasiCloseToClose() throws Exception {\n-    final OzoneConfiguration conf = new OzoneConfiguration();\n-    final DatanodeDetails datanodeDetails = randomDatanodeDetails();\n-    final OzoneContainer ozoneContainer =\n-        getOzoneContainer(conf, datanodeDetails);\n-    ozoneContainer.start(UUID.randomUUID().toString());\n-    try {\n-      final Container container =\n-          createContainer(conf, datanodeDetails, ozoneContainer);\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(1)).triggerHeartbeat();\n-      final long containerId = container.getContainerData().getContainerID();\n-      // A pipeline which doesn't exist in the datanode.\n-      final PipelineID pipelineId = PipelineID.randomId();\n-\n-      // We have created a container via ratis. Now quasi close it.\n-      final CloseContainerCommandHandler closeHandler =\n-          new CloseContainerCommandHandler();\n-      final CloseContainerCommand command = new CloseContainerCommand(\n-          containerId, pipelineId);\n-\n-      closeHandler.handle(command, ozoneContainer, context, null);\n-\n-      Assert.assertEquals(ContainerProtos.ContainerDataProto.State.QUASI_CLOSED,\n-          ozoneContainer.getContainerSet().getContainer(containerId)\n-              .getContainerState());\n-\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(3)).triggerHeartbeat();\n-\n-      // The container is quasi closed. Force close the container now.\n-      final CloseContainerCommand closeCommand = new CloseContainerCommand(\n-          containerId, pipelineId, true);\n-\n-      closeHandler.handle(closeCommand, ozoneContainer, context, null);\n-\n-      Assert.assertEquals(ContainerProtos.ContainerDataProto.State.CLOSED,\n-          ozoneContainer.getContainerSet().getContainer(containerId)\n-              .getContainerState());\n-\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(4)).triggerHeartbeat();\n-    } finally {\n-      ozoneContainer.stop();\n-    }\n+  public void forceCloseOpenContainer() throws Exception {\n+    // force-close a container that's NOT associated with an open pipeline\n+    subject.handle(forceCloseWithoutPipeline(), ozoneContainer, context, null);\n+\n+    verify(writeChannel, never())\n+        .submitRequest(any(), any());\n+    verify(containerHandler)\n+        .closeContainer(container);\n   }\n \n   @Test\n-  public void testForceCloseOpenContainer() throws Exception {\n-    final OzoneConfiguration conf = new OzoneConfiguration();\n-    final DatanodeDetails datanodeDetails = randomDatanodeDetails();\n-    final OzoneContainer ozoneContainer =\n-        getOzoneContainer(conf, datanodeDetails);\n-    ozoneContainer.start(UUID.randomUUID().toString());\n-    try {\n-      final Container container =\n-          createContainer(conf, datanodeDetails, ozoneContainer);\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(1)).triggerHeartbeat();\n-      final long containerId = container.getContainerData().getContainerID();\n-      // A pipeline which doesn't exist in the datanode.\n-      final PipelineID pipelineId = PipelineID.randomId();\n-\n-      final CloseContainerCommandHandler closeHandler =\n-          new CloseContainerCommandHandler();\n-\n-      final CloseContainerCommand closeCommand = new CloseContainerCommand(\n-          containerId, pipelineId, true);\n-\n-      closeHandler.handle(closeCommand, ozoneContainer, context, null);\n-\n-      Assert.assertEquals(ContainerProtos.ContainerDataProto.State.CLOSED,\n-          ozoneContainer.getContainerSet().getContainer(containerId)\n-              .getContainerState());\n-\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(3)).triggerHeartbeat();\n-    } finally {\n-      ozoneContainer.stop();\n-    }\n+  public void forceCloseOpenContainerWithPipeline() throws Exception {\n+    // force-close a container that's associated with an existing pipeline\n+    subject.handle(forceCloseWithPipeline(), ozoneContainer, context, null);\n+\n+    verify(containerHandler)\n+        .markContainerForClose(container);\n+    verify(writeChannel, never())\n+        .submitRequest(any(), any());\n+    verify(containerHandler, never())\n+        .quasiCloseContainer(container);\n+    verify(containerHandler, never())\n+        .closeContainer(container);\n   }\n \n   @Test\n-  public void testQuasiCloseClosedContainer()\n-      throws Exception {\n-    final OzoneConfiguration conf = new OzoneConfiguration();\n-    final DatanodeDetails datanodeDetails = randomDatanodeDetails();\n-    final OzoneContainer ozoneContainer = getOzoneContainer(\n-        conf, datanodeDetails);\n-    ozoneContainer.start(UUID.randomUUID().toString());\n-    try {\n-      final Container container = createContainer(\n-          conf, datanodeDetails, ozoneContainer);\n-      Mockito.verify(context.getParent(),\n-          Mockito.times(1)).triggerHeartbeat();\n-      final long containerId = container.getContainerData().getContainerID();\n-      final PipelineID pipelineId = PipelineID.valueOf(UUID.fromString(\n-          container.getContainerData().getOriginPipelineId()));\n-\n-      final CloseContainerCommandHandler closeHandler =\n-          new CloseContainerCommandHandler();\n-      final CloseContainerCommand closeCommand = new CloseContainerCommand(\n-          containerId, pipelineId);\n-\n-      closeHandler.handle(closeCommand, ozoneContainer, context, null);\n-\n-      Assert.assertEquals(ContainerProtos.ContainerDataProto.State.CLOSED,\n-          ozoneContainer.getContainerSet().getContainer(containerId)\n-              .getContainerState());\n-\n-      // The container is closed, now we send close command with\n-      // pipeline id which doesn't exist.\n-      // This should cause the datanode to trigger quasi close, since the\n-      // container is already closed, this should do nothing.\n-      // The command should not fail either.\n-      final PipelineID randomPipeline = PipelineID.randomId();\n-      final CloseContainerCommand quasiCloseCommand =\n-          new CloseContainerCommand(containerId, randomPipeline);\n-      closeHandler.handle(quasiCloseCommand, ozoneContainer, context, null);\n-\n-      Assert.assertEquals(ContainerProtos.ContainerDataProto.State.CLOSED,\n-          ozoneContainer.getContainerSet().getContainer(containerId)\n-              .getContainerState());\n-    } finally {\n-      ozoneContainer.stop();\n-    }\n+  public void closeAlreadyClosedContainer() throws Exception {\n+    container.getContainerData()\n+        .setState(ContainerProtos.ContainerDataProto.State.CLOSED);\n+\n+    // Since the container is already closed, these commands should do nothing,\n+    // neither should they fail\n+    subject.handle(closeWithUnknownPipeline(), ozoneContainer, context, null);\n+    subject.handle(closeWithKnownPipeline(), ozoneContainer, context, null);\n+\n+    verify(containerHandler, never())\n+        .markContainerForClose(container);\n+    verify(containerHandler, never())\n+        .quasiCloseContainer(container);\n+    verify(containerHandler, never())\n+        .closeContainer(container);\n+    verify(writeChannel, never())\n+        .submitRequest(any(), any());\n   }\n \n-  private OzoneContainer getOzoneContainer(final OzoneConfiguration conf,\n-      final DatanodeDetails datanodeDetails) throws IOException {\n-    testDir = GenericTestUtils.getTestDir(\n-        TestCloseContainerCommandHandler.class.getName() + UUID.randomUUID());\n-    conf.set(HddsConfigKeys.OZONE_METADATA_DIRS, testDir.getPath());\n-    conf.set(ScmConfigKeys.HDDS_DATANODE_DIR_KEY, testDir.getPath());\n-    conf.setBoolean(OzoneConfigKeys.DFS_CONTAINER_RATIS_IPC_RANDOM_PORT, true);\n-    conf.setBoolean(OzoneConfigKeys.DFS_CONTAINER_IPC_RANDOM_PORT, true);\n-\n-    final DatanodeStateMachine datanodeStateMachine = Mockito.mock(\n-        DatanodeStateMachine.class);\n-    Mockito.when(datanodeStateMachine.getDatanodeDetails())\n-        .thenReturn(datanodeDetails);\n-    Mockito.when(context.getParent()).thenReturn(datanodeStateMachine);\n-    final OzoneContainer ozoneContainer = new  OzoneContainer(\n-        datanodeDetails, conf, context, null);\n-    return ozoneContainer;\n+  private CloseContainerCommand closeWithKnownPipeline() {\n+    return new CloseContainerCommand(CONTAINER_ID, pipelineID);\n   }\n \n-  private Container createContainer(final Configuration conf,\n-      final DatanodeDetails datanodeDetails,\n-      final OzoneContainer ozoneContainer) throws Exception  {\n-    final PipelineID pipelineID = PipelineID.randomId();\n-    final RaftGroupId raftGroupId = RaftGroupId.valueOf(pipelineID.getId());\n-    final RetryPolicy retryPolicy = RatisHelper.createRetryPolicy(conf);\n-    final RaftPeer peer = RatisHelper.toRaftPeer(datanodeDetails);\n-    final RaftGroup group = RatisHelper.newRaftGroup(raftGroupId,\n-        Collections.singleton(datanodeDetails));\n-    final int maxOutstandingRequests = 100;\n-    final RaftClient client = RatisHelper\n-        .newRaftClient(SupportedRpcType.GRPC, peer, retryPolicy,\n-            maxOutstandingRequests,\n-            TimeDuration.valueOf(3, TimeUnit.SECONDS));\n-    Assert.assertTrue(client.groupAdd(group, peer.getId()).isSuccess());\n-    Thread.sleep(10000);\n-    final ContainerID containerId = ContainerID.valueof(\n-        random.nextLong() & Long.MAX_VALUE);\n-    ContainerProtos.ContainerCommandRequestProto.Builder request =\n-        ContainerProtos.ContainerCommandRequestProto.newBuilder();\n-    request.setCmdType(ContainerProtos.Type.CreateContainer);\n-    request.setContainerID(containerId.getId());\n-    request.setCreateContainer(\n-        ContainerProtos.CreateContainerRequestProto.getDefaultInstance());\n-    request.setDatanodeUuid(datanodeDetails.getUuidString());\n-    ozoneContainer.getWriteChannel().submitRequest(\n-        request.build(), pipelineID.getProtobuf());\n-\n-    final Container container = ozoneContainer.getContainerSet().getContainer(\n-        containerId.getId());\n-    Assert.assertEquals(ContainerProtos.ContainerDataProto.State.OPEN,\n-        container.getContainerState());\n-    return container;\n+  private CloseContainerCommand closeWithUnknownPipeline() {\n+    return new CloseContainerCommand(CONTAINER_ID, nonExistentPipelineID);\n+  }\n+\n+  private CloseContainerCommand forceCloseWithPipeline() {\n+    return new CloseContainerCommand(CONTAINER_ID, pipelineID, true);\n+  }\n+\n+  private CloseContainerCommand forceCloseWithoutPipeline() {\n+    return new CloseContainerCommand(CONTAINER_ID, nonExistentPipelineID, true);\n   }\n \n   /**\n@@ -339,9 +220,4 @@ private static DatanodeDetails randomDatanodeDetails() {\n         .addPort(restPort);\n     return builder.build();\n   }\n-\n-  @AfterClass\n-  public static void teardown() throws IOException {\n-    FileUtils.deleteDirectory(testDir);\n-  }\n }\n\\ No newline at end of file",
      "parent_sha": "c4411f7fdf745eefac32749dad4388635a0a9aae"
    }
  },
  {
    "oid": "18f9989ff2d3910afad9a0c586e3c7770feefc66",
    "message": "HDFS-17192. Add bock info when constructing remote block reader meets IOException. (#6081). Contributed by farmmamba.\n\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
    "date": "2023-09-18T12:56:45Z",
    "url": "https://github.com/apache/hadoop/commit/18f9989ff2d3910afad9a0c586e3c7770feefc66",
    "details": {
      "sha": "d013b10ef4df60f59de80a3c801ff6cef85282a1",
      "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/18f9989ff2d3910afad9a0c586e3c7770feefc66/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fclient%2Fimpl%2FBlockReaderFactory.java",
      "raw_url": "https://github.com/apache/hadoop/raw/18f9989ff2d3910afad9a0c586e3c7770feefc66/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fclient%2Fimpl%2FBlockReaderFactory.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fclient%2Fimpl%2FBlockReaderFactory.java?ref=18f9989ff2d3910afad9a0c586e3c7770feefc66",
      "patch": "@@ -769,7 +769,7 @@ private BlockReader getRemoteBlockReaderFromTcp() throws IOException {\n           LOG.debug(\"Closed potentially stale remote peer {}\", peer, ioe);\n         } else {\n           // Handle an I/O error we got when using a newly created peer.\n-          LOG.warn(\"I/O error constructing remote block reader.\", ioe);\n+          LOG.warn(\"I/O error constructing remote block reader for block {}\", block, ioe);\n           throw ioe;\n         }\n       } finally {",
      "parent_sha": "9e489b9ab5367b431849f957d0fe1ffadc43cd47"
    }
  },
  {
    "oid": "2626ec35e997c06cda975fbfb2b95da740270059",
    "message": "MAPREDUCE-7064. Flaky test TestTaskAttempt#testReducerCustomResourceTypes. Contributed by Peter Bacsko",
    "date": "2018-03-14T20:05:24Z",
    "url": "https://github.com/apache/hadoop/commit/2626ec35e997c06cda975fbfb2b95da740270059",
    "details": {
      "sha": "609923f6a142bd49a3edaf966e4cc21e12da41e8",
      "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java",
      "status": "modified",
      "additions": 88,
      "deletions": 62,
      "changes": 150,
      "blob_url": "https://github.com/apache/hadoop/blob/2626ec35e997c06cda975fbfb2b95da740270059/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fv2%2Fapp%2Fjob%2Fimpl%2FTestTaskAttempt.java",
      "raw_url": "https://github.com/apache/hadoop/raw/2626ec35e997c06cda975fbfb2b95da740270059/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fv2%2Fapp%2Fjob%2Fimpl%2FTestTaskAttempt.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fv2%2Fapp%2Fjob%2Fimpl%2FTestTaskAttempt.java?ref=2626ec35e997c06cda975fbfb2b95da740270059",
      "patch": "@@ -187,77 +187,103 @@ public void tearDown() {\n \n   @Test\n   public void testMRAppHistoryForMap() throws Exception {\n-    MRApp app = new FailingAttemptsMRApp(1, 0);\n-    testMRAppHistory(app);\n+    MRApp app = null;\n+    try {\n+      app = new FailingAttemptsMRApp(1, 0);\n+      testMRAppHistory(app);\n+    } finally {\n+      app.close();\n+    }\n   }\n \n   @Test\n   public void testMRAppHistoryForReduce() throws Exception {\n-    MRApp app = new FailingAttemptsMRApp(0, 1);\n-    testMRAppHistory(app);\n+    MRApp app = null;\n+    try {\n+      app = new FailingAttemptsMRApp(0, 1);\n+      testMRAppHistory(app);\n+    } finally {\n+      app.close();\n+    }\n   }\n \n   @Test\n   public void testMRAppHistoryForTAFailedInAssigned() throws Exception {\n     // test TA_CONTAINER_LAUNCH_FAILED for map\n-    FailingAttemptsDuringAssignedMRApp app =\n-        new FailingAttemptsDuringAssignedMRApp(1, 0,\n-            TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED);\n-    testTaskAttemptAssignedFailHistory(app);\n-\n-    // test TA_CONTAINER_LAUNCH_FAILED for reduce\n-    app =\n-        new FailingAttemptsDuringAssignedMRApp(0, 1,\n-            TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED);\n-    testTaskAttemptAssignedFailHistory(app);\n-\n-    // test TA_CONTAINER_COMPLETED for map\n-    app =\n-        new FailingAttemptsDuringAssignedMRApp(1, 0,\n-            TaskAttemptEventType.TA_CONTAINER_COMPLETED);\n-    testTaskAttemptAssignedFailHistory(app);\n-\n-    // test TA_CONTAINER_COMPLETED for reduce\n-    app =\n-        new FailingAttemptsDuringAssignedMRApp(0, 1,\n-            TaskAttemptEventType.TA_CONTAINER_COMPLETED);\n-    testTaskAttemptAssignedFailHistory(app);\n-\n-    // test TA_FAILMSG for map\n-    app =\n-        new FailingAttemptsDuringAssignedMRApp(1, 0,\n-            TaskAttemptEventType.TA_FAILMSG);\n-    testTaskAttemptAssignedFailHistory(app);\n-\n-    // test TA_FAILMSG for reduce\n-    app =\n-        new FailingAttemptsDuringAssignedMRApp(0, 1,\n-            TaskAttemptEventType.TA_FAILMSG);\n-    testTaskAttemptAssignedFailHistory(app);\n-\n-    // test TA_FAILMSG_BY_CLIENT for map\n-    app =\n-        new FailingAttemptsDuringAssignedMRApp(1, 0,\n-            TaskAttemptEventType.TA_FAILMSG_BY_CLIENT);\n-    testTaskAttemptAssignedFailHistory(app);\n-\n-    // test TA_FAILMSG_BY_CLIENT for reduce\n-    app =\n-        new FailingAttemptsDuringAssignedMRApp(0, 1,\n-            TaskAttemptEventType.TA_FAILMSG_BY_CLIENT);\n-    testTaskAttemptAssignedFailHistory(app);\n-\n-    // test TA_KILL for map\n-    app =\n-        new FailingAttemptsDuringAssignedMRApp(1, 0,\n-            TaskAttemptEventType.TA_KILL);\n-    testTaskAttemptAssignedKilledHistory(app);\n-\n-    // test TA_KILL for reduce\n-    app =\n-        new FailingAttemptsDuringAssignedMRApp(0, 1,\n-            TaskAttemptEventType.TA_KILL);\n-    testTaskAttemptAssignedKilledHistory(app);\n+    FailingAttemptsDuringAssignedMRApp app = null;\n+\n+    try {\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(1, 0,\n+              TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED);\n+      testTaskAttemptAssignedFailHistory(app);\n+      app.close();\n+\n+      // test TA_CONTAINER_LAUNCH_FAILED for reduce\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(0, 1,\n+              TaskAttemptEventType.TA_CONTAINER_LAUNCH_FAILED);\n+      testTaskAttemptAssignedFailHistory(app);\n+      app.close();\n+\n+      // test TA_CONTAINER_COMPLETED for map\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(1, 0,\n+              TaskAttemptEventType.TA_CONTAINER_COMPLETED);\n+      testTaskAttemptAssignedFailHistory(app);\n+      app.close();\n+\n+      // test TA_CONTAINER_COMPLETED for reduce\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(0, 1,\n+              TaskAttemptEventType.TA_CONTAINER_COMPLETED);\n+      testTaskAttemptAssignedFailHistory(app);\n+      app.close();\n+\n+      // test TA_FAILMSG for map\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(1, 0,\n+              TaskAttemptEventType.TA_FAILMSG);\n+      testTaskAttemptAssignedFailHistory(app);\n+      app.close();\n+\n+      // test TA_FAILMSG for reduce\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(0, 1,\n+              TaskAttemptEventType.TA_FAILMSG);\n+      testTaskAttemptAssignedFailHistory(app);\n+      app.close();\n+\n+      // test TA_FAILMSG_BY_CLIENT for map\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(1, 0,\n+              TaskAttemptEventType.TA_FAILMSG_BY_CLIENT);\n+      testTaskAttemptAssignedFailHistory(app);\n+      app.close();\n+\n+      // test TA_FAILMSG_BY_CLIENT for reduce\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(0, 1,\n+              TaskAttemptEventType.TA_FAILMSG_BY_CLIENT);\n+      testTaskAttemptAssignedFailHistory(app);\n+      app.close();\n+\n+      // test TA_KILL for map\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(1, 0,\n+              TaskAttemptEventType.TA_KILL);\n+      testTaskAttemptAssignedKilledHistory(app);\n+      app.close();\n+\n+      // test TA_KILL for reduce\n+      app =\n+          new FailingAttemptsDuringAssignedMRApp(0, 1,\n+              TaskAttemptEventType.TA_KILL);\n+      testTaskAttemptAssignedKilledHistory(app);\n+      app.close();\n+    } finally {\n+      app.close();\n+    }\n   }\n \n   @Test",
      "parent_sha": "f83716b7f2e5b63e4c2302c374982755233d4dd6"
    }
  },
  {
    "oid": "c9b63deb533274ca8ef4939f6cd13f728a067f7b",
    "message": "HDFS-13611. Unsafe use of Text as a ConcurrentHashMap key in PBHelperClient.",
    "date": "2018-05-24T16:56:23Z",
    "url": "https://github.com/apache/hadoop/commit/c9b63deb533274ca8ef4939f6cd13f728a067f7b",
    "details": {
      "sha": "490ccb453b25e3cfbc094af2741f05f5415d679e",
      "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelperClient.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/c9b63deb533274ca8ef4939f6cd13f728a067f7b/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FprotocolPB%2FPBHelperClient.java",
      "raw_url": "https://github.com/apache/hadoop/raw/c9b63deb533274ca8ef4939f6cd13f728a067f7b/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FprotocolPB%2FPBHelperClient.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FprotocolPB%2FPBHelperClient.java?ref=c9b63deb533274ca8ef4939f6cd13f728a067f7b",
      "patch": "@@ -247,7 +247,7 @@ private static ByteString getFixedByteString(Text key) {\n     ByteString value = fixedByteStringCache.get(key);\n     if (value == null) {\n       value = ByteString.copyFromUtf8(key.toString());\n-      fixedByteStringCache.put(key, value);\n+      fixedByteStringCache.put(new Text(key.copyBytes()), value);\n     }\n     return value;\n   }",
      "parent_sha": "1388de18ad51434569589a8f5b0b05c38fe02ab3"
    }
  },
  {
    "oid": "dba7a7dd9d70adfab36a78eb55059c54e553a5cb",
    "message": "HDFS-12479. Some misuses of lock in DFSStripedOutputStream. Contributed by Huafeng Wang",
    "date": "2017-09-19T09:45:41Z",
    "url": "https://github.com/apache/hadoop/commit/dba7a7dd9d70adfab36a78eb55059c54e553a5cb",
    "details": {
      "sha": "66eec7a360fbadccf84a911a9ccd4c41c673de24",
      "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedOutputStream.java",
      "status": "modified",
      "additions": 4,
      "deletions": 5,
      "changes": 9,
      "blob_url": "https://github.com/apache/hadoop/blob/dba7a7dd9d70adfab36a78eb55059c54e553a5cb/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FDFSStripedOutputStream.java",
      "raw_url": "https://github.com/apache/hadoop/raw/dba7a7dd9d70adfab36a78eb55059c54e553a5cb/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FDFSStripedOutputStream.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FDFSStripedOutputStream.java?ref=dba7a7dd9d70adfab36a78eb55059c54e553a5cb",
      "patch": "@@ -63,6 +63,7 @@\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.Callable;\n import java.util.concurrent.CompletionService;\n+import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorCompletionService;\n import java.util.concurrent.ExecutorService;\n@@ -85,11 +86,10 @@ static class MultipleBlockingQueue<T> {\n     private final List<BlockingQueue<T>> queues;\n \n     MultipleBlockingQueue(int numQueue, int queueSize) {\n-      List<BlockingQueue<T>> list = new ArrayList<>(numQueue);\n+      queues = new ArrayList<>(numQueue);\n       for (int i = 0; i < numQueue; i++) {\n-        list.add(new LinkedBlockingQueue<T>(queueSize));\n+        queues.add(new LinkedBlockingQueue<T>(queueSize));\n       }\n-      queues = Collections.synchronizedList(list);\n     }\n \n     void offer(int i, T object) {\n@@ -156,8 +156,7 @@ static class Coordinator {\n       followingBlocks = new MultipleBlockingQueue<>(numAllBlocks, 1);\n       endBlocks = new MultipleBlockingQueue<>(numAllBlocks, 1);\n       newBlocks = new MultipleBlockingQueue<>(numAllBlocks, 1);\n-      updateStreamerMap = Collections.synchronizedMap(\n-          new HashMap<StripedDataStreamer, Boolean>(numAllBlocks));\n+      updateStreamerMap = new ConcurrentHashMap<>(numAllBlocks);\n       streamerUpdateResult = new MultipleBlockingQueue<>(numAllBlocks, 1);\n     }\n ",
      "parent_sha": "2018538fdba1a95a6556187569e872fce7f9e1c3"
    }
  },
  {
    "oid": "f426b7ce8fb33d57e4187484448b9e0bfc04ccfa",
    "message": "HDDS-139. Output of createVolume can be improved. Contributed by Shweta.",
    "date": "2019-03-27T02:01:49Z",
    "url": "https://github.com/apache/hadoop/commit/f426b7ce8fb33d57e4187484448b9e0bfc04ccfa",
    "details": {
      "sha": "6ecda090817c075e2e8af9006e6761a5c70a505a",
      "filename": "hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/RpcClient.java",
      "status": "modified",
      "additions": 6,
      "deletions": 2,
      "changes": 8,
      "blob_url": "https://github.com/apache/hadoop/blob/f426b7ce8fb33d57e4187484448b9e0bfc04ccfa/hadoop-ozone%2Fclient%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fclient%2Frpc%2FRpcClient.java",
      "raw_url": "https://github.com/apache/hadoop/raw/f426b7ce8fb33d57e4187484448b9e0bfc04ccfa/hadoop-ozone%2Fclient%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fclient%2Frpc%2FRpcClient.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone%2Fclient%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fclient%2Frpc%2FRpcClient.java?ref=f426b7ce8fb33d57e4187484448b9e0bfc04ccfa",
      "patch": "@@ -270,8 +270,12 @@ public void createVolume(String volumeName, VolumeArgs volArgs)\n       builder.addOzoneAcls(OMPBHelper.convertOzoneAcl(ozoneAcl));\n     }\n \n-    LOG.info(\"Creating Volume: {}, with {} as owner and quota set to {} bytes.\",\n-        volumeName, owner, quota);\n+    if (volArgs.getQuota() == null) {\n+      LOG.info(\"Creating Volume: {}, with {} as owner.\", volumeName, owner);\n+    } else {\n+      LOG.info(\"Creating Volume: {}, with {} as owner \"\n+              + \"and quota set to {} bytes.\", volumeName, owner, quota);\n+    }\n     ozoneManagerClient.createVolume(builder.build());\n   }\n ",
      "parent_sha": "fe29b3901be1b06db92379c7b7fac4954253e6e2"
    }
  },
  {
    "oid": "41da2050bdec14709a86fa8a5cf7da82415fd989",
    "message": "HDDS-310. VolumeSet shutdown hook fails on datanode restart. Contributed by Bharat Viswanadham.",
    "date": "2018-08-02T06:05:22Z",
    "url": "https://github.com/apache/hadoop/commit/41da2050bdec14709a86fa8a5cf7da82415fd989",
    "details": {
      "sha": "06f48fc2936c9e511d37656f79130d42a48eea55",
      "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
      "status": "modified",
      "additions": 13,
      "deletions": 2,
      "changes": 15,
      "blob_url": "https://github.com/apache/hadoop/blob/41da2050bdec14709a86fa8a5cf7da82415fd989/hadoop-hdds%2Fcontainer-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Fvolume%2FVolumeSet.java",
      "raw_url": "https://github.com/apache/hadoop/raw/41da2050bdec14709a86fa8a5cf7da82415fd989/hadoop-hdds%2Fcontainer-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Fvolume%2FVolumeSet.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds%2Fcontainer-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Fvolume%2FVolumeSet.java?ref=41da2050bdec14709a86fa8a5cf7da82415fd989",
      "patch": "@@ -167,7 +167,7 @@ private void initializeVolumeSet() throws IOException {\n \n     // Ensure volume threads are stopped and scm df is saved during shutdown.\n     shutdownHook = () -> {\n-      shutdown();\n+      saveVolumeSetUsed();\n     };\n     ShutdownHookManager.get().addShutdownHook(shutdownHook,\n         SHUTDOWN_HOOK_PRIORITY);\n@@ -303,7 +303,11 @@ public HddsVolume chooseVolume(long containerSize,\n     return choosingPolicy.chooseVolume(getVolumesList(), containerSize);\n   }\n \n-  public void shutdown() {\n+  /**\n+   * This method, call shutdown on each volume to shutdown volume usage\n+   * thread and write scmUsed on each volume.\n+   */\n+  private void saveVolumeSetUsed() {\n     for (HddsVolume hddsVolume : volumeMap.values()) {\n       try {\n         hddsVolume.shutdown();\n@@ -312,7 +316,14 @@ public void shutdown() {\n             ex);\n       }\n     }\n+  }\n \n+  /**\n+   * Shutdown's the volumeset, if saveVolumeSetUsed is false, call's\n+   * {@link VolumeSet#saveVolumeSetUsed}.\n+   */\n+  public void shutdown() {\n+    saveVolumeSetUsed();\n     if (shutdownHook != null) {\n       ShutdownHookManager.get().removeShutdownHook(shutdownHook);\n     }",
      "parent_sha": "735b4925569541fb8e65dc0c668ccc2aa2ffb30b"
    }
  },
  {
    "oid": "33768724ff99d4966c24c9553eef207ed31a76d3",
    "message": "HADOOP-15377. Improve debug messages in MetricsConfig.java\n\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
    "date": "2018-05-02T08:09:22Z",
    "url": "https://github.com/apache/hadoop/commit/33768724ff99d4966c24c9553eef207ed31a76d3",
    "details": {
      "sha": "027450cb65077cf7a7911c13649ee9e0ddaa3cf3",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java",
      "status": "modified",
      "additions": 30,
      "deletions": 20,
      "changes": 50,
      "blob_url": "https://github.com/apache/hadoop/blob/33768724ff99d4966c24c9553eef207ed31a76d3/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics2%2Fimpl%2FMetricsConfig.java",
      "raw_url": "https://github.com/apache/hadoop/raw/33768724ff99d4966c24c9553eef207ed31a76d3/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics2%2Fimpl%2FMetricsConfig.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics2%2Fimpl%2FMetricsConfig.java?ref=33768724ff99d4966c24c9553eef207ed31a76d3",
      "patch": "@@ -118,20 +118,23 @@ static MetricsConfig loadFirst(String prefix, String... fileNames) {\n                 .setListDelimiterHandler(new DefaultListDelimiterHandler(',')))\n               .getConfiguration()\n               .interpolatedConfiguration();\n-        LOG.info(\"loaded properties from \"+ fname);\n-        LOG.debug(toString(cf));\n+        LOG.info(\"Loaded properties from {}\", fname);\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Properties: {}\", toString(cf));\n+        }\n         MetricsConfig mc = new MetricsConfig(cf, prefix);\n-        LOG.debug(mc.toString());\n+        LOG.debug(\"Metrics Config: {}\", mc);\n         return mc;\n       } catch (ConfigurationException e) {\n         // Commons Configuration defines the message text when file not found\n         if (e.getMessage().startsWith(\"Could not locate\")) {\n+          LOG.debug(\"Could not locate file {}\", fname, e);\n           continue;\n         }\n         throw new MetricsConfigException(e);\n       }\n     }\n-    LOG.warn(\"Cannot locate configuration: tried \"+\n+    LOG.warn(\"Cannot locate configuration: tried \" +\n              Joiner.on(\",\").join(fileNames));\n     // default to an empty configuration\n     return new MetricsConfig(new PropertiesConfiguration(), prefix);\n@@ -168,7 +171,6 @@ Map<String, MetricsConfig> getInstanceConfigs(String type) {\n \n   Iterable<String> keys() {\n     return new Iterable<String>() {\n-      @SuppressWarnings(\"unchecked\")\n       @Override\n       public Iterator<String> iterator() {\n         return (Iterator<String>) getKeys();\n@@ -186,21 +188,21 @@ public Object getPropertyInternal(String key) {\n     Object value = super.getPropertyInternal(key);\n     if (value == null) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"poking parent '\"+ getParent().getClass().getSimpleName() +\n-                  \"' for key: \"+ key);\n+        LOG.debug(\"poking parent '\" + getParent().getClass().getSimpleName() +\n+                  \"' for key: \" + key);\n       }\n       return getParent().getProperty(key.startsWith(PREFIX_DEFAULT) ? key\n                                      : PREFIX_DEFAULT + key);\n     }\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"returning '\"+ value +\"' for key: \"+ key);\n-    }\n+    LOG.debug(\"Returning '{}' for key: {}\", value, key);\n     return value;\n   }\n \n   <T extends MetricsPlugin> T getPlugin(String name) {\n     String clsName = getClassName(name);\n-    if (clsName == null) return null;\n+    if (clsName == null) {\n+      return null;\n+    }\n     try {\n       Class<?> cls = Class.forName(clsName, true, getPluginLoader());\n       @SuppressWarnings(\"unchecked\")\n@@ -213,35 +215,39 @@ <T extends MetricsPlugin> T getPlugin(String name) {\n   }\n \n   String getClassName(String prefix) {\n-    String classKey = prefix.isEmpty() ? \"class\" : prefix +\".class\";\n+    String classKey = prefix.isEmpty() ? \"class\" : prefix.concat(\".class\");\n     String clsName = getString(classKey);\n-    LOG.debug(clsName);\n+    LOG.debug(\"Class name for prefix {} is {}\", prefix, clsName);\n     if (clsName == null || clsName.isEmpty()) {\n       return null;\n     }\n     return clsName;\n   }\n \n   ClassLoader getPluginLoader() {\n-    if (pluginLoader != null) return pluginLoader;\n+    if (pluginLoader != null) {\n+      return pluginLoader;\n+    }\n     final ClassLoader defaultLoader = getClass().getClassLoader();\n     Object purls = super.getProperty(PLUGIN_URLS_KEY);\n-    if (purls == null) return defaultLoader;\n+    if (purls == null) {\n+      return defaultLoader;\n+    }\n     Iterable<String> jars = SPLITTER.split((String) purls);\n     int len = Iterables.size(jars);\n-    if ( len > 0) {\n+    if (len > 0) {\n       final URL[] urls = new URL[len];\n       try {\n         int i = 0;\n         for (String jar : jars) {\n-          LOG.debug(jar);\n+          LOG.debug(\"Parsing URL for {}\", jar);\n           urls[i++] = new URL(jar);\n         }\n       } catch (Exception e) {\n         throw new MetricsConfigException(e);\n       }\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"using plugin jars: \"+ Iterables.toString(jars));\n+        LOG.debug(\"Using plugin jars: {}\", Iterables.toString(jars));\n       }\n       pluginLoader = doPrivileged(new PrivilegedAction<ClassLoader>() {\n         @Override public ClassLoader run() {\n@@ -259,9 +265,13 @@ ClassLoader getPluginLoader() {\n   MetricsFilter getFilter(String prefix) {\n     // don't create filter instances without out options\n     MetricsConfig conf = subset(prefix);\n-    if (conf.isEmpty()) return null;\n+    if (conf.isEmpty()) {\n+      return null;\n+    }\n     MetricsFilter filter = getPlugin(prefix);\n-    if (filter != null) return filter;\n+    if (filter != null) {\n+      return filter;\n+    }\n     // glob filter is assumed if pattern is specified but class is not.\n     filter = new GlobFilter();\n     filter.init(conf);",
      "parent_sha": "1a95a4524a8c6c7be601ce8b92640a6a76164a2c"
    }
  },
  {
    "oid": "13501053dd95f41ac14091f5bbd79bcb2a3896a3",
    "message": "HADOOP-11603. Metric Snapshot log can be changed #MetricsSystemImpl.java since all the services will be initialized. Contributed By Brahma Reddy Battula.",
    "date": "2016-11-24T16:53:22Z",
    "url": "https://github.com/apache/hadoop/commit/13501053dd95f41ac14091f5bbd79bcb2a3896a3",
    "details": {
      "sha": "24173f5d01693ebbe3822b5bb22593cfc15eddfc",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/hadoop/blob/13501053dd95f41ac14091f5bbd79bcb2a3896a3/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics2%2Fimpl%2FMetricsSystemImpl.java",
      "raw_url": "https://github.com/apache/hadoop/raw/13501053dd95f41ac14091f5bbd79bcb2a3896a3/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics2%2Fimpl%2FMetricsSystemImpl.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics2%2Fimpl%2FMetricsSystemImpl.java?ref=13501053dd95f41ac14091f5bbd79bcb2a3896a3",
      "patch": "@@ -33,7 +33,6 @@\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n import com.google.common.annotations.VisibleForTesting;\n-import java.util.Locale;\n import static com.google.common.base.Preconditions.*;\n \n import org.apache.commons.configuration2.PropertiesConfiguration;\n@@ -372,7 +371,8 @@ public void run() {\n             }\n           }\n         }, millis, millis);\n-    LOG.info(\"Scheduled snapshot period at \"+ (period/1000) +\" second(s).\");\n+    LOG.info(\"Scheduled Metric snapshot period at \" + (period / 1000)\n+        + \" second(s).\");\n   }\n \n   synchronized void onTimerEvent() {",
      "parent_sha": "abb9fa7fc69ec7b25f1c44e17c4c7dd17f5de16a"
    }
  },
  {
    "oid": "e15f92829558cc4a30b10f2fccfd17c2f8088003",
    "message": "Revert \"MAPREDUCE-6286. A typo in HistoryViewer makes some code useless, which causes counter limits are not reset correctly. Contributed by Zhihai Xu.\"\n\nThis reverts commit 433542904aba5ddebf9bd9d299378647351eb13a.",
    "date": "2017-07-28T21:06:59Z",
    "url": "https://github.com/apache/hadoop/commit/e15f92829558cc4a30b10f2fccfd17c2f8088003",
    "details": {
      "sha": "25c06304565a1960aba42a30cef1ea06dfa2bb68",
      "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/e15f92829558cc4a30b10f2fccfd17c2f8088003/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fjobhistory%2FHistoryViewer.java",
      "raw_url": "https://github.com/apache/hadoop/raw/e15f92829558cc4a30b10f2fccfd17c2f8088003/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fjobhistory%2FHistoryViewer.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fjobhistory%2FHistoryViewer.java?ref=e15f92829558cc4a30b10f2fccfd17c2f8088003",
      "patch": "@@ -94,7 +94,7 @@ public HistoryViewer(String historyFile, Configuration conf, boolean printAll,\n       final Configuration jobConf = new Configuration(conf);\n       try {\n         jobConf.addResource(fs.open(jobConfPath), jobConfPath.toString());\n-        Limits.reset(jobConf);\n+        Limits.reset(conf);\n       } catch (FileNotFoundException fnf) {\n         if (LOG.isWarnEnabled()) {\n           LOG.warn(\"Missing job conf in history\", fnf);",
      "parent_sha": "746189ad8cdf90ab35baec9364b2e02956a1e70c"
    }
  },
  {
    "oid": "15df2e7a7547e12e884b624d9f17ad2799d9ccf9",
    "message": "HDDS-796. Fix failed test TestStorageContainerManagerHttpServer#testHttpPolicy.",
    "date": "2018-11-05T09:31:06Z",
    "url": "https://github.com/apache/hadoop/commit/15df2e7a7547e12e884b624d9f17ad2799d9ccf9",
    "details": {
      "sha": "5e7d7b8032ecffd524d2bbe39ddd99f03289f6cd",
      "filename": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java",
      "status": "modified",
      "additions": 2,
      "deletions": 5,
      "changes": 7,
      "blob_url": "https://github.com/apache/hadoop/blob/15df2e7a7547e12e884b624d9f17ad2799d9ccf9/hadoop-hdds%2Fframework%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdds%2Fserver%2FBaseHttpServer.java",
      "raw_url": "https://github.com/apache/hadoop/raw/15df2e7a7547e12e884b624d9f17ad2799d9ccf9/hadoop-hdds%2Fframework%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdds%2Fserver%2FBaseHttpServer.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds%2Fframework%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdds%2Fserver%2FBaseHttpServer.java?ref=15df2e7a7547e12e884b624d9f17ad2799d9ccf9",
      "patch": "@@ -115,13 +115,10 @@ protected InetSocketAddress getBindAddress(String bindHostKey,\n     final Optional<Integer> addressPort =\n         getPortNumberFromConfigKeys(conf, addressKey);\n \n-    final Optional<String> addresHost =\n+    final Optional<String> addressHost =\n         getHostNameFromConfigKeys(conf, addressKey);\n \n-    String hostName = bindHost.orElse(addresHost.get());\n-    if (hostName == null || hostName.isEmpty()) {\n-      hostName = bindHostDefault;\n-    }\n+    String hostName = bindHost.orElse(addressHost.orElse(bindHostDefault));\n \n     return NetUtils.createSocketAddr(\n         hostName + \":\" + addressPort.orElse(bindPortdefault));",
      "parent_sha": "d43cc5db0ff80958ca873df1dc2fa00054e37175"
    }
  },
  {
    "oid": "390f8603d3a54ffae743fe240b9b0195bd01de06",
    "message": "HDFS-16106. Fix flaky unit test TestDFSShell (#3168)",
    "date": "2021-07-02T07:31:25Z",
    "url": "https://github.com/apache/hadoop/commit/390f8603d3a54ffae743fe240b9b0195bd01de06",
    "details": {
      "sha": "0816c3f74fa22f2900da8f00b0de6f52cd8b34ce",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/390f8603d3a54ffae743fe240b9b0195bd01de06/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FTestDFSShell.java",
      "raw_url": "https://github.com/apache/hadoop/raw/390f8603d3a54ffae743fe240b9b0195bd01de06/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FTestDFSShell.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FTestDFSShell.java?ref=390f8603d3a54ffae743fe240b9b0195bd01de06",
      "patch": "@@ -117,7 +117,7 @@ public static void setup() throws IOException {\n         GenericTestUtils.getTestDir(\"TestDFSShell\").getAbsolutePath());\n     conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_XATTRS_ENABLED_KEY, true);\n     conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_ACLS_ENABLED_KEY, true);\n-    conf.setLong(DFSConfigKeys.DFS_NAMENODE_ACCESSTIME_PRECISION_KEY, 1000);\n+    conf.setLong(DFSConfigKeys.DFS_NAMENODE_ACCESSTIME_PRECISION_KEY, 120000);\n \n     miniCluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n     miniCluster.waitActive();",
      "parent_sha": "9be17339eb1dfda01afe255831fac36a33141d85"
    }
  },
  {
    "oid": "d95c13774e1bd5b3cc61bf4da8bae4a93ed0040c",
    "message": "HDFS-12963. Error log level in ShortCircuitRegistry#removeShm. Contributed by hu xiaodong.",
    "date": "2018-01-24T02:43:36Z",
    "url": "https://github.com/apache/hadoop/commit/d95c13774e1bd5b3cc61bf4da8bae4a93ed0040c",
    "details": {
      "sha": "ea9e72ce4433af9f56d4bc41149a923fb3e60b0a",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/d95c13774e1bd5b3cc61bf4da8bae4a93ed0040c/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FShortCircuitRegistry.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d95c13774e1bd5b3cc61bf4da8bae4a93ed0040c/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FShortCircuitRegistry.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FShortCircuitRegistry.java?ref=d95c13774e1bd5b3cc61bf4da8bae4a93ed0040c",
      "patch": "@@ -114,7 +114,7 @@ String getClientName() {\n \n   public synchronized void removeShm(ShortCircuitShm shm) {\n     if (LOG.isTraceEnabled()) {\n-      LOG.debug(\"removing shm \" + shm);\n+      LOG.trace(\"removing shm \" + shm);\n     }\n     // Stop tracking the shmId.\n     RegisteredShm removedShm = segments.remove(shm.getShmId());",
      "parent_sha": "e307edcb472207a39d1cbe4be6f7fcddc7b4fd6d"
    }
  },
  {
    "oid": "ef5c776a42697fb3958a77646b759e2a6edfe48e",
    "message": "HADOOP-15733. Correct the log when Invalid emptier Interval configured. Contributed by Ayush Saxena",
    "date": "2018-09-14T02:02:27Z",
    "url": "https://github.com/apache/hadoop/commit/ef5c776a42697fb3958a77646b759e2a6edfe48e",
    "details": {
      "sha": "6e101a26e24a6ca5509832b0efe66e8de247156c",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/ef5c776a42697fb3958a77646b759e2a6edfe48e/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FTrashPolicyDefault.java",
      "raw_url": "https://github.com/apache/hadoop/raw/ef5c776a42697fb3958a77646b759e2a6edfe48e/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FTrashPolicyDefault.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FTrashPolicyDefault.java?ref=ef5c776a42697fb3958a77646b759e2a6edfe48e",
      "patch": "@@ -249,7 +249,7 @@ protected class Emptier implements Runnable {\n       LOG.info(\"Namenode trash configuration: Deletion interval = \"\n           + (deletionInterval / MSECS_PER_MINUTE)\n           + \" minutes, Emptier interval = \"\n-          + (emptierInterval / MSECS_PER_MINUTE) + \" minutes.\");\n+          + (this.emptierInterval / MSECS_PER_MINUTE) + \" minutes.\");\n     }\n \n     @Override",
      "parent_sha": "291dcf2247f8f5f92a3290734a2ed081a99ccbc3"
    }
  },
  {
    "oid": "20cde552748cc6be1c5ca590755d4e5a67baf187",
    "message": "HADOOP-14371. License error in TestLoadBalancingKMSClientProvider.java. Contributed by hu xiaodong.",
    "date": "2017-05-02T14:36:54Z",
    "url": "https://github.com/apache/hadoop/commit/20cde552748cc6be1c5ca590755d4e5a67baf187",
    "details": {
      "sha": "499b9915d9e1353ecf043a4fd9048e49fb14c5fe",
      "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/kms/TestLoadBalancingKMSClientProvider.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/20cde552748cc6be1c5ca590755d4e5a67baf187/hadoop-common-project%2Fhadoop-common%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2FTestLoadBalancingKMSClientProvider.java",
      "raw_url": "https://github.com/apache/hadoop/raw/20cde552748cc6be1c5ca590755d4e5a67baf187/hadoop-common-project%2Fhadoop-common%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2FTestLoadBalancingKMSClientProvider.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2FTestLoadBalancingKMSClientProvider.java?ref=20cde552748cc6be1c5ca590755d4e5a67baf187",
      "patch": "@@ -1,4 +1,4 @@\n-/**    when(p1.getKMSUrl()).thenReturn(\"p1\");\n+/**\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information",
      "parent_sha": "b0f54ea035f406909f45c66b5403580919d63a4a"
    }
  },
  {
    "oid": "919a99bbb47d2ba62f61059e1e9af1e8418b82f6",
    "message": "HDFS-16163. Avoid locking entire blockPinningFailures map (#3296)",
    "date": "2021-08-16T05:40:05Z",
    "url": "https://github.com/apache/hadoop/commit/919a99bbb47d2ba62f61059e1e9af1e8418b82f6",
    "details": {
      "sha": "7394c6a28efd4b8b77009c30bb177978f5b31d26",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java",
      "status": "modified",
      "additions": 14,
      "deletions": 10,
      "changes": 24,
      "blob_url": "https://github.com/apache/hadoop/blob/919a99bbb47d2ba62f61059e1e9af1e8418b82f6/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fbalancer%2FDispatcher.java",
      "raw_url": "https://github.com/apache/hadoop/raw/919a99bbb47d2ba62f61059e1e9af1e8418b82f6/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fbalancer%2FDispatcher.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fbalancer%2FDispatcher.java?ref=919a99bbb47d2ba62f61059e1e9af1e8418b82f6",
      "patch": "@@ -40,6 +40,7 @@\n import java.util.Map;\n import java.util.Map.Entry;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n@@ -405,7 +406,8 @@ private void dispatch() {\n           // Pinned block can't be moved. Add this block into failure list.\n           // Later in the next iteration mover will exclude these blocks from\n           // pending moves.\n-          target.getDDatanode().addBlockPinningFailures(this);\n+          target.getDDatanode().addBlockPinningFailures(\n+              this.reportedBlock.getBlock().getBlockId(), this.getSource());\n           return;\n         }\n \n@@ -643,7 +645,8 @@ public boolean equals(Object obj) {\n     /** blocks being moved but not confirmed yet */\n     private final List<PendingMove> pendings;\n     private volatile boolean hasFailure = false;\n-    private Map<Long, Set<DatanodeInfo>> blockPinningFailures = new HashMap<>();\n+    private final Map<Long, Set<DatanodeInfo>> blockPinningFailures =\n+        new ConcurrentHashMap<>();\n     private volatile boolean hasSuccess = false;\n     private ExecutorService moveExecutor;\n \n@@ -729,16 +732,17 @@ void setHasFailure() {\n       this.hasFailure = true;\n     }\n \n-    void addBlockPinningFailures(PendingMove pendingBlock) {\n-      synchronized (blockPinningFailures) {\n-        long blockId = pendingBlock.reportedBlock.getBlock().getBlockId();\n-        Set<DatanodeInfo> pinnedLocations = blockPinningFailures.get(blockId);\n+    private void addBlockPinningFailures(long blockId, DatanodeInfo source) {\n+      blockPinningFailures.compute(blockId, (key, pinnedLocations) -> {\n+        Set<DatanodeInfo> newPinnedLocations;\n         if (pinnedLocations == null) {\n-          pinnedLocations = new HashSet<>();\n-          blockPinningFailures.put(blockId, pinnedLocations);\n+          newPinnedLocations = new HashSet<>();\n+        } else {\n+          newPinnedLocations = pinnedLocations;\n         }\n-        pinnedLocations.add(pendingBlock.getSource());\n-      }\n+        newPinnedLocations.add(source);\n+        return newPinnedLocations;\n+      });\n     }\n \n     Map<Long, Set<DatanodeInfo>> getBlockPinningFailureList() {",
      "parent_sha": "3aaac8a1f682ecda4d41221e8a836bc5401e3b1d"
    }
  },
  {
    "oid": "8d754c2c397f1273ad7c520fa5f1b7efa0db31f7",
    "message": "HADOOP-16351. Change \":\" to ApplicationConstants.CLASS_PATH_SEPARATOR. Contributed by kevin su.\n\nSigned-off-by: Wei-Chiu Chuang <weichiu@apache.org>",
    "date": "2019-08-17T00:26:09Z",
    "url": "https://github.com/apache/hadoop/commit/8d754c2c397f1273ad7c520fa5f1b7efa0db31f7",
    "details": {
      "sha": "8ba391e78f6e7e139a212c4ada7e15f27de6adca",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/8d754c2c397f1273ad7c520fa5f1b7efa0db31f7/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-applications-distributedshell%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fapplications%2Fdistributedshell%2FClient.java",
      "raw_url": "https://github.com/apache/hadoop/raw/8d754c2c397f1273ad7c520fa5f1b7efa0db31f7/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-applications-distributedshell%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fapplications%2Fdistributedshell%2FClient.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-applications-distributedshell%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fapplications%2Fdistributedshell%2FClient.java?ref=8d754c2c397f1273ad7c520fa5f1b7efa0db31f7",
      "patch": "@@ -925,7 +925,7 @@ public boolean run() throws IOException, YarnException {\n \n     // add the runtime classpath needed for tests to work\n     if (conf.getBoolean(YarnConfiguration.IS_MINI_YARN_CLUSTER, false)) {\n-      classPathEnv.append(':')\n+      classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR)\n           .append(System.getProperty(\"java.class.path\"));\n     }\n ",
      "parent_sha": "971a4c8e8328a4bdea65de4a0e84c82b5b2de24b"
    }
  },
  {
    "oid": "1a79dcfc457969d6a6c08ffffe4152fd7638e48a",
    "message": "HDFS-12143. Improve performance of getting and removing inode features. Contributed by Daryn Sharp.",
    "date": "2017-07-25T15:28:57Z",
    "url": "https://github.com/apache/hadoop/commit/1a79dcfc457969d6a6c08ffffe4152fd7638e48a",
    "details": {
      "sha": "9adcc3eb63c778fae70d5ffd351d6e97ae4cf6e2",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeWithAdditionalFields.java",
      "status": "modified",
      "additions": 17,
      "deletions": 7,
      "changes": 24,
      "blob_url": "https://github.com/apache/hadoop/blob/1a79dcfc457969d6a6c08ffffe4152fd7638e48a/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FINodeWithAdditionalFields.java",
      "raw_url": "https://github.com/apache/hadoop/raw/1a79dcfc457969d6a6c08ffffe4152fd7638e48a/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FINodeWithAdditionalFields.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FINodeWithAdditionalFields.java?ref=1a79dcfc457969d6a6c08ffffe4152fd7638e48a",
      "patch": "@@ -283,12 +283,14 @@ protected void addFeature(Feature f) {\n \n   protected void removeFeature(Feature f) {\n     int size = features.length;\n-    Preconditions.checkState(size > 0, \"Feature \"\n-        + f.getClass().getSimpleName() + \" not found.\");\n+    if (size == 0) {\n+      throwFeatureNotFoundException(f);\n+    }\n \n     if (size == 1) {\n-      Preconditions.checkState(features[0] == f, \"Feature \"\n-          + f.getClass().getSimpleName() + \" not found.\");\n+      if (features[0] != f) {\n+        throwFeatureNotFoundException(f);\n+      }\n       features = EMPTY_FEATURE;\n       return;\n     }\n@@ -307,14 +309,22 @@ protected void removeFeature(Feature f) {\n       }\n     }\n \n-    Preconditions.checkState(!overflow && j == size - 1, \"Feature \"\n-        + f.getClass().getSimpleName() + \" not found.\");\n+    if (overflow || j != size - 1) {\n+      throwFeatureNotFoundException(f);\n+    }\n     features = arr;\n   }\n \n+  private void throwFeatureNotFoundException(Feature f) {\n+    throw new IllegalStateException(\n+        \"Feature \" + f.getClass().getSimpleName() + \" not found.\");\n+  }\n+\n   protected <T extends Feature> T getFeature(Class<? extends Feature> clazz) {\n     Preconditions.checkArgument(clazz != null);\n-    for (Feature f : features) {\n+    final int size = features.length;\n+    for (int i=0; i < size; i++) {\n+      Feature f = features[i];\n       if (clazz.isAssignableFrom(f.getClass())) {\n         @SuppressWarnings(\"unchecked\")\n         T ret = (T) f;",
      "parent_sha": "cca51e916b7387ea358688e8f8188ead948fbdcc"
    }
  },
  {
    "oid": "733aa993134ba324c712590fa92b8ef230b0839a",
    "message": "YARN-4998. Minor cleanup to UGI use in AdminService. (Daniel Templeton via kasha)",
    "date": "2016-10-31T23:26:01Z",
    "url": "https://github.com/apache/hadoop/commit/733aa993134ba324c712590fa92b8ef230b0839a",
    "details": {
      "sha": "c060659ddb8bf9c6e6e322a7ade5665d05149879",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java",
      "status": "modified",
      "additions": 2,
      "deletions": 4,
      "changes": 6,
      "blob_url": "https://github.com/apache/hadoop/blob/733aa993134ba324c712590fa92b8ef230b0839a/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FAdminService.java",
      "raw_url": "https://github.com/apache/hadoop/raw/733aa993134ba324c712590fa92b8ef230b0839a/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FAdminService.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FAdminService.java?ref=733aa993134ba324c712590fa92b8ef230b0839a",
      "patch": "@@ -154,8 +154,7 @@ public void serviceInit(Configuration conf) throws Exception {\n         YarnConfiguration.DEFAULT_RM_ADMIN_PORT);\n     daemonUser = UserGroupInformation.getCurrentUser();\n     authorizer = YarnAuthorizationProvider.getInstance(conf);\n-    authorizer.setAdmins(getAdminAclList(conf), UserGroupInformation\n-        .getCurrentUser());\n+    authorizer.setAdmins(getAdminAclList(conf), daemonUser);\n     rmId = conf.get(YarnConfiguration.RM_HA_ID);\n \n     isCentralizedNodeLabelConfiguration =\n@@ -542,8 +541,7 @@ private RefreshAdminAclsResponse refreshAdminAcls(boolean checkRMHAState)\n     Configuration conf =\n         getConfiguration(new Configuration(false),\n             YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n-    authorizer.setAdmins(getAdminAclList(conf), UserGroupInformation\n-        .getCurrentUser());\n+    authorizer.setAdmins(getAdminAclList(conf), daemonUser);\n     RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n         \"AdminService\");\n ",
      "parent_sha": "07ab89e8bb3f647cef4f80f39237169a0c6a8520"
    }
  },
  {
    "oid": "29a6f141d4bcc5307d2ac72c742f87611d7092d5",
    "message": "YARN-10914. Simplify duplicated code for tracking ResourceUsage in AbstractCSQueue (#3402)\n\nCo-authored-by: Tamas Domok <tdomok@cloudera.com>",
    "date": "2021-09-10T13:57:46Z",
    "url": "https://github.com/apache/hadoop/commit/29a6f141d4bcc5307d2ac72c742f87611d7092d5",
    "details": {
      "sha": "2da52d55b7aca93dcfaac895611ebbd01ffb6193",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
      "status": "modified",
      "additions": 43,
      "deletions": 54,
      "changes": 97,
      "blob_url": "https://github.com/apache/hadoop/blob/29a6f141d4bcc5307d2ac72c742f87611d7092d5/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Fcapacity%2FAbstractCSQueue.java",
      "raw_url": "https://github.com/apache/hadoop/raw/29a6f141d4bcc5307d2ac72c742f87611d7092d5/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Fcapacity%2FAbstractCSQueue.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Fcapacity%2FAbstractCSQueue.java?ref=29a6f141d4bcc5307d2ac72c742f87611d7092d5",
      "patch": "@@ -73,6 +73,7 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n import java.util.Set;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n@@ -1189,84 +1190,72 @@ boolean canAssignToThisQueue(Resource clusterResource,\n \n   }\n \n+  private static String ensurePartition(String partition) {\n+    return Optional.ofNullable(partition).orElse(RMNodeLabelsManager.NO_LABEL);\n+  }\n+\n+  @FunctionalInterface\n+  interface Counter {\n+    void count(String partition, Resource resource);\n+  }\n+\n+  @FunctionalInterface\n+  interface CounterWithApp {\n+    void count(String partition, Resource reservedRes, SchedulerApplicationAttempt application);\n+  }\n+\n+  private void count(String partition, Resource resource, Counter counter, Counter parentCounter) {\n+    final String checkedPartition = ensurePartition(partition);\n+    counter.count(checkedPartition, resource);\n+    Optional.ofNullable(parentCounter).ifPresent(c -> c.count(checkedPartition, resource));\n+  }\n+\n+  private void countAndUpdate(String partition, Resource resource,\n+                              Counter counter, CounterWithApp parentCounter) {\n+    final String checkedPartition = ensurePartition(partition);\n+    counter.count(checkedPartition, resource);\n+    CSQueueUtils.updateUsedCapacity(resourceCalculator,\n+        labelManager.getResourceByLabel(checkedPartition, Resources.none()),\n+        checkedPartition, this);\n+    Optional.ofNullable(parentCounter).ifPresent(c -> c.count(checkedPartition, resource, null));\n+  }\n+\n   @Override\n   public void incReservedResource(String partition, Resource reservedRes) {\n-    if (partition == null) {\n-      partition = RMNodeLabelsManager.NO_LABEL;\n-    }\n-\n-    queueUsage.incReserved(partition, reservedRes);\n-    if(null != parent){\n-      parent.incReservedResource(partition, reservedRes);\n-    }\n+    count(partition, reservedRes, queueUsage::incReserved,\n+        parent == null ? null : parent::incReservedResource);\n   }\n \n   @Override\n   public void decReservedResource(String partition, Resource reservedRes) {\n-    if (partition == null) {\n-      partition = RMNodeLabelsManager.NO_LABEL;\n-    }\n-\n-    queueUsage.decReserved(partition, reservedRes);\n-    if(null != parent){\n-      parent.decReservedResource(partition, reservedRes);\n-    }\n+    count(partition, reservedRes, queueUsage::decReserved,\n+        parent == null ? null : parent::decReservedResource);\n   }\n \n   @Override\n   public void incPendingResource(String nodeLabel, Resource resourceToInc) {\n-    if (nodeLabel == null) {\n-      nodeLabel = RMNodeLabelsManager.NO_LABEL;\n-    }\n-    // ResourceUsage has its own lock, no addition lock needs here.\n-    queueUsage.incPending(nodeLabel, resourceToInc);\n-    if (null != parent) {\n-      parent.incPendingResource(nodeLabel, resourceToInc);\n-    }\n+    count(nodeLabel, resourceToInc, queueUsage::incPending,\n+        parent == null ? null : parent::incPendingResource);\n   }\n \n   @Override\n   public void decPendingResource(String nodeLabel, Resource resourceToDec) {\n-    if (nodeLabel == null) {\n-      nodeLabel = RMNodeLabelsManager.NO_LABEL;\n-    }\n-    // ResourceUsage has its own lock, no addition lock needs here.\n-    queueUsage.decPending(nodeLabel, resourceToDec);\n-    if (null != parent) {\n-      parent.decPendingResource(nodeLabel, resourceToDec);\n-    }\n+    count(nodeLabel, resourceToDec, queueUsage::decPending,\n+        parent == null ? null : parent::decPendingResource);\n   }\n \n   @Override\n   public void incUsedResource(String nodeLabel, Resource resourceToInc,\n       SchedulerApplicationAttempt application) {\n-    if (nodeLabel == null) {\n-      nodeLabel = RMNodeLabelsManager.NO_LABEL;\n-    }\n-    // ResourceUsage has its own lock, no addition lock needs here.\n-    queueUsage.incUsed(nodeLabel, resourceToInc);\n-    CSQueueUtils.updateUsedCapacity(resourceCalculator,\n-        labelManager.getResourceByLabel(nodeLabel, Resources.none()),\n-        nodeLabel, this);\n-    if (null != parent) {\n-      parent.incUsedResource(nodeLabel, resourceToInc, null);\n-    }\n+    countAndUpdate(nodeLabel, resourceToInc, queueUsage::incUsed,\n+        parent == null ? null : parent::incUsedResource);\n   }\n \n   @Override\n   public void decUsedResource(String nodeLabel, Resource resourceToDec,\n       SchedulerApplicationAttempt application) {\n-    if (nodeLabel == null) {\n-      nodeLabel = RMNodeLabelsManager.NO_LABEL;\n-    }\n-    // ResourceUsage has its own lock, no addition lock needs here.\n-    queueUsage.decUsed(nodeLabel, resourceToDec);\n-    CSQueueUtils.updateUsedCapacity(resourceCalculator,\n-        labelManager.getResourceByLabel(nodeLabel, Resources.none()),\n-        nodeLabel, this);\n-    if (null != parent) {\n-      parent.decUsedResource(nodeLabel, resourceToDec, null);\n-    }\n+    countAndUpdate(nodeLabel, resourceToDec, queueUsage::decUsed,\n+        parent == null ? null : parent::decUsedResource);\n   }\n \n   /**",
      "parent_sha": "dee6dc2f89609bd35a18e40188103671a38efb89"
    }
  },
  {
    "oid": "abe7fc22c154258a898b497c11de73e93e2ae2b1",
    "message": "YARN-5182. MockNodes.newNodes creates one more node per rack than requested. (Karthik Kambatla via Varun Saxena).",
    "date": "2016-06-29T18:43:28Z",
    "url": "https://github.com/apache/hadoop/commit/abe7fc22c154258a898b497c11de73e93e2ae2b1",
    "details": {
      "sha": "630a8db5aaa2527fab8a179127c26e842b8cc5da",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/hadoop/blob/abe7fc22c154258a898b497c11de73e93e2ae2b1/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FMockNodes.java",
      "raw_url": "https://github.com/apache/hadoop/raw/abe7fc22c154258a898b497c11de73e93e2ae2b1/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FMockNodes.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FMockNodes.java?ref=abe7fc22c154258a898b497c11de73e93e2ae2b1",
      "patch": "@@ -57,8 +57,7 @@ public static List<RMNode> newNodes(int racks, int nodesPerRack,\n         if (j == (nodesPerRack - 1)) {\n           // One unhealthy node per rack.\n           list.add(nodeInfo(i, perNode, NodeState.UNHEALTHY));\n-        }\n-        if (j == 0) {\n+        } else if (j == 0) {\n           // One node with label\n           list.add(nodeInfo(i, perNode, NodeState.RUNNING, ImmutableSet.of(\"x\")));\n         } else {",
      "parent_sha": "991c946593454b73b56f3c403fe128afe6f46355"
    }
  },
  {
    "oid": "39c1ea1ed454b6c61f0985fc951f20913ed964fb",
    "message": "YARN-8729. Node status updater thread could be lost after it is restarted. Contributed by Tao Yang.",
    "date": "2018-09-13T14:21:35Z",
    "url": "https://github.com/apache/hadoop/commit/39c1ea1ed454b6c61f0985fc951f20913ed964fb",
    "details": {
      "sha": "3bb9f92c27756c654cf9e0c893735c0716cc2cf7",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/39c1ea1ed454b6c61f0985fc951f20913ed964fb/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-nodemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fnodemanager%2FNodeStatusUpdaterImpl.java",
      "raw_url": "https://github.com/apache/hadoop/raw/39c1ea1ed454b6c61f0985fc951f20913ed964fb/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-nodemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fnodemanager%2FNodeStatusUpdaterImpl.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-nodemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fnodemanager%2FNodeStatusUpdaterImpl.java?ref=39c1ea1ed454b6c61f0985fc951f20913ed964fb",
      "patch": "@@ -326,8 +326,8 @@ protected void rebootNodeStatusUpdaterAndRegisterWithRM() {\n         statusUpdater.join();\n         registerWithRM();\n         statusUpdater = new Thread(statusUpdaterRunnable, \"Node Status Updater\");\n-        statusUpdater.start();\n         this.isStopped = false;\n+        statusUpdater.start();\n         LOG.info(\"NodeStatusUpdater thread is reRegistered and restarted\");\n       } catch (Exception e) {\n         String errorMessage = \"Unexpected error rebooting NodeStatusUpdater\";",
      "parent_sha": "c9e0b69ab3b8e70b804e325ebe8901c2be98edca"
    }
  },
  {
    "oid": "f3f90295770c768c8b9c1f601502999e3ea7cebe",
    "message": "Temporarily disable NameNodeRaidTestUtil.getDatanodeMap(..) for MAPREDUCE-2711.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1151351 13f79535-47bb-0310-9956-ffa450edef68",
    "date": "2011-07-27T06:41:45Z",
    "url": "https://github.com/apache/hadoop/commit/f3f90295770c768c8b9c1f601502999e3ea7cebe",
    "details": {
      "sha": "0cb2ac75444472458e8ad5818f388e5578774df3",
      "filename": "mapreduce/src/contrib/raid/src/test/org/apache/hadoop/hdfs/server/namenode/NameNodeRaidTestUtil.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/apache/hadoop/blob/f3f90295770c768c8b9c1f601502999e3ea7cebe/mapreduce%2Fsrc%2Fcontrib%2Fraid%2Fsrc%2Ftest%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FNameNodeRaidTestUtil.java",
      "raw_url": "https://github.com/apache/hadoop/raw/f3f90295770c768c8b9c1f601502999e3ea7cebe/mapreduce%2Fsrc%2Fcontrib%2Fraid%2Fsrc%2Ftest%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FNameNodeRaidTestUtil.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/mapreduce%2Fsrc%2Fcontrib%2Fraid%2Fsrc%2Ftest%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FNameNodeRaidTestUtil.java?ref=f3f90295770c768c8b9c1f601502999e3ea7cebe",
      "patch": "@@ -41,9 +41,9 @@ public static FSInodeInfo getNode(final FSDirectory dir,\n     return dir.rootDir.getNode(src, resolveLink);\n   }\n \n-  public static NavigableMap<String, DatanodeDescriptor> getDatanodeMap(\n-      final FSNamesystem namesystem) {\n-    return namesystem.datanodeMap;\n-  }\n+//  public static NavigableMap<String, DatanodeDescriptor> getDatanodeMap(\n+//      final FSNamesystem namesystem) {\n+//    return namesystem.datanodeMap;\n+//  }\n }\n ",
      "parent_sha": "d356d64479168de202110ab0df4eb75564d4dc61"
    }
  },
  {
    "oid": "8b3ee2f7e9f10073a77d53eba4a6151aaadc6191",
    "message": "YARN-9462. TestResourceTrackerService.testNodeRemovalGracefully fails sporadically. Contributed by Prabhu Joseph",
    "date": "2020-01-20T08:54:22Z",
    "url": "https://github.com/apache/hadoop/commit/8b3ee2f7e9f10073a77d53eba4a6151aaadc6191",
    "details": {
      "sha": "6690339d892b61a01d349eb4d698aa2eaa7d9ab1",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/8b3ee2f7e9f10073a77d53eba4a6151aaadc6191/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FTestResourceTrackerService.java",
      "raw_url": "https://github.com/apache/hadoop/raw/8b3ee2f7e9f10073a77d53eba4a6151aaadc6191/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FTestResourceTrackerService.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FTestResourceTrackerService.java?ref=8b3ee2f7e9f10073a77d53eba4a6151aaadc6191",
      "patch": "@@ -2210,7 +2210,7 @@ public void testNodeRemovalUtil(boolean doGraceful) throws Exception {\n                 DEFAULT_RM_NODEMANAGER_UNTRACKED_REMOVAL_TIMEOUT_MSEC);\n     int nodeRemovalInterval =\n         rmContext.getNodesListManager().getNodeRemovalCheckInterval();\n-    long maxThreadSleeptime = nodeRemovalInterval + nodeRemovalTimeout;\n+    long maxThreadSleeptime = nodeRemovalInterval + nodeRemovalTimeout + 100;\n     latch.await(maxThreadSleeptime, TimeUnit.MILLISECONDS);\n \n     rmNode = rmContext.getInactiveRMNodes().get(nm2.getNodeId());",
      "parent_sha": "57aad0f43aa34d1e522e21cdb1debf73db9f2bdc"
    }
  },
  {
    "oid": "0da54e8848764c71a31473516d23ada582013f8c",
    "message": "YARN-5672. FairScheduler: Wrong queue name in log when adding application. (Wilfred Spiegelenburg via kasha)",
    "date": "2016-10-03T13:03:46Z",
    "url": "https://github.com/apache/hadoop/commit/0da54e8848764c71a31473516d23ada582013f8c",
    "details": {
      "sha": "920052f17839c486b2b0cc3df29fbd29c0c66b56",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "status": "modified",
      "additions": 4,
      "deletions": 3,
      "changes": 7,
      "blob_url": "https://github.com/apache/hadoop/blob/0da54e8848764c71a31473516d23ada582013f8c/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Ffair%2FFairScheduler.java",
      "raw_url": "https://github.com/apache/hadoop/raw/0da54e8848764c71a31473516d23ada582013f8c/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Ffair%2FFairScheduler.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Ffair%2FFairScheduler.java?ref=0da54e8848764c71a31473516d23ada582013f8c",
      "patch": "@@ -668,11 +668,12 @@ protected synchronized void addApplication(ApplicationId applicationId,\n     queue.getMetrics().submitApp(user);\n \n     LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n-        + \", in queue: \" + queueName + \", currently num of applications: \"\n-        + applications.size());\n+        + \", in queue: \" + queue.getName()\n+        + \", currently num of applications: \" + applications.size());\n     if (isAppRecovering) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.debug(applicationId + \" is recovering. Skip notifying APP_ACCEPTED\");\n+        LOG.debug(applicationId\n+            + \" is recovering. Skip notifying APP_ACCEPTED\");\n       }\n     } else {\n       rmContext.getDispatcher().getEventHandler()",
      "parent_sha": "6e130c308cf1b97e8386b6a43c26d72d2850119c"
    }
  },
  {
    "oid": "183f09b1dadd34cd555cdb880cb653f6c2527583",
    "message": "HDFS-16717. Replace NPE with IOException in DataNode.class (#4699). Contributed by ZanderXu.\n\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
    "date": "2022-08-23T10:17:32Z",
    "url": "https://github.com/apache/hadoop/commit/183f09b1dadd34cd555cdb880cb653f6c2527583",
    "details": {
      "sha": "022c59edbe8675224199a22c81a588a41a34c39a",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "status": "modified",
      "additions": 25,
      "deletions": 11,
      "changes": 36,
      "blob_url": "https://github.com/apache/hadoop/blob/183f09b1dadd34cd555cdb880cb653f6c2527583/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FDataNode.java",
      "raw_url": "https://github.com/apache/hadoop/raw/183f09b1dadd34cd555cdb880cb653f6c2527583/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FDataNode.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FDataNode.java?ref=183f09b1dadd34cd555cdb880cb653f6c2527583",
      "patch": "@@ -889,8 +889,11 @@ private String reconfDfsUsageParameters(String property, String newVal)\n     String result = null;\n     try {\n       LOG.info(\"Reconfiguring {} to {}\", property, newVal);\n+      if (data == null) {\n+        LOG.debug(\"FsDatasetSpi has not been initialized.\");\n+        throw new IOException(\"FsDatasetSpi has not been initialized\");\n+      }\n       if (property.equals(FS_DU_INTERVAL_KEY)) {\n-        Preconditions.checkNotNull(data, \"FsDatasetSpi has not been initialized.\");\n         long interval = (newVal == null ? FS_DU_INTERVAL_DEFAULT :\n             Long.parseLong(newVal));\n         result = Long.toString(interval);\n@@ -902,7 +905,6 @@ private String reconfDfsUsageParameters(String property, String newVal)\n           }\n         }\n       } else if (property.equals(FS_GETSPACEUSED_JITTER_KEY)) {\n-        Preconditions.checkNotNull(data, \"FsDatasetSpi has not been initialized.\");\n         long jitter = (newVal == null ? FS_GETSPACEUSED_JITTER_DEFAULT :\n             Long.parseLong(newVal));\n         result = Long.toString(jitter);\n@@ -914,7 +916,6 @@ private String reconfDfsUsageParameters(String property, String newVal)\n           }\n         }\n       } else if (property.equals(FS_GETSPACEUSED_CLASSNAME)) {\n-        Preconditions.checkNotNull(data, \"FsDatasetSpi has not been initialized.\");\n         Class<? extends GetSpaceUsed> klass;\n         if (newVal == null) {\n           if (Shell.WINDOWS) {\n@@ -1174,7 +1175,7 @@ private void refreshVolumes(String newVolumes) throws IOException {\n               .newFixedThreadPool(changedVolumes.newLocations.size());\n           List<Future<IOException>> exceptions = Lists.newArrayList();\n \n-          Preconditions.checkNotNull(data, \"Storage not yet initialized\");\n+          checkStorageState(\"refreshVolumes\");\n           for (final StorageLocation location : changedVolumes.newLocations) {\n             exceptions.add(service.submit(new Callable<IOException>() {\n               @Override\n@@ -1274,7 +1275,7 @@ private synchronized void removeVolumes(\n         clearFailure, Joiner.on(\",\").join(storageLocations)));\n \n     IOException ioe = null;\n-    Preconditions.checkNotNull(data, \"Storage not yet initialized\");\n+    checkStorageState(\"removeVolumes\");\n     // Remove volumes and block infos from FsDataset.\n     data.removeVolumes(storageLocations, clearFailure);\n \n@@ -2301,7 +2302,7 @@ public BlockLocalPathInfo getBlockLocalPathInfo(ExtendedBlock block,\n       Token<BlockTokenIdentifier> token) throws IOException {\n     checkBlockLocalPathAccess();\n     checkBlockToken(block, token, BlockTokenIdentifier.AccessMode.READ);\n-    Preconditions.checkNotNull(data, \"Storage not yet initialized\");\n+    checkStorageState(\"getBlockLocalPathInfo\");\n     BlockLocalPathInfo info = data.getBlockLocalPathInfo(block);\n     if (info != null) {\n       LOG.trace(\"getBlockLocalPathInfo successful \" +\n@@ -2351,7 +2352,7 @@ FileInputStream[] requestShortCircuitFdsForRead(final ExtendedBlock blk,\n     FileInputStream fis[] = new FileInputStream[2];\n     \n     try {\n-      Preconditions.checkNotNull(data, \"Storage not yet initialized\");\n+      checkStorageState(\"requestShortCircuitFdsForRead\");\n       fis[0] = (FileInputStream)data.getBlockInputStream(blk, 0);\n       fis[1] = DatanodeUtil.getMetaDataInputStream(blk, data);\n     } catch (ClassCastException e) {\n@@ -3382,7 +3383,7 @@ public static void main(String args[]) {\n   @Override // InterDatanodeProtocol\n   public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n       throws IOException {\n-    Preconditions.checkNotNull(data, \"Storage not yet initialized\");\n+    checkStorageState(\"initReplicaRecovery\");\n     return data.initReplicaRecovery(rBlock);\n   }\n \n@@ -3393,7 +3394,7 @@ public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n   public String updateReplicaUnderRecovery(final ExtendedBlock oldBlock,\n       final long recoveryId, final long newBlockId, final long newLength)\n       throws IOException {\n-    Preconditions.checkNotNull(data, \"Storage not yet initialized\");\n+    checkStorageState(\"updateReplicaUnderRecovery\");\n     final Replica r = data.updateReplicaUnderRecovery(oldBlock,\n         recoveryId, newBlockId, newLength);\n     // Notify the namenode of the updated block info. This is important\n@@ -3679,10 +3680,23 @@ public void deleteBlockPool(String blockPoolId, boolean force)\n           \"The block pool is still running. First do a refreshNamenodes to \" +\n           \"shutdown the block pool service\");\n     }\n-    Preconditions.checkNotNull(data, \"Storage not yet initialized\");\n+    checkStorageState(\"deleteBlockPool\");\n     data.deleteBlockPool(blockPoolId, force);\n   }\n \n+  /**\n+   * Check if storage has been initialized.\n+   * @param methodName caller name\n+   * @throws IOException throw IOException if not yet initialized.\n+   */\n+  private void checkStorageState(String methodName) throws IOException {\n+    if (data == null) {\n+      String message = \"Storage not yet initialized for \" + methodName;\n+      LOG.debug(message);\n+      throw new IOException(message);\n+    }\n+  }\n+\n   @Override // ClientDatanodeProtocol\n   public synchronized void shutdownDatanode(boolean forUpgrade) throws IOException {\n     checkSuperuserPrivilege();\n@@ -4131,7 +4145,7 @@ public String getSlowDisks() {\n   @Override\n   public List<DatanodeVolumeInfo> getVolumeReport() throws IOException {\n     checkSuperuserPrivilege();\n-    Preconditions.checkNotNull(data, \"Storage not yet initialized\");\n+    checkStorageState(\"getVolumeReport\");\n     Map<String, Object> volumeInfoMap = data.getVolumeInfoMap();\n     if (volumeInfoMap == null) {\n       LOG.warn(\"DataNode volume info not available.\");",
      "parent_sha": "c249db80c25e277629865f2e9fd89496e744e7ef"
    }
  },
  {
    "oid": "ad0cff2f973edb927130f4d206fcb1ecdc079e60",
    "message": "HADOOP-18592. Sasl connection failure should log remote address. (#5294)\n\nContributed by Viraj Jasani <vjasani@apache.org>\r\n\r\nSigned-off-by: Chris Nauroth <cnauroth@apache.org>\r\nSigned-off-by: Steve Loughran <stevel@apache.org>\r\nSigned-off-by: Mingliang Liu <liuml07@apache.org>",
    "date": "2023-02-01T18:15:20Z",
    "url": "https://github.com/apache/hadoop/commit/ad0cff2f973edb927130f4d206fcb1ecdc079e60",
    "details": {
      "sha": "c0f90d98bc6b4b5e457c670cdce6f6ed6bbbfc94",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java",
      "status": "modified",
      "additions": 9,
      "deletions": 10,
      "changes": 19,
      "blob_url": "https://github.com/apache/hadoop/blob/ad0cff2f973edb927130f4d206fcb1ecdc079e60/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FClient.java",
      "raw_url": "https://github.com/apache/hadoop/raw/ad0cff2f973edb927130f4d206fcb1ecdc079e60/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FClient.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FClient.java?ref=ad0cff2f973edb927130f4d206fcb1ecdc079e60",
      "patch": "@@ -704,7 +704,7 @@ private synchronized void setupConnection(\n      * handle that, a relogin is attempted.\n      */\n     private synchronized void handleSaslConnectionFailure(\n-        final int currRetries, final int maxRetries, final Exception ex,\n+        final int currRetries, final int maxRetries, final IOException ex,\n         final Random rand, final UserGroupInformation ugi) throws IOException,\n         InterruptedException {\n       ugi.doAs(new PrivilegedExceptionAction<Object>() {\n@@ -715,10 +715,7 @@ public Object run() throws IOException, InterruptedException {\n           disposeSasl();\n           if (shouldAuthenticateOverKrb()) {\n             if (currRetries < maxRetries) {\n-              if(LOG.isDebugEnabled()) {\n-                LOG.debug(\"Exception encountered while connecting to \"\n-                    + \"the server : \" + ex);\n-              }\n+              LOG.debug(\"Exception encountered while connecting to the server {}\", remoteId, ex);\n               // try re-login\n               if (UserGroupInformation.isLoginKeytabBased()) {\n                 UserGroupInformation.getLoginUser().reloginFromKeytab();\n@@ -736,19 +733,21 @@ public Object run() throws IOException, InterruptedException {\n                   + UserGroupInformation.getLoginUser().getUserName() + \" to \"\n                   + remoteId;\n               LOG.warn(msg, ex);\n-              throw (IOException) new IOException(msg).initCause(ex);\n+              throw NetUtils.wrapException(remoteId.getAddress().getHostName(),\n+                  remoteId.getAddress().getPort(),\n+                  NetUtils.getHostname(),\n+                  0,\n+                  ex);\n             }\n           } else {\n             // With RequestHedgingProxyProvider, one rpc call will send multiple\n             // requests to all namenodes. After one request return successfully,\n             // all other requests will be interrupted. It's not a big problem,\n             // and should not print a warning log.\n             if (ex instanceof InterruptedIOException) {\n-              LOG.debug(\"Exception encountered while connecting to the server\",\n-                  ex);\n+              LOG.debug(\"Exception encountered while connecting to the server {}\", remoteId, ex);\n             } else {\n-              LOG.warn(\"Exception encountered while connecting to the server \",\n-                  ex);\n+              LOG.warn(\"Exception encountered while connecting to the server {}\", remoteId, ex);\n             }\n           }\n           if (ex instanceof RemoteException)",
      "parent_sha": "6d325d9d09cf8595e1e39e39128abd98dea75220"
    }
  },
  {
    "oid": "7030722e5d9f376245a9ab0a6a883538b6c55f82",
    "message": "HDFS-15080. Fix the issue in reading persistent memory cached data with an offset. Contributed by Feilong He.",
    "date": "2020-01-08T08:55:17Z",
    "url": "https://github.com/apache/hadoop/commit/7030722e5d9f376245a9ab0a6a883538b6c55f82",
    "details": {
      "sha": "0feb3b78aca785b8498c9684723d5c6a366e2b56",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/7030722e5d9f376245a9ab0a6a883538b6c55f82/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2Ffsdataset%2Fimpl%2FFsDatasetImpl.java",
      "raw_url": "https://github.com/apache/hadoop/raw/7030722e5d9f376245a9ab0a6a883538b6c55f82/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2Ffsdataset%2Fimpl%2FFsDatasetImpl.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2Ffsdataset%2Fimpl%2FFsDatasetImpl.java?ref=7030722e5d9f376245a9ab0a6a883538b6c55f82",
      "patch": "@@ -822,7 +822,7 @@ private InputStream getBlockInputStreamWithCheckingPmemCache(\n       if (addr != -1) {\n         LOG.debug(\"Get InputStream by cache address.\");\n         return FsDatasetUtil.getDirectInputStream(\n-            addr, info.getBlockDataLength());\n+            addr + seekOffset, info.getBlockDataLength() - seekOffset);\n       }\n       LOG.debug(\"Get InputStream by cache file path.\");\n       return FsDatasetUtil.getInputStreamAndSeek(",
      "parent_sha": "aba3f6c3e1fbb150ea7ff0411c41ffd3a2796208"
    }
  },
  {
    "oid": "83cc7263af632939dc3b2ee58d8f03f98ed4d96a",
    "message": "YARN-5722. FairScheduler hides group resolution exceptions when assigning queue (Contributed by Wilfred Spiegelenburg via Daniel Templeton)",
    "date": "2016-11-22T19:16:00Z",
    "url": "https://github.com/apache/hadoop/commit/83cc7263af632939dc3b2ee58d8f03f98ed4d96a",
    "details": {
      "sha": "354f93603a5fbdd6980c02f021672a3afd60e5fc",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/hadoop/blob/83cc7263af632939dc3b2ee58d8f03f98ed4d96a/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Ffair%2FFairScheduler.java",
      "raw_url": "https://github.com/apache/hadoop/raw/83cc7263af632939dc3b2ee58d8f03f98ed4d96a/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Ffair%2FFairScheduler.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Ffair%2FFairScheduler.java?ref=83cc7263af632939dc3b2ee58d8f03f98ed4d96a",
      "patch": "@@ -767,7 +767,8 @@ FSLeafQueue assignToQueue(RMApp rmApp, String queueName, String user) {\n     } catch (InvalidQueueNameException qne) {\n       appRejectMsg = qne.getMessage();\n     } catch (IOException ioe) {\n-      appRejectMsg = \"Error assigning app to queue \" + queueName;\n+      // IOException should only happen for a user without groups\n+      appRejectMsg = \"Error assigning app to a queue: \" + ioe.getMessage();\n     }\n \n     if (appRejectMsg != null && rmApp != null) {",
      "parent_sha": "613b902b9808c1c679674047cff15feade01dfab"
    }
  },
  {
    "oid": "06e836cd57c1db55215222be439d2239dac069fc",
    "message": "HADOOP-17501. Fix logging typo in ShutdownHookManager. Contributed by Fengnan Li.",
    "date": "2021-01-31T11:45:40Z",
    "url": "https://github.com/apache/hadoop/commit/06e836cd57c1db55215222be439d2239dac069fc",
    "details": {
      "sha": "f044295a8068d2af48c7c6d7bfa81ba6c2824933",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ShutdownHookManager.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/hadoop/blob/06e836cd57c1db55215222be439d2239dac069fc/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Futil%2FShutdownHookManager.java",
      "raw_url": "https://github.com/apache/hadoop/raw/06e836cd57c1db55215222be439d2239dac069fc/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Futil%2FShutdownHookManager.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Futil%2FShutdownHookManager.java?ref=06e836cd57c1db55215222be439d2239dac069fc",
      "patch": "@@ -147,14 +147,14 @@ private static void shutdownExecutor(final Configuration conf) {\n           shutdownTimeout,\n           TIME_UNIT_DEFAULT)) {\n         // timeout waiting for the\n-        LOG.error(\"ShutdownHookManger shutdown forcefully after\"\n+        LOG.error(\"ShutdownHookManager shutdown forcefully after\"\n             + \" {} seconds.\", shutdownTimeout);\n         EXECUTOR.shutdownNow();\n       }\n-      LOG.debug(\"ShutdownHookManger completed shutdown.\");\n+      LOG.debug(\"ShutdownHookManager completed shutdown.\");\n     } catch (InterruptedException ex) {\n       // interrupted.\n-      LOG.error(\"ShutdownHookManger interrupted while waiting for \" +\n+      LOG.error(\"ShutdownHookManager interrupted while waiting for \" +\n           \"termination.\", ex);\n       EXECUTOR.shutdownNow();\n       Thread.currentThread().interrupt();",
      "parent_sha": "ad483fd66e87a734ea985016e5dec409e1c72c99"
    }
  },
  {
    "oid": "9cfe9ccd2627571452ac2ac55a40d1f7ded07eb2",
    "message": "HDFS-17119. RBF: Logger fix for StateStoreMySQLImpl. (#5882). Contributed by Zhaohui Wang.\n\nReviewed-by: Simbarashe Dzinamarira <sdzinamarira@linkedin.com>\r\nSigned-off-by: Ayush Saxena <ayushsaxena@apache.org>",
    "date": "2023-07-24T11:56:23Z",
    "url": "https://github.com/apache/hadoop/commit/9cfe9ccd2627571452ac2ac55a40d1f7ded07eb2",
    "details": {
      "sha": "43d65e4023e32bde60fa5d0f76c66f091baf922a",
      "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/store/driver/impl/StateStoreMySQLImpl.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/9cfe9ccd2627571452ac2ac55a40d1f7ded07eb2/hadoop-hdfs-project%2Fhadoop-hdfs-rbf%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Ffederation%2Fstore%2Fdriver%2Fimpl%2FStateStoreMySQLImpl.java",
      "raw_url": "https://github.com/apache/hadoop/raw/9cfe9ccd2627571452ac2ac55a40d1f7ded07eb2/hadoop-hdfs-project%2Fhadoop-hdfs-rbf%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Ffederation%2Fstore%2Fdriver%2Fimpl%2FStateStoreMySQLImpl.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs-rbf%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Ffederation%2Fstore%2Fdriver%2Fimpl%2FStateStoreMySQLImpl.java?ref=9cfe9ccd2627571452ac2ac55a40d1f7ded07eb2",
      "patch": "@@ -67,7 +67,7 @@ public class StateStoreMySQLImpl extends StateStoreSerializableImpl {\n       SQL_STATE_STORE_CONF_PREFIX + \"connection.driver\";\n \n   private static final Logger LOG =\n-      LoggerFactory.getLogger(StateStoreSerializableImpl.class);\n+      LoggerFactory.getLogger(StateStoreMySQLImpl.class);\n   private SQLConnectionFactory connectionFactory;\n   /** If the driver has been initialized. */\n   private boolean initialized = false;",
      "parent_sha": "49c98da83850a9798608c2015119e20095b84b5b"
    }
  },
  {
    "oid": "fd264b826576b67adb04586002c3f94b7ea5a2f1",
    "message": "HDFS-14995. Use log variable directly instead of passing as argument in InvalidateBlocks#printBlockDeletionTime(). Contributed by Lisheng Sun.",
    "date": "2019-11-20T18:30:42Z",
    "url": "https://github.com/apache/hadoop/commit/fd264b826576b67adb04586002c3f94b7ea5a2f1",
    "details": {
      "sha": "bbe729c3332cc436761d6c23c2920cb638cfae61",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/InvalidateBlocks.java",
      "status": "modified",
      "additions": 4,
      "deletions": 6,
      "changes": 10,
      "blob_url": "https://github.com/apache/hadoop/blob/fd264b826576b67adb04586002c3f94b7ea5a2f1/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fblockmanagement%2FInvalidateBlocks.java",
      "raw_url": "https://github.com/apache/hadoop/raw/fd264b826576b67adb04586002c3f94b7ea5a2f1/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fblockmanagement%2FInvalidateBlocks.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fblockmanagement%2FInvalidateBlocks.java?ref=fd264b826576b67adb04586002c3f94b7ea5a2f1",
      "patch": "@@ -41,8 +41,6 @@\n \n import com.google.common.annotations.VisibleForTesting;\n \n-import org.slf4j.Logger;\n-\n /**\n  * Keeps a Collection for every named machine containing blocks\n  * that have recently been invalidated and are thought to live\n@@ -72,17 +70,17 @@ class InvalidateBlocks {\n     this.blockInvalidateLimit = blockInvalidateLimit;\n     this.pendingPeriodInMs = pendingPeriodInMs;\n     this.blockIdManager = blockIdManager;\n-    printBlockDeletionTime(BlockManager.LOG);\n+    printBlockDeletionTime();\n   }\n \n-  private void printBlockDeletionTime(final Logger log) {\n-    log.info(\"{} is set to {}\",\n+  private void printBlockDeletionTime() {\n+    BlockManager.LOG.info(\"{} is set to {}\",\n         DFSConfigKeys.DFS_NAMENODE_STARTUP_DELAY_BLOCK_DELETION_SEC_KEY,\n         DFSUtil.durationToString(pendingPeriodInMs));\n     SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy MMM dd HH:mm:ss\");\n     Calendar calendar = new GregorianCalendar();\n     calendar.add(Calendar.SECOND, (int) (this.pendingPeriodInMs / 1000));\n-    log.info(\"The block deletion will start around {}\",\n+    BlockManager.LOG.info(\"The block deletion will start around {}\",\n         sdf.format(calendar.getTime()));\n   }\n ",
      "parent_sha": "be77231452baee2c3dd68f7c0991411cae2eea1b"
    }
  },
  {
    "oid": "d0422a74ea41f53d3dabf6b558f30e3d3dac9b6d",
    "message": "HDFS-10932. Ozone: fix XceiverClient slow shutdown. Contributed by Chen Liang.",
    "date": "2018-04-25T22:53:21Z",
    "url": "https://github.com/apache/hadoop/commit/d0422a74ea41f53d3dabf6b558f30e3d3dac9b6d",
    "details": {
      "sha": "d25119b052c405ed92d45ce590a73a876884b8e6",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/ozone/container/common/transport/client/XceiverClient.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/hadoop/blob/d0422a74ea41f53d3dabf6b558f30e3d3dac9b6d/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Ftransport%2Fclient%2FXceiverClient.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d0422a74ea41f53d3dabf6b558f30e3d3dac9b6d/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Ftransport%2Fclient%2FXceiverClient.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fcontainer%2Fcommon%2Ftransport%2Fclient%2FXceiverClient.java?ref=d0422a74ea41f53d3dabf6b558f30e3d3dac9b6d",
      "patch": "@@ -36,6 +36,7 @@\n \n import java.io.Closeable;\n import java.io.IOException;\n+import java.util.concurrent.TimeUnit;\n \n /**\n  * A Client for the storageContainer protocol.\n@@ -96,7 +97,7 @@ public void connect() throws Exception {\n   @Override\n   public void close() {\n     if(group != null) {\n-      group.shutdownGracefully();\n+      group.shutdownGracefully(0, 0, TimeUnit.SECONDS);\n     }\n \n     if (channelFuture != null) {",
      "parent_sha": "4c95c3d6ab4ea2332119d9314693903709983417"
    }
  },
  {
    "oid": "b451889e8e83f7977f2b76789c61e823e2d40487",
    "message": "YARN-8011. TestOpportunisticContainerAllocatorAMService#testContainerPromoteAndDemoteBeforeContainerStart fails intermittently. Contributed by Tao Yang.",
    "date": "2018-03-08T10:13:36Z",
    "url": "https://github.com/apache/hadoop/commit/b451889e8e83f7977f2b76789c61e823e2d40487",
    "details": {
      "sha": "efa76bcf9201dee522c4c96f0f5142bb276bc443",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java",
      "status": "modified",
      "additions": 15,
      "deletions": 14,
      "changes": 29,
      "blob_url": "https://github.com/apache/hadoop/blob/b451889e8e83f7977f2b76789c61e823e2d40487/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FTestOpportunisticContainerAllocatorAMService.java",
      "raw_url": "https://github.com/apache/hadoop/raw/b451889e8e83f7977f2b76789c61e823e2d40487/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FTestOpportunisticContainerAllocatorAMService.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FTestOpportunisticContainerAllocatorAMService.java?ref=b451889e8e83f7977f2b76789c61e823e2d40487",
      "patch": "@@ -243,13 +243,13 @@ public void testContainerPromoteAndDemoteBeforeContainerStart() throws Exception\n             null, ExecutionType.GUARANTEED)));\n     // Node on same host should not result in allocation\n     sameHostDiffNode.nodeHeartbeat(true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n     allocateResponse =  am1.allocate(new ArrayList<>(), new ArrayList<>());\n     Assert.assertEquals(0, allocateResponse.getUpdatedContainers().size());\n \n     // Wait for scheduler to process all events\n     dispatcher.waitForEventThreadToWait();\n-    Thread.sleep(1000);\n+    rm.drainEvents();\n     // Verify Metrics After OPP allocation (Nothing should change again)\n     verifyMetrics(metrics, 15360, 15, 1024, 1, 1);\n \n@@ -286,7 +286,7 @@ public void testContainerPromoteAndDemoteBeforeContainerStart() throws Exception\n \n     // Ensure after correct node heartbeats, we should get the allocation\n     allocNode.nodeHeartbeat(true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n     allocateResponse =  am1.allocate(new ArrayList<>(), new ArrayList<>());\n     Assert.assertEquals(1, allocateResponse.getUpdatedContainers().size());\n     Container uc =\n@@ -303,7 +303,7 @@ public void testContainerPromoteAndDemoteBeforeContainerStart() throws Exception\n     nm2.nodeHeartbeat(true);\n     nm3.nodeHeartbeat(true);\n     nm4.nodeHeartbeat(true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n \n     // Verify that the container is still in ACQUIRED state wrt the RM.\n     RMContainer rmContainer = ((CapacityScheduler) scheduler)\n@@ -325,6 +325,7 @@ public void testContainerPromoteAndDemoteBeforeContainerStart() throws Exception\n \n     // Wait for scheduler to finish processing events\n     dispatcher.waitForEventThreadToWait();\n+    rm.drainEvents();\n     // Verify Metrics After OPP allocation :\n     // Everything should have reverted to what it was\n     verifyMetrics(metrics, 15360, 15, 1024, 1, 1);\n@@ -396,7 +397,7 @@ public void testContainerPromoteAfterContainerStart() throws Exception {\n         ContainerStatus.newInstance(container.getId(),\n             ExecutionType.OPPORTUNISTIC, ContainerState.RUNNING, \"\", 0)),\n         true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n \n     // Verify that container is actually running wrt the RM..\n     RMContainer rmContainer = ((CapacityScheduler) scheduler)\n@@ -434,7 +435,7 @@ public void testContainerPromoteAfterContainerStart() throws Exception {\n         ContainerStatus.newInstance(container.getId(),\n             ExecutionType.OPPORTUNISTIC, ContainerState.RUNNING, \"\", 0)),\n         true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n \n     allocateResponse =  am1.allocate(new ArrayList<>(), new ArrayList<>());\n     Assert.assertEquals(1, allocateResponse.getUpdatedContainers().size());\n@@ -521,7 +522,7 @@ public void testContainerPromoteAfterContainerComplete() throws Exception {\n         ContainerStatus.newInstance(container.getId(),\n             ExecutionType.OPPORTUNISTIC, ContainerState.RUNNING, \"\", 0)),\n         true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n \n     // Verify that container is actually running wrt the RM..\n     RMContainer rmContainer = ((CapacityScheduler) scheduler)\n@@ -535,7 +536,7 @@ public void testContainerPromoteAfterContainerComplete() throws Exception {\n         ContainerStatus.newInstance(container.getId(),\n             ExecutionType.OPPORTUNISTIC, ContainerState.COMPLETE, \"\", 0)),\n         true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n \n     // Verify that container has been removed..\n     rmContainer = ((CapacityScheduler) scheduler)\n@@ -618,7 +619,7 @@ public void testContainerAutoUpdateContainer() throws Exception {\n     nm1.nodeHeartbeat(Arrays.asList(ContainerStatus\n         .newInstance(container.getId(), ExecutionType.OPPORTUNISTIC,\n             ContainerState.RUNNING, \"\", 0)), true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n \n     // Verify that container is actually running wrt the RM..\n     RMContainer rmContainer = ((CapacityScheduler) scheduler)\n@@ -636,7 +637,7 @@ public void testContainerAutoUpdateContainer() throws Exception {\n     nm1.nodeHeartbeat(Arrays.asList(ContainerStatus\n         .newInstance(container.getId(), ExecutionType.OPPORTUNISTIC,\n             ContainerState.RUNNING, \"\", 0)), true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n     // Get the update response on next allocate\n     allocateResponse = am1.allocate(new ArrayList<>(), new ArrayList<>());\n     // Check the update response from YARNRM\n@@ -662,7 +663,7 @@ public void testContainerAutoUpdateContainer() throws Exception {\n         .newInstance(container.getId(), ExecutionType.GUARANTEED,\n             ContainerState.RUNNING, \"\", 0)), true);\n \n-    Thread.sleep(200);\n+    rm.drainEvents();\n     if (allocateResponse.getUpdatedContainers().size() == 0) {\n       allocateResponse = am1.allocate(new ArrayList<>(), new ArrayList<>());\n     }\n@@ -671,7 +672,7 @@ public void testContainerAutoUpdateContainer() throws Exception {\n     Assert.assertEquals(container.getId(), uc.getContainer().getId());\n     Assert.assertEquals(Resource.newInstance(2 * GB, 1),\n         uc.getContainer().getResource());\n-    Thread.sleep(1000);\n+    rm.drainEvents();\n \n     // Check that the container resources are increased in\n     // NM through NM heartbeat response\n@@ -688,7 +689,7 @@ public void testContainerAutoUpdateContainer() throws Exception {\n             ContainerUpdateType.DECREASE_RESOURCE,\n             Resources.createResource(1 * GB, 1), null)));\n     Assert.assertEquals(1, allocateResponse.getUpdatedContainers().size());\n-    Thread.sleep(1000);\n+    rm.drainEvents();\n \n     // Check that the container resources are decreased\n     // in NM through NM heartbeat response\n@@ -707,7 +708,7 @@ public void testContainerAutoUpdateContainer() throws Exception {\n     response = nm1.nodeHeartbeat(Arrays.asList(ContainerStatus\n         .newInstance(container.getId(), ExecutionType.GUARANTEED,\n             ContainerState.RUNNING, \"\", 0)), true);\n-    Thread.sleep(200);\n+    rm.drainEvents();\n     if (allocateResponse.getUpdatedContainers().size() == 0) {\n       // Get the update response on next allocate\n       allocateResponse = am1.allocate(new ArrayList<>(), new ArrayList<>());",
      "parent_sha": "4cc9a6d9bb34329d6de30706d5432c7cb675bb88"
    }
  },
  {
    "oid": "1f324e966136f69c3254f01b43b56cf077abba7f",
    "message": "HDFS-13631. TestDFSAdmin#testCheckNumOfBlocksInReportCommand should use a separate MiniDFSCluster path. Contributed by Anbang Hu.",
    "date": "2018-05-31T17:12:38Z",
    "url": "https://github.com/apache/hadoop/commit/1f324e966136f69c3254f01b43b56cf077abba7f",
    "details": {
      "sha": "647327cc3311e7379084122e8ddffa16187244c9",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdmin.java",
      "status": "modified",
      "additions": 29,
      "deletions": 30,
      "changes": 59,
      "blob_url": "https://github.com/apache/hadoop/blob/1f324e966136f69c3254f01b43b56cf077abba7f/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Ftools%2FTestDFSAdmin.java",
      "raw_url": "https://github.com/apache/hadoop/raw/1f324e966136f69c3254f01b43b56cf077abba7f/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Ftools%2FTestDFSAdmin.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Ftools%2FTestDFSAdmin.java?ref=1f324e966136f69c3254f01b43b56cf077abba7f",
      "patch": "@@ -110,11 +110,15 @@ public class TestDFSAdmin {\n   private static final PrintStream OLD_OUT = System.out;\n   private static final PrintStream OLD_ERR = System.err;\n   private String tempResource = null;\n+  private static final int NUM_DATANODES = 2;\n \n   @Before\n   public void setUp() throws Exception {\n     conf = new Configuration();\n     conf.setInt(IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 3);\n+    conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 512);\n+    conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,\n+        GenericTestUtils.getRandomizedTempPath());\n     restartCluster();\n \n     admin = new DFSAdmin(conf);\n@@ -157,7 +161,8 @@ private void restartCluster() throws IOException {\n     if (cluster != null) {\n       cluster.shutdown();\n     }\n-    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();\n+    cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(NUM_DATANODES).build();\n     cluster.waitActive();\n     datanode = cluster.getDataNodes().get(0);\n     namenode = cluster.getNameNode();\n@@ -904,40 +909,34 @@ public void testSetBalancerBandwidth() throws Exception {\n \n   @Test(timeout = 300000L)\n   public void testCheckNumOfBlocksInReportCommand() throws Exception {\n-    Configuration config = new Configuration();\n-    config.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 512);\n-    config.set(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY, \"3s\");\n-\n-    int numOfDatanodes = 1;\n-    MiniDFSCluster miniDFSCluster = new MiniDFSCluster.Builder(config)\n-        .numDataNodes(numOfDatanodes).build();\n-    try {\n-      miniDFSCluster.waitActive();\n-      DistributedFileSystem dfs = miniDFSCluster.getFileSystem();\n-      Path path= new Path(\"/tmp.txt\");\n-\n-      DatanodeInfo[] dn = dfs.getDataNodeStats();\n-      assertEquals(dn.length, numOfDatanodes);\n-      //Block count should be 0, as no files are created\n-      assertEquals(dn[0].getNumBlocks(), 0);\n-\n-\n-      //Create a file with 2 blocks\n-      DFSTestUtil.createFile(dfs, path, 1024, (short) 1, 0);\n-      int expectedBlockCount = 2;\n+    DistributedFileSystem dfs = cluster.getFileSystem();\n+    Path path = new Path(\"/tmp.txt\");\n+\n+    DatanodeInfo[] dn = dfs.getDataNodeStats();\n+    assertEquals(dn.length, NUM_DATANODES);\n+    // Block count should be 0, as no files are created\n+    int actualBlockCount = 0;\n+    for (DatanodeInfo d : dn) {\n+      actualBlockCount += d.getNumBlocks();\n+    }\n+    assertEquals(0, actualBlockCount);\n \n-      //Wait for One Heartbeat\n-      Thread.sleep(3 * 1000);\n+    // Create a file with 2 blocks\n+    DFSTestUtil.createFile(dfs, path, 1024, (short) 1, 0);\n+    int expectedBlockCount = 2;\n \n-      dn = dfs.getDataNodeStats();\n-      assertEquals(dn.length, numOfDatanodes);\n+    // Wait for One Heartbeat\n+    Thread.sleep(3 * 1000);\n \n-      //Block count should be 2, as file is created with block count 2\n-      assertEquals(dn[0].getNumBlocks(), expectedBlockCount);\n+    dn = dfs.getDataNodeStats();\n+    assertEquals(dn.length, NUM_DATANODES);\n \n-    } finally {\n-      cluster.shutdown();\n+    // Block count should be 2, as file is created with block count 2\n+    actualBlockCount = 0;\n+    for (DatanodeInfo d : dn) {\n+      actualBlockCount += d.getNumBlocks();\n     }\n+    assertEquals(expectedBlockCount, actualBlockCount);\n   }\n \n   @Test",
      "parent_sha": "1361030e59d7557a2bffac0ea8df116ce2eaae4a"
    }
  },
  {
    "oid": "b2c596cdda7c129951074bc53b4b9ecfedbf080a",
    "message": "HADOOP-13287. TestS3ACredentials#testInstantiateFromURL fails if AWS secret key contains +. Contributed by Chris Nauroth.",
    "date": "2016-06-21T18:28:52Z",
    "url": "https://github.com/apache/hadoop/commit/b2c596cdda7c129951074bc53b4b9ecfedbf080a",
    "details": {
      "sha": "8cb7c0f0b1a6b03a3baa12a29d0b11c75d262d9c",
      "filename": "hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3ACredentialsInURL.java",
      "status": "modified",
      "additions": 9,
      "deletions": 7,
      "changes": 16,
      "blob_url": "https://github.com/apache/hadoop/blob/b2c596cdda7c129951074bc53b4b9ecfedbf080a/hadoop-tools%2Fhadoop-aws%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3a%2FTestS3ACredentialsInURL.java",
      "raw_url": "https://github.com/apache/hadoop/raw/b2c596cdda7c129951074bc53b4b9ecfedbf080a/hadoop-tools%2Fhadoop-aws%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3a%2FTestS3ACredentialsInURL.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools%2Fhadoop-aws%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3a%2FTestS3ACredentialsInURL.java?ref=b2c596cdda7c129951074bc53b4b9ecfedbf080a",
      "patch": "@@ -33,7 +33,6 @@\n import java.io.IOException;\n import java.io.UnsupportedEncodingException;\n import java.net.URI;\n-import java.net.URISyntaxException;\n import java.net.URLEncoder;\n import java.nio.file.AccessDeniedException;\n \n@@ -73,7 +72,11 @@ public void testInstantiateFromURL() throws Throwable {\n         accessKey, secretKey);\n     if (secretKey.contains(\"/\")) {\n       assertTrue(\"test URI encodes the / symbol\", secretsURI.toString().\n-          contains(\"%2F\"));\n+          contains(\"%252F\"));\n+    }\n+    if (secretKey.contains(\"+\")) {\n+      assertTrue(\"test URI encodes the + symbol\", secretsURI.toString().\n+          contains(\"%252B\"));\n     }\n     assertFalse(\"Does not contain secrets\", original.equals(secretsURI));\n \n@@ -132,8 +135,7 @@ public void testInvalidCredentialsFail() throws Throwable {\n \n   private URI createUriWithEmbeddedSecrets(URI original,\n       String accessKey,\n-      String secretKey) throws URISyntaxException,\n-      UnsupportedEncodingException {\n+      String secretKey) throws UnsupportedEncodingException {\n     String encodedSecretKey = URLEncoder.encode(secretKey, \"UTF-8\");\n     String formattedString = String.format(\"%s://%s:%s@%s/%s/\",\n         original.getScheme(),\n@@ -143,10 +145,10 @@ private URI createUriWithEmbeddedSecrets(URI original,\n         original.getPath());\n     URI testURI;\n     try {\n-      testURI = new URI(formattedString);\n-    } catch (URISyntaxException e) {\n+      testURI = new Path(formattedString).toUri();\n+    } catch (IllegalArgumentException e) {\n       // inner cause is stripped to keep any secrets out of stack traces\n-      throw new URISyntaxException(\"\", \"Could not encode URI\");\n+      throw new IllegalArgumentException(\"Could not encode Path\");\n     }\n     return testURI;\n   }",
      "parent_sha": "605b4b61364781fc99ed27035c793153a20d8f71"
    }
  },
  {
    "oid": "9a19d6d48b463d4e2a598f0791d05fd2006a597c",
    "message": "HDDS-1039. OzoneManager fails to connect with secure SCM. Contributed by Ajay Kumar",
    "date": "2019-02-05T00:30:07Z",
    "url": "https://github.com/apache/hadoop/commit/9a19d6d48b463d4e2a598f0791d05fd2006a597c",
    "details": {
      "sha": "1ba698bf0e30a0e7cac1a5d6bddbc2f0dfe4b9d7",
      "filename": "hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/protocolPB/ScmBlockLocationProtocolPB.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/9a19d6d48b463d4e2a598f0791d05fd2006a597c/hadoop-hdds%2Fcommon%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdds%2Fscm%2FprotocolPB%2FScmBlockLocationProtocolPB.java",
      "raw_url": "https://github.com/apache/hadoop/raw/9a19d6d48b463d4e2a598f0791d05fd2006a597c/hadoop-hdds%2Fcommon%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdds%2Fscm%2FprotocolPB%2FScmBlockLocationProtocolPB.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdds%2Fcommon%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdds%2Fscm%2FprotocolPB%2FScmBlockLocationProtocolPB.java?ref=9a19d6d48b463d4e2a598f0791d05fd2006a597c",
      "patch": "@@ -29,7 +29,7 @@\n  * Protocol Buffers service interface to add Hadoop-specific annotations.\n  */\n @ProtocolInfo(protocolName =\n-    \"org.apache.hadoop.ozone.protocol.ScmBlockLocationProtocol\",\n+    \"org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocol\",\n     protocolVersion = 1)\n @InterfaceAudience.Private\n @KerberosInfo(",
      "parent_sha": "5f15a60e329b8d0a495a977dc5255126814c5271"
    }
  },
  {
    "oid": "4cdbdce752e192b45c2b9756c2d4bd24ceffdabd",
    "message": "HADOOP-14841 Kms client should disconnect if unable to get output stream from connection. Contributed by Rushabh S Shah",
    "date": "2018-05-04T20:36:13Z",
    "url": "https://github.com/apache/hadoop/commit/4cdbdce752e192b45c2b9756c2d4bd24ceffdabd",
    "details": {
      "sha": "45097efae918e0c2128c26ac49d0d7c21c841123",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
      "status": "modified",
      "additions": 11,
      "deletions": 2,
      "changes": 13,
      "blob_url": "https://github.com/apache/hadoop/blob/4cdbdce752e192b45c2b9756c2d4bd24ceffdabd/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2FKMSClientProvider.java",
      "raw_url": "https://github.com/apache/hadoop/raw/4cdbdce752e192b45c2b9756c2d4bd24ceffdabd/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2FKMSClientProvider.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fcrypto%2Fkey%2Fkms%2FKMSClientProvider.java?ref=4cdbdce752e192b45c2b9756c2d4bd24ceffdabd",
      "patch": "@@ -447,12 +447,21 @@ private <T> T call(HttpURLConnection conn, Object jsonOutput,\n       int expectedResponse, Class<T> klass, int authRetryCount)\n       throws IOException {\n     T ret = null;\n+    OutputStream os = null;\n     try {\n       if (jsonOutput != null) {\n-        writeJson(jsonOutput, conn.getOutputStream());\n+        os = conn.getOutputStream();\n+        writeJson(jsonOutput, os);\n       }\n     } catch (IOException ex) {\n-      IOUtils.closeStream(conn.getInputStream());\n+      // The payload is not serialized if getOutputStream fails.\n+      // Calling getInputStream will issue the put/post request with no payload\n+      // which causes HTTP 500 server error.\n+      if (os == null) {\n+        conn.disconnect();\n+      } else {\n+        IOUtils.closeStream(conn.getInputStream());\n+      }\n       throw ex;\n     }\n     if ((conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN",
      "parent_sha": "96c843f64bb424cd7544be0ccda16a6755c086de"
    }
  },
  {
    "oid": "d401e63b6c3695d1f8f3f4958b8d592b15342b17",
    "message": "MAPREDUCE-6338. MR AppMaster does not honor ephemeral port range. Contributed by Frank Nguyen.",
    "date": "2017-02-06T03:28:01Z",
    "url": "https://github.com/apache/hadoop/commit/d401e63b6c3695d1f8f3f4958b8d592b15342b17",
    "details": {
      "sha": "5669f3e2b963b908eb97931f2af73e952762ea91",
      "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
      "status": "modified",
      "additions": 8,
      "deletions": 9,
      "changes": 17,
      "blob_url": "https://github.com/apache/hadoop/blob/d401e63b6c3695d1f8f3f4958b8d592b15342b17/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTaskAttemptListenerImpl.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d401e63b6c3695d1f8f3f4958b8d592b15342b17/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTaskAttemptListenerImpl.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTaskAttemptListenerImpl.java?ref=d401e63b6c3695d1f8f3f4958b8d592b15342b17",
      "patch": "@@ -134,15 +134,14 @@ protected void registerHeartbeatHandler(Configuration conf) {\n   protected void startRpcServer() {\n     Configuration conf = getConfig();\n     try {\n-      server = \n-          new RPC.Builder(conf).setProtocol(TaskUmbilicalProtocol.class)\n-            .setInstance(this).setBindAddress(\"0.0.0.0\")\n-            .setPort(0).setNumHandlers(\n-                conf.getInt(MRJobConfig.MR_AM_TASK_LISTENER_THREAD_COUNT, \n-                    MRJobConfig.DEFAULT_MR_AM_TASK_LISTENER_THREAD_COUNT))\n-                    .setVerbose(false).setSecretManager(jobTokenSecretManager)\n-                    .build();\n-      \n+      server = new RPC.Builder(conf).setProtocol(TaskUmbilicalProtocol.class)\n+          .setInstance(this).setBindAddress(\"0.0.0.0\")\n+          .setPortRangeConfig(MRJobConfig.MR_AM_JOB_CLIENT_PORT_RANGE)\n+          .setNumHandlers(\n+          conf.getInt(MRJobConfig.MR_AM_TASK_LISTENER_THREAD_COUNT, \n+          MRJobConfig.DEFAULT_MR_AM_TASK_LISTENER_THREAD_COUNT))\n+          .setVerbose(false).setSecretManager(jobTokenSecretManager).build();\n+\n       // Enable service authorization?\n       if (conf.getBoolean(\n           CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, ",
      "parent_sha": "3ea6d3517e2f0448d91af73508ec9d10c4a3de21"
    }
  },
  {
    "oid": "1d4d0fcbe1e352b95c93dc016d3674db1126fb25",
    "message": "HDFS-15204. TestRetryCacheWithHA testRemoveCacheDescriptor fails intermittently. Contributed by Ahmed Hussein.",
    "date": "2020-03-04T18:02:54Z",
    "url": "https://github.com/apache/hadoop/commit/1d4d0fcbe1e352b95c93dc016d3674db1126fb25",
    "details": {
      "sha": "90742479c162a57e5de0dd543bd4f1859076f0d4",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java",
      "status": "modified",
      "additions": 40,
      "deletions": 33,
      "changes": 73,
      "blob_url": "https://github.com/apache/hadoop/blob/1d4d0fcbe1e352b95c93dc016d3674db1126fb25/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2Fha%2FTestRetryCacheWithHA.java",
      "raw_url": "https://github.com/apache/hadoop/raw/1d4d0fcbe1e352b95c93dc016d3674db1126fb25/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2Fha%2FTestRetryCacheWithHA.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2Fha%2FTestRetryCacheWithHA.java?ref=1d4d0fcbe1e352b95c93dc016d3674db1126fb25",
      "patch": "@@ -34,8 +34,10 @@\n import java.util.Map;\n import java.util.Random;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n+import org.apache.hadoop.test.GenericTestUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import org.apache.hadoop.conf.Configuration;\n@@ -1296,7 +1298,7 @@ public void testRemoveXAttr() throws Exception {\n    */\n   public void testClientRetryWithFailover(final AtMostOnceOp op)\n       throws Exception {\n-    final Map<String, Object> results = new HashMap<String, Object>();\n+    final Map<String, Object> results = new ConcurrentHashMap<>();\n     \n     op.prepare();\n     // set DummyRetryInvocationHandler#block to true\n@@ -1309,10 +1311,7 @@ public void run() {\n           op.invoke();\n           Object result = op.getResult();\n           LOG.info(\"Operation \" + op.name + \" finished\");\n-          synchronized (TestRetryCacheWithHA.this) {\n-            results.put(op.name, result == null ? \"SUCCESS\" : result);\n-            TestRetryCacheWithHA.this.notifyAll();\n-          }\n+          results.put(op.name, result == null ? \"SUCCESS\" : result);\n         } catch (Exception e) {\n           LOG.info(\"Got Exception while calling \" + op.name, e);\n         } finally {\n@@ -1332,40 +1331,48 @@ public void run() {\n     // disable the block in DummyHandler\n     LOG.info(\"Setting block to false\");\n     DummyRetryInvocationHandler.block.set(false);\n-    \n-    synchronized (this) {\n-      while (!results.containsKey(op.name)) {\n-        this.wait();\n-      }\n-      LOG.info(\"Got the result of \" + op.name + \": \"\n-          + results.get(op.name));\n-    }\n+\n+    GenericTestUtils.waitFor(() -> results.containsKey(op.name), 5, 10000);\n+    LOG.info(\"Got the result of \" + op.name + \": \"\n+        + results.get(op.name));\n \n     // Waiting for failover.\n-    while (cluster.getNamesystem(1).isInStandbyState()) {\n-      Thread.sleep(10);\n-    }\n-\n-    long hitNN0 = cluster.getNamesystem(0).getRetryCache().getMetricsForTests()\n-        .getCacheHit();\n-    long hitNN1 = cluster.getNamesystem(1).getRetryCache().getMetricsForTests()\n-        .getCacheHit();\n-    assertTrue(\"CacheHit: \" + hitNN0 + \", \" + hitNN1,\n-        hitNN0 + hitNN1 > 0);\n-    long updatedNN0 = cluster.getNamesystem(0).getRetryCache()\n-        .getMetricsForTests().getCacheUpdated();\n-    long updatedNN1 = cluster.getNamesystem(1).getRetryCache()\n-        .getMetricsForTests().getCacheUpdated();\n+    GenericTestUtils\n+        .waitFor(() -> !cluster.getNamesystem(1).isInStandbyState(), 5, 10000);\n+\n+    final long[] hitsNN = new long[]{0, 0};\n+    GenericTestUtils.waitFor(() -> {\n+      hitsNN[0] = cluster.getNamesystem(0).getRetryCache()\n+          .getMetricsForTests()\n+          .getCacheHit();\n+      hitsNN[1] = cluster.getNamesystem(1).getRetryCache()\n+          .getMetricsForTests()\n+          .getCacheHit();\n+      return (hitsNN[0] + hitsNN[1]) > 0;\n+    }, 5, 10000);\n+\n+    assertTrue(\"CacheHit: \" + hitsNN[0] + \", \" + hitsNN[1],\n+        +hitsNN[0] + hitsNN[1] > 0);\n+    final long[] updatesNN = new long[]{0, 0};\n+    GenericTestUtils.waitFor(() -> {\n+      updatesNN[0] = cluster.getNamesystem(0).getRetryCache()\n+          .getMetricsForTests()\n+          .getCacheUpdated();\n+      updatesNN[1] = cluster.getNamesystem(1).getRetryCache()\n+          .getMetricsForTests()\n+          .getCacheUpdated();\n+      return updatesNN[0] > 0 && updatesNN[1] > 0;\n+    }, 5, 10000);\n     // Cache updated metrics on NN0 should be >0 since the op was process on NN0\n-    assertTrue(\"CacheUpdated on NN0: \" + updatedNN0, updatedNN0 > 0);\n+    assertTrue(\"CacheUpdated on NN0: \" + updatesNN[0], updatesNN[0] > 0);\n     // Cache updated metrics on NN0 should be >0 since NN1 applied the editlog\n-    assertTrue(\"CacheUpdated on NN1: \" + updatedNN1, updatedNN1 > 0);\n+    assertTrue(\"CacheUpdated on NN1: \" + updatesNN[1], updatesNN[1] > 0);\n     long expectedUpdateCount = op.getExpectedCacheUpdateCount();\n     if (expectedUpdateCount > 0) {\n-      assertEquals(\"CacheUpdated on NN0: \" + updatedNN0, expectedUpdateCount,\n-          updatedNN0);\n-      assertEquals(\"CacheUpdated on NN0: \" + updatedNN1, expectedUpdateCount,\n-          updatedNN1);\n+      assertEquals(\"CacheUpdated on NN0: \" + updatesNN[0], expectedUpdateCount,\n+          updatesNN[0]);\n+      assertEquals(\"CacheUpdated on NN0: \" + updatesNN[1], expectedUpdateCount,\n+          updatesNN[1]);\n     }\n   }\n ",
      "parent_sha": "bbd704bb828577a1f0afe5fb0ac358fb7c5af446"
    }
  },
  {
    "oid": "d76e2655ace56490a92da70bde9e651ce515f80c",
    "message": "HDFS-14754. Erasure Coding : The number of Under-Replicated Blocks never reduced(addendum). Contributed by Surendra Singh Lilhore.",
    "date": "2019-10-09T17:50:28Z",
    "url": "https://github.com/apache/hadoop/commit/d76e2655ace56490a92da70bde9e651ce515f80c",
    "details": {
      "sha": "ac25da3fbdd0cd233aa0864033d1620959a4c0da",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestRedudantBlocks.java",
      "status": "modified",
      "additions": 9,
      "deletions": 5,
      "changes": 14,
      "blob_url": "https://github.com/apache/hadoop/blob/d76e2655ace56490a92da70bde9e651ce515f80c/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FTestRedudantBlocks.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d76e2655ace56490a92da70bde9e651ce515f80c/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FTestRedudantBlocks.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fnamenode%2FTestRedudantBlocks.java?ref=d76e2655ace56490a92da70bde9e651ce515f80c",
      "patch": "@@ -58,7 +58,7 @@ public class TestRedudantBlocks {\n   private final int cellSize = ecPolicy.getCellSize();\n   private final int stripesPerBlock = 4;\n   private final int blockSize = stripesPerBlock * cellSize;\n-  private final int numDNs = groupSize + 1;\n+  private final int numDNs = groupSize;\n \n   @Before\n   public void setup() throws IOException {\n@@ -110,12 +110,16 @@ public void testProcessOverReplicatedAndRedudantBlock() throws Exception {\n \n     // update blocksMap\n     cluster.triggerBlockReports();\n-    // add to invalidates\n+    // delete redundant block\n     cluster.triggerHeartbeats();\n-    // datanode delete block\n+    //wait for IBR\n+    Thread.sleep(1100);\n+\n+    // trigger reconstruction\n     cluster.triggerHeartbeats();\n-    // update blocksMap\n-    cluster.triggerBlockReports();\n+\n+    //wait for IBR\n+    Thread.sleep(1100);\n \n     HashSet<Long> blockIdsSet = new HashSet<Long>();\n ",
      "parent_sha": "2d81abce5ecfec555eda4819a6e2f5b22e1cd9b8"
    }
  },
  {
    "oid": "2c155afe2736a5571bbb3bdfb2fe6f9709227229",
    "message": "HDFS-10292. Add block id when client got Unable to close file exception. Contributed by Brahma Reddy Battula.",
    "date": "2016-04-14T19:25:11Z",
    "url": "https://github.com/apache/hadoop/commit/2c155afe2736a5571bbb3bdfb2fe6f9709227229",
    "details": {
      "sha": "0f8279943d2e6914feaf6e54cdee1f2978ceb3de",
      "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/2c155afe2736a5571bbb3bdfb2fe6f9709227229/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FDFSOutputStream.java",
      "raw_url": "https://github.com/apache/hadoop/raw/2c155afe2736a5571bbb3bdfb2fe6f9709227229/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FDFSOutputStream.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2FDFSOutputStream.java?ref=2c155afe2736a5571bbb3bdfb2fe6f9709227229",
      "patch": "@@ -811,7 +811,7 @@ protected void completeFile(ExtendedBlock last) throws IOException {\n         try {\n           if (retries == 0) {\n             throw new IOException(\"Unable to close file because the last block\"\n-                + \" does not have enough number of replicas.\");\n+                + last + \" does not have enough number of replicas.\");\n           }\n           retries--;\n           Thread.sleep(sleeptime);",
      "parent_sha": "3150ae8108a1fc40a67926be6254824c1e37cb38"
    }
  },
  {
    "oid": "46d61913ff2a4ed6b5c77f348ba71c2c677b61ef",
    "message": "HDDS-1954. StackOverflowError in OzoneClientInvocationHandler\n\nSigned-off-by: Anu Engineer <aengineer@apache.org>",
    "date": "2019-08-15T17:46:43Z",
    "url": "https://github.com/apache/hadoop/commit/46d61913ff2a4ed6b5c77f348ba71c2c677b61ef",
    "details": {
      "sha": "cdc7702ef2e67f974ecbab54536f96733a3c1cb4",
      "filename": "hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/OzoneClientInvocationHandler.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/46d61913ff2a4ed6b5c77f348ba71c2c677b61ef/hadoop-ozone%2Fclient%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fclient%2FOzoneClientInvocationHandler.java",
      "raw_url": "https://github.com/apache/hadoop/raw/46d61913ff2a4ed6b5c77f348ba71c2c677b61ef/hadoop-ozone%2Fclient%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fclient%2FOzoneClientInvocationHandler.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-ozone%2Fclient%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fozone%2Fclient%2FOzoneClientInvocationHandler.java?ref=46d61913ff2a4ed6b5c77f348ba71c2c677b61ef",
      "patch": "@@ -48,7 +48,7 @@ public OzoneClientInvocationHandler(ClientProtocol target) {\n   @Override\n   public Object invoke(Object proxy, Method method, Object[] args)\n       throws Throwable {\n-    LOG.trace(\"Invoking method {} on proxy {}\", method, proxy);\n+    LOG.trace(\"Invoking method {} on target {}\", method, target);\n     try {\n       long startTime = Time.monotonicNow();\n       Object result = method.invoke(target, args);",
      "parent_sha": "c801f7a26c08d367e902d3b18000853ad7ba2c82"
    }
  },
  {
    "oid": "1ccba3734d1c46aaae1cd3dd4dd504e691771167",
    "message": "YARN-10490. yarn top command not quitting completely with ctrl+c. Contributed by Agshin Kazimli",
    "date": "2021-01-21T19:30:19Z",
    "url": "https://github.com/apache/hadoop/commit/1ccba3734d1c46aaae1cd3dd4dd504e691771167",
    "details": {
      "sha": "882d2bfce20eff07a5b1f83e9649c33332561c83",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/TopCLI.java",
      "status": "modified",
      "additions": 8,
      "deletions": 1,
      "changes": 9,
      "blob_url": "https://github.com/apache/hadoop/blob/1ccba3734d1c46aaae1cd3dd4dd504e691771167/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fclient%2Fcli%2FTopCLI.java",
      "raw_url": "https://github.com/apache/hadoop/raw/1ccba3734d1c46aaae1cd3dd4dd504e691771167/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fclient%2Fcli%2FTopCLI.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-client%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fclient%2Fcli%2FTopCLI.java?ref=1ccba3734d1c46aaae1cd3dd4dd504e691771167",
      "patch": "@@ -444,6 +444,7 @@ public TopCLI() throws IOException, InterruptedException {\n \n   public static void main(String[] args) throws Exception {\n     TopCLI topImp = new TopCLI();\n+    topImp.addShutdownHook();\n     topImp.setSysOutPrintStream(System.out);\n     topImp.setSysErrPrintStream(System.err);\n     int res = ToolRunner.run(topImp, args);\n@@ -492,7 +493,6 @@ public int run(String[] args) throws Exception {\n         rmStartTime = getRMStartTime();\n       }\n     }\n-    clearScreen();\n     return 0;\n   }\n \n@@ -1220,4 +1220,11 @@ private String getCommandOutput(String[] command) throws IOException,\n     byte[] output = IOUtils.toByteArray(p.getInputStream());\n     return new String(output, \"ASCII\");\n   }\n+\n+  private void addShutdownHook() {\n+    //clear screen when the program exits\n+    Runtime.getRuntime().addShutdownHook(new Thread(() -> {\n+      clearScreen();\n+    }));\n+  }\n }",
      "parent_sha": "e2f8503ebd6201e28830acd59eac3a17a8cd3d7a"
    }
  },
  {
    "oid": "f2033de2342d20d5f540775dfe4848d452c68957",
    "message": "HADOOP-17119. Jetty upgrade to 9.4.x causes MR app fail with IOException. Contributed by Bilwa S T.",
    "date": "2020-07-20T16:43:48Z",
    "url": "https://github.com/apache/hadoop/commit/f2033de2342d20d5f540775dfe4848d452c68957",
    "details": {
      "sha": "8b69d57e8120e4479fe53f35d748c638dae25416",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/http/HttpServer2.java",
      "status": "modified",
      "additions": 5,
      "deletions": 1,
      "changes": 6,
      "blob_url": "https://github.com/apache/hadoop/blob/f2033de2342d20d5f540775dfe4848d452c68957/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FHttpServer2.java",
      "raw_url": "https://github.com/apache/hadoop/raw/f2033de2342d20d5f540775dfe4848d452c68957/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FHttpServer2.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FHttpServer2.java?ref=f2033de2342d20d5f540775dfe4848d452c68957",
      "patch": "@@ -1346,7 +1346,11 @@ private void bindForPortRange(ServerConnector listener, int startPort)\n       try {\n         bindListener(listener);\n         return;\n-      } catch (BindException ex) {\n+      } catch (IOException ex) {\n+        if (!(ex instanceof BindException)\n+            && !(ex.getCause() instanceof BindException)) {\n+          throw ex;\n+        }\n         // Ignore exception. Move to next port.\n         ioException = ex;\n       }",
      "parent_sha": "6cbd8854ee5f2c33496ac7ae397e366cf136dd07"
    }
  },
  {
    "oid": "8512e1a91be3e340d919c7cdc9c09dfb762a6a4e",
    "message": "HDFS-13746. Still occasional \"Should be different group\" failure in TestRefreshUserMappings#testGroupMappingRefresh\n(Contributed by Siyao Meng via Daniel Templeton)\n\nChange-Id: I9fad1537ace38367a463d9fe67aaa28d3178fc69",
    "date": "2018-08-16T22:00:45Z",
    "url": "https://github.com/apache/hadoop/commit/8512e1a91be3e340d919c7cdc9c09dfb762a6a4e",
    "details": {
      "sha": "2d7410a405cc93a3132c9d25ed98b9ce157facb0",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestRefreshUserMappings.java",
      "status": "modified",
      "additions": 27,
      "deletions": 24,
      "changes": 51,
      "blob_url": "https://github.com/apache/hadoop/blob/8512e1a91be3e340d919c7cdc9c09dfb762a6a4e/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FTestRefreshUserMappings.java",
      "raw_url": "https://github.com/apache/hadoop/raw/8512e1a91be3e340d919c7cdc9c09dfb762a6a4e/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FTestRefreshUserMappings.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FTestRefreshUserMappings.java?ref=8512e1a91be3e340d919c7cdc9c09dfb762a6a4e",
      "patch": "@@ -34,7 +34,6 @@\n import java.net.URL;\n import java.net.URLDecoder;\n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.List;\n \n import org.apache.hadoop.conf.Configuration;\n@@ -46,13 +45,17 @@\n import org.apache.hadoop.security.authorize.DefaultImpersonationProvider;\n import org.apache.hadoop.security.authorize.ProxyUsers;\n import org.apache.hadoop.test.GenericTestUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n import org.slf4j.event.Level;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n \n \n public class TestRefreshUserMappings {\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      TestRefreshUserMappings.class);\n   private MiniDFSCluster cluster;\n   Configuration config;\n   private static final long groupRefreshTimeoutSec = 1;\n@@ -119,42 +122,42 @@ public void testGroupMappingRefresh() throws Exception {\n     Groups groups = Groups.getUserToGroupsMappingService(config);\n     String user = UserGroupInformation.getCurrentUser().getUserName();\n \n-    System.out.println(\"First attempt:\");\n+    LOG.debug(\"First attempt:\");\n     List<String> g1 = groups.getGroups(user);\n-    String [] str_groups = new String [g1.size()];\n-    g1.toArray(str_groups);\n-    System.out.println(Arrays.toString(str_groups));\n-    \n-    System.out.println(\"Second attempt, should be the same:\");\n+    LOG.debug(g1.toString());\n+\n+    LOG.debug(\"Second attempt, should be the same:\");\n     List<String> g2 = groups.getGroups(user);\n-    g2.toArray(str_groups);\n-    System.out.println(Arrays.toString(str_groups));\n+    LOG.debug(g2.toString());\n     for(int i=0; i<g2.size(); i++) {\n       assertEquals(\"Should be same group \", g1.get(i), g2.get(i));\n     }\n \n     // Test refresh command\n     admin.run(args);\n-    System.out.println(\"Third attempt(after refresh command), should be different:\");\n+    LOG.debug(\"Third attempt(after refresh command), should be different:\");\n     List<String> g3 = groups.getGroups(user);\n-    g3.toArray(str_groups);\n-    System.out.println(Arrays.toString(str_groups));\n+    LOG.debug(g3.toString());\n     for(int i=0; i<g3.size(); i++) {\n-      assertFalse(\"Should be different group: \" + g1.get(i) + \" and \" + g3.get(i), \n-          g1.get(i).equals(g3.get(i)));\n+      assertFalse(\"Should be different group: \"\n+              + g1.get(i) + \" and \" + g3.get(i), g1.get(i).equals(g3.get(i)));\n     }\n-    \n+\n     // Test timeout\n-    Thread.sleep(groupRefreshTimeoutSec * 1500);\n-    System.out.println(\"Fourth attempt(after timeout), should be different:\");\n-    List<String> g4 = groups.getGroups(user);\n-    g4.toArray(str_groups);\n-    System.out.println(Arrays.toString(str_groups));\n-    for(int i=0; i<g4.size(); i++) {\n-      assertFalse(\"Should be different group \", g3.get(i).equals(g4.get(i)));\n-    }\n+    LOG.debug(\"Fourth attempt(after timeout), should be different:\");\n+    GenericTestUtils.waitFor(() -> {\n+      List<String> g4;\n+      try {\n+        g4 = groups.getGroups(user);\n+      } catch (IOException e) {\n+        return false;\n+      }\n+      LOG.debug(g4.toString());\n+      // if g4 is the same as g3, wait and retry\n+      return !g3.equals(g4);\n+    }, 50, Math.toIntExact(groupRefreshTimeoutSec * 1000 * 30));\n   }\n-  \n+\n   @Test\n   public void testRefreshSuperUserGroupsConfiguration() throws Exception {\n     final String SUPER_USER = \"super_user\";",
      "parent_sha": "5ef29087ad27f4f6b815dbc08ea7427d14df58e1"
    }
  },
  {
    "oid": "b98fc8249f0576e7b4e230ffc3cec5a20eefc543",
    "message": "YARN-4710. Reduce logging application reserved debug info in FSAppAttempt#assignContainer (Contributed by Yiqun Lin via Daniel Templeton)",
    "date": "2016-10-27T21:42:19Z",
    "url": "https://github.com/apache/hadoop/commit/b98fc8249f0576e7b4e230ffc3cec5a20eefc543",
    "details": {
      "sha": "cef4387794f6f57c619aad0819111b43e6de7b59",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/hadoop/blob/b98fc8249f0576e7b4e230ffc3cec5a20eefc543/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Ffair%2FFSAppAttempt.java",
      "raw_url": "https://github.com/apache/hadoop/raw/b98fc8249f0576e7b4e230ffc3cec5a20eefc543/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Ffair%2FFSAppAttempt.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Ffair%2FFSAppAttempt.java?ref=b98fc8249f0576e7b4e230ffc3cec5a20eefc543",
      "patch": "@@ -771,8 +771,8 @@ private boolean isOverAMShareLimit() {\n   }\n \n   private Resource assignContainer(FSSchedulerNode node, boolean reserved) {\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n+    if (LOG.isTraceEnabled()) {\n+      LOG.trace(\"Node offered to app: \" + getName() + \" reserved: \" + reserved);\n     }\n \n     Collection<SchedulerRequestKey> keysToTry = (reserved) ?",
      "parent_sha": "9449519a2503c55d9eac8fd7519df28aa0760059"
    }
  },
  {
    "oid": "11af08d67a8f1006c0ff270d7b314d3801d280f3",
    "message": "YARN-11489. Fix memory leak of DelegationTokenRenewer futures in DelegationTokenRenewerPoolTracker. (#5629). Contributed by Chun Chen.\n\nSigned-off-by: He Xiaoqiao <hexiaoqiao@apache.org>",
    "date": "2023-05-14T13:38:04Z",
    "url": "https://github.com/apache/hadoop/commit/11af08d67a8f1006c0ff270d7b314d3801d280f3",
    "details": {
      "sha": "be95572f92a3bbb3ea57be944971311de01dc29b",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
      "status": "modified",
      "additions": 56,
      "deletions": 28,
      "changes": 84,
      "blob_url": "https://github.com/apache/hadoop/blob/11af08d67a8f1006c0ff270d7b314d3801d280f3/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fsecurity%2FDelegationTokenRenewer.java",
      "raw_url": "https://github.com/apache/hadoop/raw/11af08d67a8f1006c0ff270d7b314d3801d280f3/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fsecurity%2FDelegationTokenRenewer.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fsecurity%2FDelegationTokenRenewer.java?ref=11af08d67a8f1006c0ff270d7b314d3801d280f3",
      "patch": "@@ -123,8 +123,8 @@ public class DelegationTokenRenewer extends AbstractService {\n   private long tokenRenewerThreadTimeout;\n   private long tokenRenewerThreadRetryInterval;\n   private int tokenRenewerThreadRetryMaxAttempts;\n-  private final Map<DelegationTokenRenewerEvent, Future<?>> futures =\n-      new ConcurrentHashMap<>();\n+  private final LinkedBlockingQueue<DelegationTokenRenewerFuture> futures =\n+      new LinkedBlockingQueue<>();\n   private boolean delegationTokenRenewerPoolTrackerFlag = true;\n \n   // this config is supposedly not used by end-users.\n@@ -227,7 +227,7 @@ private void processDelegationTokenRenewerEvent(\n       if (isServiceStarted) {\n         Future<?> future =\n             renewerService.submit(new DelegationTokenRenewerRunnable(evt));\n-        futures.put(evt, future);\n+        futures.add(new DelegationTokenRenewerFuture(evt, future));\n       } else {\n         pendingEventQueue.add(evt);\n         int qSize = pendingEventQueue.size();\n@@ -998,33 +998,35 @@ private final class DelegationTokenRenewerPoolTracker\n     @Override\n     public void run() {\n       while (true) {\n-        for (Map.Entry<DelegationTokenRenewerEvent, Future<?>> entry : futures\n-            .entrySet()) {\n-          DelegationTokenRenewerEvent evt = entry.getKey();\n-          Future<?> future = entry.getValue();\n-          try {\n-            future.get(tokenRenewerThreadTimeout, TimeUnit.MILLISECONDS);\n-          } catch (TimeoutException e) {\n-\n-            // Cancel thread and retry the same event in case of timeout\n-            if (future != null && !future.isDone() && !future.isCancelled()) {\n-              future.cancel(true);\n-              futures.remove(evt);\n-              if (evt.getAttempt() < tokenRenewerThreadRetryMaxAttempts) {\n-                renewalTimer.schedule(\n-                    getTimerTask((AbstractDelegationTokenRenewerAppEvent) evt),\n-                    tokenRenewerThreadRetryInterval);\n-              } else {\n-                LOG.info(\n-                    \"Exhausted max retry attempts {} in token renewer \"\n-                        + \"thread for {}\",\n-                    tokenRenewerThreadRetryMaxAttempts, evt.getApplicationId());\n-              }\n+        DelegationTokenRenewerFuture dtrf;\n+        try {\n+          dtrf = futures.take();\n+        } catch (InterruptedException e) {\n+          LOG.debug(\"DelegationTokenRenewer pool tracker interrupted\");\n+          return;\n+        }\n+        DelegationTokenRenewerEvent evt = dtrf.getEvt();\n+        Future<?> future = dtrf.getFuture();\n+        try {\n+          future.get(tokenRenewerThreadTimeout, TimeUnit.MILLISECONDS);\n+        } catch (TimeoutException e) {\n+          // Cancel thread and retry the same event in case of timeout.\n+          if (!future.isDone() && !future.isCancelled()) {\n+            future.cancel(true);\n+            if (evt.getAttempt() < tokenRenewerThreadRetryMaxAttempts) {\n+              renewalTimer.schedule(\n+                  getTimerTask((AbstractDelegationTokenRenewerAppEvent) evt),\n+                  tokenRenewerThreadRetryInterval);\n+            } else {\n+              LOG.info(\n+                  \"Exhausted max retry attempts {} in token renewer \"\n+                      + \"thread for {}\",\n+                  tokenRenewerThreadRetryMaxAttempts, evt.getApplicationId());\n             }\n-          } catch (Exception e) {\n-            LOG.info(\"Problem in submitting renew tasks in token renewer \"\n-                + \"thread.\", e);\n           }\n+        } catch (Exception e) {\n+          LOG.info(\"Problem in submitting renew tasks in token renewer \"\n+              + \"thread.\", e);\n         }\n       }\n     }\n@@ -1192,6 +1194,32 @@ public void setAttempt(int attempt) {\n     }\n   }\n \n+  private static class DelegationTokenRenewerFuture {\n+    private DelegationTokenRenewerEvent evt;\n+    private Future<?> future;\n+    DelegationTokenRenewerFuture(DelegationTokenRenewerEvent evt,\n+        Future<?> future) {\n+      this.future = future;\n+      this.evt = evt;\n+    }\n+\n+    public DelegationTokenRenewerEvent getEvt() {\n+      return evt;\n+    }\n+\n+    public void setEvt(DelegationTokenRenewerEvent evt) {\n+      this.evt = evt;\n+    }\n+\n+    public Future<?> getFuture() {\n+      return future;\n+    }\n+\n+    public void setFuture(Future<?> future) {\n+      this.future = future;\n+    }\n+  }\n+\n   // only for testing\n   protected ConcurrentMap<Token<?>, DelegationTokenToRenew> getAllTokens() {\n     return allTokens;",
      "parent_sha": "251439d769081e5c220d3f4ee07972aee7cfc09a"
    }
  },
  {
    "oid": "da9aa9c90964a60bef320e8786f3f767ca3e443d",
    "message": "MAPREDUCE-7250. FrameworkUploader skip replication check if timeout is 0.\n\nContributed by Peter Bacsko.",
    "date": "2019-12-05T07:02:57Z",
    "url": "https://github.com/apache/hadoop/commit/da9aa9c90964a60bef320e8786f3f767ca3e443d",
    "details": {
      "sha": "3dea023ea7ba2f6ecff6793c53482853b7068c7d",
      "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-uploader/src/main/java/org/apache/hadoop/mapred/uploader/FrameworkUploader.java",
      "status": "modified",
      "additions": 18,
      "deletions": 14,
      "changes": 32,
      "blob_url": "https://github.com/apache/hadoop/blob/da9aa9c90964a60bef320e8786f3f767ca3e443d/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-uploader%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2Fuploader%2FFrameworkUploader.java",
      "raw_url": "https://github.com/apache/hadoop/raw/da9aa9c90964a60bef320e8786f3f767ca3e443d/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-uploader%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2Fuploader%2FFrameworkUploader.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-uploader%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2Fuploader%2FFrameworkUploader.java?ref=da9aa9c90964a60bef320e8786f3f767ca3e443d",
      "patch": "@@ -298,20 +298,24 @@ private void endUpload()\n       fileSystem.setReplication(targetPath, finalReplication);\n       LOG.info(\"Set replication to \" +\n           finalReplication + \" for path: \" + targetPath);\n-      long startTime = System.currentTimeMillis();\n-      long endTime = startTime;\n-      long currentReplication = 0;\n-      while(endTime - startTime < timeout * 1000 &&\n-           currentReplication < acceptableReplication) {\n-        Thread.sleep(1000);\n-        endTime = System.currentTimeMillis();\n-        currentReplication = getSmallestReplicatedBlockCount();\n-      }\n-      if (endTime - startTime >= timeout * 1000) {\n-        LOG.error(String.format(\n-            \"Timed out after %d seconds while waiting for acceptable\" +\n-                \" replication of %d (current replication is %d)\",\n-            timeout, acceptableReplication, currentReplication));\n+      if (timeout == 0) {\n+        LOG.info(\"Timeout is set to 0. Skipping replication check.\");\n+      } else {\n+        long startTime = System.currentTimeMillis();\n+        long endTime = startTime;\n+        long currentReplication = 0;\n+        while(endTime - startTime < timeout * 1000 &&\n+             currentReplication < acceptableReplication) {\n+          Thread.sleep(1000);\n+          endTime = System.currentTimeMillis();\n+          currentReplication = getSmallestReplicatedBlockCount();\n+        }\n+        if (endTime - startTime >= timeout * 1000) {\n+          LOG.error(String.format(\n+              \"Timed out after %d seconds while waiting for acceptable\" +\n+                  \" replication of %d (current replication is %d)\",\n+              timeout, acceptableReplication, currentReplication));\n+        }\n       }\n     } else {\n       LOG.info(\"Cannot set replication to \" +",
      "parent_sha": "304e75a48b727f702a9ede8501af01cf9c0585d3"
    }
  },
  {
    "oid": "0be26811f3db49abb62d12e6a051a31553495da8",
    "message": "YARN-10328. Fixed ZK Curator NodeExists exception in YARN service AM logs\n            Contributed by Bilwa S T via eyang",
    "date": "2020-06-29T16:21:24Z",
    "url": "https://github.com/apache/hadoop/commit/0be26811f3db49abb62d12e6a051a31553495da8",
    "details": {
      "sha": "06066d546e043d8a926e46914ba849072deb5bca",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/registry/YarnRegistryViewForProviders.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/apache/hadoop/blob/0be26811f3db49abb62d12e6a051a31553495da8/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-services%2Fhadoop-yarn-services-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fservice%2Fregistry%2FYarnRegistryViewForProviders.java",
      "raw_url": "https://github.com/apache/hadoop/raw/0be26811f3db49abb62d12e6a051a31553495da8/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-services%2Fhadoop-yarn-services-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fservice%2Fregistry%2FYarnRegistryViewForProviders.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-services%2Fhadoop-yarn-services-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fservice%2Fregistry%2FYarnRegistryViewForProviders.java?ref=0be26811f3db49abb62d12e6a051a31553495da8",
      "patch": "@@ -143,7 +143,10 @@ public void putComponent(String serviceClass,\n       ServiceRecord record) throws IOException {\n     String path = RegistryUtils.componentPath(\n         user, serviceClass, serviceName, componentName);\n-    registryOperations.mknode(RegistryPathUtils.parentOf(path), true);\n+    String parentPath = RegistryPathUtils.parentOf(path);\n+    if (!registryOperations.exists(parentPath)) {\n+      registryOperations.mknode(parentPath, true);\n+    }\n     registryOperations.bind(path, record, BindFlags.OVERWRITE);\n   }\n ",
      "parent_sha": "c71ce7ac3370e220995bad0ae8b59d962c8d30a7"
    }
  },
  {
    "oid": "db4a61dc617954cf448a0d44cb4ac79f2abf9db3",
    "message": "YARN-5469. Increase timeout of TestAmFilter.testFilter. Contributed by Eric Badger",
    "date": "2016-08-03T19:51:44Z",
    "url": "https://github.com/apache/hadoop/commit/db4a61dc617954cf448a0d44cb4ac79f2abf9db3",
    "details": {
      "sha": "6f64777aace999c60233d8903b2cb403b586134d",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/amfilter/TestAmFilter.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/hadoop/blob/db4a61dc617954cf448a0d44cb4ac79f2abf9db3/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-web-proxy%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fwebproxy%2Famfilter%2FTestAmFilter.java",
      "raw_url": "https://github.com/apache/hadoop/raw/db4a61dc617954cf448a0d44cb4ac79f2abf9db3/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-web-proxy%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fwebproxy%2Famfilter%2FTestAmFilter.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-web-proxy%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fwebproxy%2Famfilter%2FTestAmFilter.java?ref=db4a61dc617954cf448a0d44cb4ac79f2abf9db3",
      "patch": "@@ -123,7 +123,7 @@ public void doFilter(ServletRequest servletRequest,\n   /**\n    * Test AmIpFilter\n    */\n-  @Test(timeout = 1000)\n+  @Test(timeout = 10000)\n   @SuppressWarnings(\"deprecation\")\n   public void testFilter() throws Exception {\n     Map<String, String> params = new HashMap<String, String>();",
      "parent_sha": "f4ba5ff1d70ef92d59851c09c4bd4b43d6c04971"
    }
  },
  {
    "oid": "f43a152b9729323e290908fbd4f188f6034efb3f",
    "message": "HDFS-15369. Refactor method VolumeScanner#runLoop(). Contributed by Yang Yun.",
    "date": "2020-05-24T12:33:41Z",
    "url": "https://github.com/apache/hadoop/commit/f43a152b9729323e290908fbd4f188f6034efb3f",
    "details": {
      "sha": "5e3d523cde382befc41465934b659a736285ec5e",
      "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java",
      "status": "modified",
      "additions": 47,
      "deletions": 34,
      "changes": 81,
      "blob_url": "https://github.com/apache/hadoop/blob/f43a152b9729323e290908fbd4f188f6034efb3f/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FVolumeScanner.java",
      "raw_url": "https://github.com/apache/hadoop/raw/f43a152b9729323e290908fbd4f188f6034efb3f/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FVolumeScanner.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project%2Fhadoop-hdfs%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhdfs%2Fserver%2Fdatanode%2FVolumeScanner.java?ref=f43a152b9729323e290908fbd4f188f6034efb3f",
      "patch": "@@ -483,6 +483,50 @@ static boolean calculateShouldScan(String storageId, long targetBytesPerSec,\n     return shouldScan;\n   }\n \n+  /**\n+   * Get next block and check if it's needed to scan.\n+   *\n+   * @return  the candidate block.\n+   */\n+  ExtendedBlock getNextBlockToScan() {\n+    ExtendedBlock block;\n+    try {\n+      block = curBlockIter.nextBlock();\n+    } catch (IOException e) {\n+      // There was an error listing the next block in the volume.  This is a\n+      // serious issue.\n+      LOG.warn(\"{}: nextBlock error on {}\", this, curBlockIter);\n+      // On the next loop iteration, curBlockIter#eof will be set to true, and\n+      // we will pick a different block iterator.\n+      return null;\n+    }\n+    if (block == null) {\n+      // The BlockIterator is at EOF.\n+      LOG.info(\"{}: finished scanning block pool {}\",\n+          this, curBlockIter.getBlockPoolId());\n+      saveBlockIterator(curBlockIter);\n+      return null;\n+    } else if (conf.skipRecentAccessed) {\n+      // Check the access time of block file to avoid scanning recently\n+      // changed blocks, reducing disk IO.\n+      try {\n+        BlockLocalPathInfo blockLocalPathInfo =\n+            volume.getDataset().getBlockLocalPathInfo(block);\n+        BasicFileAttributes attr = Files.readAttributes(\n+            new File(blockLocalPathInfo.getBlockPath()).toPath(),\n+            BasicFileAttributes.class);\n+        if (System.currentTimeMillis() - attr.lastAccessTime().\n+            to(TimeUnit.MILLISECONDS) < conf.scanPeriodMs) {\n+          return null;\n+        }\n+      } catch (IOException ioe) {\n+        LOG.debug(\"Failed to get access time of block {}\",\n+            block, ioe);\n+      }\n+    }\n+    return block;\n+  }\n+\n   /**\n    * Run an iteration of the VolumeScanner loop.\n    *\n@@ -507,10 +551,10 @@ private long runLoop(ExtendedBlock suspectBlock) {\n         return 30000L;\n       }\n \n-      // Find a usable block pool to scan.\n       if (suspectBlock != null) {\n         block = suspectBlock;\n       } else {\n+        // Find a usable block pool to scan.\n         if ((curBlockIter == null) || curBlockIter.atEnd()) {\n           long timeout = findNextUsableBlockIter();\n           if (timeout > 0) {\n@@ -528,40 +572,9 @@ private long runLoop(ExtendedBlock suspectBlock) {\n           }\n           return 0L;\n         }\n-        try {\n-          block = curBlockIter.nextBlock();\n-        } catch (IOException e) {\n-          // There was an error listing the next block in the volume.  This is a\n-          // serious issue.\n-          LOG.warn(\"{}: nextBlock error on {}\", this, curBlockIter);\n-          // On the next loop iteration, curBlockIter#eof will be set to true, and\n-          // we will pick a different block iterator.\n-          return 0L;\n-        }\n+        block = getNextBlockToScan();\n         if (block == null) {\n-          // The BlockIterator is at EOF.\n-          LOG.info(\"{}: finished scanning block pool {}\",\n-              this, curBlockIter.getBlockPoolId());\n-          saveBlockIterator(curBlockIter);\n-          return 0;\n-        } else if (conf.skipRecentAccessed) {\n-          // Check the access time of block file to avoid scanning recently\n-          // changed blocks, reducing disk IO.\n-          try {\n-            BlockLocalPathInfo blockLocalPathInfo =\n-                volume.getDataset().getBlockLocalPathInfo(block);\n-            BasicFileAttributes attr = Files.readAttributes(\n-                new File(blockLocalPathInfo.getBlockPath()).toPath(),\n-                BasicFileAttributes.class);\n-            if (System.currentTimeMillis() - attr.lastAccessTime().\n-                to(TimeUnit.MILLISECONDS) < conf.scanPeriodMs) {\n-              return 0;\n-            }\n-\n-          } catch (IOException ioe) {\n-            LOG.debug(\"Failed to get access time of block {}\",\n-                block, ioe);\n-          }\n+          return 0L;\n         }\n       }\n       if (curBlockIter != null) {",
      "parent_sha": "f4901d07781faee657f5ac2c605183ef34fe7c1a"
    }
  },
  {
    "oid": "daaf530fce4b91cf9f568b9b0c5e8b20e6774134",
    "message": "YARN-6778. In ResourceWeights, weights and setWeights() should be final. (Daniel Templeton via Yufei Gu)",
    "date": "2017-07-18T23:38:07Z",
    "url": "https://github.com/apache/hadoop/commit/daaf530fce4b91cf9f568b9b0c5e8b20e6774134",
    "details": {
      "sha": "3ce15170eb4cd26d828834f3b538c752bd8eca4d",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/resource/ResourceWeights.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/hadoop/blob/daaf530fce4b91cf9f568b9b0c5e8b20e6774134/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fresource%2FResourceWeights.java",
      "raw_url": "https://github.com/apache/hadoop/raw/daaf530fce4b91cf9f568b9b0c5e8b20e6774134/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fresource%2FResourceWeights.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fresource%2FResourceWeights.java?ref=daaf530fce4b91cf9f568b9b0c5e8b20e6774134",
      "patch": "@@ -27,7 +27,7 @@\n public class ResourceWeights {\n   public static final ResourceWeights NEUTRAL = new ResourceWeights(1.0f);\n \n-  private float[] weights = new float[ResourceType.values().length];\n+  private final float[] weights = new float[ResourceType.values().length];\n \n   public ResourceWeights(float memoryWeight, float cpuWeight) {\n     weights[ResourceType.MEMORY.ordinal()] = memoryWeight;\n@@ -40,7 +40,7 @@ public ResourceWeights(float weight) {\n \n   public ResourceWeights() { }\n \n-  public void setWeight(float weight) {\n+  public final void setWeight(float weight) {\n     for (int i = 0; i < weights.length; i++) {\n       weights[i] = weight;\n     }",
      "parent_sha": "5aa2bf231f40423865f0054ca27426ceb95ab4ba"
    }
  },
  {
    "oid": "7b93575b92e8bad889c1ef15e0baaade6de6de4d",
    "message": "YARN-9894. CapacitySchedulerPerf test for measuring hundreds of apps in a large number of queues. Contributed by Eric Payne",
    "date": "2019-12-18T21:18:11Z",
    "url": "https://github.com/apache/hadoop/commit/7b93575b92e8bad889c1ef15e0baaade6de6de4d",
    "details": {
      "sha": "b2e71cf1f3c10277dcb88697eb5312779d2fcd91",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerPerf.java",
      "status": "modified",
      "additions": 136,
      "deletions": 40,
      "changes": 176,
      "blob_url": "https://github.com/apache/hadoop/blob/7b93575b92e8bad889c1ef15e0baaade6de6de4d/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Fcapacity%2FTestCapacitySchedulerPerf.java",
      "raw_url": "https://github.com/apache/hadoop/raw/7b93575b92e8bad889c1ef15e0baaade6de6de4d/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Fcapacity%2FTestCapacitySchedulerPerf.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2Fscheduler%2Fcapacity%2FTestCapacitySchedulerPerf.java?ref=7b93575b92e8bad889c1ef15e0baaade6de6de4d",
      "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.resourcemanager.MockNodes;\n import org.apache.hadoop.yarn.server.resourcemanager.MockRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics;\n@@ -58,6 +59,9 @@\n import java.util.PriorityQueue;\n \n import static org.apache.hadoop.yarn.server.resourcemanager.resource.TestResourceProfiles.TEST_CONF_RESET_RESOURCE_TYPES;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n@@ -68,9 +72,22 @@ private String getResourceName(int idx) {\n     return \"resource-\" + idx;\n   }\n \n+  // This test is run only when when -DRunCapacitySchedulerPerfTests=true is set\n+  // on the command line. In addition, this test has tunables for the following:\n+  //   Number of queues: -DNumberOfQueues (default=100)\n+  //   Number of total apps: -DNumberOfApplications (default=200)\n+  //   Percentage of queues with apps: -DPercentActiveQueues (default=100)\n+  // E.G.:\n+  // mvn test -Dtest=TestCapacitySchedulerPerf -Dsurefire.fork.timeout=1800 \\\n+  //    -DRunCapacitySchedulerPerfTests=true -DNumberOfQueues=50 \\\n+  //    -DNumberOfApplications=200 -DPercentActiveQueues=100\n+  // Note that the surefire.fork.timeout flag is added because these tests could\n+  // take longer than the surefire timeout.\n   private void testUserLimitThroughputWithNumberOfResourceTypes(\n-      int numOfResourceTypes)\n+      int numOfResourceTypes, int numQueues, int pctActiveQueues, int appCount)\n       throws Exception {\n+    Assume.assumeTrue(Boolean.valueOf(\n+        System.getProperty(\"RunCapacitySchedulerPerfTests\")));\n     if (numOfResourceTypes > 2) {\n       // Initialize resource map\n       Map<String, ResourceInformation> riMap = new HashMap<>();\n@@ -89,22 +106,16 @@ private void testUserLimitThroughputWithNumberOfResourceTypes(\n       ResourceUtils.initializeResourcesFromResourceInformationMap(riMap);\n     }\n \n-    // Since this is more of a performance unit test, only run if\n-    // RunUserLimitThroughput is set (-DRunUserLimitThroughput=true)\n-    Assume.assumeTrue(Boolean.valueOf(\n-        System.getProperty(\"RunCapacitySchedulerPerfTests\")));\n+    final int activeQueues = (int) (numQueues * (pctActiveQueues/100f));\n+    final int totalApps = appCount + activeQueues;\n+    // extra apps to get started with user limit\n \n     CapacitySchedulerConfiguration csconf =\n-        new CapacitySchedulerConfiguration();\n-    csconf.setMaximumApplicationMasterResourcePerQueuePercent(\"root\", 100.0f);\n-    csconf.setMaximumAMResourcePercentPerPartition(\"root\", \"\", 100.0f);\n-    csconf.setMaximumApplicationMasterResourcePerQueuePercent(\"root.default\",\n-        100.0f);\n-    csconf.setMaximumAMResourcePercentPerPartition(\"root.default\", \"\", 100.0f);\n-    csconf.setResourceComparator(DominantResourceCalculator.class);\n+        createCSConfWithManyQueues(numQueues);\n \n     YarnConfiguration conf = new YarnConfiguration(csconf);\n-    // Don't reset resource types since we have already configured resource types\n+    // Don't reset resource types since we have already configured resource\n+    // types\n     conf.setBoolean(TEST_CONF_RESET_RESOURCE_TYPES, false);\n     conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class,\n         ResourceScheduler.class);\n@@ -113,25 +124,29 @@ private void testUserLimitThroughputWithNumberOfResourceTypes(\n     rm.start();\n \n     CapacityScheduler cs = (CapacityScheduler) rm.getResourceScheduler();\n-    LeafQueue qb = (LeafQueue)cs.getQueue(\"default\");\n \n-    // For now make user limit large so we can activate all applications\n-    qb.setUserLimitFactor((float)100.0);\n-    qb.setupConfigurableCapacities();\n+    LeafQueue[] lqs = new LeafQueue[numQueues];\n+    for (int i = 0; i < numQueues; i++) {\n+      String queueName = String.format(\"%03d\", i);\n+      LeafQueue qb = (LeafQueue)cs.getQueue(queueName);\n+      // For now make user limit large so we can activate all applications\n+      qb.setUserLimitFactor((float)100.0);\n+      qb.setupConfigurableCapacities();\n+      lqs[i] = qb;\n+    }\n \n     SchedulerEvent addAppEvent;\n     SchedulerEvent addAttemptEvent;\n     Container container = mock(Container.class);\n     ApplicationSubmissionContext submissionContext =\n         mock(ApplicationSubmissionContext.class);\n \n-    final int appCount = 100;\n-    ApplicationId[] appids = new ApplicationId[appCount];\n-    RMAppAttemptImpl[] attempts = new RMAppAttemptImpl[appCount];\n-    ApplicationAttemptId[] appAttemptIds = new ApplicationAttemptId[appCount];\n-    RMAppImpl[] apps = new RMAppImpl[appCount];\n-    RMAppAttemptMetrics[] attemptMetrics = new RMAppAttemptMetrics[appCount];\n-    for (int i=0; i<appCount; i++) {\n+    ApplicationId[] appids = new ApplicationId[totalApps];\n+    RMAppAttemptImpl[] attempts = new RMAppAttemptImpl[totalApps];\n+    ApplicationAttemptId[] appAttemptIds = new ApplicationAttemptId[totalApps];\n+    RMAppImpl[] apps = new RMAppImpl[totalApps];\n+    RMAppAttemptMetrics[] attemptMetrics = new RMAppAttemptMetrics[totalApps];\n+    for (int i=0; i<totalApps; i++) {\n       appids[i] = BuilderUtils.newApplicationId(100, i);\n       appAttemptIds[i] =\n           BuilderUtils.newApplicationAttemptId(appids[i], 1);\n@@ -148,34 +163,34 @@ private void testUserLimitThroughputWithNumberOfResourceTypes(\n       when(apps[i].getCurrentAppAttempt()).thenReturn(attempts[i]);\n \n       rm.getRMContext().getRMApps().put(appids[i], apps[i]);\n+      String queueName = lqs[i % activeQueues].getQueueName();\n       addAppEvent =\n-          new AppAddedSchedulerEvent(appids[i], \"default\", \"user1\");\n+          new AppAddedSchedulerEvent(appids[i], queueName, \"user1\");\n       cs.handle(addAppEvent);\n       addAttemptEvent =\n           new AppAttemptAddedSchedulerEvent(appAttemptIds[i], false);\n       cs.handle(addAttemptEvent);\n     }\n \n-    // add nodes  to cluster, so cluster has 20GB and 20 vcores\n-    Resource nodeResource = Resource.newInstance(10 * GB, 10);\n+    // add nodes to cluster with enough resources to satisfy all apps\n+    Resource newResource = Resource.newInstance(totalApps * GB, totalApps);\n     if (numOfResourceTypes > 2) {\n       for (int i = 2; i < numOfResourceTypes; i++) {\n-        nodeResource.setResourceValue(getResourceName(i), 10);\n+        newResource.setResourceValue(getResourceName(i), totalApps);\n       }\n     }\n-\n-    RMNode node = MockNodes.newNodeInfo(0, nodeResource, 1, \"127.0.0.1\");\n+    RMNode node = MockNodes.newNodeInfo(0, newResource, 1, \"127.0.0.1\");\n     cs.handle(new NodeAddedSchedulerEvent(node));\n \n-    RMNode node2 = MockNodes.newNodeInfo(0, nodeResource, 1, \"127.0.0.2\");\n+    RMNode node2 = MockNodes.newNodeInfo(0, newResource, 1, \"127.0.0.2\");\n     cs.handle(new NodeAddedSchedulerEvent(node2));\n \n     Priority u0Priority = TestUtils.createMockPriority(1);\n     RecordFactory recordFactory =\n         RecordFactoryProvider.getRecordFactory(null);\n \n-    FiCaSchedulerApp[] fiCaApps = new FiCaSchedulerApp[appCount];\n-    for (int i=0;i<appCount;i++) {\n+    FiCaSchedulerApp[] fiCaApps = new FiCaSchedulerApp[totalApps];\n+    for (int i=0;i<totalApps;i++) {\n       fiCaApps[i] =\n           cs.getSchedulerApplications().get(apps[i].getApplicationId())\n               .getCurrentAppAttempt();\n@@ -193,8 +208,30 @@ private void testUserLimitThroughputWithNumberOfResourceTypes(\n       fiCaApps[i].updateResourceRequests(\n           Collections.singletonList(resourceRequest));\n     }\n-    // Now force everything to be over user limit\n-    qb.setUserLimitFactor((float)0.0);\n+    // Now force everything to be at user limit\n+    for (int i = 0; i < numQueues; i++) {\n+      lqs[i].setUserLimitFactor((float)0.0);\n+    }\n+\n+    // allocate one container for each extra apps since\n+    //  LeafQueue.canAssignToUser() checks for used > limit, not used >= limit\n+    cs.handle(new NodeUpdateSchedulerEvent(node));\n+    cs.handle(new NodeUpdateSchedulerEvent(node2));\n+\n+    // make sure only the extra apps have allocated containers\n+    for (int i=0;i<totalApps;i++) {\n+      boolean pending = fiCaApps[i].getAppSchedulingInfo().isPending();\n+      if (i < activeQueues) {\n+        assertFalse(pending);\n+        assertEquals(0,\n+            fiCaApps[i].getTotalPendingRequestsPerPartition().size());\n+      } else {\n+        assertTrue(pending);\n+        assertEquals(1*GB,\n+            fiCaApps[i].getTotalPendingRequestsPerPartition()\n+                .get(RMNodeLabelsManager.NO_LABEL).getMemorySize());\n+      }\n+    }\n \n     // Quiet the loggers while measuring throughput\n     GenericTestUtils.setRootLogLevel(Level.WARN);\n@@ -233,27 +270,86 @@ private void testUserLimitThroughputWithNumberOfResourceTypes(\n     }\n     System.out.println(\n         \"#ResourceTypes = \" + numOfResourceTypes + \". Avg of fastest \" + entries\n-            + \": \" + numerator / (timespent / entries));\n+            + \": \" + numerator / (timespent / entries) + \" ops/sec of \"\n+            + appCount + \" apps on \" + pctActiveQueues + \"% of \" + numQueues\n+            + \" queues.\");\n+\n+    // make sure only the extra apps have allocated containers\n+    for (int i=0;i<totalApps;i++) {\n+      boolean pending = fiCaApps[i].getAppSchedulingInfo().isPending();\n+      if (i < activeQueues) {\n+        assertFalse(pending);\n+        assertEquals(0,\n+            fiCaApps[i].getTotalPendingRequestsPerPartition().size());\n+      } else {\n+        assertTrue(pending);\n+        assertEquals(1*GB,\n+            fiCaApps[i].getTotalPendingRequestsPerPartition()\n+                .get(RMNodeLabelsManager.NO_LABEL).getMemorySize());\n+      }\n+    }\n+\n+    rm.close();\n     rm.stop();\n   }\n \n   @Test(timeout = 300000)\n   public void testUserLimitThroughputForTwoResources() throws Exception {\n-    testUserLimitThroughputWithNumberOfResourceTypes(2);\n+    testUserLimitThroughputWithNumberOfResourceTypes(2, 1, 100, 100);\n   }\n \n   @Test(timeout = 300000)\n   public void testUserLimitThroughputForThreeResources() throws Exception {\n-    testUserLimitThroughputWithNumberOfResourceTypes(3);\n+    testUserLimitThroughputWithNumberOfResourceTypes(3, 1, 100, 100);\n   }\n \n   @Test(timeout = 300000)\n   public void testUserLimitThroughputForFourResources() throws Exception {\n-    testUserLimitThroughputWithNumberOfResourceTypes(4);\n+    testUserLimitThroughputWithNumberOfResourceTypes(4, 1, 100, 100);\n   }\n \n   @Test(timeout = 300000)\n   public void testUserLimitThroughputForFiveResources() throws Exception {\n-    testUserLimitThroughputWithNumberOfResourceTypes(5);\n+    testUserLimitThroughputWithNumberOfResourceTypes(5, 1, 100, 100);\n+  }\n+\n+  @Test(timeout = 1800000)\n+  public void testUserLimitThroughputWithManyQueues() throws Exception {\n+\n+    int numQueues = Integer.getInteger(\"NumberOfQueues\", 40);\n+    int pctActiveQueues = Integer.getInteger(\"PercentActiveQueues\", 100);\n+    int appCount = Integer.getInteger(\"NumberOfApplications\", 100);\n+\n+    testUserLimitThroughputWithNumberOfResourceTypes(\n+        2, numQueues, pctActiveQueues, appCount);\n+  }\n+\n+  CapacitySchedulerConfiguration createCSConfWithManyQueues(int numQueues)\n+      throws Exception {\n+    CapacitySchedulerConfiguration csconf =\n+        new CapacitySchedulerConfiguration();\n+    csconf.setResourceComparator(DominantResourceCalculator.class);\n+    csconf.setMaximumApplicationMasterResourcePerQueuePercent(\"root\", 100.0f);\n+    csconf.setMaximumAMResourcePercentPerPartition(\"root\", \"\", 100.0f);\n+    csconf.setCapacity(\"root.default\", 0.0f);\n+    csconf.setOffSwitchPerHeartbeatLimit(numQueues);\n+\n+    float capacity = 100.0f / numQueues;\n+    String[] subQueues = new String[numQueues];\n+    for (int i = 0; i < numQueues; i++) {\n+      String queueName = String.format(\"%03d\", i);\n+      String queuePath = \"root.\" + queueName;\n+      subQueues[i] = queueName;\n+      csconf.setMaximumApplicationMasterResourcePerQueuePercent(\n+          queuePath, 100.0f);\n+      csconf.setMaximumAMResourcePercentPerPartition(queuePath, \"\", 100.0f);\n+      csconf.setCapacity(queuePath, capacity);\n+      csconf.setUserLimitFactor(queuePath, 100.0f);\n+      csconf.setMaximumCapacity(queuePath, 100.0f);\n+    }\n+\n+    csconf.setQueues(\"root\", subQueues);\n+\n+    return csconf;\n   }\n }",
      "parent_sha": "fddc3d55c3e309936216b8c61944e113509999e2"
    }
  },
  {
    "oid": "221089760910743eae58eb2cbd5ac86c4ee96a17",
    "message": "YARN-9592. Use Logger format in ContainersMonitorImpl. Contributed by Inigo Goiri.",
    "date": "2019-06-01T00:35:49Z",
    "url": "https://github.com/apache/hadoop/commit/221089760910743eae58eb2cbd5ac86c4ee96a17",
    "details": {
      "sha": "43c7820e39e12ef95df86780ba5e571813a76666",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java",
      "status": "modified",
      "additions": 55,
      "deletions": 55,
      "changes": 110,
      "blob_url": "https://github.com/apache/hadoop/blob/221089760910743eae58eb2cbd5ac86c4ee96a17/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-nodemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fnodemanager%2Fcontainermanager%2Fmonitor%2FContainersMonitorImpl.java",
      "raw_url": "https://github.com/apache/hadoop/raw/221089760910743eae58eb2cbd5ac86c4ee96a17/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-nodemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fnodemanager%2Fcontainermanager%2Fmonitor%2FContainersMonitorImpl.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-nodemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fnodemanager%2Fcontainermanager%2Fmonitor%2FContainersMonitorImpl.java?ref=221089760910743eae58eb2cbd5ac86c4ee96a17",
      "patch": "@@ -133,13 +133,12 @@ protected void serviceInit(Configuration myConf) throws Exception {\n \n     this.resourceCalculatorPlugin =\n         ResourceCalculatorPlugin.getContainersMonitorPlugin(this.conf);\n-    LOG.info(\" Using ResourceCalculatorPlugin : \"\n-        + this.resourceCalculatorPlugin);\n+    LOG.info(\"Using ResourceCalculatorPlugin: {}\",\n+        this.resourceCalculatorPlugin);\n     processTreeClass = this.conf.getClass(\n             YarnConfiguration.NM_CONTAINER_MON_PROCESS_TREE, null,\n             ResourceCalculatorProcessTree.class);\n-    LOG.info(\" Using ResourceCalculatorProcessTree : \"\n-        + this.processTreeClass);\n+    LOG.info(\"Using ResourceCalculatorProcessTree: {}\", this.processTreeClass);\n \n     this.containerMetricsEnabled =\n         this.conf.getBoolean(YarnConfiguration.NM_CONTAINER_METRICS_ENABLE,\n@@ -185,10 +184,10 @@ protected void serviceInit(Configuration myConf) throws Exception {\n     strictMemoryEnforcement = conf.getBoolean(\n         YarnConfiguration.NM_MEMORY_RESOURCE_ENFORCED,\n         YarnConfiguration.DEFAULT_NM_MEMORY_RESOURCE_ENFORCED);\n-    LOG.info(\"Physical memory check enabled: \" + pmemCheckEnabled);\n-    LOG.info(\"Virtual memory check enabled: \" + vmemCheckEnabled);\n-    LOG.info(\"Elastic memory control enabled: \" + elasticMemoryEnforcement);\n-    LOG.info(\"Strict memory control enabled: \" + strictMemoryEnforcement);\n+    LOG.info(\"Physical memory check enabled: {}\", pmemCheckEnabled);\n+    LOG.info(\"Virtual memory check enabled: {}\", vmemCheckEnabled);\n+    LOG.info(\"Elastic memory control enabled: {}\", elasticMemoryEnforcement);\n+    LOG.info(\"Strict memory control enabled: {}\", strictMemoryEnforcement);\n \n     if (elasticMemoryEnforcement) {\n       if (!CGroupElasticMemoryController.isAvailable()) {\n@@ -213,7 +212,7 @@ protected void serviceInit(Configuration myConf) throws Exception {\n \n     containersMonitorEnabled =\n         isContainerMonitorEnabled() && monitoringInterval > 0;\n-    LOG.info(\"ContainersMonitor enabled: \" + containersMonitorEnabled);\n+    LOG.info(\"ContainersMonitor enabled: {}\", containersMonitorEnabled);\n \n     nodeCpuPercentageForYARN =\n         NodeManagerHardwareUtils.getNodeCpuPercentage(this.conf);\n@@ -226,20 +225,21 @@ protected void serviceInit(Configuration myConf) throws Exception {\n             .getPhysicalMemorySize();\n         if (totalPhysicalMemoryOnNM <= 0) {\n           LOG.warn(\"NodeManager's totalPmem could not be calculated. \"\n-              + \"Setting it to \" + UNKNOWN_MEMORY_LIMIT);\n+              + \"Setting it to {}\", UNKNOWN_MEMORY_LIMIT);\n           totalPhysicalMemoryOnNM = UNKNOWN_MEMORY_LIMIT;\n         }\n       }\n \n       if (totalPhysicalMemoryOnNM != UNKNOWN_MEMORY_LIMIT &&\n           this.maxPmemAllottedForContainers > totalPhysicalMemoryOnNM * 0.80f) {\n-        LOG.warn(\"NodeManager configured with \"\n-            + TraditionalBinaryPrefix.long2String(maxPmemAllottedForContainers,\n-                \"\", 1)\n-            + \" physical memory allocated to containers, which is more than \"\n-            + \"80% of the total physical memory available (\"\n-            + TraditionalBinaryPrefix.long2String(totalPhysicalMemoryOnNM, \"\",\n-                1) + \"). Thrashing might happen.\");\n+        LOG.warn(\n+            \"NodeManager configured with {} physical memory allocated to \" +\n+            \"containers, which is more than 80% of the total physical memory \" +\n+            \"available ({}). Thrashing might happen.\",\n+            TraditionalBinaryPrefix.long2String(\n+                maxPmemAllottedForContainers, \"B\", 1),\n+            TraditionalBinaryPrefix.long2String(\n+                totalPhysicalMemoryOnNM, \"B\", 1));\n       }\n     }\n     super.serviceInit(this.conf);\n@@ -264,13 +264,13 @@ private boolean isContainerMonitorEnabled() {\n \n   private boolean isResourceCalculatorAvailable() {\n     if (resourceCalculatorPlugin == null) {\n-      LOG.info(\"ResourceCalculatorPlugin is unavailable on this system. \" + this\n-          .getClass().getName() + \" is disabled.\");\n+      LOG.info(\"ResourceCalculatorPlugin is unavailable on this system. \"\n+          + \"{} is disabled.\", this.getClass().getName());\n       return false;\n     }\n     if (getResourceCalculatorProcessTree(\"0\") == null) {\n       LOG.info(\"ResourceCalculatorProcessTree is unavailable on this system. \"\n-          + this.getClass().getName() + \" is disabled.\");\n+          + \"{} is disabled.\", this.getClass().getName());\n       return false;\n     }\n     return true;\n@@ -426,15 +426,15 @@ private boolean isProcessTreeOverLimit(String containerId,\n     boolean isOverLimit = false;\n \n     if (currentMemUsage > (2 * memLimit)) {\n-      LOG.warn(\"Process tree for container: \" + containerId\n-          + \" running over twice \" + \"the configured limit. Limit=\" + memLimit\n-          + \", current usage = \" + currentMemUsage);\n+      LOG.warn(\"Process tree for container: {} running over twice \"\n+          + \"the configured limit. Limit={}, current usage = {}\",\n+          containerId, memLimit, currentMemUsage);\n       isOverLimit = true;\n     } else if (curMemUsageOfAgedProcesses > memLimit) {\n-      LOG.warn(\"Process tree for container: \" + containerId\n-          + \" has processes older than 1 \"\n-          + \"iteration running over the configured limit. Limit=\" + memLimit\n-          + \", current usage = \" + curMemUsageOfAgedProcesses);\n+      LOG.warn(\"Process tree for container: {} has processes older than 1 \"\n+          + \"iteration running over the configured limit. \"\n+          + \"Limit={}, current usage = {}\",\n+          containerId, memLimit, curMemUsageOfAgedProcesses);\n       isOverLimit = true;\n     }\n \n@@ -468,8 +468,8 @@ public void run() {\n             tmp.append(p.getPID());\n             tmp.append(\" \");\n           }\n-          LOG.debug(\"Current ProcessTree list : {}\",\n-              tmp.substring(0, tmp.length()) + \"]\");\n+          tmp.append(\"]\");\n+          LOG.debug(\"Current ProcessTree list : {}\", tmp);\n         }\n \n         // Temporary structure to calculate the total resource utilization of\n@@ -495,8 +495,9 @@ public void run() {\n             if (pId == null || !isResourceCalculatorAvailable()) {\n               continue; // processTree cannot be tracked\n             }\n-            LOG.debug(\"Constructing ProcessTree for : PID = {}\"\n-                +\" ContainerId = {}\", pId, containerId);\n+            LOG.debug(\n+                \"Constructing ProcessTree for : PID = {} ContainerId = {}\",\n+                pId, containerId);\n             ResourceCalculatorProcessTree pTree = ptInfo.getProcessTree();\n             pTree.updateProcessTree();    // update process-tree\n             long currentVmemUsage = pTree.getVirtualMemorySize();\n@@ -509,8 +510,8 @@ public void run() {\n               // CPU usage is not available likely because the container just\n               // started. Let us skip this turn and consider this container\n               // in the next iteration.\n-              LOG.info(\"Skipping monitoring container \" + containerId\n-                  + \" since CPU usage is not yet available.\");\n+              LOG.info(\"Skipping monitoring container {} since \"\n+                  + \"CPU usage is not yet available.\", containerId);\n               continue;\n             }\n \n@@ -558,8 +559,8 @@ public void run() {\n         try {\n           Thread.sleep(monitoringInterval);\n         } catch (InterruptedException e) {\n-          LOG.warn(ContainersMonitorImpl.class.getName()\n-              + \" is interrupted. Exiting.\");\n+          LOG.warn(\"{} is interrupted. Exiting.\",\n+              ContainersMonitorImpl.class.getName());\n           break;\n         }\n       }\n@@ -604,16 +605,16 @@ private void initializeProcessTrees(\n             if ((ipAndHost != null) && (ipAndHost[0] != null) &&\n                 (ipAndHost[1] != null)) {\n               container.setIpAndHost(ipAndHost);\n-              LOG.info(containerId + \"'s ip = \" + ipAndHost[0]\n-                  + \", and hostname = \" + ipAndHost[1]);\n+              LOG.info(\"{}'s ip = {}, and hostname = {}\",\n+                  containerId, ipAndHost[0], ipAndHost[1]);\n             } else {\n-              LOG.info(\"Can not get both ip and hostname: \"\n-                  + Arrays.toString(ipAndHost));\n+              LOG.info(\"Can not get both ip and hostname: {}\",\n+                  Arrays.toString(ipAndHost));\n             }\n             String exposedPorts = containerExecutor.getExposedPorts(container);\n             container.setExposedPorts(exposedPorts);\n           } else {\n-            LOG.info(containerId + \" is missing. Not setting ip and hostname\");\n+            LOG.info(\"{} is missing. Not setting ip and hostname\", containerId);\n           }\n         }\n       }\n@@ -648,15 +649,15 @@ private void recordUsage(ContainerId containerId, String pId,\n       long vmemLimit = ptInfo.getVmemLimit();\n       long pmemLimit = ptInfo.getPmemLimit();\n       if (AUDITLOG.isDebugEnabled()) {\n-        AUDITLOG.debug(String.format(\n-            \"Resource usage of ProcessTree %s for container-id %s:\" +\n-                \" %s CPU:%f CPU/core:%f\",\n-            pId, containerId.toString(),\n+        AUDITLOG.debug(\n+            \"Resource usage of ProcessTree {} for container-id {}:\" +\n+            \" {} CPU:{} CPU/core:{}\",\n+            pId, containerId,\n             formatUsageString(\n                 currentVmemUsage, vmemLimit,\n                 currentPmemUsage, pmemLimit),\n             cpuUsagePercentPerCore,\n-            cpuUsageTotalCoresPercentage));\n+            cpuUsageTotalCoresPercentage);\n       }\n \n       // Add resource utilization for this container\n@@ -754,15 +755,15 @@ && isProcessTreeOverLimit(containerId.toString(),\n         LOG.warn(msg);\n         // warn if not a leader\n         if (!pTree.checkPidPgrpidForMatch()) {\n-          LOG.error(\"Killed container process with PID \" + pId\n-                  + \" but it is not a process group leader.\");\n+          LOG.error(\"Killed container process with PID {} \"\n+              + \"but it is not a process group leader.\", pId);\n         }\n         // kill the container\n         eventDispatcher.getEventHandler().handle(\n                 new ContainerKillEvent(containerId,\n                       containerExitStatus, msg));\n         trackingContainers.remove(containerId);\n-        LOG.info(\"Removed ProcessTree with root \" + pId);\n+        LOG.info(\"Removed ProcessTree with root {}\", pId);\n       }\n     }\n \n@@ -784,7 +785,7 @@ private void reportResourceUsage(ContainerId containerId,\n                   currentPmemUsage, cpuUsagePercentPerCore);\n         }\n       } else {\n-        LOG.info(containerId + \" does not exist to report\");\n+        LOG.info(\"{} does not exist to report\", containerId);\n       }\n     }\n \n@@ -967,12 +968,11 @@ private void onChangeMonitoringContainerResource(\n     if (containersMonitorEnabled) {\n       ProcessTreeInfo processTreeInfo = trackingContainers.get(containerId);\n       if (processTreeInfo == null) {\n-        LOG.warn(\"Failed to track container \"\n-            + containerId.toString()\n-            + \". It may have already completed.\");\n+        LOG.warn(\"Failed to track container {}. It may have already completed.\",\n+            containerId);\n         return;\n       }\n-      LOG.info(\"Changing resource-monitoring for \" + containerId);\n+      LOG.info(\"Changing resource-monitoring for {}\", containerId);\n       updateContainerMetrics(monitoringEvent);\n       long pmemLimit =\n           changeEvent.getResource().getMemorySize() * 1024L * 1024L;\n@@ -984,7 +984,7 @@ private void onChangeMonitoringContainerResource(\n \n   private void onStopMonitoringContainer(\n       ContainersMonitorEvent monitoringEvent, ContainerId containerId) {\n-    LOG.info(\"Stopping resource-monitoring for \" + containerId);\n+    LOG.info(\"Stopping resource-monitoring for {}\", containerId);\n     updateContainerMetrics(monitoringEvent);\n     trackingContainers.remove(containerId);\n   }\n@@ -993,7 +993,7 @@ private void onStartMonitoringContainer(\n       ContainersMonitorEvent monitoringEvent, ContainerId containerId) {\n     ContainerStartMonitoringEvent startEvent =\n         (ContainerStartMonitoringEvent) monitoringEvent;\n-    LOG.info(\"Starting resource-monitoring for \" + containerId);\n+    LOG.info(\"Starting resource-monitoring for {}\", containerId);\n     updateContainerMetrics(monitoringEvent);\n     trackingContainers.put(containerId,\n         new ProcessTreeInfo(containerId, null, null,",
      "parent_sha": "c1d2d92187de7de6df0bcf195f3db792a269351a"
    }
  },
  {
    "oid": "d5dfee24b11c347bac4690dc5afe21a54b50a691",
    "message": "YARN-5050. Code cleanup for TestDistributedShell (Li Lu via sjlee)",
    "date": "2016-07-10T15:45:59Z",
    "url": "https://github.com/apache/hadoop/commit/d5dfee24b11c347bac4690dc5afe21a54b50a691",
    "details": {
      "sha": "c02cd852393d5f397fb996988a1b4b432ff4f61c",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java",
      "status": "modified",
      "additions": 7,
      "deletions": 6,
      "changes": 13,
      "blob_url": "https://github.com/apache/hadoop/blob/d5dfee24b11c347bac4690dc5afe21a54b50a691/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-applications-distributedshell%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fapplications%2Fdistributedshell%2FTestDistributedShell.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d5dfee24b11c347bac4690dc5afe21a54b50a691/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-applications-distributedshell%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fapplications%2Fdistributedshell%2FTestDistributedShell.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-applications%2Fhadoop-yarn-applications-distributedshell%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fapplications%2Fdistributedshell%2FTestDistributedShell.java?ref=d5dfee24b11c347bac4690dc5afe21a54b50a691",
      "patch": "@@ -135,6 +135,8 @@ private void setupInternal(int numNodeManager, float timelineVersion)\n \n     conf = new YarnConfiguration();\n     conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 128);\n+    // reduce the teardown waiting time\n+    conf.setLong(YarnConfiguration.DISPATCHER_DRAIN_EVENTS_TIMEOUT, 1000);\n     conf.set(\"yarn.log.dir\", \"target\");\n     conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\n     // mark if we need to launch the v1 timeline server\n@@ -183,9 +185,6 @@ private void setupInternal(int numNodeManager, float timelineVersion)\n       conf.set(YarnConfiguration.NM_AUX_SERVICES, TIMELINE_AUX_SERVICE_NAME);\n       conf.set(YarnConfiguration.NM_AUX_SERVICES + \".\" + TIMELINE_AUX_SERVICE_NAME\n         + \".class\", PerNodeTimelineCollectorsAuxService.class.getName());\n-      conf.setBoolean(YarnConfiguration.SYSTEM_METRICS_PUBLISHER_ENABLED, true);\n-      conf.setBoolean(YarnConfiguration.RM_SYSTEM_METRICS_PUBLISHER_ENABLED,\n-          false);\n     } else {\n       Assert.fail(\"Wrong timeline version number: \" + timelineVersion);\n     }\n@@ -280,7 +279,7 @@ public void testDSShellWithDomainV1_5() throws Exception {\n     testDSShell(true);\n   }\n \n-  @Test(timeout=90000)\n+  @Test\n   @TimelineVersion(2.0f)\n   public void testDSShellWithoutDomainV2() throws Exception {\n     testDSShell(false);\n@@ -290,12 +289,14 @@ public void testDSShell(boolean haveDomain) throws Exception {\n     testDSShell(haveDomain, true);\n   }\n \n-  @Test(timeout=90000)\n+  @Test\n+  @TimelineVersion(2.0f)\n   public void testDSShellWithoutDomainV2DefaultFlow() throws Exception {\n     testDSShell(false, true);\n   }\n \n-  @Test(timeout=90000)\n+  @Test\n+  @TimelineVersion(2.0f)\n   public void testDSShellWithoutDomainV2CustomizedFlow() throws Exception {\n     testDSShell(false, false);\n   }",
      "parent_sha": "4a7011a45f5ede25d0e07ee3bde60d1b41ec24e9"
    }
  },
  {
    "oid": "12ad63d7232ca72be9eff5680d974fc16999aac3",
    "message": "HDFS-10725. Caller context should always be constructed by a builder. (Contributed by Mingliang Liu)",
    "date": "2016-08-16T03:14:05Z",
    "url": "https://github.com/apache/hadoop/commit/12ad63d7232ca72be9eff5680d974fc16999aac3",
    "details": {
      "sha": "3d21bfe28afb9c1548981c5d187e404eeb90c87a",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/CallerContext.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/hadoop/blob/12ad63d7232ca72be9eff5680d974fc16999aac3/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FCallerContext.java",
      "raw_url": "https://github.com/apache/hadoop/raw/12ad63d7232ca72be9eff5680d974fc16999aac3/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FCallerContext.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FCallerContext.java?ref=12ad63d7232ca72be9eff5680d974fc16999aac3",
      "patch": "@@ -35,7 +35,7 @@\n @InterfaceAudience.LimitedPrivate({\"HBase\", \"HDFS\", \"Hive\", \"MapReduce\",\n     \"Pig\", \"YARN\"})\n @InterfaceStability.Evolving\n-public class CallerContext {\n+public final class CallerContext {\n   public static final Charset SIGNATURE_ENCODING = StandardCharsets.UTF_8;\n   /** The caller context.\n    *\n@@ -54,7 +54,7 @@ public class CallerContext {\n    */\n   private final byte[] signature;\n \n-  public CallerContext(Builder builder) {\n+  private CallerContext(Builder builder) {\n     this.context = builder.context;\n     this.signature = builder.signature;\n   }",
      "parent_sha": "5628b36c0872d58c9b25f23da3dab4eafad9bca3"
    }
  },
  {
    "oid": "f3b8ff54ab08545d7093bf8861b44ec9912e8dc3",
    "message": "YARN-5921. Incorrect synchronization in RMContextImpl#setHAServiceState/getHAServiceState. Contributed by Varun Saxena",
    "date": "2016-12-06T01:23:38Z",
    "url": "https://github.com/apache/hadoop/commit/f3b8ff54ab08545d7093bf8861b44ec9912e8dc3",
    "details": {
      "sha": "3f17ac69a8d6380c5262aeefd0fb58dfc098741a",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMContextImpl.java",
      "status": "modified",
      "additions": 6,
      "deletions": 4,
      "changes": 10,
      "blob_url": "https://github.com/apache/hadoop/blob/f3b8ff54ab08545d7093bf8861b44ec9912e8dc3/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FRMContextImpl.java",
      "raw_url": "https://github.com/apache/hadoop/raw/f3b8ff54ab08545d7093bf8861b44ec9912e8dc3/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FRMContextImpl.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-resourcemanager%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Fresourcemanager%2FRMContextImpl.java?ref=f3b8ff54ab08545d7093bf8861b44ec9912e8dc3",
      "patch": "@@ -80,6 +80,8 @@ public class RMContextImpl implements RMContext {\n \n   private QueueLimitCalculator queueLimitCalculator;\n \n+  private final Object haServiceStateLock = new Object();\n+\n   /**\n    * Default constructor. To be used in conjunction with setter methods for\n    * individual fields.\n@@ -254,9 +256,9 @@ void setHAEnabled(boolean isHAEnabled) {\n     this.isHAEnabled = isHAEnabled;\n   }\n \n-  void setHAServiceState(HAServiceState haServiceState) {\n-    synchronized (haServiceState) {\n-      this.haServiceState = haServiceState;\n+  void setHAServiceState(HAServiceState serviceState) {\n+    synchronized (haServiceStateLock) {\n+      this.haServiceState = serviceState;\n     }\n   }\n \n@@ -352,7 +354,7 @@ public boolean isHAEnabled() {\n \n   @Override\n   public HAServiceState getHAServiceState() {\n-    synchronized (haServiceState) {\n+    synchronized (haServiceStateLock) {\n       return haServiceState;\n     }\n   }",
      "parent_sha": "dcedb72af468128458e597f08d22f5c34b744ae5"
    }
  },
  {
    "oid": "d7a462f00261d7672b8e04e744fcb0129f3fc3f9",
    "message": "[YARN-11776] Handle NPE in the RMDelegationTokenIdentifier if localServiceAddress is null (#7431) Contributed by Abhey Rana.\n\nReviewed-by: Viraj Jasani <vjasani@apache.org>\nSigned-off-by: Shilun Fan <slfan1989@apache.org>",
    "date": "2025-03-06T06:44:26Z",
    "url": "https://github.com/apache/hadoop/commit/d7a462f00261d7672b8e04e744fcb0129f3fc3f9",
    "details": {
      "sha": "ba3242fb32167f90f13744f0e9eb9dbeca52cfcc",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/security/client/RMDelegationTokenIdentifier.java",
      "status": "modified",
      "additions": 4,
      "deletions": 2,
      "changes": 6,
      "blob_url": "https://github.com/apache/hadoop/blob/d7a462f00261d7672b8e04e744fcb0129f3fc3f9/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fsecurity%2Fclient%2FRMDelegationTokenIdentifier.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d7a462f00261d7672b8e04e744fcb0129f3fc3f9/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fsecurity%2Fclient%2FRMDelegationTokenIdentifier.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fsecurity%2Fclient%2FRMDelegationTokenIdentifier.java?ref=d7a462f00261d7672b8e04e744fcb0129f3fc3f9",
      "patch": "@@ -21,6 +21,7 @@\n \n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.net.InetAddress;\n \n import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.classification.InterfaceAudience.Public;\n@@ -139,9 +140,10 @@ private static ApplicationClientProtocol getRmClient(Token<?> token,\n       String[] services = token.getService().toString().split(\",\");\n       for (String service : services) {\n         InetSocketAddress addr = NetUtils.createSocketAddr(service);\n-        if (localSecretManager != null) {\n+        if (localSecretManager != null && localServiceAddress != null) {\n           // return null if it's our token\n-          if (localServiceAddress.getAddress().isAnyLocalAddress()) {\n+          InetAddress localServiceAddr = localServiceAddress.getAddress();\n+          if (localServiceAddr != null && localServiceAddr.isAnyLocalAddress()) {\n             if (NetUtils.isLocalAddress(addr.getAddress()) &&\n                 addr.getPort() == localServiceAddress.getPort()) {\n               return null;",
      "parent_sha": "f0430f22f735b653e9ce071001be5b93989ae627"
    }
  },
  {
    "oid": "51bd528bd34b6c01d9d1126ffed2fc6cb94604c2",
    "message": "YARN-10957. Using invokeConcurrent Overload with Collection in getClusterMetrics (#3439)",
    "date": "2021-09-28T16:51:18Z",
    "url": "https://github.com/apache/hadoop/commit/51bd528bd34b6c01d9d1126ffed2fc6cb94604c2",
    "details": {
      "sha": "3133ad5ceda6ae59c31dfed3b797dee4a277d24e",
      "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/FederationClientInterceptor.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/hadoop/blob/51bd528bd34b6c01d9d1126ffed2fc6cb94604c2/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-router%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Frouter%2Fclientrm%2FFederationClientInterceptor.java",
      "raw_url": "https://github.com/apache/hadoop/raw/51bd528bd34b6c01d9d1126ffed2fc6cb94604c2/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-router%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Frouter%2Fclientrm%2FFederationClientInterceptor.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project%2Fhadoop-yarn%2Fhadoop-yarn-server%2Fhadoop-yarn-server-router%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fyarn%2Fserver%2Frouter%2Fclientrm%2FFederationClientInterceptor.java?ref=51bd528bd34b6c01d9d1126ffed2fc6cb94604c2",
      "patch": "@@ -664,11 +664,10 @@ public GetClusterMetricsResponse getClusterMetrics(\n         federationFacade.getSubClusters(true);\n     ClientMethod remoteMethod = new ClientMethod(\"getClusterMetrics\",\n         new Class[] {GetClusterMetricsRequest.class}, new Object[] {request});\n-    ArrayList<SubClusterId> clusterList = new ArrayList<>(subclusters.keySet());\n     Map<SubClusterId, GetClusterMetricsResponse> clusterMetrics;\n \n     try {\n-      clusterMetrics = invokeConcurrent(clusterList, remoteMethod,\n+      clusterMetrics = invokeConcurrent(subclusters.keySet(), remoteMethod,\n           GetClusterMetricsResponse.class);\n \n     } catch (Exception ex) {",
      "parent_sha": "f5148ca542108d1e308a89922d765119bfa73ab4"
    }
  },
  {
    "oid": "d08b8c801a908b4242e7b21a54f3b1e4072f1eae",
    "message": "HADOOP-14893. WritableRpcEngine should use Time.monotonicNow. Contributed by Chetna Chaudhari.",
    "date": "2017-09-26T07:46:03Z",
    "url": "https://github.com/apache/hadoop/commit/d08b8c801a908b4242e7b21a54f3b1e4072f1eae",
    "details": {
      "sha": "f2b5862372a50f1005648ed8b823596fc711b31d",
      "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/WritableRpcEngine.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/hadoop/blob/d08b8c801a908b4242e7b21a54f3b1e4072f1eae/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FWritableRpcEngine.java",
      "raw_url": "https://github.com/apache/hadoop/raw/d08b8c801a908b4242e7b21a54f3b1e4072f1eae/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FWritableRpcEngine.java",
      "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project%2Fhadoop-common%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FWritableRpcEngine.java?ref=d08b8c801a908b4242e7b21a54f3b1e4072f1eae",
      "patch": "@@ -231,7 +231,7 @@ public Object invoke(Object proxy, Method method, Object[] args)\n       throws Throwable {\n       long startTime = 0;\n       if (LOG.isDebugEnabled()) {\n-        startTime = Time.now();\n+        startTime = Time.monotonicNow();\n       }\n \n       // if Tracing is on then start a new span for this rpc.\n@@ -251,7 +251,7 @@ public Object invoke(Object proxy, Method method, Object[] args)\n         if (traceScope != null) traceScope.close();\n       }\n       if (LOG.isDebugEnabled()) {\n-        long callTime = Time.now() - startTime;\n+        long callTime = Time.monotonicNow() - startTime;\n         LOG.debug(\"Call: \" + method.getName() + \" \" + callTime);\n       }\n       return value.get();",
      "parent_sha": "a2b31e355a73c37dada15a18a90a690314be7fd3"
    }
  }
]