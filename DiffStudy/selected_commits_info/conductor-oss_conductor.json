[
  {
    "oid": "bd1b0dc76c09bf3b10a72d407469a0b22443d675",
    "message": "run spotlessApply",
    "date": "2023-04-19T20:10:10Z",
    "url": "https://github.com/conductor-oss/conductor/commit/bd1b0dc76c09bf3b10a72d407469a0b22443d675",
    "details": {
      "sha": "334774c9af59cb51f92e5d43125d6aa548bb8518",
      "filename": "core/src/main/java/com/netflix/conductor/core/events/DefaultEventProcessor.java",
      "status": "modified",
      "additions": 330,
      "deletions": 327,
      "changes": 657,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/bd1b0dc76c09bf3b10a72d407469a0b22443d675/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fevents%2FDefaultEventProcessor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/bd1b0dc76c09bf3b10a72d407469a0b22443d675/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fevents%2FDefaultEventProcessor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fevents%2FDefaultEventProcessor.java?ref=bd1b0dc76c09bf3b10a72d407469a0b22443d675",
      "patch": "@@ -1,327 +1,330 @@\n-/*\n- * Copyright 2022 Netflix, Inc.\n- * <p>\n- * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- * <p>\n- * http://www.apache.org/licenses/LICENSE-2.0\n- * <p>\n- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n- * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n- * specific language governing permissions and limitations under the License.\n- */\n-package com.netflix.conductor.core.events;\n-\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.concurrent.CompletableFuture;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.ThreadFactory;\n-import com.netflix.conductor.core.exception.TransientException;\n-import org.apache.commons.lang3.StringUtils;\n-import org.apache.commons.lang3.concurrent.BasicThreadFactory;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-import org.springframework.beans.factory.annotation.Qualifier;\n-import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\n-import org.springframework.retry.support.RetryTemplate;\n-import org.springframework.stereotype.Component;\n-import org.springframework.util.CollectionUtils;\n-import com.netflix.conductor.common.metadata.events.EventExecution;\n-import com.netflix.conductor.common.metadata.events.EventExecution.Status;\n-import com.netflix.conductor.common.metadata.events.EventHandler;\n-import com.netflix.conductor.common.metadata.events.EventHandler.Action;\n-import com.netflix.conductor.core.config.ConductorProperties;\n-import com.netflix.conductor.core.events.queue.Message;\n-import com.netflix.conductor.core.events.queue.ObservableQueue;\n-import com.netflix.conductor.core.execution.evaluators.Evaluator;\n-import com.netflix.conductor.core.utils.JsonUtils;\n-import com.netflix.conductor.metrics.Monitors;\n-import com.netflix.conductor.service.ExecutionService;\n-import com.netflix.conductor.service.MetadataService;\n-import com.fasterxml.jackson.databind.ObjectMapper;\n-import com.spotify.futures.CompletableFutures;\n-\n-import static com.netflix.conductor.core.utils.Utils.isTransientException;\n-\n-/**\n- * Event Processor is used to dispatch actions configured in the event handlers, based on incoming\n- * events to the event queues.\n- *\n- * <p><code>Set conductor.default-event-processor.enabled=false</code> to disable event processing.\n- */\n-@Component\n-@ConditionalOnProperty(\n-        name = \"conductor.default-event-processor.enabled\",\n-        havingValue = \"true\",\n-        matchIfMissing = true)\n-public class DefaultEventProcessor {\n-\n-    private static final Logger LOGGER = LoggerFactory.getLogger(DefaultEventProcessor.class);\n-\n-    private final MetadataService metadataService;\n-    private final ExecutionService executionService;\n-    private final ActionProcessor actionProcessor;\n-\n-    private final ExecutorService eventActionExecutorService;\n-    private final ObjectMapper objectMapper;\n-    private final JsonUtils jsonUtils;\n-    private final boolean isEventMessageIndexingEnabled;\n-    private final Map<String, Evaluator> evaluators;\n-    private final RetryTemplate retryTemplate;\n-\n-    public DefaultEventProcessor(\n-            ExecutionService executionService,\n-            MetadataService metadataService,\n-            ActionProcessor actionProcessor,\n-            JsonUtils jsonUtils,\n-            ConductorProperties properties,\n-            ObjectMapper objectMapper,\n-            Map<String, Evaluator> evaluators,\n-            @Qualifier(\"onTransientErrorRetryTemplate\") RetryTemplate retryTemplate) {\n-        this.executionService = executionService;\n-        this.metadataService = metadataService;\n-        this.actionProcessor = actionProcessor;\n-        this.objectMapper = objectMapper;\n-        this.jsonUtils = jsonUtils;\n-        this.evaluators = evaluators;\n-        this.retryTemplate = retryTemplate;\n-\n-        if (properties.getEventProcessorThreadCount() <= 0) {\n-            throw new IllegalStateException(\n-                    \"Cannot set event processor thread count to <=0. To disable event \"\n-                            + \"processing, set conductor.default-event-processor.enabled=false.\");\n-        }\n-        ThreadFactory threadFactory =\n-                new BasicThreadFactory.Builder()\n-                        .namingPattern(\"event-action-executor-thread-%d\")\n-                        .build();\n-        eventActionExecutorService =\n-                Executors.newFixedThreadPool(\n-                        properties.getEventProcessorThreadCount(), threadFactory);\n-\n-        this.isEventMessageIndexingEnabled = properties.isEventMessageIndexingEnabled();\n-        LOGGER.info(\"Event Processing is ENABLED\");\n-    }\n-\n-    public void handle(ObservableQueue queue, Message msg) {\n-        List<EventExecution> transientFailures = null;\n-        boolean executionFailed = false;\n-        try {\n-            if (isEventMessageIndexingEnabled) {\n-                executionService.addMessage(queue.getName(), msg);\n-            }\n-            String event = queue.getType() + \":\" + queue.getName();\n-            LOGGER.debug(\"Evaluating message: {} for event: {}\", msg.getId(), event);\n-            transientFailures = executeEvent(event, msg);\n-        } catch (Exception e) {\n-            executionFailed = true;\n-            LOGGER.error(\"Error handling message: {} on queue:{}\", msg, queue.getName(), e);\n-            Monitors.recordEventQueueMessagesError(queue.getType(), queue.getName());\n-        } finally {\n-            if (!executionFailed && CollectionUtils.isEmpty(transientFailures)) {\n-                queue.ack(Collections.singletonList(msg));\n-                LOGGER.debug(\"Message: {} acked on queue: {}\", msg.getId(), queue.getName());\n-            } else if (queue.rePublishIfNoAck() || !CollectionUtils.isEmpty(transientFailures)) {\n-                // re-submit this message to the queue, to be retried later\n-                // This is needed for queues with no unack timeout, since messages are removed\n-                // from the queue\n-                queue.publish(Collections.singletonList(msg));\n-                LOGGER.debug(\"Message: {} published to queue: {}\", msg.getId(), queue.getName());\n-            } else {\n-                queue.nack(Collections.singletonList(msg));\n-                LOGGER.debug(\"Message: {} nacked on queue: {}\", msg.getId(), queue.getName());\n-            }\n-            Monitors.recordEventQueueMessagesHandled(queue.getType(), queue.getName());\n-        }\n-    }\n-\n-    /**\n-     * Executes all the actions configured on all the event handlers triggered by the {@link\n-     * Message} on the queue If any of the actions on an event handler fails due to a transient\n-     * failure, the execution is not persisted such that it can be retried\n-     *\n-     * @return a list of {@link EventExecution} that failed due to transient failures.\n-     */\n-    protected List<EventExecution> executeEvent(String event, Message msg) throws Exception {\n-        List<EventHandler> eventHandlerList;\n-        List<EventExecution> transientFailures = new ArrayList<>();\n-\n-        try {\n-            eventHandlerList = metadataService.getEventHandlersForEvent(event, true);\n-        } catch (TransientException transientException) {\n-            transientFailures.add(new EventExecution(event, msg.getId()));\n-            return transientFailures;\n-        }\n-\n-        Object payloadObject = getPayloadObject(msg.getPayload());\n-        for (EventHandler eventHandler : eventHandlerList) {\n-            String condition = eventHandler.getCondition();\n-            String evaluatorType = eventHandler.getEvaluatorType();\n-            // Set default to true so that if condition is not specified, it falls through\n-            // to process the event.\n-            boolean success = true;\n-            if (StringUtils.isNotEmpty(condition) && evaluators.get(evaluatorType) != null) {\n-                Object result =\n-                        evaluators\n-                                .get(evaluatorType)\n-                                .evaluate(condition, jsonUtils.expand(payloadObject));\n-                success = ScriptEvaluator.toBoolean(result);\n-            } else if (StringUtils.isNotEmpty(condition)) {\n-                LOGGER.debug(\"Checking condition: {} for event: {}\", condition, event);\n-                success = ScriptEvaluator.evalBool(condition, jsonUtils.expand(payloadObject));\n-            }\n-\n-            if (!success) {\n-                String id = msg.getId() + \"_\" + 0;\n-                EventExecution eventExecution = new EventExecution(id, msg.getId());\n-                eventExecution.setCreated(System.currentTimeMillis());\n-                eventExecution.setEvent(eventHandler.getEvent());\n-                eventExecution.setName(eventHandler.getName());\n-                eventExecution.setStatus(Status.SKIPPED);\n-                eventExecution.getOutput().put(\"msg\", msg.getPayload());\n-                eventExecution.getOutput().put(\"condition\", condition);\n-                executionService.addEventExecution(eventExecution);\n-                LOGGER.debug(\n-                        \"Condition: {} not successful for event: {} with payload: {}\",\n-                        condition,\n-                        eventHandler.getEvent(),\n-                        msg.getPayload());\n-                continue;\n-            }\n-\n-            CompletableFuture<List<EventExecution>> future =\n-                    executeActionsForEventHandler(eventHandler, msg);\n-            future.whenComplete(\n-                            (result, error) ->\n-                                    result.forEach(\n-                                            eventExecution -> {\n-                                                if (error != null\n-                                                        || eventExecution.getStatus()\n-                                                                == Status.IN_PROGRESS) {\n-                                                    transientFailures.add(eventExecution);\n-                                                } else {\n-                                                    executionService.updateEventExecution(\n-                                                            eventExecution);\n-                                                }\n-                                            }))\n-                    .get();\n-        }\n-        return processTransientFailures(transientFailures);\n-    }\n-\n-    /**\n-     * Remove the event executions which failed temporarily.\n-     *\n-     * @param eventExecutions The event executions which failed with a transient error.\n-     * @return The event executions which failed with a transient error.\n-     */\n-    protected List<EventExecution> processTransientFailures(List<EventExecution> eventExecutions) {\n-        eventExecutions.forEach(executionService::removeEventExecution);\n-        return eventExecutions;\n-    }\n-\n-    /**\n-     * @param eventHandler the {@link EventHandler} for which the actions are to be executed\n-     * @param msg the {@link Message} that triggered the event\n-     * @return a {@link CompletableFuture} holding a list of {@link EventExecution}s for the {@link\n-     *     Action}s executed in the event handler\n-     */\n-    protected CompletableFuture<List<EventExecution>> executeActionsForEventHandler(\n-            EventHandler eventHandler, Message msg) {\n-        List<CompletableFuture<EventExecution>> futuresList = new ArrayList<>();\n-        int i = 0;\n-        for (Action action : eventHandler.getActions()) {\n-            String id = msg.getId() + \"_\" + i++;\n-            EventExecution eventExecution = new EventExecution(id, msg.getId());\n-            eventExecution.setCreated(System.currentTimeMillis());\n-            eventExecution.setEvent(eventHandler.getEvent());\n-            eventExecution.setName(eventHandler.getName());\n-            eventExecution.setAction(action.getAction());\n-            eventExecution.setStatus(Status.IN_PROGRESS);\n-            if (executionService.addEventExecution(eventExecution)) {\n-                futuresList.add(\n-                        CompletableFuture.supplyAsync(\n-                                () ->\n-                                        execute(\n-                                                eventExecution,\n-                                                action,\n-                                                getPayloadObject(msg.getPayload())),\n-                                eventActionExecutorService));\n-            } else {\n-                LOGGER.warn(\"Duplicate delivery/execution of message: {}\", msg.getId());\n-            }\n-        }\n-        return CompletableFutures.allAsList(futuresList);\n-    }\n-\n-    /**\n-     * @param eventExecution the instance of {@link EventExecution}\n-     * @param action the {@link Action} to be executed for the event\n-     * @param payload the {@link Message#getPayload()}\n-     * @return the event execution updated with execution output, if the execution is\n-     *     completed/failed with non-transient error the input event execution, if the execution\n-     *     failed due to transient error\n-     */\n-    protected EventExecution execute(EventExecution eventExecution, Action action, Object payload) {\n-        try {\n-            LOGGER.debug(\n-                    \"Executing action: {} for event: {} with messageId: {} with payload: {}\",\n-                    action.getAction(),\n-                    eventExecution.getId(),\n-                    eventExecution.getMessageId(),\n-                    payload);\n-\n-            // TODO: Switch to @Retryable annotation on SimpleActionProcessor.execute()\n-            Map<String, Object> output =\n-                    retryTemplate.execute(\n-                            context ->\n-                                    actionProcessor.execute(\n-                                            action,\n-                                            payload,\n-                                            eventExecution.getEvent(),\n-                                            eventExecution.getMessageId()));\n-            if (output != null) {\n-                eventExecution.getOutput().putAll(output);\n-            }\n-            eventExecution.setStatus(Status.COMPLETED);\n-            Monitors.recordEventExecutionSuccess(\n-                    eventExecution.getEvent(),\n-                    eventExecution.getName(),\n-                    eventExecution.getAction().name());\n-        } catch (RuntimeException e) {\n-            LOGGER.error(\n-                    \"Error executing action: {} for event: {} with messageId: {}\",\n-                    action.getAction(),\n-                    eventExecution.getEvent(),\n-                    eventExecution.getMessageId(),\n-                    e);\n-            if (!isTransientException(e)) {\n-                // not a transient error, fail the event execution\n-                eventExecution.setStatus(Status.FAILED);\n-                eventExecution.getOutput().put(\"exception\", e.getMessage());\n-                Monitors.recordEventExecutionError(\n-                        eventExecution.getEvent(),\n-                        eventExecution.getName(),\n-                        eventExecution.getAction().name(),\n-                        e.getClass().getSimpleName());\n-            }\n-        }\n-        return eventExecution;\n-    }\n-\n-    private Object getPayloadObject(String payload) {\n-        Object payloadObject = null;\n-        if (payload != null) {\n-            try {\n-                payloadObject = objectMapper.readValue(payload, Object.class);\n-            } catch (Exception e) {\n-                payloadObject = payload;\n-            }\n-        }\n-        return payloadObject;\n-    }\n-}\n+/*\r\n+ * Copyright 2022 Netflix, Inc.\r\n+ * <p>\r\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\r\n+ * the License. You may obtain a copy of the License at\r\n+ * <p>\r\n+ * http://www.apache.org/licenses/LICENSE-2.0\r\n+ * <p>\r\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\r\n+ * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\r\n+ * specific language governing permissions and limitations under the License.\r\n+ */\r\n+package com.netflix.conductor.core.events;\r\n+\r\n+import java.util.ArrayList;\r\n+import java.util.Collections;\r\n+import java.util.List;\r\n+import java.util.Map;\r\n+import java.util.concurrent.CompletableFuture;\r\n+import java.util.concurrent.ExecutorService;\r\n+import java.util.concurrent.Executors;\r\n+import java.util.concurrent.ThreadFactory;\r\n+\r\n+import org.apache.commons.lang3.StringUtils;\r\n+import org.apache.commons.lang3.concurrent.BasicThreadFactory;\r\n+import org.slf4j.Logger;\r\n+import org.slf4j.LoggerFactory;\r\n+import org.springframework.beans.factory.annotation.Qualifier;\r\n+import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\r\n+import org.springframework.retry.support.RetryTemplate;\r\n+import org.springframework.stereotype.Component;\r\n+import org.springframework.util.CollectionUtils;\r\n+\r\n+import com.netflix.conductor.common.metadata.events.EventExecution;\r\n+import com.netflix.conductor.common.metadata.events.EventExecution.Status;\r\n+import com.netflix.conductor.common.metadata.events.EventHandler;\r\n+import com.netflix.conductor.common.metadata.events.EventHandler.Action;\r\n+import com.netflix.conductor.core.config.ConductorProperties;\r\n+import com.netflix.conductor.core.events.queue.Message;\r\n+import com.netflix.conductor.core.events.queue.ObservableQueue;\r\n+import com.netflix.conductor.core.exception.TransientException;\r\n+import com.netflix.conductor.core.execution.evaluators.Evaluator;\r\n+import com.netflix.conductor.core.utils.JsonUtils;\r\n+import com.netflix.conductor.metrics.Monitors;\r\n+import com.netflix.conductor.service.ExecutionService;\r\n+import com.netflix.conductor.service.MetadataService;\r\n+\r\n+import com.fasterxml.jackson.databind.ObjectMapper;\r\n+import com.spotify.futures.CompletableFutures;\r\n+\r\n+import static com.netflix.conductor.core.utils.Utils.isTransientException;\r\n+\r\n+/**\r\n+ * Event Processor is used to dispatch actions configured in the event handlers, based on incoming\r\n+ * events to the event queues.\r\n+ *\r\n+ * <p><code>Set conductor.default-event-processor.enabled=false</code> to disable event processing.\r\n+ */\r\n+@Component\r\n+@ConditionalOnProperty(\r\n+        name = \"conductor.default-event-processor.enabled\",\r\n+        havingValue = \"true\",\r\n+        matchIfMissing = true)\r\n+public class DefaultEventProcessor {\r\n+\r\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DefaultEventProcessor.class);\r\n+\r\n+    private final MetadataService metadataService;\r\n+    private final ExecutionService executionService;\r\n+    private final ActionProcessor actionProcessor;\r\n+\r\n+    private final ExecutorService eventActionExecutorService;\r\n+    private final ObjectMapper objectMapper;\r\n+    private final JsonUtils jsonUtils;\r\n+    private final boolean isEventMessageIndexingEnabled;\r\n+    private final Map<String, Evaluator> evaluators;\r\n+    private final RetryTemplate retryTemplate;\r\n+\r\n+    public DefaultEventProcessor(\r\n+            ExecutionService executionService,\r\n+            MetadataService metadataService,\r\n+            ActionProcessor actionProcessor,\r\n+            JsonUtils jsonUtils,\r\n+            ConductorProperties properties,\r\n+            ObjectMapper objectMapper,\r\n+            Map<String, Evaluator> evaluators,\r\n+            @Qualifier(\"onTransientErrorRetryTemplate\") RetryTemplate retryTemplate) {\r\n+        this.executionService = executionService;\r\n+        this.metadataService = metadataService;\r\n+        this.actionProcessor = actionProcessor;\r\n+        this.objectMapper = objectMapper;\r\n+        this.jsonUtils = jsonUtils;\r\n+        this.evaluators = evaluators;\r\n+        this.retryTemplate = retryTemplate;\r\n+\r\n+        if (properties.getEventProcessorThreadCount() <= 0) {\r\n+            throw new IllegalStateException(\r\n+                    \"Cannot set event processor thread count to <=0. To disable event \"\r\n+                            + \"processing, set conductor.default-event-processor.enabled=false.\");\r\n+        }\r\n+        ThreadFactory threadFactory =\r\n+                new BasicThreadFactory.Builder()\r\n+                        .namingPattern(\"event-action-executor-thread-%d\")\r\n+                        .build();\r\n+        eventActionExecutorService =\r\n+                Executors.newFixedThreadPool(\r\n+                        properties.getEventProcessorThreadCount(), threadFactory);\r\n+\r\n+        this.isEventMessageIndexingEnabled = properties.isEventMessageIndexingEnabled();\r\n+        LOGGER.info(\"Event Processing is ENABLED\");\r\n+    }\r\n+\r\n+    public void handle(ObservableQueue queue, Message msg) {\r\n+        List<EventExecution> transientFailures = null;\r\n+        boolean executionFailed = false;\r\n+        try {\r\n+            if (isEventMessageIndexingEnabled) {\r\n+                executionService.addMessage(queue.getName(), msg);\r\n+            }\r\n+            String event = queue.getType() + \":\" + queue.getName();\r\n+            LOGGER.debug(\"Evaluating message: {} for event: {}\", msg.getId(), event);\r\n+            transientFailures = executeEvent(event, msg);\r\n+        } catch (Exception e) {\r\n+            executionFailed = true;\r\n+            LOGGER.error(\"Error handling message: {} on queue:{}\", msg, queue.getName(), e);\r\n+            Monitors.recordEventQueueMessagesError(queue.getType(), queue.getName());\r\n+        } finally {\r\n+            if (!executionFailed && CollectionUtils.isEmpty(transientFailures)) {\r\n+                queue.ack(Collections.singletonList(msg));\r\n+                LOGGER.debug(\"Message: {} acked on queue: {}\", msg.getId(), queue.getName());\r\n+            } else if (queue.rePublishIfNoAck() || !CollectionUtils.isEmpty(transientFailures)) {\r\n+                // re-submit this message to the queue, to be retried later\r\n+                // This is needed for queues with no unack timeout, since messages are removed\r\n+                // from the queue\r\n+                queue.publish(Collections.singletonList(msg));\r\n+                LOGGER.debug(\"Message: {} published to queue: {}\", msg.getId(), queue.getName());\r\n+            } else {\r\n+                queue.nack(Collections.singletonList(msg));\r\n+                LOGGER.debug(\"Message: {} nacked on queue: {}\", msg.getId(), queue.getName());\r\n+            }\r\n+            Monitors.recordEventQueueMessagesHandled(queue.getType(), queue.getName());\r\n+        }\r\n+    }\r\n+\r\n+    /**\r\n+     * Executes all the actions configured on all the event handlers triggered by the {@link\r\n+     * Message} on the queue If any of the actions on an event handler fails due to a transient\r\n+     * failure, the execution is not persisted such that it can be retried\r\n+     *\r\n+     * @return a list of {@link EventExecution} that failed due to transient failures.\r\n+     */\r\n+    protected List<EventExecution> executeEvent(String event, Message msg) throws Exception {\r\n+        List<EventHandler> eventHandlerList;\r\n+        List<EventExecution> transientFailures = new ArrayList<>();\r\n+\r\n+        try {\r\n+            eventHandlerList = metadataService.getEventHandlersForEvent(event, true);\r\n+        } catch (TransientException transientException) {\r\n+            transientFailures.add(new EventExecution(event, msg.getId()));\r\n+            return transientFailures;\r\n+        }\r\n+\r\n+        Object payloadObject = getPayloadObject(msg.getPayload());\r\n+        for (EventHandler eventHandler : eventHandlerList) {\r\n+            String condition = eventHandler.getCondition();\r\n+            String evaluatorType = eventHandler.getEvaluatorType();\r\n+            // Set default to true so that if condition is not specified, it falls through\r\n+            // to process the event.\r\n+            boolean success = true;\r\n+            if (StringUtils.isNotEmpty(condition) && evaluators.get(evaluatorType) != null) {\r\n+                Object result =\r\n+                        evaluators\r\n+                                .get(evaluatorType)\r\n+                                .evaluate(condition, jsonUtils.expand(payloadObject));\r\n+                success = ScriptEvaluator.toBoolean(result);\r\n+            } else if (StringUtils.isNotEmpty(condition)) {\r\n+                LOGGER.debug(\"Checking condition: {} for event: {}\", condition, event);\r\n+                success = ScriptEvaluator.evalBool(condition, jsonUtils.expand(payloadObject));\r\n+            }\r\n+\r\n+            if (!success) {\r\n+                String id = msg.getId() + \"_\" + 0;\r\n+                EventExecution eventExecution = new EventExecution(id, msg.getId());\r\n+                eventExecution.setCreated(System.currentTimeMillis());\r\n+                eventExecution.setEvent(eventHandler.getEvent());\r\n+                eventExecution.setName(eventHandler.getName());\r\n+                eventExecution.setStatus(Status.SKIPPED);\r\n+                eventExecution.getOutput().put(\"msg\", msg.getPayload());\r\n+                eventExecution.getOutput().put(\"condition\", condition);\r\n+                executionService.addEventExecution(eventExecution);\r\n+                LOGGER.debug(\r\n+                        \"Condition: {} not successful for event: {} with payload: {}\",\r\n+                        condition,\r\n+                        eventHandler.getEvent(),\r\n+                        msg.getPayload());\r\n+                continue;\r\n+            }\r\n+\r\n+            CompletableFuture<List<EventExecution>> future =\r\n+                    executeActionsForEventHandler(eventHandler, msg);\r\n+            future.whenComplete(\r\n+                            (result, error) ->\r\n+                                    result.forEach(\r\n+                                            eventExecution -> {\r\n+                                                if (error != null\r\n+                                                        || eventExecution.getStatus()\r\n+                                                                == Status.IN_PROGRESS) {\r\n+                                                    transientFailures.add(eventExecution);\r\n+                                                } else {\r\n+                                                    executionService.updateEventExecution(\r\n+                                                            eventExecution);\r\n+                                                }\r\n+                                            }))\r\n+                    .get();\r\n+        }\r\n+        return processTransientFailures(transientFailures);\r\n+    }\r\n+\r\n+    /**\r\n+     * Remove the event executions which failed temporarily.\r\n+     *\r\n+     * @param eventExecutions The event executions which failed with a transient error.\r\n+     * @return The event executions which failed with a transient error.\r\n+     */\r\n+    protected List<EventExecution> processTransientFailures(List<EventExecution> eventExecutions) {\r\n+        eventExecutions.forEach(executionService::removeEventExecution);\r\n+        return eventExecutions;\r\n+    }\r\n+\r\n+    /**\r\n+     * @param eventHandler the {@link EventHandler} for which the actions are to be executed\r\n+     * @param msg the {@link Message} that triggered the event\r\n+     * @return a {@link CompletableFuture} holding a list of {@link EventExecution}s for the {@link\r\n+     *     Action}s executed in the event handler\r\n+     */\r\n+    protected CompletableFuture<List<EventExecution>> executeActionsForEventHandler(\r\n+            EventHandler eventHandler, Message msg) {\r\n+        List<CompletableFuture<EventExecution>> futuresList = new ArrayList<>();\r\n+        int i = 0;\r\n+        for (Action action : eventHandler.getActions()) {\r\n+            String id = msg.getId() + \"_\" + i++;\r\n+            EventExecution eventExecution = new EventExecution(id, msg.getId());\r\n+            eventExecution.setCreated(System.currentTimeMillis());\r\n+            eventExecution.setEvent(eventHandler.getEvent());\r\n+            eventExecution.setName(eventHandler.getName());\r\n+            eventExecution.setAction(action.getAction());\r\n+            eventExecution.setStatus(Status.IN_PROGRESS);\r\n+            if (executionService.addEventExecution(eventExecution)) {\r\n+                futuresList.add(\r\n+                        CompletableFuture.supplyAsync(\r\n+                                () ->\r\n+                                        execute(\r\n+                                                eventExecution,\r\n+                                                action,\r\n+                                                getPayloadObject(msg.getPayload())),\r\n+                                eventActionExecutorService));\r\n+            } else {\r\n+                LOGGER.warn(\"Duplicate delivery/execution of message: {}\", msg.getId());\r\n+            }\r\n+        }\r\n+        return CompletableFutures.allAsList(futuresList);\r\n+    }\r\n+\r\n+    /**\r\n+     * @param eventExecution the instance of {@link EventExecution}\r\n+     * @param action the {@link Action} to be executed for the event\r\n+     * @param payload the {@link Message#getPayload()}\r\n+     * @return the event execution updated with execution output, if the execution is\r\n+     *     completed/failed with non-transient error the input event execution, if the execution\r\n+     *     failed due to transient error\r\n+     */\r\n+    protected EventExecution execute(EventExecution eventExecution, Action action, Object payload) {\r\n+        try {\r\n+            LOGGER.debug(\r\n+                    \"Executing action: {} for event: {} with messageId: {} with payload: {}\",\r\n+                    action.getAction(),\r\n+                    eventExecution.getId(),\r\n+                    eventExecution.getMessageId(),\r\n+                    payload);\r\n+\r\n+            // TODO: Switch to @Retryable annotation on SimpleActionProcessor.execute()\r\n+            Map<String, Object> output =\r\n+                    retryTemplate.execute(\r\n+                            context ->\r\n+                                    actionProcessor.execute(\r\n+                                            action,\r\n+                                            payload,\r\n+                                            eventExecution.getEvent(),\r\n+                                            eventExecution.getMessageId()));\r\n+            if (output != null) {\r\n+                eventExecution.getOutput().putAll(output);\r\n+            }\r\n+            eventExecution.setStatus(Status.COMPLETED);\r\n+            Monitors.recordEventExecutionSuccess(\r\n+                    eventExecution.getEvent(),\r\n+                    eventExecution.getName(),\r\n+                    eventExecution.getAction().name());\r\n+        } catch (RuntimeException e) {\r\n+            LOGGER.error(\r\n+                    \"Error executing action: {} for event: {} with messageId: {}\",\r\n+                    action.getAction(),\r\n+                    eventExecution.getEvent(),\r\n+                    eventExecution.getMessageId(),\r\n+                    e);\r\n+            if (!isTransientException(e)) {\r\n+                // not a transient error, fail the event execution\r\n+                eventExecution.setStatus(Status.FAILED);\r\n+                eventExecution.getOutput().put(\"exception\", e.getMessage());\r\n+                Monitors.recordEventExecutionError(\r\n+                        eventExecution.getEvent(),\r\n+                        eventExecution.getName(),\r\n+                        eventExecution.getAction().name(),\r\n+                        e.getClass().getSimpleName());\r\n+            }\r\n+        }\r\n+        return eventExecution;\r\n+    }\r\n+\r\n+    private Object getPayloadObject(String payload) {\r\n+        Object payloadObject = null;\r\n+        if (payload != null) {\r\n+            try {\r\n+                payloadObject = objectMapper.readValue(payload, Object.class);\r\n+            } catch (Exception e) {\r\n+                payloadObject = payload;\r\n+            }\r\n+        }\r\n+        return payloadObject;\r\n+    }\r\n+}\r",
      "parent_sha": "cafb898ad49175fa1a786a8c22e5b333367cda6a"
    }
  },
  {
    "oid": "7576c306bb20dbafc4bdc2437ad35e2f12c78ad6",
    "message": "Prevent workflow to tasks mapping from being deleted  immediately when removing workflow with ttl - Redis DAO",
    "date": "2021-08-10T19:57:44Z",
    "url": "https://github.com/conductor-oss/conductor/commit/7576c306bb20dbafc4bdc2437ad35e2f12c78ad6",
    "details": {
      "sha": "37471341fd6d9e0d230acb7799fea088589e8770",
      "filename": "redis-persistence/src/main/java/com/netflix/conductor/redis/dao/RedisExecutionDAO.java",
      "status": "modified",
      "additions": 12,
      "deletions": 1,
      "changes": 13,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/7576c306bb20dbafc4bdc2437ad35e2f12c78ad6/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Fdao%2FRedisExecutionDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/7576c306bb20dbafc4bdc2437ad35e2f12c78ad6/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Fdao%2FRedisExecutionDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Fdao%2FRedisExecutionDAO.java?ref=7576c306bb20dbafc4bdc2437ad35e2f12c78ad6",
      "patch": "@@ -276,6 +276,15 @@ private void removeTaskMappings(Task task) {\n         jedisProxy.zrem(nsKey(TASK_LIMIT_BUCKET, task.getTaskDefName()), task.getTaskId());\n     }\n \n+    private void removeTaskMappingsWithExpiry(Task task) {\n+        String taskKey = task.getReferenceTaskName() + \"\" + task.getRetryCount();\n+\n+        jedisProxy.hdel(nsKey(SCHEDULED_TASKS, task.getWorkflowInstanceId()), taskKey);\n+        jedisProxy.srem(nsKey(IN_PROGRESS_TASKS, task.getTaskDefName()), task.getTaskId());\n+        jedisProxy.srem(nsKey(TASKS_IN_PROGRESS_STATUS, task.getTaskDefName()), task.getTaskId());\n+        jedisProxy.zrem(nsKey(TASK_LIMIT_BUCKET, task.getTaskDefName()), task.getTaskId());\n+    }\n+\n     @Override\n     public boolean removeTask(String taskId) {\n         Task task = getTask(taskId);\n@@ -296,7 +305,7 @@ private boolean removeTaskWithExpiry(String taskId, int ttlSeconds) {\n             LOGGER.warn(\"No such task found by id {}\", taskId);\n             return false;\n         }\n-        removeTaskMappings(task);\n+        removeTaskMappingsWithExpiry(task);\n \n         jedisProxy.expire(nsKey(TASK, task.getTaskId()), ttlSeconds);\n         recordRedisDaoRequests(\"removeTask\", task.getTaskType(), task.getWorkflowType());\n@@ -397,6 +406,8 @@ public boolean removeWorkflowWithExpiry(String workflowId, int ttlSeconds) {\n             for (Task task : workflow.getTasks()) {\n                 removeTaskWithExpiry(task.getTaskId(), ttlSeconds);\n             }\n+            jedisProxy.expire(nsKey(WORKFLOW_TO_TASKS, workflowId), ttlSeconds);\n+\n             return true;\n         }\n         return false;",
      "parent_sha": "b831d7e6d4e09e73e5e371cdbd10b28f34ca23a8"
    }
  },
  {
    "oid": "1735708367db2c65db40130484a07ac4b8eac996",
    "message": "issue-3719: removed any from location in test setup",
    "date": "2023-09-07T19:30:19Z",
    "url": "https://github.com/conductor-oss/conductor/commit/1735708367db2c65db40130484a07ac4b8eac996",
    "details": {
      "sha": "e5fd2c04d2795ba6cf53ea46b04c7942ed9b6974",
      "filename": "core/src/test/java/com/netflix/conductor/core/storage/DummyPayloadStorageTest.java",
      "status": "modified",
      "additions": 5,
      "deletions": 2,
      "changes": 7,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/1735708367db2c65db40130484a07ac4b8eac996/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fstorage%2FDummyPayloadStorageTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/1735708367db2c65db40130484a07ac4b8eac996/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fstorage%2FDummyPayloadStorageTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fstorage%2FDummyPayloadStorageTest.java?ref=1735708367db2c65db40130484a07ac4b8eac996",
      "patch": "@@ -24,6 +24,7 @@\n import org.junit.Test;\n \n import com.netflix.conductor.common.run.ExternalStorageLocation;\n+import com.netflix.conductor.common.utils.ExternalPayloadStorage;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n \n@@ -33,7 +34,6 @@\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n-import static org.mockito.ArgumentMatchers.any;\n \n public class DummyPayloadStorageTest {\n \n@@ -52,7 +52,10 @@ public void setup() {\n         dummyPayloadStorage = new DummyPayloadStorage();\n         objectMapper = new ObjectMapper();\n         location =\n-                dummyPayloadStorage.getLocation(any(), PayloadType.TASK_OUTPUT, TEST_STORAGE_PATH);\n+                dummyPayloadStorage.getLocation(\n+                        ExternalPayloadStorage.Operation.WRITE,\n+                        PayloadType.TASK_OUTPUT,\n+                        TEST_STORAGE_PATH);\n         try {\n             byte[] payloadBytes = MOCK_PAYLOAD.getBytes(\"UTF-8\");\n             dummyPayloadStorage.upload(",
      "parent_sha": "13a689d133a414e223e19fc3a7ee75a094a12d56"
    }
  },
  {
    "oid": "d4fabb4a183a1f820739f77324c28d501d5290a3",
    "message": "Incorporate review comment. Add a warn log statement when workflow is null and remove unnecessary check",
    "date": "2022-10-13T20:12:00Z",
    "url": "https://github.com/conductor-oss/conductor/commit/d4fabb4a183a1f820739f77324c28d501d5290a3",
    "details": {
      "sha": "335b8fbd79790232b0e93af4f14bfa6b8794219d",
      "filename": "core/src/main/java/com/netflix/conductor/core/reconciliation/WorkflowSweeper.java",
      "status": "modified",
      "additions": 3,
      "deletions": 10,
      "changes": 13,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/d4fabb4a183a1f820739f77324c28d501d5290a3/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Freconciliation%2FWorkflowSweeper.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/d4fabb4a183a1f820739f77324c28d501d5290a3/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Freconciliation%2FWorkflowSweeper.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Freconciliation%2FWorkflowSweeper.java?ref=d4fabb4a183a1f820739f77324c28d501d5290a3",
      "patch": "@@ -24,7 +24,6 @@\n import com.netflix.conductor.annotations.VisibleForTesting;\n import com.netflix.conductor.common.metadata.tasks.TaskDef;\n import com.netflix.conductor.common.metadata.tasks.TaskType;\n-import com.netflix.conductor.common.metadata.workflow.WorkflowTask;\n import com.netflix.conductor.core.WorkflowContext;\n import com.netflix.conductor.core.config.ConductorProperties;\n import com.netflix.conductor.core.exception.NotFoundException;\n@@ -99,6 +98,9 @@ public void sweep(String workflowId) {\n         if (workflow != null) {\n             unack(workflow);\n         } else {\n+            LOGGER.warn(\n+                    \"Workflow with {} id can not be found. Attempting to unack using the id\",\n+                    workflowId);\n             queueDAO.setUnackTimeout(\n                     DECIDER_QUEUE, workflowId, properties.getWorkflowOffsetTimeout().toMillis());\n         }\n@@ -125,15 +127,6 @@ void unack(WorkflowModel workflowModel) {\n             }\n             if (taskModel.getStatus() == Status.SCHEDULED) {\n                 Optional<TaskDef> taskDefinition = taskModel.getTaskDefinition();\n-                if (taskDefinition.isEmpty()) {\n-                    taskDefinition =\n-                            Optional.ofNullable(\n-                                            workflowModel\n-                                                    .getWorkflowDefinition()\n-                                                    .getTaskByRefName(\n-                                                            taskModel.getReferenceTaskName()))\n-                                    .map(WorkflowTask::getTaskDefinition);\n-                }\n                 if (taskDefinition.isPresent()) {\n                     TaskDef taskDef = taskDefinition.get();\n                     if (taskDef.getPollTimeoutSeconds() != null",
      "parent_sha": "7a328dd1ae56337c8379e1469b508a5ae180ecd9"
    }
  },
  {
    "oid": "698c973958faaf06f911867ec056887b3b03f1b3",
    "message": "revert WorkflowDef changes",
    "date": "2024-04-03T22:26:11Z",
    "url": "https://github.com/conductor-oss/conductor/commit/698c973958faaf06f911867ec056887b3b03f1b3",
    "details": {
      "sha": "de0a2b95f0a506ba2e95d4285596543fce54196c",
      "filename": "common/src/main/java/com/netflix/conductor/common/metadata/workflow/WorkflowDef.java",
      "status": "modified",
      "additions": 3,
      "deletions": 45,
      "changes": 48,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/698c973958faaf06f911867ec056887b3b03f1b3/common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FWorkflowDef.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/698c973958faaf06f911867ec056887b3b03f1b3/common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FWorkflowDef.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FWorkflowDef.java?ref=698c973958faaf06f911867ec056887b3b03f1b3",
      "patch": "@@ -101,12 +101,6 @@ public enum TimeoutPolicy {\n     @ProtoField(id = 15)\n     private Map<String, Object> inputTemplate = new HashMap<>();\n \n-    @ProtoField(id = 16)\n-    private String webhookUrl;\n-\n-    @ProtoField(id = 17)\n-    private String webhookAuthToken;\n-\n     /**\n      * @return the name\n      */\n@@ -311,34 +305,6 @@ public void setVariables(Map<String, Object> variables) {\n         this.variables = variables;\n     }\n \n-    /**\n-     * @return the webhookUrl\n-     */\n-    public String getWebhookUrl() {\n-        return webhookUrl;\n-    }\n-\n-    /**\n-     * @param webhookUrl the webhookUrl to set\n-     */\n-    public void setWebhookUrl(String webhookUrl) {\n-        this.webhookUrl = webhookUrl;\n-    }\n-\n-    /**\n-     * @return the webhookAuthToken\n-     */\n-    public String getWebhookAuthToken() {\n-        return webhookAuthToken;\n-    }\n-\n-    /**\n-     * @param webhookAuthToken the webhookAuthToken to set\n-     */\n-    public void setWebhookAuthToken(String webhookAuthToken) {\n-        this.webhookAuthToken = webhookAuthToken;\n-    }\n-\n     public Map<String, Object> getInputTemplate() {\n         return inputTemplate;\n     }\n@@ -427,9 +393,7 @@ && getSchemaVersion() == that.getSchemaVersion()\n                 && Objects.equals(getOutputParameters(), that.getOutputParameters())\n                 && Objects.equals(getFailureWorkflow(), that.getFailureWorkflow())\n                 && Objects.equals(getOwnerEmail(), that.getOwnerEmail())\n-                && Objects.equals(getTimeoutSeconds(), that.getTimeoutSeconds())\n-                && Objects.equals(getWebhookUrl(), that.getWebhookUrl())\n-                && Objects.equals(getWebhookAuthToken(), that.getWebhookAuthToken());\n+                && Objects.equals(getTimeoutSeconds(), that.getTimeoutSeconds());\n     }\n \n     @Override\n@@ -444,9 +408,7 @@ public int hashCode() {\n                 getFailureWorkflow(),\n                 getSchemaVersion(),\n                 getOwnerEmail(),\n-                getTimeoutSeconds(),\n-                getWebhookUrl(),\n-                getWebhookAuthToken());\n+                getTimeoutSeconds());\n     }\n \n     @Override\n@@ -477,10 +439,6 @@ public String toString() {\n                 + workflowStatusListenerEnabled\n                 + \", timeoutSeconds=\"\n                 + timeoutSeconds\n-                + \", webhookUrl=\"\n-                + webhookUrl\n-                + \", webhookAuthToken=\"\n-                + webhookAuthToken\n                 + '}';\n     }\n-}\n+}\n\\ No newline at end of file",
      "parent_sha": "14c0c8d205a99daf37518f96a227053b46d6b159"
    }
  },
  {
    "oid": "d9f6a202a43fad7219e47cd8c73052c7c736e713",
    "message": "fix concurrent workflow state modify problem.",
    "date": "2019-01-30T06:21:05Z",
    "url": "https://github.com/conductor-oss/conductor/commit/d9f6a202a43fad7219e47cd8c73052c7c736e713",
    "details": {
      "sha": "8afea6da25a6e2c8acb2448bd604d9f97c3e1b36",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/WorkflowExecutor.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/d9f6a202a43fad7219e47cd8c73052c7c736e713/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/d9f6a202a43fad7219e47cd8c73052c7c736e713/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java?ref=d9f6a202a43fad7219e47cd8c73052c7c736e713",
      "patch": "@@ -831,8 +831,6 @@ public boolean decide(String workflowId) {\n                 }\n             }\n \n-            stateChanged = scheduleTask(workflow, tasksToBeScheduled) || stateChanged;\n-\n             if (!outcome.tasksToBeUpdated.isEmpty()) {\n                 for (Task task : tasksToBeUpdated) {\n                     if (task.getStatus() != null && (!task.getStatus().equals(Task.Status.IN_PROGRESS)\n@@ -848,6 +846,8 @@ public boolean decide(String workflowId) {\n                 queueDAO.push(DECIDER_QUEUE, workflow.getWorkflowId(), config.getSweepFrequency());\n             }\n \n+            stateChanged = scheduleTask(workflow, tasksToBeScheduled) || stateChanged;\n+\n             if (stateChanged) {\n                 decide(workflowId);\n             }",
      "parent_sha": "2cbc92c92bc04d52ab6b0ccc6631882a62ff2b10"
    }
  },
  {
    "oid": "10a8ce5f7e66baeb9bdf146a504182343a2db85a",
    "message": "fixes formatting",
    "date": "2018-07-15T04:08:06Z",
    "url": "https://github.com/conductor-oss/conductor/commit/10a8ce5f7e66baeb9bdf146a504182343a2db85a",
    "details": {
      "sha": "3d8bdcacc67aa30e61daf639dd39aaed03d2e5ae",
      "filename": "client/src/main/java/com/netflix/conductor/client/task/WorkflowTaskCoordinator.java",
      "status": "modified",
      "additions": 7,
      "deletions": 7,
      "changes": 14,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/10a8ce5f7e66baeb9bdf146a504182343a2db85a/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Ftask%2FWorkflowTaskCoordinator.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/10a8ce5f7e66baeb9bdf146a504182343a2db85a/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Ftask%2FWorkflowTaskCoordinator.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Ftask%2FWorkflowTaskCoordinator.java?ref=10a8ce5f7e66baeb9bdf146a504182343a2db85a",
      "patch": "@@ -273,15 +273,15 @@ public synchronized void init() {\n \t\t});\n \t}\n \n-    public void shutdown() {\n-        this.scheduledExecutorService.shutdown();\n-        this.executorService.shutdown();\n+    \tpublic void shutdown() {\n+        \tthis.scheduledExecutorService.shutdown();\n+        \tthis.executorService.shutdown();\n \n-        shutdownExecutorService(this.scheduledExecutorService, SHUTDOWN_WAIT_TIME_IN_SEC);\n-        shutdownExecutorService(this.executorService, SHUTDOWN_WAIT_TIME_IN_SEC);\n-    }\n+        \tshutdownExecutorService(this.scheduledExecutorService, SHUTDOWN_WAIT_TIME_IN_SEC);\n+        \tshutdownExecutorService(this.executorService, SHUTDOWN_WAIT_TIME_IN_SEC);\n+    \t}\n \n-    private void shutdownExecutorService(ExecutorService executorService, long timeout) {\n+    \tprivate void shutdownExecutorService(ExecutorService executorService, long timeout) {\n \t\ttry {\n \t\t\tif (executorService.awaitTermination(timeout, TimeUnit.SECONDS)) {\n \t\t\t\tlogger.debug(\"tasks completed, shutting down\");",
      "parent_sha": "4723993a227b7c3c05a188036579a6f7f058c26e"
    }
  },
  {
    "oid": "87671d7964417f3ccf7413d027dbb3ec9937ea43",
    "message": "Fixes compilation for ES6 configuration\n\nFixes #1147",
    "date": "2019-05-18T14:32:53Z",
    "url": "https://github.com/conductor-oss/conductor/commit/87671d7964417f3ccf7413d027dbb3ec9937ea43",
    "details": {
      "sha": "8fd5a06c10a628404b2ceeb0d38921ba7ce09dd7",
      "filename": "es6-persistence/src/main/java/com/netflix/conductor/dao/es6/index/ElasticSearchRestDAOV6.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/87671d7964417f3ccf7413d027dbb3ec9937ea43/es6-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fes6%2Findex%2FElasticSearchRestDAOV6.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/87671d7964417f3ccf7413d027dbb3ec9937ea43/es6-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fes6%2Findex%2FElasticSearchRestDAOV6.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/es6-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fes6%2Findex%2FElasticSearchRestDAOV6.java?ref=87671d7964417f3ccf7413d027dbb3ec9937ea43",
      "patch": "@@ -141,7 +141,7 @@ public ElasticSearchRestDAOV6(RestClientBuilder restClientBuilder, ElasticSearch\n \n         this.objectMapper = objectMapper;\n         this.elasticSearchAdminClient = restClientBuilder.build();\n-        this.elasticSearchClient = new RestHighLevelClient(restClientBuilder.build());\n+        this.elasticSearchClient = new RestHighLevelClient(restClientBuilder);\n         this.clusterHealthColor = config.getClusterHealthColor();\n \n         this.indexPrefix = config.getIndexName();",
      "parent_sha": "a1443d013ee7cd90db813e6ba2ec342911fc2c01"
    }
  },
  {
    "oid": "f660fccab19f9159bee85fd119955543a223c39b",
    "message": "Update TaskResourceTest.java",
    "date": "2024-12-09T07:24:08Z",
    "url": "https://github.com/conductor-oss/conductor/commit/f660fccab19f9159bee85fd119955543a223c39b",
    "details": {
      "sha": "99eec55badc490c74845e07518651f0728b64dce",
      "filename": "rest/src/test/java/com/netflix/conductor/rest/controllers/TaskResourceTest.java",
      "status": "modified",
      "additions": 8,
      "deletions": 2,
      "changes": 10,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/f660fccab19f9159bee85fd119955543a223c39b/rest%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Frest%2Fcontrollers%2FTaskResourceTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/f660fccab19f9159bee85fd119955543a223c39b/rest%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Frest%2Fcontrollers%2FTaskResourceTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/rest%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Frest%2Fcontrollers%2FTaskResourceTest.java?ref=f660fccab19f9159bee85fd119955543a223c39b",
      "patch": "@@ -18,6 +18,8 @@\n import java.util.List;\n import java.util.Map;\n \n+import com.netflix.conductor.model.TaskModel;\n+import com.netflix.conductor.service.WorkflowService;\n import org.junit.Before;\n import org.junit.Test;\n import org.springframework.http.ResponseEntity;\n@@ -47,11 +49,13 @@ public class TaskResourceTest {\n     private TaskService mockTaskService;\n \n     private TaskResource taskResource;\n+    private WorkflowService workflowService;\n \n     @Before\n     public void before() {\n         this.mockTaskService = mock(TaskService.class);\n-        this.taskResource = new TaskResource(this.mockTaskService);\n+        this.workflowService = mock(WorkflowService.class);\n+        this.taskResource = new TaskResource(this.mockTaskService, this.workflowService);\n     }\n \n     @Test\n@@ -86,7 +90,9 @@ public void testUpdateTask() {\n         TaskResult taskResult = new TaskResult();\n         taskResult.setStatus(TaskResult.Status.COMPLETED);\n         taskResult.setTaskId(\"123\");\n-        when(mockTaskService.updateTask(any(TaskResult.class))).thenReturn(\"123\");\n+        TaskModel taskModel = new TaskModel();\n+        taskModel.setTaskId(\"123\");\n+        when(mockTaskService.updateTask(any(TaskResult.class))).thenReturn(taskModel);\n         assertEquals(\"123\", taskResource.updateTask(taskResult));\n     }\n ",
      "parent_sha": "0779107adf1d263a351433617ed0c3312910b16a"
    }
  },
  {
    "oid": "69852d115d7b5dc2295ae33d1b2e09b6e803063d",
    "message": "Changing HttpTask test URI to fix failing test with conflicting IP Address.",
    "date": "2019-11-13T18:33:52Z",
    "url": "https://github.com/conductor-oss/conductor/commit/69852d115d7b5dc2295ae33d1b2e09b6e803063d",
    "details": {
      "sha": "18d7dda71ad813b0eb25467474ba8c652219712b",
      "filename": "contribs/src/test/java/com/netflix/conductor/contribs/http/TestHttpTask.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/69852d115d7b5dc2295ae33d1b2e09b6e803063d/contribs%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FTestHttpTask.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/69852d115d7b5dc2295ae33d1b2e09b6e803063d/contribs%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FTestHttpTask.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/contribs%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FTestHttpTask.java?ref=69852d115d7b5dc2295ae33d1b2e09b6e803063d",
      "patch": "@@ -292,7 +292,7 @@ public void testHTTPGetConnectionTimeOut() throws Exception{\n         Instant start  = Instant.now();\n         input.setConnectionTimeOut(110);\n         input.setMethod(\"GET\");\n-        input.setUri(\"http://10.255.255.255\");\n+        input.setUri(\"http://10.255.14.15\");\n         task.getInputData().put(HttpTask.REQUEST_PARAMETER_NAME, input);\n         task.setStatus(Status.SCHEDULED);\n         task.setScheduledTime(0);",
      "parent_sha": "77b719952499e520509a45cc3e5da603f190a422"
    }
  },
  {
    "oid": "ddf59b2942a39584ddf381ce6c4a255b771bccd7",
    "message": "Test for Priority field - addressing review comments",
    "date": "2025-03-27T15:49:57Z",
    "url": "https://github.com/conductor-oss/conductor/commit/ddf59b2942a39584ddf381ce6c4a255b771bccd7",
    "details": {
      "sha": "769dbfc4ed635bf91f03fa43b5597b09b198fe4d",
      "filename": "conductor-clients/java/conductor-java-sdk/conductor-client/src/test/java/com/netflix/conductor/common/metadata/workflow/WorkflowDefDeserializationTest.java",
      "status": "renamed",
      "additions": 6,
      "deletions": 9,
      "changes": 15,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/ddf59b2942a39584ddf381ce6c4a255b771bccd7/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FWorkflowDefDeserializationTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/ddf59b2942a39584ddf381ce6c4a255b771bccd7/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FWorkflowDefDeserializationTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FWorkflowDefDeserializationTest.java?ref=ddf59b2942a39584ddf381ce6c4a255b771bccd7",
      "patch": "@@ -10,21 +10,20 @@\n  * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n  * specific language governing permissions and limitations under the License.\n  */\n-package com.netflix.conductor.client.worker;\n+package com.netflix.conductor.common.metadata.workflow;\n \n import java.io.IOException;\n import java.io.InputStream;\n \n+import org.junit.jupiter.api.Assertions;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.DisplayName;\n import org.junit.jupiter.api.Test;\n \n import com.netflix.conductor.common.config.ObjectMapperProvider;\n-import com.netflix.conductor.common.metadata.workflow.WorkflowDef;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n \n-import static org.junit.jupiter.api.Assertions.assertEquals;\n \n class WorkflowDefDeserializationTest {\n \n@@ -38,16 +37,14 @@ public void setup() {\n     @Test\n     @DisplayName(\"Should correctly deserialize subworkflow priority as a dynamic expression\")\n     public void testSubworkflowPriorityDeserialization() throws Exception {\n-        WorkflowDef mainWorkflowDef;\n         try (InputStream inputStream = getClass().getResourceAsStream(\"/workflows/main_workflow.json\")) {\n             if (inputStream == null) {\n                 throw new IOException(\"Resource not found: /workflows/main_workflow.json\");\n             }\n-            mainWorkflowDef = objectMapper.readValue(inputStream, WorkflowDef.class);\n+            WorkflowDef mainWorkflowDef = objectMapper.readValue(inputStream, WorkflowDef.class);\n+            Assertions.assertEquals(\"${fetchPriority.output.priority}\",\n+                    mainWorkflowDef.getTasks().get(1).getSubWorkflowParam().getPriority().toString(),\n+                    \"Subworkflow priority should be deserialized as a dynamic expression\");\n         }\n-\n-        assertEquals(\"${fetchPriority.output.priority}\",\n-                mainWorkflowDef.getTasks().get(1).getSubWorkflowParam().getPriority().toString(),\n-                \"Subworkflow priority should be deserialized as a dynamic expression\");\n     }\n }",
      "previous_filename": "conductor-clients/java/conductor-java-sdk/conductor-client/src/test/java/com/netflix/conductor/client/worker/WorkflowDefDeserializationTest.java",
      "parent_sha": "2249a23e6f58673d80f93c4d02576b0d8de0fe59"
    }
  },
  {
    "oid": "caad53d6888de8cbbdc4e7d6974da1302bda2421",
    "message": "corrected JerseyRequestHandler initialization in ClientBase",
    "date": "2022-06-28T17:21:38Z",
    "url": "https://github.com/conductor-oss/conductor/commit/caad53d6888de8cbbdc4e7d6974da1302bda2421",
    "details": {
      "sha": "603697751b2aa240451afc9ad1da7fada782c2b3",
      "filename": "client/src/main/java/com/netflix/conductor/client/http/ClientBase.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/caad53d6888de8cbbdc4e7d6974da1302bda2421/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Fhttp%2FClientBase.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/caad53d6888de8cbbdc4e7d6974da1302bda2421/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Fhttp%2FClientBase.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Fhttp%2FClientBase.java?ref=caad53d6888de8cbbdc4e7d6974da1302bda2421",
      "patch": "@@ -75,7 +75,9 @@ protected ClientBase(\n             objectMapper.registerModule(new JavaTimeModule());\n         }\n \n-        this.requestHandler = ObjectUtils.defaultIfNull(requestHandler, new JerseyRequestHandler());\n+        // we do not want to use defaultIfNull here since creation of JerseyRequestHandler requires\n+        // classes that may not be in the classpath\n+        this.requestHandler = requestHandler != null ? requestHandler : new JerseyRequestHandler();\n         this.conductorClientConfiguration =\n                 ObjectUtils.defaultIfNull(\n                         clientConfiguration, new DefaultConductorClientConfiguration());",
      "parent_sha": "83841ac065b98e845cf446a920b0c1c4a0c8d102"
    }
  },
  {
    "oid": "6a6b7eaf6a4a2212325b676c881387e8a1f74307",
    "message": "add an ability to specify caller app namd and username",
    "date": "2018-12-18T22:31:51Z",
    "url": "https://github.com/conductor-oss/conductor/commit/6a6b7eaf6a4a2212325b676c881387e8a1f74307",
    "details": {
      "sha": "c56c2e9bc947eafbcf05bd2dda36dc8af4b01856",
      "filename": "core/src/main/java/com/netflix/conductor/core/WorkflowContext.java",
      "status": "modified",
      "additions": 19,
      "deletions": 8,
      "changes": 27,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/6a6b7eaf6a4a2212325b676c881387e8a1f74307/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2FWorkflowContext.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/6a6b7eaf6a4a2212325b676c881387e8a1f74307/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2FWorkflowContext.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2FWorkflowContext.java?ref=6a6b7eaf6a4a2212325b676c881387e8a1f74307",
      "patch": "@@ -1,5 +1,5 @@\n /**\n- * Copyright 2016 Netflix, Inc.\n+ * Copyright 2018 Netflix, Inc.\n  *\n  * Licensed under the Apache License, Version 2.0 (the \"License\");\n  * you may not use this file except in compliance with the License.\n@@ -18,20 +18,25 @@\n  */\n package com.netflix.conductor.core;\n \n-\n /**\n- * @author Viren\n- *\n+ * Store the authentication context, app or user name or both\n  */\n public class WorkflowContext {\n \n-\tpublic static final ThreadLocal<WorkflowContext> threadLocal = InheritableThreadLocal.withInitial(() -> new WorkflowContext(\"\"));\n+\tpublic static final ThreadLocal<WorkflowContext> threadLocal = InheritableThreadLocal.withInitial(() -> new WorkflowContext(\"\", \"\"));\n \t\n \tprivate String clientApp;\n-    \n-\t\n+\n+\tprivate String userName;\n+\n \tpublic WorkflowContext(String clientApp){\n \t\tthis.clientApp = clientApp;\n+\t\tthis.userName = null;\n+\t}\n+\n+\tpublic WorkflowContext(String clientApp, String userName){\n+\t\tthis.clientApp = clientApp;\n+\t\tthis.userName = userName;\n \t}\n \t\n \tpublic static WorkflowContext get(){\n@@ -47,11 +52,17 @@ public static void unset(){\n \t}\n \n \t/**\n-\t * todo: rename to more generic callerName\n \t * @return the clientApp\n \t */\n \tpublic String getClientApp() {\n \t\treturn clientApp;\n \t}\n+\n+\t/**\n+\t * @return the username\n+\t */\n+\tpublic String getUserName() {\n+\t\treturn userName;\n+\t}\n \t\n }",
      "parent_sha": "b86280d5e72f7507b2a0a61c6b6dac613fac9395"
    }
  },
  {
    "oid": "01971b0bb6acc727c5db0dfffac48cd95e929106",
    "message": "BuildProperties could be null during test runs",
    "date": "2022-01-27T22:42:21Z",
    "url": "https://github.com/conductor-oss/conductor/commit/01971b0bb6acc727c5db0dfffac48cd95e929106",
    "details": {
      "sha": "6794f186d7cf8f7fc3b8ead5769ac11d63ce32e2",
      "filename": "core/src/main/java/com/netflix/conductor/service/AdminServiceImpl.java",
      "status": "modified",
      "additions": 5,
      "deletions": 2,
      "changes": 7,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/01971b0bb6acc727c5db0dfffac48cd95e929106/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fservice%2FAdminServiceImpl.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/01971b0bb6acc727c5db0dfffac48cd95e929106/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fservice%2FAdminServiceImpl.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fservice%2FAdminServiceImpl.java?ref=01971b0bb6acc727c5db0dfffac48cd95e929106",
      "patch": "@@ -22,6 +22,7 @@\n import com.netflix.conductor.dao.QueueDAO;\n import org.springframework.boot.info.BuildProperties;\n \n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n@@ -44,13 +45,13 @@ public class AdminServiceImpl implements AdminService {\n \n     public AdminServiceImpl(ConductorProperties properties, ExecutionService executionService, QueueDAO queueDAO,\n         Optional<WorkflowRepairService> workflowRepairService, Optional<EventQueueManager> eventQueueManager,\n-        BuildProperties buildProperties) {\n+        Optional<BuildProperties> buildProperties) {\n         this.properties = properties;\n         this.executionService = executionService;\n         this.queueDAO = queueDAO;\n         this.workflowRepairService = workflowRepairService.orElse(null);\n         this.eventQueueManager = eventQueueManager.orElse(null);\n-        this.buildProperties = buildProperties;\n+        this.buildProperties = buildProperties.orElse(null);\n     }\n \n     /**\n@@ -69,6 +70,8 @@ public Map<String, Object> getAllConfig() {\n      * @return all the build properties.\n      */\n     private Map<String, Object> getBuildProperties(){\n+        if(buildProperties == null) return Collections.emptyMap();\n+\n         Map<String, Object> buildProps = new HashMap<>();\n         buildProps.put(\"version\", buildProperties.getVersion());\n         buildProps.put(\"buildDate\", buildProperties.getTime());",
      "parent_sha": "bb33492e77d4d3d5ac04a818ff30bd1f79911ab1"
    }
  },
  {
    "oid": "1261b1d45e7c54a0d0584c48843bb6e12a6bc879",
    "message": "when rerunning workflow, to find the rerunFromTaskId task, check tasks first directly under the workflow before checking sub_workflow tasks",
    "date": "2020-09-18T12:35:45Z",
    "url": "https://github.com/conductor-oss/conductor/commit/1261b1d45e7c54a0d0584c48843bb6e12a6bc879",
    "details": {
      "sha": "5993dee249eb842bdb28cb14b6356724de574f3f",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/WorkflowExecutor.java",
      "status": "modified",
      "additions": 6,
      "deletions": 2,
      "changes": 8,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/1261b1d45e7c54a0d0584c48843bb6e12a6bc879/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/1261b1d45e7c54a0d0584c48843bb6e12a6bc879/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java?ref=1261b1d45e7c54a0d0584c48843bb6e12a6bc879",
      "patch": "@@ -1541,8 +1541,12 @@ private boolean rerunWF(String workflowId, String taskId, Map<String, Object> ta\n             if (task.getTaskId().equals(taskId)) {\n                 rerunFromTask = task;\n                 break;\n-            } else {\n-                // If not found look into sub workflows\n+            }\n+        }\n+\n+        // If not found look into sub workflows\n+        if(rerunFromTask == null) { \n+\t        for (Task task : workflow.getTasks()) {\n                 if (task.getTaskType().equalsIgnoreCase(SubWorkflow.NAME)) {\n                     String subWorkflowId = task.getSubWorkflowId();\n                     if (rerunWF(subWorkflowId, taskId, taskInput, null, null)) {",
      "parent_sha": "749f922e46f7dedc08046b7fa5d0ec0d6694dda7"
    }
  },
  {
    "oid": "ea27cc5f4e9b463b51cddb0fc1c4690401cadd78",
    "message": "Trying to increase timeouts for the search to be successful.",
    "date": "2019-03-08T02:56:05Z",
    "url": "https://github.com/conductor-oss/conductor/commit/ea27cc5f4e9b463b51cddb0fc1c4690401cadd78",
    "details": {
      "sha": "5e468cd3335ad5a3abca023be8412ee5e55e706b",
      "filename": "es5-persistence/src/test/java/com/netflix/conductor/dao/es5/index/TestElasticSearchRestDAOV5.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/ea27cc5f4e9b463b51cddb0fc1c4690401cadd78/es5-persistence%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fes5%2Findex%2FTestElasticSearchRestDAOV5.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/ea27cc5f4e9b463b51cddb0fc1c4690401cadd78/es5-persistence%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fes5%2Findex%2FTestElasticSearchRestDAOV5.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/es5-persistence%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fes5%2Findex%2FTestElasticSearchRestDAOV5.java?ref=ea27cc5f4e9b463b51cddb0fc1c4690401cadd78",
      "patch": "@@ -292,7 +292,7 @@ public void testSearchRecentRunningWorkflows() {\n     @Test\n     public void testSearchArchivableWorkflows() throws IOException {\n         String workflowId = \"search-workflow-id\";\n-        Long time = DateTime.now().minusDays(2).toDate().getTime();\n+        Long time = DateTime.now().minusDays(7).toDate().getTime();\n \n         workflow.setWorkflowId(workflowId);\n         workflow.setStatus(Workflow.WorkflowStatus.COMPLETED);\n@@ -305,7 +305,7 @@ public void testSearchArchivableWorkflows() throws IOException {\n         assertTrue(indexExists(\"conductor\"));\n \n         await()\n-                .atMost(3, TimeUnit.SECONDS)\n+                .atMost(5, TimeUnit.SECONDS)\n                 .untilAsserted(\n                         () -> {\n                             List<String> searchIds = indexDAO.searchArchivableWorkflows(\"conductor\",1);",
      "parent_sha": "4bd147c2a6d4f2d46a309abb2521b8252c4f9acb"
    }
  },
  {
    "oid": "8947e6732cf30c6451c953499071d85dd5218a43",
    "message": "back to async",
    "date": "2024-07-01T08:31:26Z",
    "url": "https://github.com/conductor-oss/conductor/commit/8947e6732cf30c6451c953499071d85dd5218a43",
    "details": {
      "sha": "5b0db258b5f985c74eee2df61664f62b81be3914",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/tasks/Join.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/8947e6732cf30c6451c953499071d85dd5218a43/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FJoin.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/8947e6732cf30c6451c953499071d85dd5218a43/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FJoin.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FJoin.java?ref=8947e6732cf30c6451c953499071d85dd5218a43",
      "patch": "@@ -126,6 +126,6 @@ public Optional<Long> getEvaluationOffset(TaskModel taskModel, long defaultOffse\n     }\n \n     public boolean isAsync() {\n-        return false;\n+        return true;\n     }\n }",
      "parent_sha": "52f0ddb77acd07c63df51106bd04a8e4b8484fdb"
    }
  },
  {
    "oid": "ee8dff3c85e4352cc2af3cac6e7d9caa4fc9ea93",
    "message": "grpc-client: Add missing curly braces",
    "date": "2018-06-28T16:25:11Z",
    "url": "https://github.com/conductor-oss/conductor/commit/ee8dff3c85e4352cc2af3cac6e7d9caa4fc9ea93",
    "details": {
      "sha": "7e2b62786ddb3b3f498adf820d06e3377149c629",
      "filename": "grpc-client/src/main/java/com/netflix/conductor/client/grpc/TaskClient.java",
      "status": "modified",
      "additions": 6,
      "deletions": 3,
      "changes": 9,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/ee8dff3c85e4352cc2af3cac6e7d9caa4fc9ea93/grpc-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Fgrpc%2FTaskClient.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/ee8dff3c85e4352cc2af3cac6e7d9caa4fc9ea93/grpc-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Fgrpc%2FTaskClient.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/grpc-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Fgrpc%2FTaskClient.java?ref=ee8dff3c85e4352cc2af3cac6e7d9caa4fc9ea93",
      "patch": "@@ -100,10 +100,12 @@ public List<Task> getPendingTasksByType(String taskType, @Nullable String startK\n \n         TaskServicePb.TasksInProgressRequest.Builder request = TaskServicePb.TasksInProgressRequest.newBuilder();\n         request.setTaskType(taskType);\n-        if (startKey != null)\n+        if (startKey != null) {\n             request.setStartKey(startKey);\n-        if (count != null)\n+        }\n+        if (count != null) {\n             request.setCount(count);\n+        }\n \n         return stub.getTasksInProgress(request.build())\n                 .getTasksList()\n@@ -157,8 +159,9 @@ public boolean ack(String taskId, @Nullable String workerId) {\n \n         TaskServicePb.AckTaskRequest.Builder request = TaskServicePb.AckTaskRequest.newBuilder();\n         request.setTaskId(taskId);\n-        if (workerId != null)\n+        if (workerId != null) {\n             request.setWorkerId(workerId);\n+        }\n \n         return stub.ackTask(request.build()).getAck();\n     }",
      "parent_sha": "855a302b525a3a9271c9f903ce8eb76bb7555f07"
    }
  },
  {
    "oid": "bb0bcc507edf4f23f01179fbbee8ecca85f690d3",
    "message": "Add HTTP reason phrase in the response metadata",
    "date": "2017-07-13T20:08:22Z",
    "url": "https://github.com/conductor-oss/conductor/commit/bb0bcc507edf4f23f01179fbbee8ecca85f690d3",
    "details": {
      "sha": "0e0c53a2ab5668d8e48ce32f8742cb15714493f0",
      "filename": "contribs/src/main/java/com/netflix/conductor/contribs/http/HttpTask.java",
      "status": "modified",
      "additions": 6,
      "deletions": 1,
      "changes": 7,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/bb0bcc507edf4f23f01179fbbee8ecca85f690d3/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FHttpTask.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/bb0bcc507edf4f23f01179fbbee8ecca85f690d3/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FHttpTask.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FHttpTask.java?ref=bb0bcc507edf4f23f01179fbbee8ecca85f690d3",
      "patch": "@@ -177,6 +177,7 @@ protected HttpResponse httpCall(Input input) throws Exception {\n \t\t\t\tresponse.body = extractBody(cr);\n \t\t\t}\n \t\t\tresponse.statusCode = cr.getStatus();\n+\t\t\tresponse.reasonPhrase = cr.getStatusInfo().getReasonPhrase();\n \t\t\tresponse.headers = cr.getHeaders();\n \t\t\treturn response;\n \n@@ -191,6 +192,7 @@ protected HttpResponse httpCall(Input input) throws Exception {\n \t\t\t\t}\n \t\t\t\tresponse.headers = cr.getHeaders();\n \t\t\t\tresponse.statusCode = cr.getStatus();\n+\t\t\t\tresponse.reasonPhrase = cr.getStatusInfo().getReasonPhrase();\n \t\t\t\treturn response;\n \t\t\t\t\n \t\t\t}else {\n@@ -263,9 +265,11 @@ public static class HttpResponse {\n \t\t\n \t\tpublic int statusCode;\n \n+\t\tpublic String reasonPhrase;\n+\n \t\t@Override\n \t\tpublic String toString() {\n-\t\t\treturn \"HttpResponse [body=\" + body + \", headers=\" + headers + \", statusCode=\" + statusCode + \"]\";\n+\t\t\treturn \"HttpResponse [body=\" + body + \", headers=\" + headers + \", statusCode=\" + statusCode + \", reasonPhrase=\" + reasonPhrase + \"]\";\n \t\t}\n \t\t\n \t\tpublic Map<String, Object> asMap() {\n@@ -274,6 +278,7 @@ public Map<String, Object> asMap() {\n \t\t\tmap.put(\"body\", body);\n \t\t\tmap.put(\"headers\", headers);\n \t\t\tmap.put(\"statusCode\", statusCode);\n+\t\t\tmap.put(\"reasonPhrase\", reasonPhrase);\n \t\t\t\n \t\t\treturn map;\n \t\t}",
      "parent_sha": "398131c619b85b6ee4101a96dd6929d80de39d0f"
    }
  },
  {
    "oid": "317cfd0f13d0a19bcc6436ac4eaed75a7dc9faee",
    "message": "using the same instance of JedisMock for both default and read client",
    "date": "2020-12-05T08:05:29Z",
    "url": "https://github.com/conductor-oss/conductor/commit/317cfd0f13d0a19bcc6436ac4eaed75a7dc9faee",
    "details": {
      "sha": "46da34135e049a09ebe9a7dc6fa5fde8f74b8993",
      "filename": "redis-persistence/src/main/java/com/netflix/conductor/redis/configuration/InMemoryRedisConfiguration.java",
      "status": "modified",
      "additions": 8,
      "deletions": 8,
      "changes": 16,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/317cfd0f13d0a19bcc6436ac4eaed75a7dc9faee/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Fconfiguration%2FInMemoryRedisConfiguration.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/317cfd0f13d0a19bcc6436ac4eaed75a7dc9faee/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Fconfiguration%2FInMemoryRedisConfiguration.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Fconfiguration%2FInMemoryRedisConfiguration.java?ref=317cfd0f13d0a19bcc6436ac4eaed75a7dc9faee",
      "patch": "@@ -12,29 +12,29 @@\n  */\n package com.netflix.conductor.redis.configuration;\n \n-import com.netflix.conductor.redis.configuration.JedisCommandsConfigurer;\n import com.netflix.conductor.redis.config.utils.RedisProperties;\n import com.netflix.conductor.redis.jedis.JedisMock;\n-import com.netflix.conductor.redis.jedis.LocalHostSupplierProvider;\n+import com.netflix.conductor.redis.jedis.LocalhostHostSupplier;\n import com.netflix.dyno.connectionpool.HostSupplier;\n-import com.netflix.dyno.connectionpool.TokenMapSupplier;\n import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\n import org.springframework.context.annotation.Bean;\n import org.springframework.context.annotation.Configuration;\n-import redis.clients.jedis.commands.JedisCommands;\n+\n+import static com.netflix.conductor.redis.configuration.RedisCommonConfiguration.DEFAULT_CLIENT_INJECTION_NAME;\n+import static com.netflix.conductor.redis.configuration.RedisCommonConfiguration.READ_CLIENT_INJECTION_NAME;\n \n @SuppressWarnings(\"SpringJavaInjectionPointsAutowiringInspection\")\n @Configuration(proxyBeanMethods = false)\n @ConditionalOnProperty(name = \"db\", havingValue = \"memory\", matchIfMissing = true)\n-public class InMemoryRedisConfiguration extends JedisCommandsConfigurer {\n+public class InMemoryRedisConfiguration {\n \n     @Bean\n     public HostSupplier hostSupplier(RedisProperties properties) {\n-        return new LocalHostSupplierProvider(properties).get();\n+        return new LocalhostHostSupplier(properties);\n     }\n \n-    @Override\n-    protected JedisCommands createJedisCommands(RedisProperties properties, HostSupplier hostSupplier, TokenMapSupplier tokenMapSupplier) {\n+    @Bean(name = {DEFAULT_CLIENT_INJECTION_NAME, READ_CLIENT_INJECTION_NAME})\n+    public JedisMock jedisMock() {\n         return new JedisMock();\n     }\n }",
      "parent_sha": "2b39a4dc8404566908a58e63aee35c64293c6c47"
    }
  },
  {
    "oid": "93362c06a566757a0d55eb78a40931ec9d5fe853",
    "message": "Removed code changes from sdk v4",
    "date": "2024-11-26T14:51:24Z",
    "url": "https://github.com/conductor-oss/conductor/commit/93362c06a566757a0d55eb78a40931ec9d5fe853",
    "details": {
      "sha": "d4bfc0fc1c51639719a64d7dcf635aabb4641c0d",
      "filename": "conductor-clients/java/conductor-java-sdk/conductor-client/src/main/java/com/netflix/conductor/common/metadata/tasks/TaskDef.java",
      "status": "modified",
      "additions": 2,
      "deletions": 12,
      "changes": 14,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/93362c06a566757a0d55eb78a40931ec9d5fe853/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Ftasks%2FTaskDef.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/93362c06a566757a0d55eb78a40931ec9d5fe853/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Ftasks%2FTaskDef.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Ftasks%2FTaskDef.java?ref=93362c06a566757a0d55eb78a40931ec9d5fe853",
      "patch": "@@ -47,8 +47,6 @@ public enum RetryLogic {\n \n     private long timeoutSeconds;\n \n-    private long totalTimeoutSeconds;\n-\n     private List<String> inputKeys = new ArrayList<>();\n \n     private List<String> outputKeys = new ArrayList<>();\n@@ -174,14 +172,6 @@ public void setTimeoutSeconds(long timeoutSeconds) {\n         this.timeoutSeconds = timeoutSeconds;\n     }\n \n-    public long getTotalTimeoutSeconds() {\n-        return totalTimeoutSeconds;\n-    }\n-\n-    public void setTotalTimeoutSeconds(long totalTimeoutSeconds) {\n-        this.totalTimeoutSeconds = totalTimeoutSeconds;\n-    }\n-\n     /**\n      * @return Returns the input keys\n      */\n@@ -438,10 +428,10 @@ public boolean equals(Object o) {\n             return false;\n         }\n         TaskDef taskDef = (TaskDef) o;\n-        return getRetryCount() == taskDef.getRetryCount() && getTimeoutSeconds() == taskDef.getTimeoutSeconds() && getRetryDelaySeconds() == taskDef.getRetryDelaySeconds() && getBackoffScaleFactor() == taskDef.getBackoffScaleFactor() && getResponseTimeoutSeconds() == taskDef.getResponseTimeoutSeconds() && getTotalTimeoutSeconds() == taskDef.getTotalTimeoutSeconds() && Objects.equals(getName(), taskDef.getName()) && Objects.equals(getDescription(), taskDef.getDescription()) && Objects.equals(getInputKeys(), taskDef.getInputKeys()) && Objects.equals(getOutputKeys(), taskDef.getOutputKeys()) && getTimeoutPolicy() == taskDef.getTimeoutPolicy() && getRetryLogic() == taskDef.getRetryLogic() && Objects.equals(getConcurrentExecLimit(), taskDef.getConcurrentExecLimit()) && Objects.equals(getRateLimitPerFrequency(), taskDef.getRateLimitPerFrequency()) && Objects.equals(getInputTemplate(), taskDef.getInputTemplate()) && Objects.equals(getIsolationGroupId(), taskDef.getIsolationGroupId()) && Objects.equals(getExecutionNameSpace(), taskDef.getExecutionNameSpace()) && Objects.equals(getOwnerEmail(), taskDef.getOwnerEmail()) && Objects.equals(getBaseType(), taskDef.getBaseType()) && Objects.equals(getInputSchema(), taskDef.getInputSchema()) && Objects.equals(getOutputSchema(), taskDef.getOutputSchema());\n+        return getRetryCount() == taskDef.getRetryCount() && getTimeoutSeconds() == taskDef.getTimeoutSeconds() && getRetryDelaySeconds() == taskDef.getRetryDelaySeconds() && getBackoffScaleFactor() == taskDef.getBackoffScaleFactor() && getResponseTimeoutSeconds() == taskDef.getResponseTimeoutSeconds() && Objects.equals(getName(), taskDef.getName()) && Objects.equals(getDescription(), taskDef.getDescription()) && Objects.equals(getInputKeys(), taskDef.getInputKeys()) && Objects.equals(getOutputKeys(), taskDef.getOutputKeys()) && getTimeoutPolicy() == taskDef.getTimeoutPolicy() && getRetryLogic() == taskDef.getRetryLogic() && Objects.equals(getConcurrentExecLimit(), taskDef.getConcurrentExecLimit()) && Objects.equals(getRateLimitPerFrequency(), taskDef.getRateLimitPerFrequency()) && Objects.equals(getInputTemplate(), taskDef.getInputTemplate()) && Objects.equals(getIsolationGroupId(), taskDef.getIsolationGroupId()) && Objects.equals(getExecutionNameSpace(), taskDef.getExecutionNameSpace()) && Objects.equals(getOwnerEmail(), taskDef.getOwnerEmail()) && Objects.equals(getBaseType(), taskDef.getBaseType()) && Objects.equals(getInputSchema(), taskDef.getInputSchema()) && Objects.equals(getOutputSchema(), taskDef.getOutputSchema());\n     }\n \n     public int hashCode() {\n-        return Objects.hash(getName(), getDescription(), getRetryCount(), getTimeoutSeconds(), getInputKeys(), getOutputKeys(), getTimeoutPolicy(), getRetryLogic(), getRetryDelaySeconds(), getBackoffScaleFactor(), getResponseTimeoutSeconds(), getTotalTimeoutSeconds(), getConcurrentExecLimit(), getRateLimitPerFrequency(), getInputTemplate(), getIsolationGroupId(), getExecutionNameSpace(), getOwnerEmail(), getBaseType(), getInputSchema(), getOutputSchema());\n+        return Objects.hash(getName(), getDescription(), getRetryCount(), getTimeoutSeconds(), getInputKeys(), getOutputKeys(), getTimeoutPolicy(), getRetryLogic(), getRetryDelaySeconds(), getBackoffScaleFactor(), getResponseTimeoutSeconds(), getConcurrentExecLimit(), getRateLimitPerFrequency(), getInputTemplate(), getIsolationGroupId(), getExecutionNameSpace(), getOwnerEmail(), getBaseType(), getInputSchema(), getOutputSchema());\n     }\n }",
      "parent_sha": "f9784677e066a39dbfe65721e6cd65e8231a4a98"
    }
  },
  {
    "oid": "e9673605e7dc3b7530d53428647613444ee21b16",
    "message": "Use thread-safe wrapper for BulkRequest in ElasticSearchRestDAO",
    "date": "2020-04-04T05:05:46Z",
    "url": "https://github.com/conductor-oss/conductor/commit/e9673605e7dc3b7530d53428647613444ee21b16",
    "details": {
      "sha": "6482378ed00baa8c6c252f309fe4c326db6b1142",
      "filename": "es5-persistence/src/main/java/com/netflix/conductor/dao/es5/index/ElasticSearchRestDAOV5.java",
      "status": "modified",
      "additions": 47,
      "deletions": 8,
      "changes": 55,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/e9673605e7dc3b7530d53428647613444ee21b16/es5-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fes5%2Findex%2FElasticSearchRestDAOV5.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/e9673605e7dc3b7530d53428647613444ee21b16/es5-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fes5%2Findex%2FElasticSearchRestDAOV5.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/es5-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fes5%2Findex%2FElasticSearchRestDAOV5.java?ref=e9673605e7dc3b7530d53428647613444ee21b16",
      "patch": "@@ -47,6 +47,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.TimeZone;\n+import java.util.Objects;\n import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ExecutorService;\n@@ -56,6 +57,7 @@\n import java.util.concurrent.TimeUnit;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n+import javax.annotation.Nonnull;\n import javax.annotation.PreDestroy;\n import javax.inject.Inject;\n import javax.inject.Singleton;\n@@ -805,8 +807,10 @@ private void indexObject(final String index, final String docType, final String\n \n     private synchronized void indexBulkRequest(String docType) {\n         if (bulkRequests.get(docType).getBulkRequest() != null && bulkRequests.get(docType).getBulkRequest().numberOfActions() > 0) {\n-            indexWithRetry(bulkRequests.get(docType).getBulkRequest(), \"Bulk Indexing \" + docType, docType);\n-            bulkRequests.put(docType, new BulkRequests(System.currentTimeMillis(), new BulkRequest()));\n+            synchronized (bulkRequests.get(docType).getBulkRequest()) {\n+                indexWithRetry(bulkRequests.get(docType).getBulkRequest().get(), \"Bulk Indexing \" + docType, docType);\n+                bulkRequests.put(docType, new BulkRequests(System.currentTimeMillis(), new BulkRequest()));\n+            }\n         }\n     }\n \n@@ -934,19 +938,54 @@ private void flushBulkRequests() {\n \n     private static class BulkRequests {\n         private final long lastFlushTime;\n-        private final BulkRequest bulkRequest;\n+        private final BulkRequestWrapper bulkRequestWrapper;\n \n-        public long getLastFlushTime() {\n+        long getLastFlushTime() {\n             return lastFlushTime;\n         }\n \n-        public BulkRequest getBulkRequest() {\n-            return bulkRequest;\n+        BulkRequestWrapper getBulkRequest() {\n+            return bulkRequestWrapper;\n         }\n \n-        BulkRequests(long lastFlushTime, BulkRequest bulkRequest) {\n+        BulkRequests(long lastFlushTime, BulkRequest bulkRequestWrapper) {\n             this.lastFlushTime = lastFlushTime;\n-            this.bulkRequest = bulkRequest;\n+            this.bulkRequestWrapper = new BulkRequestWrapper(bulkRequestWrapper);\n+        }\n+\n+        /**\n+         * Thread-safe wrapper for {@link BulkRequest}.\n+         */\n+        private static class BulkRequestWrapper\n+        {\n+            private final BulkRequest bulkRequest;\n+\n+            BulkRequestWrapper(@Nonnull BulkRequest bulkRequest) {\n+                this.bulkRequest = Objects.requireNonNull(bulkRequest);\n+            }\n+\n+            public void add(@Nonnull UpdateRequest req) {\n+                synchronized (bulkRequest) {\n+                    bulkRequest.add(Objects.requireNonNull(req));\n+                }\n+            }\n+\n+            public void add(@Nonnull IndexRequest req) {\n+                synchronized (bulkRequest) {\n+                    bulkRequest.add(Objects.requireNonNull(req));\n+                }\n+            }\n+\n+            private BulkRequest get()\n+            {\n+                return bulkRequest;\n+            }\n+\n+            int numberOfActions() {\n+                synchronized (bulkRequest) {\n+                    return bulkRequest.numberOfActions();\n+                }\n+            }\n         }\n     }\n }",
      "parent_sha": "1f6bad2992a45354bb87bb1f184f4d1b34023f7e"
    }
  },
  {
    "oid": "0ef1190177ef862281c5ee392aa16c8d908c0df7",
    "message": "fork join 2 iteration integration test.",
    "date": "2019-08-26T18:39:01Z",
    "url": "https://github.com/conductor-oss/conductor/commit/0ef1190177ef862281c5ee392aa16c8d908c0df7",
    "details": {
      "sha": "52afe287c339387098856488180098229f32b46a",
      "filename": "test-harness/src/test/java/com/netflix/conductor/tests/integration/AbstractWorkflowServiceTest.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/0ef1190177ef862281c5ee392aa16c8d908c0df7/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FAbstractWorkflowServiceTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/0ef1190177ef862281c5ee392aa16c8d908c0df7/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FAbstractWorkflowServiceTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FAbstractWorkflowServiceTest.java?ref=0ef1190177ef862281c5ee392aa16c8d908c0df7",
      "patch": "@@ -774,6 +774,9 @@ public void testDoWhileTwoIteration() throws Exception {\n         assertTrue(task.getReferenceTaskName().endsWith(DoWhileTaskMapper.LOOP_TASK_DELIMITER + task.getIteration()));\n         assertTrue(workflowExecutionService.ackTaskReceived(task.getTaskId()));\n \n+        task.setStatus(COMPLETED);\n+        workflowExecutionService.updateTask(task);\n+\n         task = workflowExecutionService.poll(\"HTTP\", \"test\");\n         assertNotNull(task);\n         assertTrue(task.getReferenceTaskName().endsWith(DoWhileTaskMapper.LOOP_TASK_DELIMITER + task.getIteration()));\n@@ -787,7 +790,7 @@ public void testDoWhileTwoIteration() throws Exception {\n \n         workflow = workflowExecutionService.getExecutionStatus(workflowId, true);\n         assertNotNull(workflow);\n-        assertEquals(\"Found \" + workflow.getTasks(), COMPLETED, workflow.getStatus());\n+        assertEquals(\"Found \" + workflow.getTasks(), WorkflowStatus.COMPLETED, workflow.getStatus());\n     }\n \n     @Test",
      "parent_sha": "29970fd512ff82dfcb31f7219b9ece7711b3cf93"
    }
  },
  {
    "oid": "ed0f54b48b6ffa3fbbcc46794593f77d85244a5d",
    "message": "improved logging in returnTask",
    "date": "2018-04-24T07:40:15Z",
    "url": "https://github.com/conductor-oss/conductor/commit/ed0f54b48b6ffa3fbbcc46794593f77d85244a5d",
    "details": {
      "sha": "a7d3917c4dc4fa96ce6c79263ce27ffb8013071e",
      "filename": "client/src/main/java/com/netflix/conductor/client/task/WorkflowTaskCoordinator.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/ed0f54b48b6ffa3fbbcc46794593f77d85244a5d/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Ftask%2FWorkflowTaskCoordinator.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/ed0f54b48b6ffa3fbbcc46794593f77d85244a5d/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Ftask%2FWorkflowTaskCoordinator.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Ftask%2FWorkflowTaskCoordinator.java?ref=ed0f54b48b6ffa3fbbcc46794593f77d85244a5d",
      "patch": "@@ -450,7 +450,7 @@ private void handleException(Throwable t, TaskResult result, Worker worker, Task\n \t * in IN_PROGRESS status forever when these errors occur if task is not returned.\n \t */\n \tprivate void returnTask(Worker worker, Task task) {\n-\t\tlogger.error(\"Returning task back to conductor\");\n+\t\tlogger.warn(\"Returning task {} back to conductor\", task.getTaskId());\n \t\tupdateWithRetry(updateRetryCount, task, new TaskResult(task), worker);\n \t}\n }",
      "parent_sha": "e4dcd3177e071a188479f023407e6867640b930e"
    }
  },
  {
    "oid": "3c428d229e191b767140e636730a67d0ddf0d507",
    "message": "Address missing time part for task logs.\n\n(cherry picked from commit c84b5f4bd94cf4a1afb32d2e3e7996849c0b0728)",
    "date": "2024-08-02T00:57:43Z",
    "url": "https://github.com/conductor-oss/conductor/commit/3c428d229e191b767140e636730a67d0ddf0d507",
    "details": {
      "sha": "b0481a29351b3c73a3e1010d99f4d028c42e4b0c",
      "filename": "postgres-persistence/src/main/java/com/netflix/conductor/postgres/dao/PostgresIndexDAO.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/3c428d229e191b767140e636730a67d0ddf0d507/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresIndexDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/3c428d229e191b767140e636730a67d0ddf0d507/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresIndexDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresIndexDAO.java?ref=3c428d229e191b767140e636730a67d0ddf0d507",
      "patch": "@@ -233,7 +233,7 @@ public List<TaskExecLog> getTaskExecutionLogs(String taskId) {\n                                                 log.setLog(rs.getString(\"log\"));\n                                                 log.setTaskId(rs.getString(\"task_id\"));\n                                                 log.setCreatedTime(\n-                                                        rs.getDate(\"created_time\").getTime());\n+                                                        rs.getTimestamp(\"created_time\").getTime());\n                                                 result.add(log);\n                                             }\n                                             return result;",
      "parent_sha": "9da6a4ebdbd76f0b095aed2c26dad5a02b4ae795"
    }
  },
  {
    "oid": "f43aa00c779a2637a83c4d5d3fd4decf05bacb85",
    "message": "When starting a new workflow execution, store workflow definition if it is ephemeral",
    "date": "2018-07-26T19:13:03Z",
    "url": "https://github.com/conductor-oss/conductor/commit/f43aa00c779a2637a83c4d5d3fd4decf05bacb85",
    "details": {
      "sha": "e6298ae337e1a9eab519b7d8e15d4caec9748c04",
      "filename": "jersey/src/main/java/com/netflix/conductor/server/resources/WorkflowResource.java",
      "status": "modified",
      "additions": 8,
      "deletions": 1,
      "changes": 9,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/f43aa00c779a2637a83c4d5d3fd4decf05bacb85/jersey%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2Fresources%2FWorkflowResource.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/f43aa00c779a2637a83c4d5d3fd4decf05bacb85/jersey%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2Fresources%2FWorkflowResource.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/jersey%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2Fresources%2FWorkflowResource.java?ref=f43aa00c779a2637a83c4d5d3fd4decf05bacb85",
      "patch": "@@ -86,6 +86,9 @@ public String startWorkflow(StartWorkflowRequest request) {\n         if (Strings.isNullOrEmpty(request.getName())) {\n             throw new ApplicationException(Code.INVALID_INPUT,  \"A name is required to start a workflow.\");\n         }\n+        if (request.getWorkflowDef() != null) {\n+            metadata.registerWorkflowDef(request.getWorkflowDef());\n+        }\n         return executor.startWorkflow(\n                 request.getName(),\n                 request.getVersion(),\n@@ -107,10 +110,14 @@ public String startWorkflow(@PathParam(\"name\") String name, StartWorkflowRequest\n             throw new ApplicationException(\n                     Code.INVALID_INPUT,\n                     \"Cannot run workflow with name inconsistencies. \" +\n-                    \"Make sure the name on the url and the name on the payload matches.\"\n+                    \"Make sure the name on the url and the name on the payload match.\"\n             );\n         }\n \n+        if (request.getWorkflowDef() != null) {\n+            metadata.registerWorkflowDef(request.getWorkflowDef());\n+        }\n+\n         return executor.startWorkflow(name, request.getVersion(), request.getCorrelationId(),\n                 request.getInput(), null, request.getTaskToDomain(), request.getWorkflowDef());\n     }",
      "parent_sha": "343b4d9c2157aa288d7c00a2d6341bf2887eb3dd"
    }
  },
  {
    "oid": "bfa0aa2925e38b0031494e4483169e96ba0a6f67",
    "message": "updated the execution log message when sub workflow is changed",
    "date": "2021-03-29T17:03:33Z",
    "url": "https://github.com/conductor-oss/conductor/commit/bfa0aa2925e38b0031494e4483169e96ba0a6f67",
    "details": {
      "sha": "ec995f1846efc246ec4d9d561d0fbc1f9c373a03",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/WorkflowExecutor.java",
      "status": "modified",
      "additions": 13,
      "deletions": 13,
      "changes": 26,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/bfa0aa2925e38b0031494e4483169e96ba0a6f67/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/bfa0aa2925e38b0031494e4483169e96ba0a6f67/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java?ref=bfa0aa2925e38b0031494e4483169e96ba0a6f67",
      "patch": "@@ -1,14 +1,14 @@\n /*\n- * Copyright 2021 Netflix, Inc.\n+ *  Copyright 2021 Netflix, Inc.\n  *  <p>\n  *  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n- *   the License. You may obtain a copy of the License at\n- *   <p>\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *   <p>\n- *   Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n- *   an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n- *   specific language governing permissions and limitations under the License.\n+ *  the License. You may obtain a copy of the License at\n+ *  <p>\n+ *  http://www.apache.org/licenses/LICENSE-2.0\n+ *  <p>\n+ *  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+ *  an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+ *  specific language governing permissions and limitations under the License.\n  */\n package com.netflix.conductor.core.execution;\n \n@@ -547,7 +547,7 @@ public void restart(String workflowId, boolean useLatestDefinitions) {\n \n         decide(workflowId);\n \n-        updateAndPushParents(workflow);\n+        updateAndPushParents(workflow, \"restarted\");\n     }\n \n     /**\n@@ -570,15 +570,15 @@ public void retry(String workflowId, boolean resumeSubworkflowTasks) {\n             if (taskToRetry.isPresent()) {\n                 workflow = findLastFailedSubWorkflowIfAny(taskToRetry.get(), workflow);\n                 retry(workflow);\n-                updateAndPushParents(workflow);\n+                updateAndPushParents(workflow, \"retried\");\n             }\n         } else {\n             retry(workflow);\n-            updateAndPushParents(workflow);\n+            updateAndPushParents(workflow, \"retried\");\n         }\n     }\n \n-    private void updateAndPushParents(Workflow workflow) {\n+    private void updateAndPushParents(Workflow workflow, String operation) {\n         String workflowIdentifier = \"\";\n         while (workflow.hasParent()) {\n             // update parent's sub workflow task\n@@ -590,7 +590,7 @@ private void updateAndPushParents(Workflow workflow) {\n             // add an execution log\n             String currentWorkflowIdentifier = workflow.toShortString();\n             workflowIdentifier = !workflowIdentifier.equals(\"\") ? String.format(\"%s -> %s\", currentWorkflowIdentifier, workflowIdentifier) : currentWorkflowIdentifier;\n-            TaskExecLog log = new TaskExecLog(String.format(\"Workflow %s changed.\", workflowIdentifier));\n+            TaskExecLog log = new TaskExecLog(String.format(\"Sub workflow %s %s.\", workflowIdentifier, operation));\n             log.setTaskId(subWorkflowTask.getTaskId());\n             executionDAOFacade.addTaskExecLog(Collections.singletonList(log));\n             LOGGER.info(\"Task {} updated. {}\", log.getTaskId(), log.getLog());",
      "parent_sha": "6474e2b8cd6432ac1d8b992aee9897d0629dbb2b"
    }
  },
  {
    "oid": "97c0a5fe8014df0ba7c8bc37a8e3e72db496019b",
    "message": "Fixed formatting issues",
    "date": "2024-04-16T11:09:11Z",
    "url": "https://github.com/conductor-oss/conductor/commit/97c0a5fe8014df0ba7c8bc37a8e3e72db496019b",
    "details": {
      "sha": "b50997c89cf5cbce73b90585b18e49d91fe43473",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/tasks/DoWhile.java",
      "status": "modified",
      "additions": 8,
      "deletions": 5,
      "changes": 13,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/97c0a5fe8014df0ba7c8bc37a8e3e72db496019b/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhile.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/97c0a5fe8014df0ba7c8bc37a8e3e72db496019b/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhile.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhile.java?ref=97c0a5fe8014df0ba7c8bc37a8e3e72db496019b",
      "patch": "@@ -112,12 +112,15 @@ public boolean execute(\n         }\n         doWhileTaskModel.addOutput(String.valueOf(doWhileTaskModel.getIteration()), output);\n \n-        Optional<Integer> keepLastN = Optional.ofNullable(doWhileTaskModel.getWorkflowTask().getInputParameters())\n-                .map(parameters -> parameters.get(\"keepLastN\"))\n-                .map(value -> (Integer) value);\n-        if(keepLastN.isPresent() && doWhileTaskModel.getIteration() > keepLastN.get()) {\n+        Optional<Integer> keepLastN =\n+                Optional.ofNullable(doWhileTaskModel.getWorkflowTask().getInputParameters())\n+                        .map(parameters -> parameters.get(\"keepLastN\"))\n+                        .map(value -> (Integer) value);\n+        if (keepLastN.isPresent() && doWhileTaskModel.getIteration() > keepLastN.get()) {\n             Integer iteration = doWhileTaskModel.getIteration();\n-            IntStream.range(0, iteration - keepLastN.get()).mapToObj(Integer::toString).forEach(doWhileTaskModel::removeOutput);\n+            IntStream.range(0, iteration - keepLastN.get())\n+                    .mapToObj(Integer::toString)\n+                    .forEach(doWhileTaskModel::removeOutput);\n         }\n \n         if (hasFailures) {",
      "parent_sha": "eb46749fe18cf7cfdd4c0d092ff303abae3d54a8"
    }
  },
  {
    "oid": "f9f0568b31a2ab743fd54a268d972d3acf3818b2",
    "message": "end point to get the event executions",
    "date": "2017-02-15T06:52:11Z",
    "url": "https://github.com/conductor-oss/conductor/commit/f9f0568b31a2ab743fd54a268d972d3acf3818b2",
    "details": {
      "sha": "a40ca4b7e0d5128436405e97c8cb303bc7f88ab8",
      "filename": "jersey/src/main/java/com/netflix/conductor/server/resources/EventResource.java",
      "status": "modified",
      "additions": 20,
      "deletions": 3,
      "changes": 23,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/f9f0568b31a2ab743fd54a268d972d3acf3818b2/jersey%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2Fresources%2FEventResource.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/f9f0568b31a2ab743fd54a268d972d3acf3818b2/jersey%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2Fresources%2FEventResource.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/jersey%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2Fresources%2FEventResource.java?ref=f9f0568b31a2ab743fd54a268d972d3acf3818b2",
      "patch": "@@ -35,11 +35,13 @@\n import javax.ws.rs.QueryParam;\n import javax.ws.rs.core.MediaType;\n \n+import com.netflix.conductor.common.metadata.events.EventExecution;\n import com.netflix.conductor.common.metadata.events.EventHandler;\n import com.netflix.conductor.core.events.EventProcessor;\n import com.netflix.conductor.core.events.EventQueueProvider;\n import com.netflix.conductor.core.events.EventQueues;\n import com.netflix.conductor.core.events.EventQueues.QueueType;\n+import com.netflix.conductor.service.ExecutionService;\n import com.netflix.conductor.service.MetadataService;\n \n import io.swagger.annotations.Api;\n@@ -61,10 +63,13 @@ public class EventResource {\n \t\n \tprivate EventProcessor ep;\n \t\n+\tprivate ExecutionService es;\n+\t\n \t@Inject\n-\tpublic EventResource(MetadataService service, EventProcessor ep) {\n+\tpublic EventResource(MetadataService service, EventProcessor ep, ExecutionService es) {\n \t\tthis.service = service;\n \t\tthis.ep = ep;\n+\t\tthis.es = es;\n \t}\n \n \t@POST\n@@ -102,16 +107,28 @@ public List<EventHandler> getEventHandlersForEvent(@PathParam(\"event\") String ev\n \t\n \t@GET\n \t@Path(\"/queues\")\n-\t@ApiOperation(\"Get Registered queues\")\n+\t@ApiOperation(\"Get registered queues\")\n \tpublic Map<String, ?> getEventQueues(@QueryParam(\"verbose\") @DefaultValue(\"false\") boolean verbose) {\n \t\treturn (verbose ? ep.getQueueSizes() : ep.getQueues());\n \t}\n \n \t@GET\n \t@Path(\"/queues/providers\")\n-\t@ApiOperation(\"Get Registered queue providers\")\n+\t@ApiOperation(\"Get registered queue providers\")\n \tpublic Map<QueueType, EventQueueProvider> getEventQueueProviders() {\n \t\treturn EventQueues.providers();\n \t}\n \t\n+\t@GET\n+\t@Path(\"/executions/{eventHandlerName}/{eventName}\")\n+\t@ApiOperation(\"Get Event executions\")\n+\tpublic List<EventExecution> getEventExecutions(\n+\t\t\t@PathParam(\"eventHandlerName\") String eventHandlerName, @PathParam(\"eventName\") String eventName, \n+\t\t\t@QueryParam(\"startTime\") @DefaultValue(\"0\") long startTime, @QueryParam(\"endTime\") @DefaultValue(\"-1\") long endTime, @QueryParam(\"count\") @DefaultValue(\"100\") int count) {\n+\t\tif(endTime == -1) {\n+\t\t\tendTime = System.currentTimeMillis();\n+\t\t}\n+\t\treturn es.getEventExecutions(eventHandlerName, eventName, startTime, endTime, count);\n+\t}\n+\t\n }",
      "parent_sha": "a0668a6b676dfa7173231258c0760dbef445ad27"
    }
  },
  {
    "oid": "0899863fa58bb7c111bb518b28c9d07b92ced38f",
    "message": "Adjust unit tests for the HTTP task fix",
    "date": "2017-02-14T20:18:27Z",
    "url": "https://github.com/conductor-oss/conductor/commit/0899863fa58bb7c111bb518b28c9d07b92ced38f",
    "details": {
      "sha": "356f72a06ed63209a4a5eb3eacb538f42e11bd4e",
      "filename": "contribs/src/test/java/com/netflix/conductor/contribs/http/TestHttpTask.java",
      "status": "modified",
      "additions": 13,
      "deletions": 12,
      "changes": 25,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/0899863fa58bb7c111bb518b28c9d07b92ced38f/contribs%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FTestHttpTask.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/0899863fa58bb7c111bb518b28c9d07b92ced38f/contribs%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FTestHttpTask.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/contribs%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FTestHttpTask.java?ref=0899863fa58bb7c111bb518b28c9d07b92ced38f",
      "patch": "@@ -58,6 +58,7 @@\n  * @author Viren\n  *\n  */\n+@SuppressWarnings(\"unchecked\")\n public class TestHttpTask {\n \n \tprivate static final String ERROR_RESPONSE = \"Something went wrong!\";\n@@ -126,8 +127,8 @@ public void testPost() throws Exception {\n \t\t\n \t\thttpTask.start(workflow, task, executor);\n \t\tassertEquals(task.getReasonForIncompletion(), Task.Status.COMPLETED, task.getStatus());\n-\t\tHttpResponse hr = (HttpResponse) task.getOutputData().get(\"response\");\n-\t\tObject response = hr.body;\n+\t\tMap<String, Object> hr = (Map<String, Object>) task.getOutputData().get(\"response\");\n+\t\tObject response = hr.get(\"body\");\n \t\tassertEquals(Task.Status.COMPLETED, task.getStatus());\n \t\tassertTrue(\"response is: \" + response, response instanceof Map);\n \t\tMap<String, Object> map = (Map<String, Object>) response;\n@@ -153,8 +154,8 @@ public void testPostNoContent() throws Exception {\n \t\t\n \t\thttpTask.start(workflow, task, executor);\n \t\tassertEquals(task.getReasonForIncompletion(), Task.Status.COMPLETED, task.getStatus());\n-\t\tHttpResponse hr = (HttpResponse) task.getOutputData().get(\"response\");\n-\t\tObject response = hr.body;\n+\t\tMap<String, Object> hr = (Map<String, Object>) task.getOutputData().get(\"response\");\n+\t\tObject response = hr.get(\"body\");\n \t\tassertEquals(Task.Status.COMPLETED, task.getStatus());\n \t\tassertNull(\"response is: \" + response, response);\n \t}\n@@ -189,8 +190,8 @@ public void testTextGET() throws Exception {\n \t\ttask.getInputData().put(HttpTask.REQUEST_PARAMETER_NAME, input);\n \t\t\n \t\thttpTask.start(workflow, task, executor);\n-\t\tHttpResponse hr = (HttpResponse) task.getOutputData().get(\"response\");\n-\t\tObject response = hr.body;\n+\t\tMap<String, Object> hr = (Map<String, Object>) task.getOutputData().get(\"response\");\n+\t\tObject response = hr.get(\"body\");\n \t\tassertEquals(Task.Status.COMPLETED, task.getStatus());\n \t\tassertEquals(TEXT_RESPONSE, response);\t\n \t}\n@@ -205,8 +206,8 @@ public void testNumberGET() throws Exception {\n \t\ttask.getInputData().put(HttpTask.REQUEST_PARAMETER_NAME, input);\n \t\t\n \t\thttpTask.start(workflow, task, executor);\n-\t\tHttpResponse hr = (HttpResponse) task.getOutputData().get(\"response\");\n-\t\tObject response = hr.body;\n+\t\tMap<String, Object> hr = (Map<String, Object>) task.getOutputData().get(\"response\");\n+\t\tObject response = hr.get(\"body\");\n \t\tassertEquals(Task.Status.COMPLETED, task.getStatus());\n \t\tassertEquals(NUM_RESPONSE, response);\n \t\tassertTrue(response instanceof Number);\n@@ -223,8 +224,8 @@ public void testJsonGET() throws Exception {\n \t\ttask.getInputData().put(HttpTask.REQUEST_PARAMETER_NAME, input);\n \t\t\n \t\thttpTask.start(workflow, task, executor);\n-\t\tHttpResponse hr = (HttpResponse) task.getOutputData().get(\"response\");\n-\t\tObject response = hr.body;\n+\t\tMap<String, Object> hr = (Map<String, Object>) task.getOutputData().get(\"response\");\n+\t\tObject response = hr.get(\"body\");\n \t\tassertEquals(Task.Status.COMPLETED, task.getStatus());\n \t\tassertTrue(response instanceof Map);\n \t\tMap<String, Object> map = (Map<String, Object>) response;\n@@ -244,8 +245,8 @@ public void testExecute() throws Exception {\n \t\ttask.setScheduledTime(0);\n \t\thttpTask.execute(workflow, task, executor);\n \n-\t\tHttpResponse hr = (HttpResponse) task.getOutputData().get(\"response\");\n-\t\tObject response = hr.body;\n+\t\tMap<String, Object> hr = (Map<String, Object>) task.getOutputData().get(\"response\");\n+\t\tObject response = hr.get(\"body\");\n \t\tassertEquals(Task.Status.COMPLETED, task.getStatus());\n \t\tassertTrue(response instanceof Map);\n \t\tMap<String, Object> map = (Map<String, Object>) response;",
      "parent_sha": "f896c70fde514fd0292a01d74400ac817ef8ad11"
    }
  },
  {
    "oid": "ee190d52ba127f6812ad65fc403e18eda3c08ba8",
    "message": "Wait until calculation fix.",
    "date": "2023-09-27T08:13:26Z",
    "url": "https://github.com/conductor-oss/conductor/commit/ee190d52ba127f6812ad65fc403e18eda3c08ba8",
    "details": {
      "sha": "6a3ca95d12965a3da0e6178044f19079daf2a311",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/mapper/WaitTaskMapper.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/ee190d52ba127f6812ad65fc403e18eda3c08ba8/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Fmapper%2FWaitTaskMapper.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/ee190d52ba127f6812ad65fc403e18eda3c08ba8/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Fmapper%2FWaitTaskMapper.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Fmapper%2FWaitTaskMapper.java?ref=ee190d52ba127f6812ad65fc403e18eda3c08ba8",
      "patch": "@@ -110,7 +110,7 @@ void setCallbackAfter(TaskModel task) {\n                 Date expiryDate = parseDate(until);\n                 long timeInMS = expiryDate.getTime();\n                 long now = System.currentTimeMillis();\n-                long seconds = ((timeInMS - now) / 1000) + 1;\n+                long seconds = ((timeInMS - now) / 1000);\n                 if (seconds < 0) {\n                     seconds = 0;\n                 }",
      "parent_sha": "1c899db50778179bce5a95b058db69562b5b02e1"
    }
  },
  {
    "oid": "6bcec07c42108a80821d785dc50ba1bfcb3a569f",
    "message": "Resolving conflits.",
    "date": "2018-05-08T00:07:45Z",
    "url": "https://github.com/conductor-oss/conductor/commit/6bcec07c42108a80821d785dc50ba1bfcb3a569f",
    "details": {
      "sha": "30501b7b5c0d87adb1105621884e3985ea75b79e",
      "filename": "mysql-persistence/src/main/java/com/netflix/conductor/dao/mysql/MySQLExecutionDAO.java",
      "status": "modified",
      "additions": 804,
      "deletions": 735,
      "changes": 1539,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/6bcec07c42108a80821d785dc50ba1bfcb3a569f/mysql-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fmysql%2FMySQLExecutionDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/6bcec07c42108a80821d785dc50ba1bfcb3a569f/mysql-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fmysql%2FMySQLExecutionDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/mysql-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fmysql%2FMySQLExecutionDAO.java?ref=6bcec07c42108a80821d785dc50ba1bfcb3a569f",
      "patch": "@@ -1,19 +1,5 @@\n package com.netflix.conductor.dao.mysql;\n \n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Comparator;\n-import java.util.Date;\n-import java.util.LinkedList;\n-import java.util.List;\n-import java.util.stream.Collectors;\n-\n-import javax.inject.Inject;\n-\n-import org.sql2o.Connection;\n-import org.sql2o.ResultSetHandler;\n-import org.sql2o.Sql2o;\n-\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.Lists;\n@@ -29,264 +15,276 @@\n import com.netflix.conductor.dao.IndexDAO;\n import com.netflix.conductor.dao.MetadataDAO;\n import com.netflix.conductor.metrics.Monitors;\n-\n-class MySQLExecutionDAO extends MySQLBaseDAO implements ExecutionDAO {\n-\n-\tprivate static final String ARCHIVED_FIELD = \"archived\";\n-\tprivate static final String RAW_JSON_FIELD = \"rawJSON\";\n-\n-\tprivate IndexDAO indexer;\n-\n-\tprivate MetadataDAO metadata;\n-\n-\t@Inject\n-\tMySQLExecutionDAO(IndexDAO indexer, MetadataDAO metadata, ObjectMapper om, Sql2o sql2o) {\n-\t\tsuper(om, sql2o);\n-\t\tthis.indexer = indexer;\n-\t\tthis.metadata = metadata;\n-\t}\n-\n-\t@Override\n-\tpublic List<Task> getPendingTasksByWorkflow(String taskDefName, String workflowId) {\n-\t\treturn getWithTransaction(connection -> {\n-\n-\t\t\tString GET_IN_PROGRESS_TASKS_FOR_WORKFLOW =\n-\t\t\t\t\"SELECT json_data FROM task_in_progress tip INNER JOIN task t ON t.task_id = tip.task_id \\n\" +\n-\t\t\t\t\" WHERE task_def_name = :taskDefName AND workflow_id = :workflowId\";\n-\n-\t\t\tResultSetHandler<Task> resultSetHandler = resultSet -> readValue(resultSet.getString(\"json_data\"), Task.class);\n-\n-\t\t\treturn connection.createQuery(GET_IN_PROGRESS_TASKS_FOR_WORKFLOW)\n-\t\t\t\t\t.addParameter(\"taskDefName\", taskDefName)\n-\t\t\t\t\t.addParameter(\"workflowId\", workflowId)\n-\t\t\t\t\t.executeAndFetch(resultSetHandler);\n-\t\t});\n-\t}\n-\n-\t@Override\n-\tpublic List<Task> getTasks(String taskDefName, String startKey, int count) {\n-\t\tList<Task> tasks = new ArrayList<>(count);\n-\n-\t\tList<Task> pendingTasks = getPendingTasksForTaskType(taskDefName);\n-\t\tboolean startKeyFound = startKey == null;\n-\t\tint found = 0;\n-\t\tfor (Task pendingTask : pendingTasks) {\n-\t\t\tif (!startKeyFound) {\n-\t\t\t\tif (pendingTask.getTaskId().equals(startKey)) {\n-\t\t\t\t\tstartKeyFound = true;\n-\t\t\t\t\tif (startKey != null) {\n-\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif (startKeyFound && found < count) {\n-\t\t\t\ttasks.add(pendingTask);\n-\t\t\t\tfound++;\n-\t\t\t}\n-\t\t}\n-\n-\t\treturn tasks;\n-\t}\n-\n-\t@Override\n-\tpublic List<Task> createTasks(List<Task> tasks) {\n-\t\tList<Task> created = Lists.newLinkedList();\n-\n-\t\twithTransaction(connection -> {\n-\t\t\tfor (Task task : tasks) {\n-\t\t\t\tvalidate(task);\n-\n-\t\t\t\ttask.setScheduledTime(System.currentTimeMillis());\n-\n-\t\t\t\tString taskKey = task.getReferenceTaskName() + \"_\" + task.getRetryCount();\n-\n-\t\t\t\tboolean scheduledTaskAdded = addScheduledTask(connection, task, taskKey);\n-\n-\t\t\t\tif (!scheduledTaskAdded) {\n-\t\t\t\t\tlogger.info(\"Task already scheduled, skipping the run \" + task.getTaskId() + \", ref=\" + task.getReferenceTaskName() + \", key=\" + taskKey);\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n-\n-\t\t\t\tinsertOrUpdateTaskData(connection, task);\n-\t\t\t\taddWorkflowToTaskMapping(connection, task);\n-\t\t\t\taddTaskInProgress(connection, task);\n-\t\t\t\tupdateTask(connection, task);\n-\n-\t\t\t\tcreated.add(task);\n-\t\t\t}\n-\t\t});\n-\n-\t\treturn created;\n-\t}\n-\n-\t@Override\n-\tpublic void updateTask(Task task) {\n-\t\twithTransaction(connection -> updateTask(connection, task));\n-\t}\n-\n-\t@Override\n-\tpublic boolean exceedsInProgressLimit(Task task) {\n-\t\tTaskDef taskDef = metadata.getTaskDef(task.getTaskDefName());\n-\t\tif (taskDef == null) return false;\n-\n-\t\tint limit = taskDef.concurrencyLimit();\n-\t\tif (limit <= 0) return false;\n-\n-\t\tlong current = getInProgressTaskCount(task.getTaskDefName());\n-\n-\t\tif (current >= limit) {\n-\t\t\tMonitors.recordTaskRateLimited(task.getTaskDefName(), limit);\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\tlogger.info(\"Task execution count for {}: limit={}, current={}\", task.getTaskDefName(), limit, getInProgressTaskCount(task.getTaskDefName()));\n-\n-\t\tString taskId = task.getTaskId();\n-\n-\t\tList<String> tasksInProgressInOrderOfArrival = findAllTasksInProgressInOrderOfArrival(task, limit);\n-\n-\t\tboolean rateLimited = !tasksInProgressInOrderOfArrival.contains(taskId);\n-\n-\t\tif (rateLimited) {\n-\t\t\tlogger.info(\"Task execution count limited. {}, limit {}, current {}\", task.getTaskDefName(), limit, getInProgressTaskCount(task.getTaskDefName()));\n-\t\t\tMonitors.recordTaskRateLimited(task.getTaskDefName(), limit);\n-\t\t}\n-\n-\t\treturn rateLimited;\n-\t}\n-\n-\t@Override\n-\tpublic void updateTasks(List<Task> tasks) {\n-\t\twithTransaction(connection -> tasks.forEach(task -> updateTask(connection, task)));\n-\t}\n-\n-\t@Override\n-\tpublic void addTaskExecLog(List<TaskExecLog> log) {\n-\t\tindexer.addTaskExecutionLogs(log);\n-\t}\n-\n-\t@Override\n-\tpublic void removeTask(String taskId) {\n-\t\tTask task = getTask(taskId);\n-\n-\t\tif(task == null) {\n-\t\t\tlogger.warn(\"No such Task by id {}\", taskId);\n-\t\t\treturn;\n-\t\t}\n-\n-\t\tString taskKey = task.getReferenceTaskName() + \"\" + task.getRetryCount();\n-\n-\t\twithTransaction(connection -> {\n-\t\t\tremoveScheduledTask(connection, task, taskKey);\n-\t\t\tremoveWorkflowToTaskMapping(connection, task);\n-\t\t\tremoveTaskInProgress(connection, task);\n-\t\t\tremoveTaskData(connection, task);\n-\t\t});\n-\t}\n-\n-\t@Override\n-\tpublic Task getTask(String taskId) {\n-\t\tString GET_TASK = \"SELECT json_data FROM task WHERE task_id = :taskId\";\n-\t\tString taskJsonStr = getWithTransaction(c -> c.createQuery(GET_TASK).addParameter(\"taskId\", taskId).executeScalar(String.class));\n-\t\treturn taskJsonStr != null ? readValue(taskJsonStr, Task.class) : null;\n-\t}\n-\n-\t@Override\n-\tpublic List<Task> getTasks(List<String> taskIds) {\n-\t\tif (taskIds.isEmpty()) return Lists.newArrayList();\n-\t\treturn getWithTransaction(c -> getTasks(c, taskIds));\n-\t}\n-\n-\tprivate List<Task> getTasks(Connection connection, List<String> taskIds) {\n-\t\tif (taskIds.isEmpty()) return Lists.newArrayList();\n-\n-\t\tString GET_TASKS_FOR_IDS = \"SELECT json_data FROM task WHERE task_id IN (%s) AND json_data IS NOT NULL\";\n-\t\tString query = generateQueryWithParametersListPlaceholders(GET_TASKS_FOR_IDS, taskIds.size());\n-\n-\t\tResultSetHandler<Task> resultSetHandler = resultSet -> readValue(resultSet.getString(\"json_data\"), Task.class);\n-\n-\t\treturn connection.createQuery(query).withParams(taskIds.toArray()).executeAndFetch(resultSetHandler);\n-\t}\n-\n-\t@Override\n-\tpublic List<Task> getPendingTasksForTaskType(String taskName) {\n-\t\tPreconditions.checkNotNull(taskName, \"task name cannot be null\");\n-\t\treturn getWithTransaction(connection -> {\n-\n-\t\t\tString GET_IN_PROGRESS_TASKS_FOR_TYPE =\n-\t\t\t\t\"SELECT json_data FROM task_in_progress tip INNER JOIN task t ON t.task_id = tip.task_id WHERE task_def_name = :taskDefName\";\n-\n-\t\t\tResultSetHandler<Task> resultSetHandler = resultSet -> readValue(resultSet.getString(\"json_data\"), Task.class);\n-\n-\t\t\treturn connection.createQuery(GET_IN_PROGRESS_TASKS_FOR_TYPE).addParameter(\"taskDefName\", taskName).executeAndFetch(resultSetHandler);\n-\t\t});\n-\t}\n-\n-\t@Override\n-\tpublic List<Task> getTasksForWorkflow(String workflowId) {\n-\t\treturn getWithTransaction(connection -> {\n-\t\t\tString GET_TASKS_FOR_WORKFLOW = \"SELECT task_id FROM workflow_to_task WHERE workflow_id = :workflowId\";\n-\t\t\tList<String> taskIds = connection.createQuery(GET_TASKS_FOR_WORKFLOW).addParameter(\"workflowId\", workflowId).executeScalarList(String.class);\n-\t\t\treturn getTasks(connection, taskIds);\n-\t\t});\n-\t}\n-\n-\t@Override\n-\tpublic String createWorkflow(Workflow workflow) {\n-\t\tworkflow.setCreateTime(System.currentTimeMillis());\n-\t\treturn insertOrUpdateWorkflow(workflow, false);\n-\t}\n-\n-\t@Override\n-\tpublic String updateWorkflow(Workflow workflow) {\n-\t\tworkflow.setUpdateTime(System.currentTimeMillis());\n-\t\treturn insertOrUpdateWorkflow(workflow, true);\n-\t}\n-\n-\t@Override\n-\tpublic void removeWorkflow(String workflowId, boolean archiveWorkflow) {\n-\t\ttry {\n-\t\t\tWorkflow wf = getWorkflow(workflowId, true);\n-\n-\t\t\tif (archiveWorkflow) {\n-\t\t\t\t//Add to elasticsearch\n-\t\t\t\tindexer.updateWorkflow(workflowId,\n-\t\t\t\t               new String[] {RAW_JSON_FIELD, ARCHIVED_FIELD},\n-\t\t\t\t               new Object[] {om.writeValueAsString(wf), true});\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\t// Not archiving, also remove workflowId from index\n-\t\t\t\tindexer.removeWorkflow(workflowId);\n-\t\t\t}\n-\n-\t\t\twithTransaction(connection -> {\n-\t\t\t\tremoveWorkflowDefToWorkflowMapping(connection, wf);\n-\t\t\t\tremoveWorkflow(connection, workflowId);\n-\t\t\t\tremovePendingWorkflow(connection, wf.getWorkflowType(), workflowId);\n-\t\t\t});\n-\n-\t\t\tfor(Task task : wf.getTasks()) {\n-\t\t\t\tremoveTask(task.getTaskId());\n-\t\t\t}\n-\n-\t\t} catch(Exception e) {\n-\t\t\tthrow new ApplicationException(\"Unable to remove workflow \" + workflowId, e);\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void removeFromPendingWorkflow(String workflowType, String workflowId) {\n-\t\twithTransaction(connection -> removePendingWorkflow(connection, workflowType, workflowId));\n-\t}\n-\n-\t@Override\n-\tpublic Workflow getWorkflow(String workflowId) {\n-\t\treturn getWorkflow(workflowId, true);\n-\t}\n-\n-\t@Override\n-\tpublic Workflow getWorkflow(String workflowId, boolean includeTasks) {\n-\t\tWorkflow workflow = getWithTransaction(tx -> readWorkflow(tx, workflowId));\n+import java.sql.Connection;\n+import java.text.SimpleDateFormat;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import javax.inject.Inject;\n+import javax.sql.DataSource;\n+\n+public class MySQLExecutionDAO extends MySQLBaseDAO implements ExecutionDAO {\n+\n+    private static final String ARCHIVED_FIELD = \"archived\";\n+    private static final String RAW_JSON_FIELD = \"rawJSON\";\n+\n+    private IndexDAO indexer;\n+\n+    private MetadataDAO metadata;\n+\n+    @Inject\n+    public MySQLExecutionDAO(IndexDAO indexer, MetadataDAO metadata, ObjectMapper om, DataSource dataSource) {\n+        super(om, dataSource);\n+        this.indexer = indexer;\n+        this.metadata = metadata;\n+    }\n+\n+    private static String dateStr(Long timeInMs) {\n+        Date date = new Date(timeInMs);\n+        return dateStr(date);\n+    }\n+\n+    private static String dateStr(Date date) {\n+        SimpleDateFormat format = new SimpleDateFormat(\"yyyyMMdd\");\n+        return format.format(date);\n+    }\n+\n+    @Override\n+    public List<Task> getPendingTasksByWorkflow(String taskDefName, String workflowId) {\n+        //@formatter:off\n+        String GET_IN_PROGRESS_TASKS_FOR_WORKFLOW =\n+                \"SELECT json_data FROM task_in_progress tip \"\n+                        + \"INNER JOIN task t ON t.task_id = tip.task_id \"\n+                        + \"WHERE task_def_name = ? AND workflow_id = ?\";\n+        //@formatter:on\n+\n+        return queryWithTransaction(GET_IN_PROGRESS_TASKS_FOR_WORKFLOW,\n+                q -> q.addParameter(taskDefName).addParameter(workflowId).executeAndFetch(Task.class));\n+    }\n+\n+    @Override\n+    public List<Task> getTasks(String taskDefName, String startKey, int count) {\n+        List<Task> tasks = new ArrayList<>(count);\n+\n+        List<Task> pendingTasks = getPendingTasksForTaskType(taskDefName);\n+        boolean startKeyFound = startKey == null;\n+        int found = 0;\n+        for (Task pendingTask : pendingTasks) {\n+            if (!startKeyFound) {\n+                if (pendingTask.getTaskId().equals(startKey)) {\n+                    startKeyFound = true;\n+                    //noinspection ConstantConditions\n+                    if (startKey != null) {\n+                        continue;\n+                    }\n+                }\n+            }\n+            if (startKeyFound && found < count) {\n+                tasks.add(pendingTask);\n+                found++;\n+            }\n+        }\n+\n+        return tasks;\n+    }\n+\n+    @Override\n+    public List<Task> createTasks(List<Task> tasks) {\n+        List<Task> created = Lists.newArrayListWithCapacity(tasks.size());\n+\n+        withTransaction(connection -> {\n+            for (Task task : tasks) {\n+                validate(task);\n+\n+                task.setScheduledTime(System.currentTimeMillis());\n+\n+                String taskKey = task.getReferenceTaskName() + \"\" + task.getRetryCount();\n+\n+                boolean scheduledTaskAdded = addScheduledTask(connection, task, taskKey);\n+\n+                if (!scheduledTaskAdded) {\n+                    logger.info(\"Task already scheduled, skipping the run \" + task.getTaskId() + \", ref=\" +\n+                            task.getReferenceTaskName() + \", key=\" + taskKey);\n+                    continue;\n+                }\n+\n+                insertOrUpdateTaskData(connection, task);\n+                addWorkflowToTaskMapping(connection, task);\n+                addTaskInProgress(connection, task);\n+                updateTask(connection, task);\n+\n+                created.add(task);\n+            }\n+        });\n+\n+        return created;\n+    }\n+\n+    @Override\n+    public void updateTask(Task task) {\n+        withTransaction(connection -> updateTask(connection, task));\n+    }\n+\n+    @Override\n+    public boolean exceedsInProgressLimit(Task task) {\n+        TaskDef taskDef = metadata.getTaskDef(task.getTaskDefName());\n+        if (taskDef == null) {\n+            return false;\n+        }\n+\n+        int limit = taskDef.concurrencyLimit();\n+        if (limit <= 0) {\n+            return false;\n+        }\n+\n+        long current = getInProgressTaskCount(task.getTaskDefName());\n+\n+        if (current >= limit) {\n+            Monitors.recordTaskRateLimited(task.getTaskDefName(), limit);\n+            return true;\n+        }\n+\n+        logger.info(\"Task execution count for {}: limit={}, current={}\", task.getTaskDefName(), limit,\n+                getInProgressTaskCount(task.getTaskDefName()));\n+\n+        String taskId = task.getTaskId();\n+\n+        List<String> tasksInProgressInOrderOfArrival = findAllTasksInProgressInOrderOfArrival(task, limit);\n+\n+        boolean rateLimited = !tasksInProgressInOrderOfArrival.contains(taskId);\n+\n+        if (rateLimited) {\n+            logger.info(\"Task execution count limited. {}, limit {}, current {}\", task.getTaskDefName(), limit,\n+                    getInProgressTaskCount(task.getTaskDefName()));\n+            Monitors.recordTaskRateLimited(task.getTaskDefName(), limit);\n+        }\n+\n+        return rateLimited;\n+    }\n+\n+    @Override\n+    public void updateTasks(List<Task> tasks) {\n+        withTransaction(connection -> tasks.forEach(task -> updateTask(connection, task)));\n+    }\n+\n+    @Override\n+    public void addTaskExecLog(List<TaskExecLog> log) {\n+        indexer.addTaskExecutionLogs(log);\n+    }\n+\n+    @Override\n+    public void removeTask(String taskId) {\n+        Task task = getTask(taskId);\n+\n+        if (task == null) {\n+            logger.warn(\"No such Task by id {}\", taskId);\n+            return;\n+        }\n+\n+        String taskKey = task.getReferenceTaskName() + \"_\" + task.getRetryCount();\n+\n+        withTransaction(connection -> {\n+            removeScheduledTask(connection, task, taskKey);\n+            removeWorkflowToTaskMapping(connection, task);\n+            removeTaskInProgress(connection, task);\n+            removeTaskData(connection, task);\n+        });\n+    }\n+\n+    @Override\n+    public Task getTask(String taskId) {\n+        String GET_TASK = \"SELECT json_data FROM task WHERE task_id = ?\";\n+        return queryWithTransaction(GET_TASK, q -> q.addParameter(taskId).executeAndFetchFirst(Task.class));\n+    }\n+\n+    @Override\n+    public List<Task> getTasks(List<String> taskIds) {\n+        if (taskIds.isEmpty()) {\n+            return Lists.newArrayList();\n+        }\n+        return getWithTransaction(c -> getTasks(c, taskIds));\n+    }\n+\n+    @Override\n+    public List<Task> getPendingTasksForTaskType(String taskName) {\n+        Preconditions.checkNotNull(taskName, \"task name cannot be null\");\n+        //@formatter:off\n+        String GET_IN_PROGRESS_TASKS_FOR_TYPE =\n+                \"SELECT json_data FROM task_in_progress tip \"\n+                        + \"INNER JOIN task t ON t.task_id = tip.task_id \"\n+                        + \"WHERE task_def_name = ?\";\n+        //@formatter:on\n+\n+        return queryWithTransaction(GET_IN_PROGRESS_TASKS_FOR_TYPE,\n+                q -> q.addParameter(taskName).executeAndFetch(Task.class));\n+    }\n+\n+    @Override\n+    public List<Task> getTasksForWorkflow(String workflowId) {\n+        String GET_TASKS_FOR_WORKFLOW = \"SELECT task_id FROM workflow_to_task WHERE workflow_id = ?\";\n+        return getWithTransaction(tx -> query(tx, GET_TASKS_FOR_WORKFLOW, q -> {\n+            List<String> taskIds = q.addParameter(workflowId).executeScalarList(String.class);\n+            return getTasks(tx, taskIds);\n+        }));\n+    }\n+\n+    @Override\n+    public String createWorkflow(Workflow workflow) {\n+        workflow.setCreateTime(System.currentTimeMillis());\n+        return insertOrUpdateWorkflow(workflow, false);\n+    }\n+\n+    @Override\n+    public String updateWorkflow(Workflow workflow) {\n+        workflow.setUpdateTime(System.currentTimeMillis());\n+        return insertOrUpdateWorkflow(workflow, true);\n+    }\n+\n+    @Override\n+    public void removeWorkflow(String workflowId, boolean archiveWorkflow) {\n+        try {\n+            Workflow wf = getWorkflow(workflowId, true);\n+\n+            if (archiveWorkflow) {\n+                //Add to elasticsearch\n+                indexer.updateWorkflow(workflowId, new String[]{RAW_JSON_FIELD, ARCHIVED_FIELD},\n+                        new Object[]{objectMapper.writeValueAsString(wf), true});\n+            } else {\n+                // Not archiving, also remove workflowId from index\n+                indexer.removeWorkflow(workflowId);\n+            }\n+\n+            withTransaction(connection -> {\n+                removeWorkflowDefToWorkflowMapping(connection, wf);\n+                removeWorkflow(connection, workflowId);\n+                removePendingWorkflow(connection, wf.getWorkflowType(), workflowId);\n+            });\n+\n+            for (Task task : wf.getTasks()) {\n+                removeTask(task.getTaskId());\n+            }\n+\n+        } catch (Exception e) {\n+            throw new ApplicationException(\"Unable to remove workflow \" + workflowId, e);\n+        }\n+    }\n+\n+    @Override\n+    public void removeFromPendingWorkflow(String workflowType, String workflowId) {\n+        withTransaction(connection -> removePendingWorkflow(connection, workflowType, workflowId));\n+    }\n+\n+    @Override\n+    public Workflow getWorkflow(String workflowId) {\n+        return getWorkflow(workflowId, true);\n+    }\n+\n+    @Override\n+    public Workflow getWorkflow(String workflowId, boolean includeTasks) {\n+        Workflow workflow = getWithTransaction(tx -> readWorkflow(tx, workflowId));\n \n \t\tif(workflow != null){\n \t\t\tif (includeTasks) {\n@@ -307,468 +305,539 @@ public Workflow getWorkflow(String workflowId, boolean includeTasks) {\n \t\treturn workflow;\n \t}\n \n-\t@Override\n-\tpublic List<String> getRunningWorkflowIds(String workflowName) {\n-\t\tPreconditions.checkNotNull(workflowName, \"workflowName cannot be null\");\n-\t\tString GET_PENDING_WORKFLOW_IDS = \"SELECT workflow_id FROM workflow_pending WHERE workflow_type = :workflowType\";\n-\t\treturn getWithTransaction(tx -> tx.createQuery(GET_PENDING_WORKFLOW_IDS).addParameter(\"workflowType\", workflowName).executeScalarList(String.class));\n-\t}\n-\n-\t@Override\n-\tpublic List<Workflow> getPendingWorkflowsByType(String workflowName) {\n-\t\tPreconditions.checkNotNull(workflowName, \"workflowName cannot be null\");\n-\t\treturn getRunningWorkflowIds(workflowName).stream().map(this::getWorkflow).collect(Collectors.toList());\n-\t}\n-\n-\t@Override\n-\tpublic long getPendingWorkflowCount(String workflowName) {\n-\t\tPreconditions.checkNotNull(workflowName, \"workflowName cannot be null\");\n-\t\tString GET_PENDING_WORKFLOW_COUNT = \"SELECT COUNT(*) FROM workflow_pending WHERE workflow_type = :workflowType\";\n-\t\treturn getWithTransaction(tx -> tx.createQuery(GET_PENDING_WORKFLOW_COUNT).addParameter(\"workflowType\", workflowName).executeScalar(Long.class));\n-\t}\n-\n-\t@Override\n-\tpublic long getInProgressTaskCount(String taskDefName) {\n-\t\tString GET_IN_PROGRESS_TASK_COUNT = \"SELECT COUNT(*) FROM task_in_progress WHERE task_def_name = :taskDefName AND in_progress_status = true\";\n-\t\treturn getWithTransaction(c -> c.createQuery(GET_IN_PROGRESS_TASK_COUNT)\n-\t\t\t\t.addParameter(\"taskDefName\", taskDefName)\n-\t\t\t\t.executeScalar(Long.class));\n-\t}\n-\n-\t@Override\n-\tpublic List<Workflow> getWorkflowsByType(String workflowName, Long startTime, Long endTime) {\n-\t\tPreconditions.checkNotNull(workflowName, \"workflowName cannot be null\");\n-\t\tPreconditions.checkNotNull(startTime, \"startTime cannot be null\");\n-\t\tPreconditions.checkNotNull(endTime, \"endTime cannot be null\");\n-\n-\t\tList<Workflow> workflows = new LinkedList<Workflow>();\n-\n-\t\twithTransaction(tx -> {\n-\t\t\tString GET_ALL_WORKFLOWS_FOR_WORKFLOW_DEF = \"SELECT workflow_id FROM workflow_def_to_workflow WHERE workflow_def = :workflowType AND date_str BETWEEN :start AND :end\";\n-\t\t\tList<String> workflowIds = tx.createQuery(GET_ALL_WORKFLOWS_FOR_WORKFLOW_DEF)\n-\t\t\t\t\t.addParameter(\"workflowType\", workflowName)\n-\t\t\t\t\t.addParameter(\"start\", dateStr(startTime))\n-\t\t\t\t\t.addParameter(\"end\", dateStr(endTime))\n-\t\t\t\t\t.executeScalarList(String.class);\n-\n-\t\t\tworkflowIds.forEach(workflowId -> {\n-\t\t\t\ttry {\n-\t\t\t\t\tWorkflow wf = getWorkflow(workflowId);\n-\t\t\t\t\tif (wf.getCreateTime() >= startTime && wf.getCreateTime() <= endTime) {\n-\t\t\t\t\t\tworkflows.add(wf);\n-\t\t\t\t\t}\n-\t\t\t\t} catch(Exception e) {\n-\t\t\t\t\tlogger.error(\"Unable to load workflow id {} with name {}\", workflowId, workflowName, e);\n-\t\t\t\t}\n-\t\t\t});\n-\t\t});\n-\n-\t\treturn workflows;\n-\t}\n-\n-\t@Override\n-\tpublic List<Workflow> getWorkflowsByCorrelationId(String correlationId, boolean includeTasks) {\n-\t\tPreconditions.checkNotNull(correlationId, \"correlationId cannot be null\");\n-\t\tString GET_WORKFLOWS_BY_CORRELATION_ID = \"SELECT workflow_id FROM workflow WHERE correlation_id = :correlationId\";\n-\t\treturn getWithTransaction(tx -> tx.createQuery(GET_WORKFLOWS_BY_CORRELATION_ID)\n-\t\t\t\t.addParameter(\"correlationId\", correlationId)\n-\t\t\t\t.executeScalarList(String.class)).stream()\n+    @Override\n+    public List<String> getRunningWorkflowIds(String workflowName) {\n+        Preconditions.checkNotNull(workflowName, \"workflowName cannot be null\");\n+        String GET_PENDING_WORKFLOW_IDS = \"SELECT workflow_id FROM workflow_pending WHERE workflow_type = ?\";\n+\n+        return queryWithTransaction(GET_PENDING_WORKFLOW_IDS,\n+                q -> q.addParameter(workflowName).executeScalarList(String.class));\n+    }\n+\n+    @Override\n+    public List<Workflow> getPendingWorkflowsByType(String workflowName) {\n+        Preconditions.checkNotNull(workflowName, \"workflowName cannot be null\");\n+        return getRunningWorkflowIds(workflowName).stream().map(this::getWorkflow).collect(Collectors.toList());\n+    }\n+\n+    @Override\n+    public long getPendingWorkflowCount(String workflowName) {\n+        Preconditions.checkNotNull(workflowName, \"workflowName cannot be null\");\n+        String GET_PENDING_WORKFLOW_COUNT = \"SELECT COUNT(*) FROM workflow_pending WHERE workflow_type = ?\";\n+\n+        return queryWithTransaction(GET_PENDING_WORKFLOW_COUNT, q -> q.addParameter(workflowName).executeCount());\n+    }\n+\n+    @Override\n+    public long getInProgressTaskCount(String taskDefName) {\n+        String GET_IN_PROGRESS_TASK_COUNT =\n+                \"SELECT COUNT(*) FROM task_in_progress WHERE task_def_name = ? AND in_progress_status = true\";\n+\n+        return queryWithTransaction(GET_IN_PROGRESS_TASK_COUNT, q -> q.addParameter(taskDefName).executeCount());\n+    }\n+\n+    @Override\n+    public List<Workflow> getWorkflowsByType(String workflowName, Long startTime, Long endTime) {\n+        Preconditions.checkNotNull(workflowName, \"workflowName cannot be null\");\n+        Preconditions.checkNotNull(startTime, \"startTime cannot be null\");\n+        Preconditions.checkNotNull(endTime, \"endTime cannot be null\");\n+\n+        List<Workflow> workflows = new LinkedList<>();\n+\n+        withTransaction(tx -> {\n+            //@formatter:off\n+            String GET_ALL_WORKFLOWS_FOR_WORKFLOW_DEF =\n+                    \"SELECT workflow_id FROM workflow_def_to_workflow \"\n+                            + \"WHERE workflow_def = ? AND date_str BETWEEN ? AND ?\";\n+            //@formatter:on\n+\n+            List<String> workflowIds = query(tx, GET_ALL_WORKFLOWS_FOR_WORKFLOW_DEF, q -> q.addParameter(workflowName)\n+                    .addParameter(\n+                            dateStr(startTime))\n+                    .addParameter(\n+                            dateStr(endTime))\n+                    .executeScalarList(\n+                            String.class));\n+            workflowIds.forEach(workflowId -> {\n+                try {\n+                    Workflow wf = getWorkflow(workflowId);\n+                    if (wf.getCreateTime() >= startTime && wf.getCreateTime() <= endTime) {\n+                        workflows.add(wf);\n+                    }\n+                } catch (Exception e) {\n+                    logger.error(\"Unable to load workflow id {} with name {}\", workflowId, workflowName, e);\n+                }\n+            });\n+        });\n+\n+        return workflows;\n+    }\n+\n+    @Override\n+    public List<Workflow> getWorkflowsByCorrelationId(String correlationId, boolean includeTasks) {\n+        Preconditions.checkNotNull(correlationId, \"correlationId cannot be null\");\n+        String GET_WORKFLOWS_BY_CORRELATION_ID =\n+                \"SELECT workflow_id FROM workflow WHERE correlation_id = ?\";\n+\n+        return queryWithTransaction(GET_WORKFLOWS_BY_CORRELATION_ID, q -> q.addParameter(correlationId)\n+                .executeScalarList(String.class)\n+                .stream()\n \t\t\t\t.map(workflowId -> getWorkflow(workflowId, includeTasks))\n-\t\t\t\t.collect(Collectors.toList());\n-\t}\n-\n-\t@Override\n-\tpublic boolean addEventExecution(EventExecution eventExecution) {\n-\t\ttry {\n-\t\t\tboolean added = getWithTransaction(tx -> insertEventExecution(tx, eventExecution));\n-\t\t\tif (added) {\n-\t\t\t\tindexer.addEventExecution(eventExecution);\n-\t\t\t\treturn true;\n-\t\t\t}\n-\t\t\treturn false;\n-\t\t} catch (Exception e) {\n-\t\t\tthrow new ApplicationException(ApplicationException.Code.BACKEND_ERROR, \"Unable to add event execution \" + eventExecution.getId(), e);\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void updateEventExecution(EventExecution eventExecution) {\n-\t\ttry {\n-\t\t\twithTransaction(tx -> updateEventExecution(tx, eventExecution));\n-\t\t\tindexer.addEventExecution(eventExecution);\n-\t\t} catch (Exception e) {\n-\t\t\tthrow new ApplicationException(ApplicationException.Code.BACKEND_ERROR, \"Unable to update event execution \" + eventExecution.getId(), e);\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic List<EventExecution> getEventExecutions(String eventHandlerName, String eventName, String messageId, int max) {\n-\t\ttry {\n-\t\t\tList<EventExecution> executions = Lists.newLinkedList();\n-\t\t\twithTransaction(tx ->  {\n-\t\t\t\tfor(int i = 0; i < max; i++) {\n-\t\t\t\t\tString executionId = messageId + \"_\" + i; //see EventProcessor.handle to understand how the execution id is set\n-\t\t\t\t\tEventExecution ee = readEventExecution(tx, eventHandlerName, eventName, messageId, executionId);\n-\t\t\t\t\tif (ee == null) break;\n-\t\t\t\t\texecutions.add(ee);\n-\t\t\t\t}\n-\t\t\t});\n-\t\t\treturn executions;\n-\t\t} catch (Exception e) {\n-\t\t\tString message = String.format(\"Unable to get event executions for eventHandlerName=%s, eventName=%s, messageId=%s\", eventHandlerName, eventName, messageId);\n-\t\t\tthrow new ApplicationException(ApplicationException.Code.BACKEND_ERROR, message, e);\n-\t\t}\n-\t}\n-\n-\t@Override\n-\tpublic void addMessage(String queue, Message msg) {\n-\t\tindexer.addMessage(queue, msg);\n-\t}\n-\n-\t@Override\n-\tpublic void updateLastPoll(String taskDefName, String domain, String workerId) {\n-\t\tPreconditions.checkNotNull(taskDefName, \"taskDefName name cannot be null\");\n-\t\tPollData pollData = new PollData(taskDefName, domain, workerId, System.currentTimeMillis());\n-\t\tString effectiveDomain = (domain == null) ? \"DEFAULT\" : domain;\n-\t\twithTransaction(tx -> insertOrUpdatePollData(tx, pollData, effectiveDomain));\n-\t}\n-\n-\t@Override\n-\tpublic PollData getPollData(String taskDefName, String domain) {\n-\t\tPreconditions.checkNotNull(taskDefName, \"taskDefName name cannot be null\");\n-\t\tString effectiveDomain = (domain == null) ? \"DEFAULT\" : domain;\n-\t\treturn getWithTransaction(tx -> readPollData(tx, taskDefName, effectiveDomain));\n-\t}\n-\n-\t@Override\n-\tpublic List<PollData> getPollData(String taskDefName) {\n-\t\tPreconditions.checkNotNull(taskDefName, \"taskDefName name cannot be null\");\n-\t\treturn readAllPollData(taskDefName);\n-\t}\n-\n-\tprivate String insertOrUpdateWorkflow(Workflow workflow, boolean update) {\n-\t\tPreconditions.checkNotNull(workflow, \"workflow object cannot be null\");\n-\n-\t\tboolean terminal = workflow.getStatus().isTerminal();\n-\n-\t\tif (terminal) workflow.setEndTime(System.currentTimeMillis());\n-\n-\t\tList<Task> tasks = workflow.getTasks();\n-\t\tworkflow.setTasks(Lists.newLinkedList());\n-\n-\t\twithTransaction(tx -> {\n-\t\t\tif (!update) {\n-\t\t\t\taddWorkflow(tx, workflow);\n-\t\t\t\taddWorkflowDefToWorkflowMapping(tx, workflow);\n-\t\t\t} else {\n-\t\t\t\tupdateWorkflow(tx, workflow);\n-\t\t\t}\n-\n-\t\t\tif (terminal) {\n-\t\t\t\tremovePendingWorkflow(tx, workflow.getWorkflowType(), workflow.getWorkflowId());\n-\t\t\t} else {\n-\t\t\t\taddPendingWorkflow(tx, workflow.getWorkflowType(), workflow.getWorkflowId());\n-\t\t\t}\n-\t\t});\n-\n-\t\tworkflow.setTasks(tasks);\n-\t\tindexer.indexWorkflow(workflow);\n-\t\treturn workflow.getWorkflowId();\n-\t}\n-\n-\tprivate void updateTask(Connection connection, Task task) {\n-\t\ttask.setUpdateTime(System.currentTimeMillis());\n-\t\tif (task.getStatus() != null && task.getStatus().isTerminal()) {\n-\t\t\ttask.setEndTime(System.currentTimeMillis());\n-\t\t}\n-\n-\t\tTaskDef taskDef = metadata.getTaskDef(task.getTaskDefName());\n-\n-\t\tif (taskDef != null && taskDef.concurrencyLimit() > 0) {\n-\t\t\tboolean inProgress = task.getStatus() != null && task.getStatus().equals(Task.Status.IN_PROGRESS);\n-\t\t\tupdateInProgressStatus(connection, task, inProgress);\n-\t\t}\n-\n-\t\tinsertOrUpdateTaskData(connection, task);\n-\n-\t\tif (task.getStatus() != null && task.getStatus().isTerminal()) {\n-\t\t\tremoveTaskInProgress(connection, task);\n-\t\t}\n-\n-\t\tindexer.indexTask(task);\n-\t}\n-\n-\tprivate Workflow readWorkflow(Connection connection, String workflowId) {\n-\t\tString GET_WORKFLOW = \"SELECT json_data FROM workflow WHERE workflow_id = :workflowId\";\n-\t\tString json = connection.createQuery(GET_WORKFLOW).addParameter(\"workflowId\", workflowId).executeScalar(String.class);\n-\t\treturn json != null ? readValue(json, Workflow.class) : null;\n-\t}\n-\n-\tprivate Workflow readWorkflowFromArchive(String workflowId) {\n-\t\tString json = indexer.get(workflowId, RAW_JSON_FIELD);\n-\t\tif (json != null) {\n-\t\t\treturn readValue(json, Workflow.class);\n-\t\t} else {\n-\t\t\tthrow new ApplicationException(ApplicationException.Code.NOT_FOUND, \"No such workflow found by id: \" + workflowId);\n-\t\t}\n-\t}\n-\n-\tprivate void addWorkflow(Connection connection, Workflow workflow) {\n-\t\tString INSERT_WORKFLOW = \"INSERT INTO workflow (workflow_id, correlation_id, json_data) VALUES (:workflowId, :correlationId, :jsonData)\";\n-\t\tconnection.createQuery(INSERT_WORKFLOW)\n-\t\t\t\t.addParameter(\"workflowId\", workflow.getWorkflowId())\n-\t\t\t\t.addParameter(\"correlationId\", workflow.getCorrelationId())\n-\t\t\t\t.addParameter(\"jsonData\", toJson(workflow))\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate void updateWorkflow(Connection connection, Workflow workflow) {\n-\t\tString UPDATE_WORKFLOW = \"UPDATE workflow SET json_data = :jsonData, modified_on = CURRENT_TIMESTAMP WHERE workflow_id = :workflowId\";\n-\t\tconnection.createQuery(UPDATE_WORKFLOW)\n-\t\t\t\t.addParameter(\"workflowId\", workflow.getWorkflowId())\n-\t\t\t\t.addParameter(\"jsonData\", toJson(workflow))\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate void removeWorkflow(Connection connection, String workflowId) {\n-\t\tString REMOVE_WORKFLOW = \"DELETE FROM workflow WHERE workflow_id = :workflowId\";\n-\t\tconnection.createQuery(REMOVE_WORKFLOW)\n-\t\t\t\t.addParameter(\"workflowId\", workflowId)\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate void addPendingWorkflow(Connection connection, String workflowType, String workflowId) {\n-\t\tString EXISTS_PENDING_WORKFLOW = \"SELECT EXISTS(SELECT 1 FROM workflow_pending WHERE workflow_type = :workflowType AND workflow_id = :workflowId)\";\n-\t\tboolean exist = connection.createQuery(EXISTS_PENDING_WORKFLOW)\n-\t\t\t\t.addParameter(\"workflowType\", workflowType)\n-\t\t\t\t.addParameter(\"workflowId\", workflowId)\n-\t\t\t\t.executeScalar(Boolean.class);\n-\n-\t\tif (!exist) {\n-\t\t\tString INSERT_PENDING_WORKFLOW = \"INSERT INTO workflow_pending (workflow_type, workflow_id) VALUES (:workflowType, :workflowId)\";\n-\t\t\tconnection.createQuery(INSERT_PENDING_WORKFLOW)\n-\t\t\t\t\t.addParameter(\"workflowType\", workflowType)\n-\t\t\t\t\t.addParameter(\"workflowId\", workflowId)\n-\t\t\t\t\t.executeUpdate();\n-\t\t}\n-\t}\n-\n-\tprivate void removePendingWorkflow(Connection connection, String workflowType, String workflowId) {\n-\t\tString REMOVE_PENDING_WORKFLOW = \"DELETE FROM workflow_pending WHERE workflow_type = :workflowType AND workflow_id = :workflowId\";\n-\t\tconnection.createQuery(REMOVE_PENDING_WORKFLOW)\n-\t\t\t\t.addParameter(\"workflowType\", workflowType)\n-\t\t\t\t.addParameter(\"workflowId\", workflowId)\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate void insertOrUpdateTaskData(Connection connection, Task task) {\n-\t\tString UPDATE_TASK = \"UPDATE task SET json_data = :jsonData, modified_on = CURRENT_TIMESTAMP WHERE task_id = :taskId\";\n-\t\tint result = connection.createQuery(UPDATE_TASK).addParameter(\"taskId\", task.getTaskId()).addParameter(\"jsonData\", toJson(task)).executeUpdate().getResult();\n-\t\tif (result == 0) {\n-\t\t\tString INSERT_TASK = \"INSERT INTO task (task_id, json_data) VALUES (:taskId, :jsonData)\";\n-\t\t\tconnection.createQuery(INSERT_TASK).addParameter(\"taskId\", task.getTaskId()).addParameter(\"jsonData\", toJson(task)).executeUpdate().getResult();\n-\t\t}\n-\t}\n-\n-\tprivate void removeTaskData(Connection connection, Task task) {\n-\t\tString REMOVE_TASK = \"DELETE FROM task WHERE task_id = :taskId\";\n-\t\tconnection.createQuery(REMOVE_TASK)\n-\t\t\t\t.addParameter(\"taskId\", task.getTaskId())\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate void addWorkflowToTaskMapping(Connection connection, Task task) {\n-\t\tString EXISTS_WORKFLOW_TO_TASK = \"SELECT EXISTS(SELECT 1 FROM workflow_to_task WHERE workflow_id = :workflowId AND task_id = :taskId)\";\n-\t\tboolean exist = connection.createQuery(EXISTS_WORKFLOW_TO_TASK)\n-\t\t\t\t.addParameter(\"workflowId\", task.getWorkflowInstanceId())\n-\t\t\t\t.addParameter(\"taskId\", task.getTaskId())\n-\t\t\t\t.executeScalar(Boolean.class);\n-\n-\t\tif (!exist) {\n-\t\t\tString INSERT_WORKFLOW_TO_TASK = \"INSERT INTO workflow_to_task (workflow_id, task_id) VALUES (:workflowId, :taskId)\";\n-\t\t\tconnection.createQuery(INSERT_WORKFLOW_TO_TASK)\n-\t\t\t\t\t.addParameter(\"workflowId\", task.getWorkflowInstanceId())\n-\t\t\t\t\t.addParameter(\"taskId\", task.getTaskId())\n-\t\t\t\t\t.executeUpdate();\n-\t\t}\n-\t}\n-\n-\tprivate void removeWorkflowToTaskMapping(Connection connection, Task task) {\n-\t\tString REMOVE_WORKFLOW_TO_TASK = \"DELETE FROM workflow_to_task WHERE workflow_id = :workflowId AND task_id = :taskId\";\n-\t\tconnection.createQuery(REMOVE_WORKFLOW_TO_TASK)\n-\t\t\t\t.addParameter(\"workflowId\", task.getWorkflowInstanceId())\n-\t\t\t\t.addParameter(\"taskId\", task.getTaskId())\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate void addWorkflowDefToWorkflowMapping(Connection connection, Workflow workflow) {\n-\t\tString INSERT_WORKFLOW_DEF_TO_WORKFLOW = \"INSERT INTO workflow_def_to_workflow (workflow_def, date_str, workflow_id) VALUES (:workflowType, :dateStr, :workflowId)\";\n-\t\tconnection.createQuery(INSERT_WORKFLOW_DEF_TO_WORKFLOW)\n-\t\t\t\t.bind(workflow)\n-\t\t\t\t.addParameter(\"dateStr\", dateStr(workflow.getCreateTime()))\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate void removeWorkflowDefToWorkflowMapping(Connection connection, Workflow workflow) {\n-\t\tString REMOVE_WORKFLOW_DEF_TO_WORKFLOW = \"DELETE FROM workflow_def_to_workflow WHERE workflow_def = :workflowType AND date_str = :dateStr AND workflow_id = :workflowId\";\n-\t\tconnection.createQuery(REMOVE_WORKFLOW_DEF_TO_WORKFLOW)\n-\t\t\t\t.bind(workflow)\n-\t\t\t\t.addParameter(\"dateStr\", dateStr(workflow.getCreateTime()))\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate boolean addScheduledTask(Connection connection, Task task, String taskKey) {\n-\t\tString EXISTS_SCHEDULED_TASK = \"SELECT EXISTS(SELECT 1 FROM task_scheduled WHERE workflow_id = :workflowId AND task_key = :taskKey)\";\n-\t\tboolean exist = connection.createQuery(EXISTS_SCHEDULED_TASK)\n-\t\t\t\t.addParameter(\"workflowId\", task.getWorkflowInstanceId())\n-\t\t\t\t.addParameter(\"taskKey\", taskKey)\n-\t\t\t\t.executeScalar(Boolean.class);\n-\n-\t\tif (!exist) {\n-\t\t\tString INSERT_SCHEDULED_TASK = \"INSERT INTO task_scheduled (workflow_id, task_key, task_id) VALUES (:workflowId, :taskKey, :taskId)\";\n-\t\t\tconnection.createQuery(INSERT_SCHEDULED_TASK)\n-\t\t\t\t\t.addParameter(\"workflowId\", task.getWorkflowInstanceId())\n-\t\t\t\t\t.addParameter(\"taskKey\", taskKey)\n-\t\t\t\t\t.addParameter(\"taskId\", task.getTaskId())\n-\t\t\t\t\t.executeUpdate()\n-\t\t\t\t\t.getResult();\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\treturn false;\n-\t}\n-\n-\tprivate void removeScheduledTask(Connection connection, Task task, String taskKey) {\n-\t\tString REMOVE_SCHEDULED_TASK = \"DELETE FROM task_scheduled WHERE workflow_id = :workflowId AND task_key = :taskKey\";\n-\t\tconnection.createQuery(REMOVE_SCHEDULED_TASK)\n-\t\t\t\t.addParameter(\"workflowId\", task.getWorkflowInstanceId())\n-\t\t\t\t.addParameter(\"taskKey\", taskKey)\n-\t\t\t\t.executeUpdate()\n-\t\t\t\t.getResult();\n-\t}\n-\n-\tprivate void addTaskInProgress(Connection connection, Task task) {\n-\t\tString EXISTS_IN_PROGRESS_TASK = \"SELECT EXISTS(SELECT 1 FROM task_in_progress WHERE task_def_name = :taskDefName AND task_id = :taskId)\";\n-\t\tboolean exist = connection.createQuery(EXISTS_IN_PROGRESS_TASK)\n-\t\t\t\t.addParameter(\"taskDefName\", task.getTaskDefName())\n-\t\t\t\t.addParameter(\"taskId\", task.getTaskId())\n-\t\t\t\t.executeScalar(Boolean.class);\n-\n-\t\tif (!exist) {\n-\t\t\tString INSERT_IN_PROGRESS_TASK = \"INSERT INTO task_in_progress (task_def_name, task_id, workflow_id) VALUES (:taskDefName, :taskId, :workflowId)\";\n-\t\t\tconnection.createQuery(INSERT_IN_PROGRESS_TASK)\n-\t\t\t\t\t.addParameter(\"taskDefName\", task.getTaskDefName())\n-\t\t\t\t\t.addParameter(\"taskId\", task.getTaskId())\n-\t\t\t\t\t.addParameter(\"workflowId\", task.getWorkflowInstanceId())\n-\t\t\t\t\t.executeUpdate();\n-\t\t}\n-\t}\n-\n-\tprivate void removeTaskInProgress(Connection connection, Task task) {\n-\t\tString REMOVE_IN_PROGRESS_TASK = \"DELETE FROM task_in_progress WHERE task_def_name = :taskDefName AND task_id = :taskId\";\n-\t\tconnection.createQuery(REMOVE_IN_PROGRESS_TASK)\n-\t\t\t\t.addParameter(\"taskDefName\", task.getTaskDefName())\n-\t\t\t\t.addParameter(\"taskId\", task.getTaskId())\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate void updateInProgressStatus(Connection connection, Task task, boolean inProgress) {\n-\t\tString UPDATE_IN_PROGRESS_TASK_STATUS = \"UPDATE task_in_progress SET in_progress_status = :inProgress, modified_on = CURRENT_TIMESTAMP WHERE task_def_name = :taskDefName AND task_id = :taskId\";\n-\t\tconnection.createQuery(UPDATE_IN_PROGRESS_TASK_STATUS)\n-\t\t\t\t.addParameter(\"taskDefName\", task.getTaskDefName())\n-\t\t\t\t.addParameter(\"taskId\", task.getTaskId())\n-\t\t\t\t.addParameter(\"inProgress\", inProgress)\n-\t\t\t\t.executeUpdate();\n-\t}\n-\n-\tprivate boolean insertEventExecution(Connection  connection, EventExecution eventExecution) {\n-\t\tString EXISTS_EVENT_EXECUTION = \"SELECT EXISTS(SELECT 1 FROM event_execution WHERE event_handler_name = :name AND event_name = :event AND message_id = :messageId AND execution_id = :id)\";\n-\t\tboolean exist = connection.createQuery(EXISTS_EVENT_EXECUTION).bind(eventExecution).executeScalar(Boolean.class);\n-\t\tif (!exist) {\n-\t\t\tString INSERT_EVENT_EXECUTION = \"INSERT INTO event_execution (event_handler_name, event_name, message_id, execution_id, json_data) VALUES (:name, :event, :messageId, :id, :jsonData)\";\n-\t\t\tconnection.createQuery(INSERT_EVENT_EXECUTION).bind(eventExecution).addParameter(\"jsonData\", toJson(eventExecution)).executeUpdate();\n-\t\t\treturn true;\n-\t\t}\n-\t\treturn false;\n-\t}\n-\n-\tprivate void updateEventExecution(Connection  connection, EventExecution eventExecution) {\n-\t\tString UPDATE_EVENT_EXECUTION = \"UPDATE event_execution SET json_data = :jsonData, modified_on = CURRENT_TIMESTAMP WHERE event_handler_name = :name AND execution_event = :event AND message_id = :messageId AND execution_id = :id\";\n-\t\tconnection.createQuery(UPDATE_EVENT_EXECUTION).bind(eventExecution).addParameter(\"jsonData\", toJson(eventExecution)).executeUpdate();\n-\t}\n-\n-\tprivate EventExecution readEventExecution(Connection connection, String eventHandlerName, String eventName, String messageId, String executionId) {\n-\t\tString GET_EVENT_EXECUTION = \"SELECT json_data FROM event_execution WHERE event_handler_name = :name AND event_name = :event AND message_id = :messageId AND execution_id = :id\";\n-\t\tString jsonStr = connection.createQuery(GET_EVENT_EXECUTION)\n-\t\t\t\t.addParameter(\"name\", eventHandlerName)\n-\t\t\t\t.addParameter(\"event\", eventName)\n-\t\t\t\t.addParameter(\"messageId\", messageId)\n-\t\t\t\t.addParameter(\"id\", executionId)\n-\t\t\t\t.executeScalar(String.class);\n-\t\treturn jsonStr != null ? readValue(jsonStr, EventExecution.class) : null;\n-\t}\n-\n-\tprivate void insertOrUpdatePollData(Connection connection, PollData pollData, String domain) {\n-\t\tString UPDATE_POLL_DATA = \"UPDATE poll_data SET json_data = :jsonData, modified_on = CURRENT_TIMESTAMP WHERE queue_name = :queueName AND domain = :domain\";\n-\t\tint result = connection.createQuery(UPDATE_POLL_DATA)\n-\t\t\t\t.addParameter(\"queueName\", pollData.getQueueName())\n-\t\t\t\t.addParameter(\"domain\", domain)\n-\t\t\t\t.addParameter(\"jsonData\", toJson(pollData))\n-\t\t\t\t.executeUpdate()\n-\t\t\t\t.getResult();\n-\t\tif (result == 0) {\n-\t\t\tString INSERT_POLL_DATA = \"INSERT INTO poll_data (queue_name, domain, json_data) VALUES (:queueName, :domain, :jsonData)\";\n-\t\t\tconnection.createQuery(INSERT_POLL_DATA)\n-\t\t\t\t\t.addParameter(\"queueName\", pollData.getQueueName())\n-\t\t\t\t\t.addParameter(\"domain\", domain)\n-\t\t\t\t\t.addParameter(\"jsonData\", toJson(pollData))\n-\t\t\t\t\t.executeUpdate()\n-\t\t\t\t\t.getResult();\n-\t\t}\n-\t}\n-\n-\tprivate PollData readPollData(Connection connection, String queueName, String domain) {\n-\t\tString GET_POLL_DATA = \"SELECT json_data FROM poll_data WHERE queue_name = :queueName AND domain = :domain\";\n-\t\tString jsonStr = connection.createQuery(GET_POLL_DATA)\n-\t\t\t\t.addParameter(\"queueName\", queueName)\n-\t\t\t\t.addParameter(\"domain\", domain)\n-\t\t\t\t.executeScalar(String.class);\n-\t\treturn jsonStr != null ? readValue(jsonStr, PollData.class) : null;\n-\t}\n-\n-\tprivate List<PollData> readAllPollData(String queueName) {\n-\t\tString GET_ALL_POLL_DATA = \"SELECT json_data FROM poll_data WHERE queue_name = :queueName\";\n-\t\treturn getWithTransaction(tx -> tx.createQuery(GET_ALL_POLL_DATA)\n-\t\t\t\t.addParameter(\"queueName\", queueName)\n-\t\t\t\t.executeScalarList(String.class)\n-\t\t\t\t.stream()\n-\t\t\t\t.map(jsonData -> readValue(jsonData, PollData.class))\n-\t\t\t\t.collect(Collectors.toList()));\n-\t}\n-\n-\tprivate List<String> findAllTasksInProgressInOrderOfArrival(Task task, int limit) {\n-\t\tString GET_IN_PROGRESS_TASKS_WITH_LIMIT = \"SELECT task_id FROM task_in_progress WHERE task_def_name = :taskDefName ORDER BY id LIMIT :limit\";\n-\t\treturn getWithTransaction(connection ->\n-\t\t\tconnection.createQuery(GET_IN_PROGRESS_TASKS_WITH_LIMIT)\n-\t\t\t\t\t.addParameter(\"taskDefName\", task.getTaskDefName())\n-\t\t\t\t\t.addParameter(\"limit\", limit)\n-\t\t\t\t\t.executeScalarList(String.class));\n-\t}\n-\n-\tprivate void validate(Task task) {\n-\t\tPreconditions.checkNotNull(task, \"task object cannot be null\");\n-\t\tPreconditions.checkNotNull(task.getTaskId(), \"Task id cannot be null\");\n-\t\tPreconditions.checkNotNull(task.getWorkflowInstanceId(), \"Workflow instance id cannot be null\");\n-\t\tPreconditions.checkNotNull(task.getReferenceTaskName(), \"Task reference name cannot be null\");\n-\t}\n-\n-\tprivate static String dateStr(Long timeInMs) {\n-\t\tDate date = new Date(timeInMs);\n-\t\treturn dateStr(date);\n-\t}\n-\n-\tprivate static String dateStr(Date date) {\n-\t\tSimpleDateFormat format = new SimpleDateFormat(\"yyyyMMdd\");\n-\t\treturn format.format(date);\n-\t}\n+                .collect(Collectors.toList()));\n+    }\n+\n+    @Override\n+    public boolean addEventExecution(EventExecution eventExecution) {\n+        try {\n+            boolean added = getWithTransaction(tx -> insertEventExecution(tx, eventExecution));\n+            if (added) {\n+                indexer.addEventExecution(eventExecution);\n+                return true;\n+            }\n+            return false;\n+        } catch (Exception e) {\n+            throw new ApplicationException(ApplicationException.Code.BACKEND_ERROR,\n+                    \"Unable to add event execution \" + eventExecution.getId(), e);\n+        }\n+    }\n+\n+    @Override\n+    public void updateEventExecution(EventExecution eventExecution) {\n+        try {\n+            withTransaction(tx -> updateEventExecution(tx, eventExecution));\n+            indexer.addEventExecution(eventExecution);\n+        } catch (Exception e) {\n+            throw new ApplicationException(ApplicationException.Code.BACKEND_ERROR,\n+                    \"Unable to update event execution \" + eventExecution.getId(), e);\n+        }\n+    }\n+\n+    @Override\n+    public List<EventExecution> getEventExecutions(String eventHandlerName, String eventName, String messageId,\n+            int max) {\n+        try {\n+            List<EventExecution> executions = Lists.newLinkedList();\n+            withTransaction(tx -> {\n+                for (int i = 0; i < max; i++) {\n+                    String executionId =\n+                            messageId + \"_\" + i; //see EventProcessor.handle to understand how the execution id is set\n+                    EventExecution ee = readEventExecution(tx, eventHandlerName, eventName, messageId, executionId);\n+                    if (ee == null) {\n+                        break;\n+                    }\n+                    executions.add(ee);\n+                }\n+            });\n+            return executions;\n+        } catch (Exception e) {\n+            String message = String.format(\n+                    \"Unable to get event executions for eventHandlerName=%s, eventName=%s, messageId=%s\",\n+                    eventHandlerName,\n+                    eventName, messageId);\n+            throw new ApplicationException(ApplicationException.Code.BACKEND_ERROR, message, e);\n+        }\n+    }\n+\n+    @Override\n+    public void addMessage(String queue, Message msg) {\n+        indexer.addMessage(queue, msg);\n+    }\n+\n+    @Override\n+    public void updateLastPoll(String taskDefName, String domain, String workerId) {\n+        Preconditions.checkNotNull(taskDefName, \"taskDefName name cannot be null\");\n+        PollData pollData = new PollData(taskDefName, domain, workerId, System.currentTimeMillis());\n+        String effectiveDomain = (domain == null) ? \"DEFAULT\" : domain;\n+        withTransaction(tx -> insertOrUpdatePollData(tx, pollData, effectiveDomain));\n+    }\n+\n+    @Override\n+    public PollData getPollData(String taskDefName, String domain) {\n+        Preconditions.checkNotNull(taskDefName, \"taskDefName name cannot be null\");\n+        String effectiveDomain = (domain == null) ? \"DEFAULT\" : domain;\n+        return getWithTransaction(tx -> readPollData(tx, taskDefName, effectiveDomain));\n+    }\n+\n+    @Override\n+    public List<PollData> getPollData(String taskDefName) {\n+        Preconditions.checkNotNull(taskDefName, \"taskDefName name cannot be null\");\n+        return readAllPollData(taskDefName);\n+    }\n+\n+    private List<Task> getTasks(Connection connection, List<String> taskIds) {\n+        if (taskIds.isEmpty()) {\n+            return Lists.newArrayList();\n+        }\n+\n+        // Generate a formatted query string with a variable number of bind params based on taskIds.size()\n+        final String GET_TASKS_FOR_IDS = String.format(\n+                \"SELECT json_data FROM task WHERE task_id IN (%s) AND json_data IS NOT NULL\",\n+                Query.generateInBindings(taskIds.size()));\n+\n+        return query(connection, GET_TASKS_FOR_IDS, q -> q.addParameters(taskIds).executeAndFetch(Task.class));\n+    }\n+\n+    private String insertOrUpdateWorkflow(Workflow workflow, boolean update) {\n+        Preconditions.checkNotNull(workflow, \"workflow object cannot be null\");\n+\n+        boolean terminal = workflow.getStatus().isTerminal();\n+\n+        if (terminal) {\n+            workflow.setEndTime(System.currentTimeMillis());\n+        }\n+\n+        List<Task> tasks = workflow.getTasks();\n+        workflow.setTasks(Lists.newLinkedList());\n+\n+        withTransaction(tx -> {\n+            if (!update) {\n+                addWorkflow(tx, workflow);\n+                addWorkflowDefToWorkflowMapping(tx, workflow);\n+            } else {\n+                updateWorkflow(tx, workflow);\n+            }\n+\n+            if (terminal) {\n+                removePendingWorkflow(tx, workflow.getWorkflowType(), workflow.getWorkflowId());\n+            } else {\n+                addPendingWorkflow(tx, workflow.getWorkflowType(), workflow.getWorkflowId());\n+            }\n+        });\n+\n+        workflow.setTasks(tasks);\n+        indexer.indexWorkflow(workflow);\n+        return workflow.getWorkflowId();\n+    }\n+\n+    private void updateTask(Connection connection, Task task) {\n+        task.setUpdateTime(System.currentTimeMillis());\n+        if (task.getStatus() != null && task.getStatus().isTerminal()) {\n+            task.setEndTime(System.currentTimeMillis());\n+        }\n+\n+        TaskDef taskDef = metadata.getTaskDef(task.getTaskDefName());\n+\n+        if (taskDef != null && taskDef.concurrencyLimit() > 0) {\n+            boolean inProgress = task.getStatus() != null && task.getStatus().equals(Task.Status.IN_PROGRESS);\n+            updateInProgressStatus(connection, task, inProgress);\n+        }\n+\n+        insertOrUpdateTaskData(connection, task);\n+\n+        if (task.getStatus() != null && task.getStatus().isTerminal()) {\n+            removeTaskInProgress(connection, task);\n+        }\n+\n+        indexer.indexTask(task);\n+    }\n+\n+    private Workflow readWorkflow(Connection connection, String workflowId) {\n+        String GET_WORKFLOW = \"SELECT json_data FROM workflow WHERE workflow_id = ?\";\n+\n+        return query(connection, GET_WORKFLOW, q -> q.addParameter(workflowId).executeAndFetchFirst(Workflow.class));\n+    }\n+\n+    private Workflow readWorkflowFromArchive(String workflowId) {\n+        String json = indexer.get(workflowId, RAW_JSON_FIELD);\n+        if (json != null) {\n+            return readValue(json, Workflow.class);\n+        } else {\n+            throw new ApplicationException(ApplicationException.Code.NOT_FOUND,\n+                    \"No such workflow found by id: \" + workflowId);\n+        }\n+    }\n+\n+    private void addWorkflow(Connection connection, Workflow workflow) {\n+        String INSERT_WORKFLOW = \"INSERT INTO workflow (workflow_id, correlation_id, json_data) VALUES (?, ?, ?)\";\n+\n+        execute(connection, INSERT_WORKFLOW, q -> q.addParameter(workflow.getWorkflowId())\n+                .addParameter(workflow.getCorrelationId())\n+                .addJsonParameter(workflow)\n+                .executeUpdate());\n+    }\n+\n+    private void updateWorkflow(Connection connection, Workflow workflow) {\n+        String UPDATE_WORKFLOW =\n+                \"UPDATE workflow SET json_data = ?, modified_on = CURRENT_TIMESTAMP WHERE workflow_id = ?\";\n+\n+        execute(connection, UPDATE_WORKFLOW,\n+                q -> q.addJsonParameter(workflow).addParameter(workflow.getWorkflowId()).executeUpdate());\n+    }\n+\n+    private void removeWorkflow(Connection connection, String workflowId) {\n+        String REMOVE_WORKFLOW = \"DELETE FROM workflow WHERE workflow_id = ?\";\n+        execute(connection, REMOVE_WORKFLOW, q -> q.addParameter(workflowId).executeDelete());\n+    }\n+\n+    private void addPendingWorkflow(Connection connection, String workflowType, String workflowId) {\n+        String EXISTS_PENDING_WORKFLOW =\n+                \"SELECT EXISTS(SELECT 1 FROM workflow_pending WHERE workflow_type = ? AND workflow_id = ?)\";\n+\n+        boolean exist = query(connection, EXISTS_PENDING_WORKFLOW,\n+                q -> q.addParameter(workflowType).addParameter(workflowId).exists());\n+\n+        if (!exist) {\n+            String INSERT_PENDING_WORKFLOW = \"INSERT INTO workflow_pending (workflow_type, workflow_id) VALUES (?, ?)\";\n+\n+            execute(connection, INSERT_PENDING_WORKFLOW,\n+                    q -> q.addParameter(workflowType).addParameter(workflowId).executeUpdate());\n+        }\n+    }\n+\n+    private void removePendingWorkflow(Connection connection, String workflowType, String workflowId) {\n+        String REMOVE_PENDING_WORKFLOW = \"DELETE FROM workflow_pending WHERE workflow_type = ? AND workflow_id = ?\";\n+\n+        execute(connection, REMOVE_PENDING_WORKFLOW,\n+                q -> q.addParameter(workflowType).addParameter(workflowId).executeDelete());\n+    }\n+\n+    private void insertOrUpdateTaskData(Connection connection, Task task) {\n+        String UPDATE_TASK =\n+                \"UPDATE task SET json_data = ?, modified_on = CURRENT_TIMESTAMP WHERE task_id = ?\";\n+\n+        int result = query(connection, UPDATE_TASK,\n+                q -> q.addJsonParameter(task).addParameter(task.getTaskId()).executeUpdate());\n+\n+        if (result == 0) {\n+            String INSERT_TASK = \"INSERT INTO task (task_id, json_data) VALUES (?, ?)\";\n+\n+            execute(connection, INSERT_TASK,\n+                    q -> q.addParameter(task.getTaskId()).addJsonParameter(task).executeUpdate());\n+        }\n+    }\n+\n+    private void removeTaskData(Connection connection, Task task) {\n+        String REMOVE_TASK = \"DELETE FROM task WHERE task_id = ?\";\n+        execute(connection, REMOVE_TASK, q -> q.addParameter(task.getTaskId()).executeDelete());\n+    }\n+\n+    private void addWorkflowToTaskMapping(Connection connection, Task task) {\n+        String EXISTS_WORKFLOW_TO_TASK =\n+                \"SELECT EXISTS(SELECT 1 FROM workflow_to_task WHERE workflow_id = ? AND task_id = ?)\";\n+\n+        boolean exist = query(connection, EXISTS_WORKFLOW_TO_TASK, q -> q.addParameter(task.getWorkflowInstanceId())\n+                .addParameter(task.getTaskId())\n+                .exists());\n+\n+        if (!exist) {\n+            String INSERT_WORKFLOW_TO_TASK = \"INSERT INTO workflow_to_task (workflow_id, task_id) VALUES (?, ?)\";\n+\n+            execute(connection, INSERT_WORKFLOW_TO_TASK,\n+                    q -> q.addParameter(task.getWorkflowInstanceId()).addParameter(task.getTaskId()).executeUpdate());\n+        }\n+    }\n+\n+    private void removeWorkflowToTaskMapping(Connection connection, Task task) {\n+        String REMOVE_WORKFLOW_TO_TASK = \"DELETE FROM workflow_to_task WHERE workflow_id = ? AND task_id = ?\";\n+\n+        execute(connection, REMOVE_WORKFLOW_TO_TASK,\n+                q -> q.addParameter(task.getWorkflowInstanceId()).addParameter(task.getTaskId()).executeDelete());\n+    }\n+\n+    private void addWorkflowDefToWorkflowMapping(Connection connection, Workflow workflow) {\n+        String INSERT_WORKFLOW_DEF_TO_WORKFLOW =\n+                \"INSERT INTO workflow_def_to_workflow (workflow_def, date_str, workflow_id) VALUES (?, ?, ?)\";\n+\n+        execute(connection, INSERT_WORKFLOW_DEF_TO_WORKFLOW, q -> q.addParameter(workflow.getWorkflowType())\n+                .addParameter(dateStr(workflow.getCreateTime()))\n+                .addParameter(workflow.getWorkflowId())\n+                .executeUpdate());\n+    }\n+\n+    private void removeWorkflowDefToWorkflowMapping(Connection connection, Workflow workflow) {\n+        String REMOVE_WORKFLOW_DEF_TO_WORKFLOW =\n+                \"DELETE FROM workflow_def_to_workflow WHERE workflow_def = ? AND date_str = ? AND workflow_id = ?\";\n+\n+        execute(connection, REMOVE_WORKFLOW_DEF_TO_WORKFLOW, q -> q.addParameter(workflow.getWorkflowType())\n+                .addParameter(dateStr(workflow.getCreateTime()))\n+                .addParameter(workflow.getWorkflowId())\n+                .executeUpdate());\n+    }\n+\n+    private boolean addScheduledTask(Connection connection, Task task, String taskKey) {\n+        String EXISTS_SCHEDULED_TASK =\n+                \"SELECT EXISTS(SELECT 1 FROM task_scheduled WHERE workflow_id = ? AND task_key = ?)\";\n+        boolean exist = query(connection, EXISTS_SCHEDULED_TASK,\n+                q -> q.addParameter(task.getWorkflowInstanceId())\n+                        .addParameter(taskKey).exists());\n+\n+        if (!exist) {\n+            String INSERT_SCHEDULED_TASK =\n+                    \"INSERT INTO task_scheduled (workflow_id, task_key, task_id) VALUES (?, ?, ?)\";\n+\n+            execute(connection, INSERT_SCHEDULED_TASK,\n+                    q -> q.addParameter(task.getWorkflowInstanceId()).addParameter(taskKey)\n+                            .addParameter(task.getTaskId()).executeUpdate());\n+\n+            return true;\n+        }\n+\n+        return false;\n+    }\n+\n+    private void removeScheduledTask(Connection connection, Task task, String taskKey) {\n+        String REMOVE_SCHEDULED_TASK = \"DELETE FROM task_scheduled WHERE workflow_id = ? AND task_key = ?\";\n+        execute(connection, REMOVE_SCHEDULED_TASK,\n+                q -> q.addParameter(task.getWorkflowInstanceId()).addParameter(taskKey).executeDelete());\n+    }\n+\n+    private void addTaskInProgress(Connection connection, Task task) {\n+        String EXISTS_IN_PROGRESS_TASK =\n+                \"SELECT EXISTS(SELECT 1 FROM task_in_progress WHERE task_def_name = ? AND task_id = ?)\";\n+\n+        boolean exist = query(connection, EXISTS_IN_PROGRESS_TASK,\n+                q -> q.addParameter(task.getTaskDefName()).addParameter(task.getTaskId()).exists());\n+\n+        if (!exist) {\n+            String INSERT_IN_PROGRESS_TASK =\n+                    \"INSERT INTO task_in_progress (task_def_name, task_id, workflow_id) VALUES (?, ?, ?)\";\n+\n+            execute(connection, INSERT_IN_PROGRESS_TASK,\n+                    q -> q.addParameter(task.getTaskDefName())\n+                            .addParameter(task.getTaskId())\n+                            .addParameter(task.getWorkflowInstanceId())\n+                            .executeUpdate());\n+        }\n+    }\n+\n+    private void removeTaskInProgress(Connection connection, Task task) {\n+        String REMOVE_IN_PROGRESS_TASK =\n+                \"DELETE FROM task_in_progress WHERE task_def_name = ? AND task_id = ?\";\n+\n+        execute(connection, REMOVE_IN_PROGRESS_TASK, q ->\n+                q.addParameter(task.getTaskDefName())\n+                        .addParameter(task.getTaskId())\n+                        .executeUpdate());\n+    }\n+\n+    private void updateInProgressStatus(Connection connection, Task task, boolean inProgress) {\n+        String UPDATE_IN_PROGRESS_TASK_STATUS =\n+                \"UPDATE task_in_progress SET in_progress_status = ?, modified_on = CURRENT_TIMESTAMP \"\n+                        + \"WHERE task_def_name = ? AND task_id = ?\";\n+\n+        execute(connection, UPDATE_IN_PROGRESS_TASK_STATUS, q ->\n+                q.addParameter(inProgress)\n+                        .addParameter(task.getTaskDefName())\n+                        .addParameter(task.getTaskId())\n+                        .executeUpdate());\n+    }\n+\n+    private boolean insertEventExecution(Connection connection, EventExecution eventExecution) {\n+        //@formatter:off\n+        String EXISTS_EVENT_EXECUTION =\n+                \"SELECT EXISTS(SELECT 1 FROM event_execution \"\n+                        + \"WHERE event_handler_name = ? \"\n+                        + \"AND event_name = ? \"\n+                        + \"AND message_id = ? \"\n+                        + \"AND execution_id = ?)\";\n+        //@formatter:on\n+\n+        boolean exist = query(connection, EXISTS_EVENT_EXECUTION, q ->\n+                q.addParameter(eventExecution.getName())\n+                        .addParameter(eventExecution.getEvent())\n+                        .addParameter(eventExecution.getMessageId())\n+                        .addParameter(eventExecution.getId()).exists());\n+\n+        if (!exist) {\n+            String INSERT_EVENT_EXECUTION =\n+                    \"INSERT INTO event_execution (event_handler_name, event_name, message_id, execution_id, json_data) \"\n+                            + \"VALUES (?, ?, ?, ?, ?)\";\n+\n+            execute(connection, INSERT_EVENT_EXECUTION, q ->\n+                    q.addParameter(eventExecution.getName())\n+                            .addParameter(eventExecution.getEvent())\n+                            .addParameter(eventExecution.getMessageId())\n+                            .addParameter(eventExecution.getId())\n+                            .addJsonParameter(eventExecution).executeUpdate());\n+        }\n+        return false;\n+    }\n+\n+    private void updateEventExecution(Connection connection, EventExecution eventExecution) {\n+        //@formatter:off\n+        String UPDATE_EVENT_EXECUTION = \"UPDATE event_execution SET \"\n+                + \"json_data = ?, \"\n+                + \"modified_on = CURRENT_TIMESTAMP \"\n+                + \"WHERE event_handler_name = ? \"\n+                + \"AND execution_event = ? \"\n+                + \"AND message_id = ? \"\n+                + \"AND execution_id = ?\";\n+        //@formatter:on\n+\n+        execute(connection, UPDATE_EVENT_EXECUTION, q ->\n+                q.addJsonParameter(eventExecution)\n+                        .addParameter(eventExecution.getName())\n+                        .addParameter(eventExecution.getEvent())\n+                        .addParameter(eventExecution.getMessageId()).addParameter(eventExecution.getId())\n+                        .executeUpdate());\n+    }\n+\n+    private EventExecution readEventExecution(Connection connection, String eventHandlerName, String eventName,\n+            String messageId, String executionId) {\n+        //@formatter:off\n+        String GET_EVENT_EXECUTION =\n+                \"SELECT json_data FROM event_execution \"\n+                        + \"WHERE event_handler_name = ? \"\n+                        + \"AND event_name = ? \"\n+                        + \"AND message_id = ? \"\n+                        + \"AND execution_id = ?\";\n+        //@formatter:on\n+        return query(connection, GET_EVENT_EXECUTION, q ->\n+                q.addParameter(eventHandlerName)\n+                        .addParameter(eventName)\n+                        .addParameter(messageId)\n+                        .addParameter(executionId)\n+                        .executeAndFetchFirst(EventExecution.class));\n+    }\n+\n+    private void insertOrUpdatePollData(Connection connection, PollData pollData, String domain) {\n+        String UPDATE_POLL_DATA =\n+                \"UPDATE poll_data SET json_data = ?, modified_on = CURRENT_TIMESTAMP \"\n+                        + \"WHERE queue_name = ? AND domain = ?\";\n+\n+        int result = query(connection, UPDATE_POLL_DATA, q ->\n+                q.addJsonParameter(pollData)\n+                        .addParameter(pollData.getQueueName())\n+                        .addParameter(domain)\n+                        .executeUpdate());\n+\n+        if (result == 0) {\n+            String INSERT_POLL_DATA = \"INSERT INTO poll_data (queue_name, domain, json_data) VALUES (?, ?, ?)\";\n+            execute(connection, INSERT_POLL_DATA, q ->\n+                    q.addParameter(pollData.getQueueName())\n+                            .addParameter(domain)\n+                            .addJsonParameter(pollData)\n+                            .executeUpdate());\n+        }\n+    }\n+\n+    private PollData readPollData(Connection connection, String queueName, String domain) {\n+        String GET_POLL_DATA = \"SELECT json_data FROM poll_data WHERE queue_name = ? AND domain = ?\";\n+        return query(connection, GET_POLL_DATA, q ->\n+                q.addParameter(queueName)\n+                        .addParameter(domain)\n+                        .executeAndFetchFirst(PollData.class));\n+    }\n+\n+    private List<PollData> readAllPollData(String queueName) {\n+        String GET_ALL_POLL_DATA = \"SELECT json_data FROM poll_data WHERE queue_name = ?\";\n+        return queryWithTransaction(GET_ALL_POLL_DATA, q -> q.addParameter(queueName).executeAndFetch(PollData.class));\n+    }\n+\n+    private List<String> findAllTasksInProgressInOrderOfArrival(Task task, int limit) {\n+        String GET_IN_PROGRESS_TASKS_WITH_LIMIT =\n+                \"SELECT task_id FROM task_in_progress WHERE task_def_name = ? ORDER BY id LIMIT ?\";\n+\n+        return queryWithTransaction(GET_IN_PROGRESS_TASKS_WITH_LIMIT, q ->\n+                q.addParameter(task.getTaskDefName())\n+                        .addParameter(limit)\n+                        .executeScalarList(String.class));\n+    }\n+\n+    private void validate(Task task) {\n+        Preconditions.checkNotNull(task, \"task object cannot be null\");\n+        Preconditions.checkNotNull(task.getTaskId(), \"Task id cannot be null\");\n+        Preconditions.checkNotNull(task.getWorkflowInstanceId(), \"Workflow instance id cannot be null\");\n+        Preconditions.checkNotNull(task.getReferenceTaskName(), \"Task reference name cannot be null\");\n+    }\n }",
      "parent_sha": "2e0c8c9949845785e3114a734b7fb680c67d7ade"
    }
  },
  {
    "oid": "dba2b9cc755c863671c5a55c0a8a8594993bc72b",
    "message": "close connections, log the error before propagting it up",
    "date": "2019-08-29T07:56:50Z",
    "url": "https://github.com/conductor-oss/conductor/commit/dba2b9cc755c863671c5a55c0a8a8594993bc72b",
    "details": {
      "sha": "85a3fdc92290efb16ff1d5d8d69dcc523cb01c0a",
      "filename": "mysql-persistence/src/main/java/com/netflix/conductor/mysql/MySQLDataSourceProvider.java",
      "status": "modified",
      "additions": 12,
      "deletions": 4,
      "changes": 16,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/dba2b9cc755c863671c5a55c0a8a8594993bc72b/mysql-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmysql%2FMySQLDataSourceProvider.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/dba2b9cc755c863671c5a55c0a8a8594993bc72b/mysql-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmysql%2FMySQLDataSourceProvider.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/mysql-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmysql%2FMySQLDataSourceProvider.java?ref=dba2b9cc755c863671c5a55c0a8a8594993bc72b",
      "patch": "@@ -24,10 +24,18 @@ public MySQLDataSourceProvider(MySQLConfiguration configuration) {\n \n     @Override\n     public DataSource get() {\n-        HikariDataSource dataSource = new HikariDataSource(createConfiguration());\n-        flywayMigrate(dataSource);\n-\n-        return dataSource;\n+        HikariDataSource dataSource = null;\n+        try {\n+            dataSource = new HikariDataSource(createConfiguration());\n+            flywayMigrate(dataSource);\n+            return dataSource;\n+        } catch (final Throwable t) {\n+            if(null != dataSource && !dataSource.isClosed()){\n+                dataSource.close();\n+            }\n+            logger.error(\"error migration DB\", t);\n+            throw t;\n+        }\n     }\n \n     private HikariConfig createConfiguration(){",
      "parent_sha": "2e44b5ae8a4e3916713c246b7c94cd670a8ca743"
    }
  },
  {
    "oid": "4e993d354c16a8e375a3150af038b8b73b84cbd6",
    "message": "issue-3719: Added disk based approach to prevent oom",
    "date": "2023-09-02T08:14:08Z",
    "url": "https://github.com/conductor-oss/conductor/commit/4e993d354c16a8e375a3150af038b8b73b84cbd6",
    "details": {
      "sha": "a04a43710b1cb8de4c155d1cbf926205ad04a029",
      "filename": "core/src/main/java/com/netflix/conductor/core/storage/DummyPayloadStorage.java",
      "status": "modified",
      "additions": 51,
      "deletions": 21,
      "changes": 72,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/4e993d354c16a8e375a3150af038b8b73b84cbd6/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fstorage%2FDummyPayloadStorage.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/4e993d354c16a8e375a3150af038b8b73b84cbd6/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fstorage%2FDummyPayloadStorage.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fstorage%2FDummyPayloadStorage.java?ref=4e993d354c16a8e375a3150af038b8b73b84cbd6",
      "patch": "@@ -12,17 +12,23 @@\n  */\n package com.netflix.conductor.core.storage;\n \n-import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n import java.io.InputStream;\n-import java.util.Map;\n-import java.util.concurrent.ConcurrentHashMap;\n+import java.nio.file.Files;\n+import java.util.UUID;\n \n+import org.apache.commons.io.IOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import com.netflix.conductor.common.run.ExternalStorageLocation;\n import com.netflix.conductor.common.utils.ExternalPayloadStorage;\n \n+import com.fasterxml.jackson.databind.ObjectMapper;\n+\n /**\n  * A dummy implementation of {@link ExternalPayloadStorage} used when no external payload is\n  * configured\n@@ -31,40 +37,64 @@ public class DummyPayloadStorage implements ExternalPayloadStorage {\n \n     private static final Logger LOGGER = LoggerFactory.getLogger(DummyPayloadStorage.class);\n \n-    private final Map<String, byte[]> dummyDataStore = new ConcurrentHashMap<>();\n+    private ObjectMapper objectMapper;\n+    private File payloadDir;\n \n-    private static final String DUMMY_DATA_STORE_KEY = \"DUMMY_PAYLOAD_STORE_KEY\";\n+    public DummyPayloadStorage() {\n+        try {\n+            this.objectMapper = new ObjectMapper();\n+            this.payloadDir = Files.createTempDirectory(\"payloads\").toFile();\n+            LOGGER.info(\n+                    \"{} initialized in directory: {}\",\n+                    this.getClass().getSimpleName(),\n+                    payloadDir.getAbsolutePath());\n+        } catch (IOException ioException) {\n+            LOGGER.error(\n+                    \"Exception encountered while creating payloads directory : {}\",\n+                    ioException.getMessage());\n+        }\n+    }\n \n     @Override\n     public ExternalStorageLocation getLocation(\n             Operation operation, PayloadType payloadType, String path) {\n-        ExternalStorageLocation externalStorageLocation = new ExternalStorageLocation();\n-        externalStorageLocation.setPath(path != null ? path : \"\");\n-        return externalStorageLocation;\n+        ExternalStorageLocation location = new ExternalStorageLocation();\n+        location.setPath(path + UUID.randomUUID() + \".json\");\n+        return location;\n     }\n \n     @Override\n     public void upload(String path, InputStream payload, long payloadSize) {\n+        File file = new File(payloadDir, path);\n+        String filePath = file.getAbsolutePath();\n         try {\n-            final byte[] payloadBytes = new byte[(int) payloadSize];\n-            final int bytesRead = payload.read(new byte[(int) payloadSize]);\n-\n-            if (bytesRead > 0) {\n-                dummyDataStore.put(\n-                        path == null || path.isEmpty() ? DUMMY_DATA_STORE_KEY : path, payloadBytes);\n+            if (!file.exists() && file.createNewFile()) {\n+                LOGGER.debug(\"Created file: {}\", filePath);\n+            }\n+            IOUtils.copy(payload, new FileOutputStream(file));\n+            LOGGER.debug(\"Written to {}\", filePath);\n+        } catch (IOException e) {\n+            // just handle this exception here and return empty map so that test will fail in case\n+            // this exception is thrown\n+            LOGGER.error(\"Error writing to {}\", filePath);\n+        } finally {\n+            try {\n+                if (payload != null) {\n+                    payload.close();\n+                }\n+            } catch (IOException e) {\n+                LOGGER.warn(\"Unable to close input stream when writing to file\");\n             }\n-        } catch (Exception e) {\n-            LOGGER.error(\"Error encountered while uploading payload {}\", e.getMessage());\n         }\n     }\n \n     @Override\n     public InputStream download(String path) {\n-        final byte[] data =\n-                dummyDataStore.get(path == null || path.isEmpty() ? DUMMY_DATA_STORE_KEY : path);\n-        if (data != null) {\n-            return new ByteArrayInputStream(data);\n-        } else {\n+        try {\n+            LOGGER.debug(\"Reading from {}\", path);\n+            return new FileInputStream(new File(payloadDir, path));\n+        } catch (IOException e) {\n+            LOGGER.error(\"Error reading {}\", path, e);\n             return null;\n         }\n     }",
      "parent_sha": "46dd3df90320912eddcae050738bc19a8ed9702e"
    }
  },
  {
    "oid": "6022b53eece36c76bafaa26b2da927ba34e8d50a",
    "message": "condition fix.",
    "date": "2020-02-21T19:20:33Z",
    "url": "https://github.com/conductor-oss/conductor/commit/6022b53eece36c76bafaa26b2da927ba34e8d50a",
    "details": {
      "sha": "ef9732cd42d35843d4f860da692847de7c58dd77",
      "filename": "test-harness/src/test/java/com/netflix/conductor/tests/integration/AbstractWorkflowServiceTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/6022b53eece36c76bafaa26b2da927ba34e8d50a/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FAbstractWorkflowServiceTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/6022b53eece36c76bafaa26b2da927ba34e8d50a/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FAbstractWorkflowServiceTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FAbstractWorkflowServiceTest.java?ref=6022b53eece36c76bafaa26b2da927ba34e8d50a",
      "patch": "@@ -1849,7 +1849,7 @@ private void createDoWhileWorkflowWithIteration(int iteration, boolean isInputPa\n \n         workflowDef.getTasks().add(loopTask);\n \n-        if (iteration ==2) {\n+        if (iteration == 2 && isInputParameter == false) {\n             TaskDef taskDef2 = new TaskDef();\n             taskDef2.setName(\"loopTask2\");\n             taskDef2.setTimeoutSeconds(200);",
      "parent_sha": "02515f5eb91eeaf527cd39438d12efee244d1b1d"
    }
  },
  {
    "oid": "4a2002e5c3e68ff3cdc009d915163877e25a128a",
    "message": "checkin for perf.",
    "date": "2019-05-30T05:42:03Z",
    "url": "https://github.com/conductor-oss/conductor/commit/4a2002e5c3e68ff3cdc009d915163877e25a128a",
    "details": {
      "sha": "6c8f3a435047c2e7ebff61c256813bd90addf6a2",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/tasks/DoWhile.java",
      "status": "modified",
      "additions": 4,
      "deletions": 2,
      "changes": 6,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/4a2002e5c3e68ff3cdc009d915163877e25a128a/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhile.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/4a2002e5c3e68ff3cdc009d915163877e25a128a/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhile.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhile.java?ref=4a2002e5c3e68ff3cdc009d915163877e25a128a",
      "patch": "@@ -18,6 +18,7 @@\n  */\n package com.netflix.conductor.core.execution.tasks;\n \n+import com.google.common.annotations.VisibleForTesting;\n import com.netflix.conductor.common.metadata.tasks.Task;\n import com.netflix.conductor.common.metadata.tasks.Task.Status;\n import com.netflix.conductor.common.metadata.workflow.WorkflowTask;\n@@ -128,6 +129,7 @@ public boolean execute(Workflow workflow, Task task, WorkflowExecutor provider)\n \n \t}\n \n+\t@VisibleForTesting\n \tboolean getEvaluatedCondition(WorkflowTask taskToSchedule, Map<String, Object> taskInput) {\n \t\tString condition = taskToSchedule.getLoopCondition();\n \t\tboolean caseValue = false;\n@@ -141,8 +143,8 @@ boolean getEvaluatedCondition(WorkflowTask taskToSchedule, Map<String, Object> t\n \t\t\t\tthrow new RuntimeException(\"Error while evaluating the script \" + condition, e);\n \t\t\t}\n \t\t}\n-\n-\t\treturn caseValue;\n+\t\tint rand = (int)(Math.random() * 10);\n+\t\treturn rand %2 == 00 ;\n \t}\n \n }",
      "parent_sha": "5c7bb5ea498b411f494dac41cfd10b208d5fa835"
    }
  },
  {
    "oid": "131d3466e2f93169c9926a243fed3a12f996b683",
    "message": "fix zookeeper lock can't release (#1758) (#1791)",
    "date": "2020-07-20T23:18:49Z",
    "url": "https://github.com/conductor-oss/conductor/commit/131d3466e2f93169c9926a243fed3a12f996b683",
    "details": {
      "sha": "339499365808125fc1d57e67a71f2b836655d631",
      "filename": "zookeeper-lock/src/main/java/com/netflix/conductor/zookeeper/config/ZookeeperModule.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/131d3466e2f93169c9926a243fed3a12f996b683/zookeeper-lock%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fzookeeper%2Fconfig%2FZookeeperModule.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/131d3466e2f93169c9926a243fed3a12f996b683/zookeeper-lock%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fzookeeper%2Fconfig%2FZookeeperModule.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/zookeeper-lock%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fzookeeper%2Fconfig%2FZookeeperModule.java?ref=131d3466e2f93169c9926a243fed3a12f996b683",
      "patch": "@@ -18,8 +18,8 @@\n \n import com.google.inject.AbstractModule;\n import com.google.inject.Provides;\n+import com.google.inject.Singleton;\n import com.netflix.conductor.core.utils.Lock;\n-import com.netflix.conductor.service.ExecutionLockService;\n import com.netflix.conductor.zookeeper.ZookeeperLock;\n \n public class ZookeeperModule extends AbstractModule {\n@@ -30,6 +30,7 @@ protected void configure() {\n     }\n \n     @Provides\n+    @Singleton\n     protected Lock provideLock(ZookeeperConfiguration config) {\n         return new ZookeeperLock(config);\n     }",
      "parent_sha": "83db8e2b60fbc5f528739efdc478316b4cd3b3d2"
    }
  },
  {
    "oid": "419ca79504362f8890fe9d4db7dc23ec965a9427",
    "message": "no changes made, but want start the build on the pull request",
    "date": "2018-09-23T04:24:45Z",
    "url": "https://github.com/conductor-oss/conductor/commit/419ca79504362f8890fe9d4db7dc23ec965a9427",
    "details": {
      "sha": "71f617499f6618940dbd53d8d144a754f6d1c577",
      "filename": "test-harness/src/test/java/com/netflix/conductor/tests/integration/End2EndTests.java",
      "status": "modified",
      "additions": 14,
      "deletions": 13,
      "changes": 27,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/419ca79504362f8890fe9d4db7dc23ec965a9427/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FEnd2EndTests.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/419ca79504362f8890fe9d4db7dc23ec965a9427/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FEnd2EndTests.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FEnd2EndTests.java?ref=419ca79504362f8890fe9d4db7dc23ec965a9427",
      "patch": "@@ -18,6 +18,19 @@\n  */\n package com.netflix.conductor.tests.integration;\n \n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n import com.netflix.conductor.client.exceptions.ConductorClientException;\n import com.netflix.conductor.client.http.MetadataClient;\n import com.netflix.conductor.client.http.TaskClient;\n@@ -37,18 +50,6 @@\n import com.netflix.conductor.common.run.WorkflowSummary;\n import com.netflix.conductor.server.ConductorConfig;\n import com.netflix.conductor.server.ConductorServer;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n-\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.LinkedList;\n-import java.util.List;\n-\n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertNotNull;\n-import static org.junit.Assert.assertTrue;\n \n \n /**\n@@ -57,7 +58,7 @@\n  */\n public class End2EndTests {\n \n-\tstatic {\n+   static {\n \t\tSystem.setProperty(\"EC2_REGION\", \"us-east-1\");\n \t\tSystem.setProperty(\"EC2_AVAILABILITY_ZONE\", \"us-east-1c\");\n \t\tSystem.setProperty(\"workflow.elasticsearch.url\", \"localhost:9300\");",
      "parent_sha": "bb52dee55cb915f71f3a3adb89e0b68cffebe1f4"
    }
  },
  {
    "oid": "caaf639c69f7f90282a656e32ad9865784bfb474",
    "message": "fix workflow status in listener",
    "date": "2019-02-11T23:40:00Z",
    "url": "https://github.com/conductor-oss/conductor/commit/caaf639c69f7f90282a656e32ad9865784bfb474",
    "details": {
      "sha": "786a6888143ea6d18fbea7146b44caeeb4910e87",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/WorkflowExecutor.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/caaf639c69f7f90282a656e32ad9865784bfb474/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/caaf639c69f7f90282a656e32ad9865784bfb474/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java?ref=caaf639c69f7f90282a656e32ad9865784bfb474",
      "patch": "@@ -538,8 +538,8 @@ void completeWorkflow(Workflow wf) {\n         Monitors.recordWorkflowCompletion(workflow.getWorkflowName(), workflow.getEndTime() - workflow.getStartTime(), wf.getOwnerApp());\n         queueDAO.remove(DECIDER_QUEUE, workflow.getWorkflowId());    //remove from the sweep queue\n \n-        if (wf.getWorkflowDefinition().isWorkflowStatusListenerEnabled()) {\n-            workflowStatusListener.onWorkflowCompleted(wf);\n+        if (workflow.getWorkflowDefinition().isWorkflowStatusListenerEnabled()) {\n+            workflowStatusListener.onWorkflowCompleted(workflow);\n         }\n     }\n ",
      "parent_sha": "d7fbb3053bd2110a7f212f6ad3c30239a4c86f9a"
    }
  },
  {
    "oid": "0031d2128d46769ccc90770c613774588ce81cb9",
    "message": "javadoc",
    "date": "2019-03-07T14:01:52Z",
    "url": "https://github.com/conductor-oss/conductor/commit/0031d2128d46769ccc90770c613774588ce81cb9",
    "details": {
      "sha": "598ffa04b170d2e94ddf0503d21ef4829b19b09b",
      "filename": "contribs/src/main/java/com/netflix/conductor/contribs/listener/DynoQueueStatusPublisher.java",
      "status": "modified",
      "additions": 3,
      "deletions": 4,
      "changes": 7,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/0031d2128d46769ccc90770c613774588ce81cb9/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Flistener%2FDynoQueueStatusPublisher.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/0031d2128d46769ccc90770c613774588ce81cb9/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Flistener%2FDynoQueueStatusPublisher.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Flistener%2FDynoQueueStatusPublisher.java?ref=0031d2128d46769ccc90770c613774588ce81cb9",
      "patch": "@@ -32,7 +32,9 @@\n import javax.inject.Inject;\n import java.util.Collections;\n \n-\n+/**\n+ * Publishes a @see Message containing a @see WorkflowSummary to a DynoQueue on a workflow completion or termination event.\n+ */\n public class DynoQueueStatusPublisher implements WorkflowStatusListener {\n \n     private static final Logger LOG = LoggerFactory.getLogger(DynoQueueStatusPublisher.class);\n@@ -42,9 +44,6 @@ public class DynoQueueStatusPublisher implements WorkflowStatusListener {\n     private final String successStatusQueue;\n     private final String failureStatusQueue;\n \n-    /**\n-     * Publishes a message containing a WorkflowSummary on a configured DynoQueue on workflow completion or termination.\n-     */\n     @Inject\n     public DynoQueueStatusPublisher(QueueDAO queueDAO, ObjectMapper objectMapper, Configuration config) {\n         this.queueDAO = queueDAO;",
      "parent_sha": "5da74e7eb8252fb61a721889ff258dba3efcadae"
    }
  },
  {
    "oid": "8ea34fc16defd6a25cb5d4f3126bfd1fd800f981",
    "message": "Use I/O scheduler instead of computation scheduler",
    "date": "2020-07-08T00:18:19Z",
    "url": "https://github.com/conductor-oss/conductor/commit/8ea34fc16defd6a25cb5d4f3126bfd1fd800f981",
    "details": {
      "sha": "d1465610867e12058b035af3077e7597f7f6be39",
      "filename": "core/src/main/java/com/netflix/conductor/core/events/queue/dyno/DynoObservableQueue.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/8ea34fc16defd6a25cb5d4f3126bfd1fd800f981/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fevents%2Fqueue%2Fdyno%2FDynoObservableQueue.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/8ea34fc16defd6a25cb5d4f3126bfd1fd800f981/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fevents%2Fqueue%2Fdyno%2FDynoObservableQueue.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fevents%2Fqueue%2Fdyno%2FDynoObservableQueue.java?ref=8ea34fc16defd6a25cb5d4f3126bfd1fd800f981",
      "patch": "@@ -28,6 +28,8 @@\n import org.slf4j.LoggerFactory;\n import rx.Observable;\n import rx.Observable.OnSubscribe;\n+import rx.Scheduler;\n+import rx.schedulers.Schedulers;\n \n import javax.inject.Inject;\n import javax.inject.Singleton;\n@@ -121,7 +123,7 @@ private List<Message> receiveMessages() {\n     @VisibleForTesting\n     private OnSubscribe<Message> getOnSubscribe() {\n         return subscriber -> {\n-            Observable<Long> interval = Observable.interval(pollTimeInMS, TimeUnit.MILLISECONDS);\n+            Observable<Long> interval = Observable.interval(pollTimeInMS, TimeUnit.MILLISECONDS, Schedulers.io());\n             interval.flatMap((Long x) -> {\n                 List<Message> msgs = receiveMessages();\n                 return Observable.from(msgs);",
      "parent_sha": "a14d7dbcd14f62a4264e9d22442bcdb458983121"
    }
  },
  {
    "oid": "3c6dd71a60fd40561c166242d49122b8bdda1321",
    "message": "update the test with correct workflow name",
    "date": "2017-03-21T18:54:14Z",
    "url": "https://github.com/conductor-oss/conductor/commit/3c6dd71a60fd40561c166242d49122b8bdda1321",
    "details": {
      "sha": "d76a4420c3e49c5749654790c7c13f01c18200c9",
      "filename": "test-harness/src/test/java/com/netflix/conductor/tests/integration/WorkflowServiceTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/3c6dd71a60fd40561c166242d49122b8bdda1321/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FWorkflowServiceTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/3c6dd71a60fd40561c166242d49122b8bdda1321/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FWorkflowServiceTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FWorkflowServiceTest.java?ref=3c6dd71a60fd40561c166242d49122b8bdda1321",
      "patch": "@@ -1142,7 +1142,7 @@ public void testLongRunning() throws Exception {\n \t\tString inputParam1 = \"p1 value\";\n \t\tinput.put(\"param1\", inputParam1);\n \t\tinput.put(\"param2\", \"p2 value\");\n-\t\tString wfid = provider.startWorkflow(LINEAR_WORKFLOW_T1_T2, 1, correlationId , input);\n+\t\tString wfid = provider.startWorkflow(LONG_RUNNING, 1, correlationId , input);\n \t\tSystem.out.println(\"testLongRunning.wfid=\" + wfid);\n \t\tassertNotNull(wfid);\n \t\t",
      "parent_sha": "fc5326b8f250d6b116e59e55b59cfd8ef5488079"
    }
  },
  {
    "oid": "319e2ba3a4a77b51f0e1ab93ad6182653dd932cd",
    "message": "fix the json formatting",
    "date": "2017-02-09T23:09:01Z",
    "url": "https://github.com/conductor-oss/conductor/commit/319e2ba3a4a77b51f0e1ab93ad6182653dd932cd",
    "details": {
      "sha": "be3374b16f7c2ddc5781229e9e8281825a3c38d8",
      "filename": "jersey/src/main/java/com/netflix/conductor/server/resources/TaskResource.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/319e2ba3a4a77b51f0e1ab93ad6182653dd932cd/jersey%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2Fresources%2FTaskResource.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/319e2ba3a4a77b51f0e1ab93ad6182653dd932cd/jersey%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2Fresources%2FTaskResource.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/jersey%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2Fresources%2FTaskResource.java?ref=319e2ba3a4a77b51f0e1ab93ad6182653dd932cd",
      "patch": "@@ -122,7 +122,7 @@ public String updateTask(TaskResult task) throws Exception {\n \t@ApiOperation(\"Ack Task is recieved\")\n \t@Consumes({ MediaType.WILDCARD })\n \tpublic String ack(@PathParam(\"taskId\") String taskId, @QueryParam(\"workerid\") String workerId) throws Exception {\n-\t\treturn \"\\\"\" + taskService.ackTaskRecieved(taskId, workerId) + \"\\\"\";\n+\t\treturn \"\" + taskService.ackTaskRecieved(taskId, workerId);\n \t}\n \n \t@GET",
      "parent_sha": "80f7b881e80eeef57f0a00fc8b4f5a9d03e4e552"
    }
  },
  {
    "oid": "4a51a487ec0c5a91b0e9ef22d90e1f13b461a46e",
    "message": "Fix import EmbeddedElasticSearch",
    "date": "2017-06-27T02:26:55Z",
    "url": "https://github.com/conductor-oss/conductor/commit/4a51a487ec0c5a91b0e9ef22d90e1f13b461a46e",
    "details": {
      "sha": "20df3c6306f83be87f4bf31f3049afbb37aa1a22",
      "filename": "server/src/main/java/com/netflix/conductor/server/ConductorServer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/4a51a487ec0c5a91b0e9ef22d90e1f13b461a46e/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorServer.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/4a51a487ec0c5a91b0e9ef22d90e1f13b461a46e/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorServer.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorServer.java?ref=4a51a487ec0c5a91b0e9ef22d90e1f13b461a46e",
      "patch": "@@ -41,7 +41,7 @@\n import com.google.inject.servlet.GuiceFilter;\n import com.netflix.conductor.common.metadata.tasks.TaskDef;\n import com.netflix.conductor.redis.utils.JedisMock;\n-import com.netflix.conductor.server.es.EmbeddedElasticSearch;\n+import com.netflix.conductor.dao.es.EmbeddedElasticSearch;\n import com.netflix.dyno.connectionpool.Host;\n import com.netflix.dyno.connectionpool.Host.Status;\n import com.netflix.dyno.connectionpool.HostSupplier;",
      "parent_sha": "606c498899334686a8528795718a457a08e82f9d"
    }
  },
  {
    "oid": "0be0e8011912f65d7daf99bb42f44a191d752b67",
    "message": "Initialize workflow monitor (#1676)\n\n* Initialize workflow monitor\r\n\r\n* Moved workflow monitor binding to Server module",
    "date": "2020-05-12T01:00:59Z",
    "url": "https://github.com/conductor-oss/conductor/commit/0be0e8011912f65d7daf99bb42f44a191d752b67",
    "details": {
      "sha": "747b57becb781523c96ff1c47b7d23c8573ca0c3",
      "filename": "server/src/main/java/com/netflix/conductor/server/ServerModule.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/0be0e8011912f65d7daf99bb42f44a191d752b67/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FServerModule.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/0be0e8011912f65d7daf99bb42f44a191d752b67/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FServerModule.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FServerModule.java?ref=0be0e8011912f65d7daf99bb42f44a191d752b67",
      "patch": "@@ -12,13 +12,11 @@\n  */\n package com.netflix.conductor.server;\n \n-import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.inject.AbstractModule;\n import com.google.inject.Scopes;\n import com.google.inject.matcher.Matchers;\n import com.netflix.archaius.guice.ArchaiusModule;\n import com.netflix.conductor.annotations.Service;\n-import com.netflix.conductor.common.utils.JsonMapperProvider;\n import com.netflix.conductor.core.config.Configuration;\n import com.netflix.conductor.core.config.CoreModule;\n import com.netflix.conductor.core.config.ValidationModule;\n@@ -27,6 +25,7 @@\n import com.netflix.conductor.grpc.server.GRPCModule;\n import com.netflix.conductor.interceptors.ServiceInterceptor;\n import com.netflix.conductor.jetty.server.JettyModule;\n+import com.netflix.conductor.service.WorkflowMonitor;\n import com.netflix.runtime.health.guice.HealthModule;\n \n import javax.validation.Validator;\n@@ -50,5 +49,6 @@ protected void configure() {\n         bind(Configuration.class).to(SystemPropertiesDynomiteConfiguration.class);\n         bind(ExecutorService.class).toProvider(ExecutorServiceProvider.class).in(Scopes.SINGLETON);\n         bind(WorkflowSweeper.class).asEagerSingleton();\n+        bind(WorkflowMonitor.class).asEagerSingleton();\n     }\n }",
      "parent_sha": "1382644cca7ff5bb44a3c217aa8dceb201878760"
    }
  },
  {
    "oid": "1d3330fb1407a0fd2c920e33b2aebade9dba728b",
    "message": "handle null",
    "date": "2017-04-05T19:37:53Z",
    "url": "https://github.com/conductor-oss/conductor/commit/1d3330fb1407a0fd2c920e33b2aebade9dba728b",
    "details": {
      "sha": "4ecad00eb2d4ea1917b7b7528b473b41309c116b",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/ParametersUtils.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/1d3330fb1407a0fd2c920e33b2aebade9dba728b/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FParametersUtils.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/1d3330fb1407a0fd2c920e33b2aebade9dba728b/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FParametersUtils.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FParametersUtils.java?ref=1d3330fb1407a0fd2c920e33b2aebade9dba728b",
      "patch": "@@ -81,7 +81,9 @@ public Map<String, Object> getTaskInputV2(Map<String, Object> input, Workflow wo\n \t\t\ttaskIO.put(\"input\", task.getInputData());\n \t\t\ttaskIO.put(\"output\", task.getOutputData());\n \t\t\ttaskIO.put(\"taskType\", task.getTaskType());\n-\t\t\ttaskIO.put(\"status\", task.getStatus().toString());\t\t\t\n+\t\t\tif(task.getStatus() != null) {\n+\t\t\t\ttaskIO.put(\"status\", task.getStatus().toString());\n+\t\t\t}\n \t\t\ttaskIO.put(\"referenceTaskName\", task.getReferenceTaskName());\n \t\t\ttaskIO.put(\"retryCount\", task.getRetryCount());\n \t\t\ttaskIO.put(\"correlationId\", task.getCorrelationId());",
      "parent_sha": "dcbca3e38ea1e3598aa1bdb9d10a49a2a154dc8f"
    }
  },
  {
    "oid": "4cea47f7c4753e0ed4b33b8fdb5fb8e72c4ed4a0",
    "message": "fix: Fix error with headers in webhook",
    "date": "2024-04-15T11:48:14Z",
    "url": "https://github.com/conductor-oss/conductor/commit/4cea47f7c4753e0ed4b33b8fdb5fb8e72c4ed4a0",
    "details": {
      "sha": "37401be1969f0feacdc9d06bb43069b30e62c03a",
      "filename": "task-status-listener/src/main/java/com/netflix/conductor/contribs/listener/RestClientManager.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/4cea47f7c4753e0ed4b33b8fdb5fb8e72c4ed4a0/task-status-listener%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Flistener%2FRestClientManager.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/4cea47f7c4753e0ed4b33b8fdb5fb8e72c4ed4a0/task-status-listener%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Flistener%2FRestClientManager.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/task-status-listener%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Flistener%2FRestClientManager.java?ref=4cea47f7c4753e0ed4b33b8fdb5fb8e72c4ed4a0",
      "patch": "@@ -188,7 +188,8 @@ public void postNotification(\n         String url = prepareUrl(notifType, statusNotifier);\n \n         Map<String, String> headers = new HashMap<>();\n-        headers.put(config.getHeaderPrefer(), config.getHeaderPreferValue());\n+        if(config.getHeaderPrefer() != \"\" && config.getHeaderPreferValue() != \"\")\n+            headers.put(config.getHeaderPrefer(), config.getHeaderPreferValue());\n \n         HttpPost request = createPostRequest(url, data, headers);\n         long start = System.currentTimeMillis();",
      "parent_sha": "55268f0633969379874cc425ef32c9048bbddbfa"
    }
  },
  {
    "oid": "7ff9ad0e8e903a66af21efd621d1dae907dc094a",
    "message": "chore(workflow): improved validation and error message",
    "date": "2018-07-25T20:50:08Z",
    "url": "https://github.com/conductor-oss/conductor/commit/7ff9ad0e8e903a66af21efd621d1dae907dc094a",
    "details": {
      "sha": "4597ce30c7bea5e74be2782ec7e6168b811eabb0",
      "filename": "redis-persistence/src/main/java/com/netflix/conductor/dao/dynomite/RedisMetadataDAO.java",
      "status": "modified",
      "additions": 3,
      "deletions": 2,
      "changes": 5,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/7ff9ad0e8e903a66af21efd621d1dae907dc094a/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fdynomite%2FRedisMetadataDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/7ff9ad0e8e903a66af21efd621d1dae907dc094a/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fdynomite%2FRedisMetadataDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fdynomite%2FRedisMetadataDAO.java?ref=7ff9ad0e8e903a66af21efd621d1dae907dc094a",
      "patch": "@@ -39,6 +39,7 @@\n import com.netflix.conductor.core.execution.ApplicationException.Code;\n import com.netflix.conductor.dao.MetadataDAO;\n import com.netflix.conductor.metrics.Monitors;\n+import org.apache.commons.lang.StringUtils;\n \n @Singleton\n @Trace\n@@ -219,8 +220,8 @@ public WorkflowDef get(String name, int version) {\n \n     @Override\n     public void removeWorkflowDef(String name, int version) {\n-        Preconditions.checkNotNull(name, \"WorkflowDef name cannot be null\");\n-        Preconditions.checkNotNull(version, \"Version cannot be null\");\n+        Preconditions.checkArgument(StringUtils.isNotBlank(name), \"WorkflowDef name cannot be null\");\n+        Preconditions.checkArgument(version > 0, \"Input version is not valid\");\n         Long result = dynoClient.hdel(nsKey(WORKFLOW_DEF, name), String.valueOf(version));\n         if (!result.equals(1L)) {\n             throw new ApplicationException(Code.NOT_FOUND, String.format(\"Cannot remove the workflow - no such workflow\" +",
      "parent_sha": "1904de400889c832292de0c3f8d8d623d21d1183"
    }
  },
  {
    "oid": "9b915e349e145bba6399793bd65447b53f92d869",
    "message": "Introduce properties that control event processing and event queues.",
    "date": "2020-12-17T17:56:06Z",
    "url": "https://github.com/conductor-oss/conductor/commit/9b915e349e145bba6399793bd65447b53f92d869",
    "details": {
      "sha": "ce3c31764ec6c9e832b74f27e2c07377593f36e1",
      "filename": "core/src/main/java/com/netflix/conductor/core/config/EventConfiguration.java",
      "status": "modified",
      "additions": 22,
      "deletions": 8,
      "changes": 30,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/9b915e349e145bba6399793bd65447b53f92d869/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fconfig%2FEventConfiguration.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/9b915e349e145bba6399793bd65447b53f92d869/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fconfig%2FEventConfiguration.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fconfig%2FEventConfiguration.java?ref=9b915e349e145bba6399793bd65447b53f92d869",
      "patch": "@@ -27,6 +27,7 @@\n import com.netflix.conductor.dao.QueueDAO;\n import com.netflix.conductor.service.ExecutionService;\n import com.netflix.conductor.service.MetadataService;\n+import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\n import org.springframework.context.annotation.Bean;\n import org.springframework.context.annotation.Configuration;\n import rx.Scheduler;\n@@ -36,37 +37,50 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.ThreadFactory;\n \n+/**\n+ * Default event processing configuration that is enabled by default.\n+ *\n+ * <p>Set <code>workflow.event.processing.enabled=false</code> to <i>disable</i> event processing.</p>\n+ */\n @Configuration(proxyBeanMethods = false)\n+@ConditionalOnProperty(prefix = \"workflow\", name = \"event.processing.enabled\", havingValue = \"true\", matchIfMissing = true)\n public class EventConfiguration {\n \n     @Bean\n     public ActionProcessor actionProcessor(WorkflowExecutor workflowExecutor, ParametersUtils parametersUtils,\n-        JsonUtils jsonUtils) {\n+                                           JsonUtils jsonUtils) {\n         return new SimpleActionProcessor(workflowExecutor, parametersUtils, jsonUtils);\n     }\n \n     @Bean\n     public EventProcessor eventProcessor(ExecutionService executionService, MetadataService metadataService,\n-        ActionProcessor actionProcessor, EventQueues eventQueues, JsonUtils jsonUtils, ConductorProperties properties,\n-        ObjectMapper objectMapper) {\n+                                         ActionProcessor actionProcessor, EventQueues eventQueues, JsonUtils jsonUtils, ConductorProperties properties,\n+                                         ObjectMapper objectMapper) {\n         return new SimpleEventProcessor(executionService, metadataService, actionProcessor, eventQueues, jsonUtils,\n-            properties, objectMapper);\n+                properties, objectMapper);\n     }\n \n     @Bean\n     public Scheduler scheduler(ConductorProperties properties) {\n         ThreadFactory threadFactory = new ThreadFactoryBuilder()\n-            .setNameFormat(\"event-queue-poll-scheduler-thread-%d\")\n-            .build();\n+                .setNameFormat(\"event-queue-poll-scheduler-thread-%d\")\n+                .build();\n         Executor executorService = Executors\n-            .newFixedThreadPool(properties.getEventSchedulerPollThreadCount(), threadFactory);\n+                .newFixedThreadPool(properties.getEventSchedulerPollThreadCount(), threadFactory);\n \n         return Schedulers.from(executorService);\n     }\n \n+    /**\n+     * Default provider for {@link com.netflix.conductor.core.events.queue.ObservableQueue}\n+     * that listens on the <i>conductor</i> queue prefix.\n+     *\n+     * <p><code>Set workflow.default.event.queue.enabled=false</code> to disable the default queue.</p>\n+     */\n     @Bean\n+    @ConditionalOnProperty(prefix = \"workflow\", name = \"default.event.queue.enabled\", havingValue = \"true\", matchIfMissing = true)\n     public EventQueueProvider conductorEventQueueProvider(QueueDAO queueDAO, ConductorProperties properties,\n-        Scheduler scheduler) {\n+                                                          Scheduler scheduler) {\n         return new ConductorEventQueueProvider(queueDAO, properties, scheduler);\n     }\n }",
      "parent_sha": "17754477138bbc750a8dd9bba838184106b81914"
    }
  },
  {
    "oid": "8c952e42faf4812517cf47ce771f3484950e8959",
    "message": "set workflow input threshold to 5MB",
    "date": "2018-06-08T18:39:08Z",
    "url": "https://github.com/conductor-oss/conductor/commit/8c952e42faf4812517cf47ce771f3484950e8959",
    "details": {
      "sha": "1baaac6aa6c9673c022fe6144ce3611c35ca118a",
      "filename": "redis-persistence/src/main/java/com/netflix/conductor/dao/dynomite/RedisExecutionDAO.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/8c952e42faf4812517cf47ce771f3484950e8959/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fdynomite%2FRedisExecutionDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/8c952e42faf4812517cf47ce771f3484950e8959/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fdynomite%2FRedisExecutionDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fdynomite%2FRedisExecutionDAO.java?ref=8c952e42faf4812517cf47ce771f3484950e8959",
      "patch": "@@ -80,8 +80,8 @@ public class RedisExecutionDAO extends BaseDynoDAO implements ExecutionDAO {\n \tprivate final static String POLL_DATA = \"POLL_DATA\";\n \n \tprivate final static String EVENT_EXECUTION = \"EVENT_EXECUTION\";\n-\tpublic static final String WORKFLOW_DYNOMITE_PAYLOAD_THRESHOLD = \"workflow.dynomite.task.payload.threshold\";\n-\tpublic static final String WORKFLOW_DYNOMITE_WORKFLOW_INPUT_THRESHOLD = \"workflow.dynomite.workflow.input.threshold\";\n+\tprivate static final String WORKFLOW_DYNOMITE_TASK_PAYLOAD_THRESHOLD = \"workflow.dynomite.task.payload.threshold\";\n+\tprivate static final String WORKFLOW_DYNOMITE_WORKFLOW_INPUT_THRESHOLD = \"workflow.dynomite.workflow.input.threshold\";\n \n \tprivate IndexDAO indexDAO;\n \n@@ -97,8 +97,8 @@ public RedisExecutionDAO(DynoProxy dynoClient, ObjectMapper objectMapper,\n \t\tsuper(dynoClient, objectMapper, config);\n \t\tthis.indexDAO = indexDAO;\n \t\tthis.metadataDA0 = metadataDA0;\n-\t\tthis.taskPayloadThreshold = config.getLongProperty(WORKFLOW_DYNOMITE_PAYLOAD_THRESHOLD,5 * FileUtils.ONE_MB);\n-\t\tthis.workflowInputPayloadThreshold = config.getLongProperty(WORKFLOW_DYNOMITE_WORKFLOW_INPUT_THRESHOLD,2 * FileUtils.ONE_MB);\n+\t\tthis.taskPayloadThreshold = config.getLongProperty(WORKFLOW_DYNOMITE_TASK_PAYLOAD_THRESHOLD,5 * FileUtils.ONE_MB);\n+\t\tthis.workflowInputPayloadThreshold = config.getLongProperty(WORKFLOW_DYNOMITE_WORKFLOW_INPUT_THRESHOLD,5 * FileUtils.ONE_MB);\n \t}\n \n \t@Override",
      "parent_sha": "861ab8efc6907a13339f762844950472c6b97306"
    }
  },
  {
    "oid": "76a1a4bb70ec1f362d3aa5d18a9c3a521096b757",
    "message": "handle the condition where the task input can be null.",
    "date": "2016-12-12T19:52:06Z",
    "url": "https://github.com/conductor-oss/conductor/commit/76a1a4bb70ec1f362d3aa5d18a9c3a521096b757",
    "details": {
      "sha": "377b8e8c5c8030a22fa65f7a8e93a4ec0a15d7b5",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/DeciderService.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/76a1a4bb70ec1f362d3aa5d18a9c3a521096b757/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FDeciderService.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/76a1a4bb70ec1f362d3aa5d18a9c3a521096b757/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FDeciderService.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FDeciderService.java?ref=76a1a4bb70ec1f362d3aa5d18a9c3a521096b757",
      "patch": "@@ -750,7 +750,9 @@ private Map<String, Object> getTaskInputV2_Prev(Map<String, String> inputParams,\n \t@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n \tprivate Map<String, Object> getTaskInputV2(Map input, Workflow workflow, String taskId, TaskDef taskDef) {\n \t\tMap<String, Object> inputParams = new HashMap<>();\n-\t\tinputParams.putAll(input);\n+\t\tif(input != null) {\n+\t\t\tinputParams.putAll(input);\n+\t\t}\n \t\tif(taskDef != null && taskDef.getInputTemplate() != null) {\n \t\t\tinputParams.putAll(taskDef.getInputTemplate());\n \t\t}",
      "parent_sha": "f37507e833ef0a68f3de63869d8ee8f1dfcaf8d4"
    }
  },
  {
    "oid": "2d25ae8d9b9b723ba5296776048ae5bc1b73ef48",
    "message": "doc: expand comments in WorkflowSystemTask (#2456)",
    "date": "2021-09-13T21:56:37Z",
    "url": "https://github.com/conductor-oss/conductor/commit/2d25ae8d9b9b723ba5296776048ae5bc1b73ef48",
    "details": {
      "sha": "94da0f711a2bb5b329225c22c5b3e83d7daf6899",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/tasks/WorkflowSystemTask.java",
      "status": "modified",
      "additions": 8,
      "deletions": 1,
      "changes": 9,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/2d25ae8d9b9b723ba5296776048ae5bc1b73ef48/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FWorkflowSystemTask.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/2d25ae8d9b9b723ba5296776048ae5bc1b73ef48/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FWorkflowSystemTask.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FWorkflowSystemTask.java?ref=2d25ae8d9b9b723ba5296776048ae5bc1b73ef48",
      "patch": "@@ -28,7 +28,9 @@ public WorkflowSystemTask(String taskType) {\n     }\n \n     /**\n-     * Start the task execution\n+     * Start the task execution.\n+     *\n+     * Called only once, and first, when the task status is SCHEDULED.\n      *\n      * @param workflow         Workflow for which the task is being started\n      * @param task             Instance of the Task\n@@ -39,6 +41,11 @@ public void start(Workflow workflow, Task task, WorkflowExecutor workflowExecuto\n     }\n \n     /**\n+     * \"Execute\" the task.\n+     *\n+     * Called after {@link #start(Workflow, Task, WorkflowExecutor)}, if the task status is not terminal.\n+     * Can be called more than once.\n+     *\n      * @param workflow         Workflow for which the task is being started\n      * @param task             Instance of the Task\n      * @param workflowExecutor Workflow Executor",
      "parent_sha": "ff4df0ea266f612a2de15d9d8bc7d3f4871bba32"
    }
  },
  {
    "oid": "358493862d79569061b4f788afd86229ea196bae",
    "message": "replaced '.' with '_' when using env vars",
    "date": "2017-11-17T14:17:01Z",
    "url": "https://github.com/conductor-oss/conductor/commit/358493862d79569061b4f788afd86229ea196bae",
    "details": {
      "sha": "c8956366f62d9cf5bfb88f12d0828969f0b4e463",
      "filename": "server/src/main/java/com/netflix/conductor/server/ConductorConfig.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/358493862d79569061b4f788afd86229ea196bae/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorConfig.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/358493862d79569061b4f788afd86229ea196bae/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorConfig.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorConfig.java?ref=358493862d79569061b4f788afd86229ea196bae",
      "patch": "@@ -104,9 +104,10 @@ public int getIntProperty(String key, int defaultValue) {\n \n \t@Override\n \tpublic String getProperty(String key, String defaultValue) {\n+\n \t\tString val = null;\n \t\ttry{\n-\t\t\tval = System.getenv(key);\n+\t\t\tval = System.getenv(key.replace('.','_'));\n \t\t\tif (val == null || val.isEmpty()) {\n \t\t\t\tval = Optional.ofNullable(System.getProperty(key)).orElse(defaultValue);\n \t\t\t}",
      "parent_sha": "b2ac16bd4365bfc028222d9d2f661887b8222a37"
    }
  },
  {
    "oid": "e6f5e2ab64d6ca2ef5e73f29288d00f67f5d35f5",
    "message": "Fix chronounit issue in java sdk",
    "date": "2024-10-29T07:16:03Z",
    "url": "https://github.com/conductor-oss/conductor/commit/e6f5e2ab64d6ca2ef5e73f29288d00f67f5d35f5",
    "details": {
      "sha": "0b47c6ff3bae99baac16e1e5adb5e19ea09c58e2",
      "filename": "conductor-clients/java/conductor-java-sdk/orkes-client/src/main/java/io/orkes/conductor/client/http/OrkesWorkflowClient.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/e6f5e2ab64d6ca2ef5e73f29288d00f67f5d35f5/conductor-clients%2Fjava%2Fconductor-java-sdk%2Forkes-client%2Fsrc%2Fmain%2Fjava%2Fio%2Forkes%2Fconductor%2Fclient%2Fhttp%2FOrkesWorkflowClient.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/e6f5e2ab64d6ca2ef5e73f29288d00f67f5d35f5/conductor-clients%2Fjava%2Fconductor-java-sdk%2Forkes-client%2Fsrc%2Fmain%2Fjava%2Fio%2Forkes%2Fconductor%2Fclient%2Fhttp%2FOrkesWorkflowClient.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/conductor-clients%2Fjava%2Fconductor-java-sdk%2Forkes-client%2Fsrc%2Fmain%2Fjava%2Fio%2Forkes%2Fconductor%2Fclient%2Fhttp%2FOrkesWorkflowClient.java?ref=e6f5e2ab64d6ca2ef5e73f29288d00f67f5d35f5",
      "patch": "@@ -112,7 +112,7 @@ public CompletableFuture<WorkflowRun> executeWorkflow(StartWorkflowRequest reque\n      */\n     public WorkflowRun executeWorkflow(StartWorkflowRequest request, String waitUntilTask, Duration waitTimeout) throws ExecutionException, InterruptedException, TimeoutException {\n         CompletableFuture<WorkflowRun> future = executeWorkflow(request, waitUntilTask);\n-        return future.get(waitTimeout.get(ChronoUnit.MILLIS), TimeUnit.MILLISECONDS);\n+        return future.get(waitTimeout.get(ChronoUnit.SECONDS), TimeUnit.SECONDS);\n     }\n \n     public void terminateWorkflowWithFailure(String workflowId, String reason, boolean triggerFailureWorkflow) {",
      "parent_sha": "6e77cc53c9eba20c7b4d1e6894794bfc5d686f2d"
    }
  },
  {
    "oid": "f878d5c0dbbdb0a9b163e4a1b56c373d2d183cd9",
    "message": "code changes for WorkflowBulkResource",
    "date": "2024-11-18T09:23:54Z",
    "url": "https://github.com/conductor-oss/conductor/commit/f878d5c0dbbdb0a9b163e4a1b56c373d2d183cd9",
    "details": {
      "sha": "56c3e92a56e7870a6c830f814a7aab874c729ab2",
      "filename": "conductor-clients/java/conductor-java-sdk/orkes-client/src/main/java/io/orkes/conductor/client/http/WorkflowBulkResource.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/f878d5c0dbbdb0a9b163e4a1b56c373d2d183cd9/conductor-clients%2Fjava%2Fconductor-java-sdk%2Forkes-client%2Fsrc%2Fmain%2Fjava%2Fio%2Forkes%2Fconductor%2Fclient%2Fhttp%2FWorkflowBulkResource.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/f878d5c0dbbdb0a9b163e4a1b56c373d2d183cd9/conductor-clients%2Fjava%2Fconductor-java-sdk%2Forkes-client%2Fsrc%2Fmain%2Fjava%2Fio%2Forkes%2Fconductor%2Fclient%2Fhttp%2FWorkflowBulkResource.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/conductor-clients%2Fjava%2Fconductor-java-sdk%2Forkes-client%2Fsrc%2Fmain%2Fjava%2Fio%2Forkes%2Fconductor%2Fclient%2Fhttp%2FWorkflowBulkResource.java?ref=f878d5c0dbbdb0a9b163e4a1b56c373d2d183cd9",
      "patch": "@@ -31,7 +31,7 @@ class WorkflowBulkResource {\n         this.client = client;\n     }\n \n-    BulkResponse pauseWorkflows(List<String> workflowIds) {\n+    BulkResponse<String> pauseWorkflows(List<String> workflowIds) {\n         ConductorClientRequest request = ConductorClientRequest.builder()\n                 .method(Method.PUT)\n                 .path(\"/workflow/bulk/pause\")\n@@ -44,7 +44,7 @@ BulkResponse pauseWorkflows(List<String> workflowIds) {\n         return resp.getData();\n     }\n \n-    BulkResponse restartWorkflows(List<String> workflowIds, Boolean useLatestDefinitions) {\n+    BulkResponse<String> restartWorkflows(List<String> workflowIds, Boolean useLatestDefinitions) {\n         ConductorClientRequest request = ConductorClientRequest.builder()\n                 .method(Method.POST)\n                 .path(\"/workflow/bulk/restart\")\n@@ -58,7 +58,7 @@ BulkResponse restartWorkflows(List<String> workflowIds, Boolean useLatestDefinit\n         return resp.getData();\n     }\n \n-    BulkResponse resumeWorkflows(List<String> workflowIds) {\n+    BulkResponse<String> resumeWorkflows(List<String> workflowIds) {\n         ConductorClientRequest request = ConductorClientRequest.builder()\n                 .method(Method.PUT)\n                 .path(\"/workflow/bulk/resume\")\n@@ -71,7 +71,7 @@ BulkResponse resumeWorkflows(List<String> workflowIds) {\n         return resp.getData();\n     }\n \n-    BulkResponse retryWorkflows(List<String> workflowIds) {\n+    BulkResponse<String> retryWorkflows(List<String> workflowIds) {\n         ConductorClientRequest request = ConductorClientRequest.builder()\n                 .method(Method.POST)\n                 .path(\"/workflow/bulk/retry\")",
      "parent_sha": "9906b1557db4e7fb0e3d3bc6bf23ef1ba5755cd1"
    }
  },
  {
    "oid": "ad5d68ec62324df17ce2dfe1b63a42baa9d20374",
    "message": "Logger statements fix",
    "date": "2018-03-22T17:50:26Z",
    "url": "https://github.com/conductor-oss/conductor/commit/ad5d68ec62324df17ce2dfe1b63a42baa9d20374",
    "details": {
      "sha": "a262b508503ac798be39f6dfdffbf3a0ed5bca72",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/WorkflowExecutor.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/ad5d68ec62324df17ce2dfe1b63a42baa9d20374/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/ad5d68ec62324df17ce2dfe1b63a42baa9d20374/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java?ref=ad5d68ec62324df17ce2dfe1b63a42baa9d20374",
      "patch": "@@ -324,7 +324,7 @@ public void completeWorkflow(Workflow wf) throws Exception {\n \n         if (workflow.getStatus().equals(WorkflowStatus.COMPLETED)) {\n             executionDAO.removeFromPendingWorkflow(workflow.getWorkflowType(), workflow.getWorkflowId());\n-            logger.info(\"Workflow has already been completed.  Current status=\" + workflow.getStatus() + \", workflowId=\" + wf.getWorkflowId());\n+            logger.info(\"Workflow has already been completed.  Current status={}, workflowId= {}\", workflow.getStatus(), wf.getWorkflowId());\n             return;\n         }\n \n@@ -429,7 +429,7 @@ public void updateTask(TaskResult result) throws Exception {\n         if (workflowInstance.getStatus().isTerminal()) {\n             // Workflow is in terminal state\n             queueDAO.remove(taskQueueName, result.getTaskId());\n-            logger.debug(\"Workflow: {} is in terminal state Task: {} removed from Queue: {} during update task\", task, workflowInstance);\n+            logger.debug(\"Workflow: {} is in terminal state Task: {} removed from Queue: {} during update task\", task, workflowInstance, taskQueueName);\n             if (!task.getStatus().isTerminal()) {\n                 task.setStatus(Status.COMPLETED);\n             }\n@@ -472,7 +472,7 @@ public void updateTask(TaskResult result) throws Exception {\n         if (Status.FAILED.equals(task.getStatus())) {\n             workflowInstance.getFailedReferenceTaskNames().add(task.getReferenceTaskName());\n             executionDAO.updateWorkflow(workflowInstance);\n-            logger.debug(\"Task: {} has a FAILED status and the Workflow has been updated with failed task reference\");\n+            logger.debug(\"Task: {} has a FAILED status and the Workflow has been updated with failed task reference\", task);\n         }\n \n         result.getLogs().forEach(tl -> tl.setTaskId(task.getTaskId()));",
      "parent_sha": "791a7128ba03fa7941268e427062c14702cb7072"
    }
  },
  {
    "oid": "ecf43366b8916dcced43f6cd1c4fd3eb8f663589",
    "message": "when the sub workflow updates the state of the parent, it sets up the parent to be evaluated as soon as possible by setting a priority",
    "date": "2021-04-01T16:16:05Z",
    "url": "https://github.com/conductor-oss/conductor/commit/ecf43366b8916dcced43f6cd1c4fd3eb8f663589",
    "details": {
      "sha": "c2a8c9393c67e0bdbbe1b49e3678feb5bd9ade4f",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/WorkflowExecutor.java",
      "status": "modified",
      "additions": 15,
      "deletions": 3,
      "changes": 18,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/ecf43366b8916dcced43f6cd1c4fd3eb8f663589/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/ecf43366b8916dcced43f6cd1c4fd3eb8f663589/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java?ref=ecf43366b8916dcced43f6cd1c4fd3eb8f663589",
      "patch": "@@ -92,6 +92,7 @@\n public class WorkflowExecutor {\n \n     private static final Logger LOGGER = LoggerFactory.getLogger(WorkflowExecutor.class);\n+    private static final int PARENT_WF_PRIORITY = 10;\n \n     private final MetadataDAO metadataDAO;\n     private final QueueDAO queueDAO;\n@@ -601,7 +602,7 @@ private void updateAndPushParents(Workflow workflow, String operation) {\n             parentWorkflow.setStatus(WorkflowStatus.RUNNING);\n             parentWorkflow.setLastRetriedTime(System.currentTimeMillis());\n             executionDAOFacade.updateWorkflow(parentWorkflow);\n-            queueDAO.pushIfNotExists(DECIDER_QUEUE, parentWorkflowId, properties.getSweepFrequency().getSeconds());\n+            pushParentWorkflow(parentWorkflowId);\n \n             workflow = parentWorkflow;\n         }\n@@ -775,7 +776,7 @@ Workflow completeWorkflow(Workflow workflow) {\n         if (workflow.hasParent()) {\n             updateParentWorkflowTask(workflow);\n             LOGGER.info(\"{} updated parent {} task {}\", workflow.toShortString(), workflow.getParentWorkflowId(), workflow.getParentWorkflowTaskId());\n-            queueDAO.pushIfNotExists(DECIDER_QUEUE, workflow.getParentWorkflowId(), properties.getSweepFrequency().getSeconds());\n+            pushParentWorkflow(workflow.getParentWorkflowId());\n         }\n \n         executionLockService.releaseLock(workflow.getWorkflowId());\n@@ -829,7 +830,7 @@ public Workflow terminateWorkflow(Workflow workflow, String reason, String failu\n             if (workflow.hasParent()) {\n                 updateParentWorkflowTask(workflow);\n                 LOGGER.info(\"{} updated parent {} task {}\", workflow.toShortString(), workflow.getParentWorkflowId(), workflow.getParentWorkflowTaskId());\n-                queueDAO.pushIfNotExists(DECIDER_QUEUE, workflow.getParentWorkflowId(), properties.getSweepFrequency().getSeconds());\n+                pushParentWorkflow(workflow.getParentWorkflowId());\n             }\n \n             if (!StringUtils.isBlank(failureWorkflow)) {\n@@ -1822,4 +1823,15 @@ private void executeSubworkflowTaskAndSyncData(Workflow subWorkflow, Task subWor\n             deciderService.externalizeTaskData(subWorkflowTask);\n         }\n     }\n+\n+    /**\n+     * Pushes parent workflow id into the decider queue with a priority.\n+     */\n+    private void pushParentWorkflow(String parentWorkflowId) {\n+        if (queueDAO.containsMessage(DECIDER_QUEUE, parentWorkflowId)) {\n+            queueDAO.postpone(DECIDER_QUEUE, parentWorkflowId, PARENT_WF_PRIORITY, 0);\n+        } else {\n+            queueDAO.push(DECIDER_QUEUE, parentWorkflowId, PARENT_WF_PRIORITY, 0);\n+        }\n+    }\n }",
      "parent_sha": "7d5bcd2239923d2b38facde4b6e1c83b1d0baa94"
    }
  },
  {
    "oid": "7cdd645470701416722f211bbfd899dd1e89987a",
    "message": "Set Task scheduledTime only after successful added!",
    "date": "2018-12-19T09:51:18Z",
    "url": "https://github.com/conductor-oss/conductor/commit/7cdd645470701416722f211bbfd899dd1e89987a",
    "details": {
      "sha": "83c0f0b587d13d6be0a45a05c09628c30ef5a455",
      "filename": "redis-persistence/src/main/java/com/netflix/conductor/dao/dynomite/RedisExecutionDAO.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/7cdd645470701416722f211bbfd899dd1e89987a/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fdynomite%2FRedisExecutionDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/7cdd645470701416722f211bbfd899dd1e89987a/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fdynomite%2FRedisExecutionDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fdao%2Fdynomite%2FRedisExecutionDAO.java?ref=7cdd645470701416722f211bbfd899dd1e89987a",
      "patch": "@@ -127,15 +127,15 @@ public List<Task> createTasks(List<Task> tasks) {\n \n \t\t\trecordRedisDaoRequests(\"createTask\", task.getTaskType(), task.getWorkflowType());\n \n-\t\t\ttask.setScheduledTime(System.currentTimeMillis());\n-\n \t\t\tString taskKey = task.getReferenceTaskName() + \"\" + task.getRetryCount();\n \t\t\tLong added = dynoClient.hset(nsKey(SCHEDULED_TASKS, task.getWorkflowInstanceId()), taskKey, task.getTaskId());\n \t\t\tif (added < 1) {\n \t\t\t\tlogger.debug(\"Task already scheduled, skipping the run \" + task.getTaskId() + \", ref=\" + task.getReferenceTaskName() + \", key=\" + taskKey);\n \t\t\t\tcontinue;\n \t\t\t}\n \n+\t\t\ttask.setScheduledTime(System.currentTimeMillis());\n+\n \t\t\tcorrelateTaskToWorkflowInDS(task.getTaskId(), task.getWorkflowInstanceId());\n \t\t\tlogger.debug(\"Scheduled task added to WORKFLOW_TO_TASKS workflowId: {}, taskId: {}, taskType: {} during createTasks\",\n                     task.getWorkflowInstanceId(), task.getTaskId(), task.getTaskType());",
      "parent_sha": "e99fdc0839878c99bfe33c292f4eca027e66667d"
    }
  },
  {
    "oid": "a1684b1f932e828e979e0e87df90e5cdaa35226c",
    "message": "loading the resources as a stream to avoid reading them as file - and OS level compatibility issues.",
    "date": "2017-01-18T05:31:33Z",
    "url": "https://github.com/conductor-oss/conductor/commit/a1684b1f932e828e979e0e87df90e5cdaa35226c",
    "details": {
      "sha": "c2f1e625450cbc07771b30590d982b92c79efe51",
      "filename": "server/src/main/java/com/netflix/conductor/server/ConductorServer.java",
      "status": "modified",
      "additions": 5,
      "deletions": 12,
      "changes": 17,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/a1684b1f932e828e979e0e87df90e5cdaa35226c/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorServer.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/a1684b1f932e828e979e0e87df90e5cdaa35226c/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorServer.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorServer.java?ref=a1684b1f932e828e979e0e87df90e5cdaa35226c",
      "patch": "@@ -18,9 +18,7 @@\n  */\n package com.netflix.conductor.server;\n \n-import java.net.URL;\n-import java.nio.file.Files;\n-import java.nio.file.Paths;\n+import java.io.InputStream;\n import java.util.Collection;\n import java.util.EnumSet;\n import java.util.LinkedList;\n@@ -211,7 +209,6 @@ public synchronized void stop() throws Exception {\n \t\n \tprivate static void createKitchenSink(int port) throws Exception {\n \t\t\n-\n \t\tList<TaskDef> taskDefs = new LinkedList<>();\n \t\tfor(int i = 0; i < 40; i++) {\n \t\t\ttaskDefs.add(new TaskDef(\"task_\" + i, \"task_\" + i, 1, 0));\n@@ -222,15 +219,11 @@ private static void createKitchenSink(int port) throws Exception {\n \t\tObjectMapper om = new ObjectMapper();\n \t\tclient.resource(\"http://localhost:\" + port + \"/api/metadata/taskdefs\").type(MediaType.APPLICATION_JSON).post(om.writeValueAsString(taskDefs));\n \t\t\n-\t\tURL template = Main.class.getClassLoader().getResource(\"kitchensink.json\");\n-\t\tbyte[] source = Files.readAllBytes(Paths.get(ClassLoader.getSystemResource(template.getFile()).toURI()));\n-\t\tString json = new String(source);\n-\t\tclient.resource(\"http://localhost:\" + port + \"/api/metadata/workflow\").type(MediaType.APPLICATION_JSON).post(json);\n+\t\tInputStream stream = Main.class.getResourceAsStream(\"/kitchensink.json\");\n+\t\tclient.resource(\"http://localhost:\" + port + \"/api/metadata/workflow\").type(MediaType.APPLICATION_JSON).post(stream);\n \t\t\n-\t\ttemplate = Main.class.getClassLoader().getResource(\"sub_flow_1.json\");\n-\t\tsource = Files.readAllBytes(Paths.get(ClassLoader.getSystemResource(template.getFile()).toURI()));\n-\t\tjson = new String(source);\n-\t\tclient.resource(\"http://localhost:\" + port + \"/api/metadata/workflow\").type(MediaType.APPLICATION_JSON).post(json);\n+\t\tstream = Main.class.getResourceAsStream(\"/sub_flow_1.json\");\n+\t\tclient.resource(\"http://localhost:\" + port + \"/api/metadata/workflow\").type(MediaType.APPLICATION_JSON).post(stream);\n \t\t\n \t\tlogger.info(\"Kitchen sink workflows are created!\");\n \t}",
      "parent_sha": "82c3ffcc4fc0f6c079a2193788a3423a7cfc8ecd"
    }
  },
  {
    "oid": "307fade96f0c320211f251749995236c4834a75d",
    "message": "updated test to account for the change in TaskResource method signature",
    "date": "2021-02-25T18:35:15Z",
    "url": "https://github.com/conductor-oss/conductor/commit/307fade96f0c320211f251749995236c4834a75d",
    "details": {
      "sha": "11933dd62e783aa82f940e765de8fce8634f6c23",
      "filename": "rest/src/test/java/com/netflix/conductor/rest/controllers/TaskResourceTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/307fade96f0c320211f251749995236c4834a75d/rest%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Frest%2Fcontrollers%2FTaskResourceTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/307fade96f0c320211f251749995236c4834a75d/rest%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Frest%2Fcontrollers%2FTaskResourceTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/rest%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Frest%2Fcontrollers%2FTaskResourceTest.java?ref=307fade96f0c320211f251749995236c4834a75d",
      "patch": "@@ -59,7 +59,7 @@ public void testPoll() {\n         task.setDomain(\"test\");\n \n         when(mockTaskService.poll(anyString(), anyString(), anyString())).thenReturn(task);\n-        assertEquals(task, taskResource.poll(\"SIMPLE\", \"123\", \"test\"));\n+        assertEquals(ResponseEntity.ok(task), taskResource.poll(\"SIMPLE\", \"123\", \"test\"));\n     }\n \n     @Test\n@@ -73,7 +73,7 @@ public void testBatchPoll() {\n \n         when(mockTaskService.batchPoll(anyString(), anyString(), anyString(), anyInt(), anyInt()))\n             .thenReturn(listOfTasks);\n-        assertEquals(listOfTasks, taskResource.batchPoll(\"SIMPLE\", \"123\",\n+        assertEquals(ResponseEntity.ok(listOfTasks), taskResource.batchPoll(\"SIMPLE\", \"123\",\n             \"test\", 1, 100));\n     }\n ",
      "parent_sha": "facec91e85bfd3741ccc7066073836d6fd89f8a1"
    }
  },
  {
    "oid": "1a1b459f5d966043eb2f61b55888aeb23ec21812",
    "message": "Updating DoWhileTask keepLastN iteration endIndex",
    "date": "2024-04-17T11:34:20Z",
    "url": "https://github.com/conductor-oss/conductor/commit/1a1b459f5d966043eb2f61b55888aeb23ec21812",
    "details": {
      "sha": "2e61b4fe76b6035f2365745b82a2c77e68df5077",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/tasks/DoWhile.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/1a1b459f5d966043eb2f61b55888aeb23ec21812/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhile.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/1a1b459f5d966043eb2f61b55888aeb23ec21812/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhile.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhile.java?ref=1a1b459f5d966043eb2f61b55888aeb23ec21812",
      "patch": "@@ -118,7 +118,7 @@ public boolean execute(\n                         .map(value -> (Integer) value);\n         if (keepLastN.isPresent() && doWhileTaskModel.getIteration() > keepLastN.get()) {\n             Integer iteration = doWhileTaskModel.getIteration();\n-            IntStream.range(0, iteration - keepLastN.get())\n+            IntStream.range(0, iteration - keepLastN.get() - 1)\n                     .mapToObj(Integer::toString)\n                     .forEach(doWhileTaskModel::removeOutput);\n         }",
      "parent_sha": "97c0a5fe8014df0ba7c8bc37a8e3e72db496019b"
    }
  },
  {
    "oid": "e96f8f3914e5fc8a61dcc8bfe881ae77448ab637",
    "message": "Clean up some logic and error handling.",
    "date": "2018-06-27T10:48:29Z",
    "url": "https://github.com/conductor-oss/conductor/commit/e96f8f3914e5fc8a61dcc8bfe881ae77448ab637",
    "details": {
      "sha": "578309fb6d5323f50277645d2b94dc76234990a6",
      "filename": "es5-persistence/src/main/java/com/netflix/conductor/elasticsearch/EmbeddedElasticSearch.java",
      "status": "modified",
      "additions": 5,
      "deletions": 4,
      "changes": 9,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/e96f8f3914e5fc8a61dcc8bfe881ae77448ab637/es5-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Felasticsearch%2FEmbeddedElasticSearch.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/e96f8f3914e5fc8a61dcc8bfe881ae77448ab637/es5-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Felasticsearch%2FEmbeddedElasticSearch.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/es5-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Felasticsearch%2FEmbeddedElasticSearch.java?ref=e96f8f3914e5fc8a61dcc8bfe881ae77448ab637",
      "patch": "@@ -16,14 +16,15 @@ public interface EmbeddedElasticSearch extends Lifecycle {\n     Logger logger = LoggerFactory.getLogger(EmbeddedElasticSearch.class);\n \n     default void cleanDataDir(String path) {\n+        File dataDir = new File(path);\n+\n         try {\n             logger.info(\"Deleting contents of data dir {}\", path);\n-            File f = new File(path);\n-            if (f.exists()) {\n-                FileUtils.cleanDirectory(new File(path));\n+            if (dataDir.exists()) {\n+                FileUtils.cleanDirectory(dataDir);\n             }\n         } catch (IOException e) {\n-            logger.error(\"Failed to delete ES data dir\");\n+            logger.error(String.format(\"Failed to delete ES data dir: %s\", dataDir.getAbsolutePath()), e);\n         }\n     }\n ",
      "parent_sha": "b5d6bfc911da8b4ee9cc73b561e529ba869605b2"
    }
  },
  {
    "oid": "8d5eee316e75fc5b1626aea9d2a6237020ab8e98",
    "message": "#2926 added a null check for TaskModel",
    "date": "2022-04-20T04:53:34Z",
    "url": "https://github.com/conductor-oss/conductor/commit/8d5eee316e75fc5b1626aea9d2a6237020ab8e98",
    "details": {
      "sha": "5b905a2853ce72efdb1d1515e82abd9c0d8e6688",
      "filename": "core/src/main/java/com/netflix/conductor/core/dal/ExecutionDAOFacade.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/8d5eee316e75fc5b1626aea9d2a6237020ab8e98/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fdal%2FExecutionDAOFacade.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/8d5eee316e75fc5b1626aea9d2a6237020ab8e98/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fdal%2FExecutionDAOFacade.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fdal%2FExecutionDAOFacade.java?ref=8d5eee316e75fc5b1626aea9d2a6237020ab8e98",
      "patch": "@@ -449,7 +449,9 @@ public List<Task> getTasksForWorkflow(String workflowId) {\n \n     public TaskModel getTaskModel(String taskId) {\n         TaskModel taskModel = getTaskFromDatastore(taskId);\n-        populateTaskData(taskModel);\n+        if (taskModel != null) {\n+            populateTaskData(taskModel);\n+        }\n         return taskModel;\n     }\n ",
      "parent_sha": "4ad982a281c9dc46b3aad317070ed3de9a327ce0"
    }
  },
  {
    "oid": "e8d17df0a6945ad93e6db2675808a536b7c6d65e",
    "message": "Update RedisLockTest.java",
    "date": "2023-12-19T09:06:22Z",
    "url": "https://github.com/conductor-oss/conductor/commit/e8d17df0a6945ad93e6db2675808a536b7c6d65e",
    "details": {
      "sha": "24953dd7b2bea30337d8368d073c9d06b1b6d8b1",
      "filename": "redis-lock/src/test/java/com/netflix/conductor/redis/lock/RedisLockTest.java",
      "status": "modified",
      "additions": 3,
      "deletions": 4,
      "changes": 7,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/e8d17df0a6945ad93e6db2675808a536b7c6d65e/redis-lock%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Flock%2FRedisLockTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/e8d17df0a6945ad93e6db2675808a536b7c6d65e/redis-lock%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Flock%2FRedisLockTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/redis-lock%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Flock%2FRedisLockTest.java?ref=e8d17df0a6945ad93e6db2675808a536b7c6d65e",
      "patch": "@@ -21,13 +21,12 @@\n import org.redisson.api.RLock;\n import org.redisson.api.RedissonClient;\n import org.redisson.config.Config;\n+import org.testcontainers.containers.*;\n \n import com.netflix.conductor.redislock.config.RedisLockProperties;\n import com.netflix.conductor.redislock.config.RedisLockProperties.REDIS_SERVER_TYPE;\n import com.netflix.conductor.redislock.lock.RedisLock;\n \n-import org.testcontainers.containers.*;\n-\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Mockito.mock;\n@@ -39,8 +38,8 @@ public class RedisLockTest {\n     private static Config config;\n     private static RedissonClient redisson;\n \n-    static GenericContainer redis = new GenericContainer(\"redis:5.0.3-alpine\")\n-            .withExposedPorts(6379);\n+    static GenericContainer redis =\n+            new GenericContainer(\"redis:5.0.3-alpine\").withExposedPorts(6379);\n \n     @BeforeClass\n     public static void setUp() throws Exception {",
      "parent_sha": "7dcb6c209b66785eaf273ba0dad0f30dace7e252"
    }
  },
  {
    "oid": "904c2b02e7e6efa374869054886fb36ee17e479c",
    "message": "Added integration tests for ephemeral workflows and tasks\n- Integration test for an ephemeral workflow with stored tasks\n- Integration test for an ephemeral workflow with ephemeral tasks",
    "date": "2018-07-31T03:00:59Z",
    "url": "https://github.com/conductor-oss/conductor/commit/904c2b02e7e6efa374869054886fb36ee17e479c",
    "details": {
      "sha": "30a6d96b384f5789910233fad5206295980a4d35",
      "filename": "test-harness/src/test/java/com/netflix/conductor/tests/integration/End2EndTests.java",
      "status": "modified",
      "additions": 135,
      "deletions": 41,
      "changes": 176,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/904c2b02e7e6efa374869054886fb36ee17e479c/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FEnd2EndTests.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/904c2b02e7e6efa374869054886fb36ee17e479c/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FEnd2EndTests.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FEnd2EndTests.java?ref=904c2b02e7e6efa374869054886fb36ee17e479c",
      "patch": "@@ -27,6 +27,7 @@\n import com.netflix.conductor.common.metadata.tasks.TaskDef;\n import com.netflix.conductor.common.metadata.tasks.TaskDef.TimeoutPolicy;\n import com.netflix.conductor.common.metadata.tasks.TaskResult;\n+import com.netflix.conductor.common.metadata.workflow.StartWorkflowRequest;\n import com.netflix.conductor.common.metadata.workflow.WorkflowDef;\n import com.netflix.conductor.common.metadata.workflow.WorkflowTask;\n import com.netflix.conductor.common.metadata.workflow.WorkflowTask.Type;\n@@ -47,6 +48,7 @@\n import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Optional;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n@@ -57,10 +59,13 @@\n  *\n  */\n public class End2EndTests {\n-    private static TaskClient tc;\n-    private static WorkflowClient wc;\n+    private static TaskClient taskClient;\n+    private static WorkflowClient workflowClient;\n     private static EmbeddedElasticSearch search;\n \n+    private static final int SERVER_PORT = 8080;\n+    private static final String TASK_DEFINITION_PREFIX = \"task_\";\n+\n     @BeforeClass\n     public static void setup() throws Exception {\n         TestEnvironment.setup();\n@@ -73,14 +78,14 @@ public static void setup() throws Exception {\n         search = serverInjector.getInstance(EmbeddedElasticSearchProvider.class).get().get();\n         search.start();\n \n-        JettyServer server = new JettyServer(8080, false);\n+        JettyServer server = new JettyServer(SERVER_PORT, false);\n         server.start();\n \n-        tc = new TaskClient();\n-        tc.setRootURI(\"http://localhost:8080/api/\");\n+        taskClient = new TaskClient();\n+        taskClient.setRootURI(\"http://localhost:8080/api/\");\n \n-        wc = new WorkflowClient();\n-        wc.setRootURI(\"http://localhost:8080/api/\");\n+        workflowClient = new WorkflowClient();\n+        workflowClient.setRootURI(\"http://localhost:8080/api/\");\n     }\n \n     @AfterClass\n@@ -91,17 +96,11 @@ public static void teardown() throws Exception {\n \n     @Test\n     public void testAll() throws Exception {\n-        assertNotNull(tc);\n-        List<TaskDef> defs = new LinkedList<>();\n-        for (int i = 0; i < 5; i++) {\n-            TaskDef def = new TaskDef(\"t\" + i, \"task \" + i);\n-            def.setTimeoutPolicy(TimeoutPolicy.RETRY);\n-            defs.add(def);\n-        }\n-        tc.registerTaskDefs(defs);\n-        List<TaskDef> found = tc.getTaskDef();\n+        List<TaskDef> definitions = createAndRegisterTaskDefinitions(\"t\", 5);\n+\n+        List<TaskDef> found = taskClient.getTaskDef();\n         assertNotNull(found);\n-        assertEquals(defs.size(), found.size());\n+        assertEquals(definitions.size(), found.size());\n \n         WorkflowDef def = new WorkflowDef();\n         def.setName(\"test\");\n@@ -119,56 +118,56 @@ public void testAll() throws Exception {\n         def.getTasks().add(t0);\n         def.getTasks().add(t1);\n \n-        wc.registerWorkflow(def);\n-        WorkflowDef foundd = wc.getWorkflowDef(def.getName(), null);\n-        assertNotNull(foundd);\n-        assertEquals(def.getName(), foundd.getName());\n-        assertEquals(def.getVersion(), foundd.getVersion());\n+        workflowClient.registerWorkflow(def);\n+        WorkflowDef workflowDefinitionFromSystem = workflowClient.getWorkflowDef(def.getName(), null);\n+        assertNotNull(workflowDefinitionFromSystem);\n+        assertEquals(def.getName(), workflowDefinitionFromSystem.getName());\n+        assertEquals(def.getVersion(), workflowDefinitionFromSystem.getVersion());\n \n         String correlationId = \"test_corr_id\";\n-        String workflowId = wc.startWorkflow(def.getName(), null, correlationId, new HashMap<>());\n+        String workflowId = workflowClient.startWorkflow(def.getName(), null, correlationId, new HashMap<>());\n         assertNotNull(workflowId);\n         System.out.println(workflowId);\n \n-        Workflow wf = wc.getWorkflow(workflowId, false);\n+        Workflow wf = workflowClient.getWorkflow(workflowId, false);\n         assertEquals(0, wf.getTasks().size());\n         assertEquals(workflowId, wf.getWorkflowId());\n \n-        wf = wc.getWorkflow(workflowId, true);\n+        wf = workflowClient.getWorkflow(workflowId, true);\n         assertNotNull(wf);\n         assertEquals(WorkflowStatus.RUNNING, wf.getStatus());\n         assertEquals(1, wf.getTasks().size());\n         assertEquals(t0.getTaskReferenceName(), wf.getTasks().get(0).getReferenceTaskName());\n         assertEquals(workflowId, wf.getWorkflowId());\n \n-        List<String> runningIds = wc.getRunningWorkflow(def.getName(), def.getVersion());\n+        List<String> runningIds = workflowClient.getRunningWorkflow(def.getName(), def.getVersion());\n         assertNotNull(runningIds);\n         assertEquals(1, runningIds.size());\n         assertEquals(workflowId, runningIds.get(0));\n \n-        List<Task> polled = tc.batchPollTasksByTaskType(\"non existing task\", \"test\", 1, 100);\n+        List<Task> polled = taskClient.batchPollTasksByTaskType(\"non existing task\", \"test\", 1, 100);\n         assertNotNull(polled);\n         assertEquals(0, polled.size());\n \n-        polled = tc.batchPollTasksByTaskType(t0.getName(), \"test\", 1, 100);\n+        polled = taskClient.batchPollTasksByTaskType(t0.getName(), \"test\", 1, 100);\n         assertNotNull(polled);\n         assertEquals(1, polled.size());\n         assertEquals(t0.getName(), polled.get(0).getTaskDefName());\n         Task task = polled.get(0);\n \n-        Boolean acked = tc.ack(task.getTaskId(), \"test\");\n+        Boolean acked = taskClient.ack(task.getTaskId(), \"test\");\n         assertNotNull(acked);\n         assertTrue(acked.booleanValue());\n \n         task.getOutputData().put(\"key1\", \"value1\");\n         task.setStatus(Status.COMPLETED);\n-        tc.updateTask(new TaskResult(task), task.getTaskType());\n+        taskClient.updateTask(new TaskResult(task), task.getTaskType());\n \n-        polled = tc.batchPollTasksByTaskType(t0.getName(), \"test\", 1, 100);\n+        polled = taskClient.batchPollTasksByTaskType(t0.getName(), \"test\", 1, 100);\n         assertNotNull(polled);\n         assertTrue(polled.toString(), polled.isEmpty());\n \n-        wf = wc.getWorkflow(workflowId, true);\n+        wf = workflowClient.getWorkflow(workflowId, true);\n         assertNotNull(wf);\n         assertEquals(WorkflowStatus.RUNNING, wf.getStatus());\n         assertEquals(2, wf.getTasks().size());\n@@ -177,42 +176,137 @@ public void testAll() throws Exception {\n         assertEquals(Task.Status.COMPLETED, wf.getTasks().get(0).getStatus());\n         assertEquals(Task.Status.SCHEDULED, wf.getTasks().get(1).getStatus());\n \n-        Task taskById = tc.getTaskDetails(task.getTaskId());\n+        Task taskById = taskClient.getTaskDetails(task.getTaskId());\n         assertNotNull(taskById);\n         assertEquals(task.getTaskId(), taskById.getTaskId());\n \n \n-        List<Task> getTasks = tc.getPendingTasksByType(t0.getName(), null, 1);\n+        List<Task> getTasks = taskClient.getPendingTasksByType(t0.getName(), null, 1);\n         assertNotNull(getTasks);\n         assertEquals(0, getTasks.size());        //getTasks only gives pending tasks\n \n \n-        getTasks = tc.getPendingTasksByType(t1.getName(), null, 1);\n+        getTasks = taskClient.getPendingTasksByType(t1.getName(), null, 1);\n         assertNotNull(getTasks);\n         assertEquals(1, getTasks.size());\n \n \n-        Task pending = tc.getPendingTaskForWorkflow(workflowId, t1.getTaskReferenceName());\n+        Task pending = taskClient.getPendingTaskForWorkflow(workflowId, t1.getTaskReferenceName());\n         assertNotNull(pending);\n         assertEquals(t1.getTaskReferenceName(), pending.getReferenceTaskName());\n         assertEquals(workflowId, pending.getWorkflowInstanceId());\n \n         Thread.sleep(1000);\n-        SearchResult<WorkflowSummary> searchResult = wc.search(\"workflowType='\" + def.getName() + \"'\");\n+        SearchResult<WorkflowSummary> searchResult = workflowClient.search(\"workflowType='\" + def.getName() + \"'\");\n         assertNotNull(searchResult);\n         assertEquals(1, searchResult.getTotalHits());\n \n-        wc.terminateWorkflow(workflowId, \"terminate reason\");\n-        wf = wc.getWorkflow(workflowId, true);\n+        workflowClient.terminateWorkflow(workflowId, \"terminate reason\");\n+        wf = workflowClient.getWorkflow(workflowId, true);\n         assertNotNull(wf);\n         assertEquals(WorkflowStatus.TERMINATED, wf.getStatus());\n \n-        wc.restart(workflowId);\n-        wf = wc.getWorkflow(workflowId, true);\n+        workflowClient.restart(workflowId);\n+        wf = workflowClient.getWorkflow(workflowId, true);\n         assertNotNull(wf);\n         assertEquals(WorkflowStatus.RUNNING, wf.getStatus());\n         assertEquals(1, wf.getTasks().size());\n+    }\n+\n+    @Test\n+    public void testEphemeralWorkflowsWithStoredTasks() throws Exception {\n+        List<TaskDef> definitions = createAndRegisterTaskDefinitions(\"storedTaskDef\", 5);\n+\n+        List<TaskDef> found = taskClient.getTaskDef();\n+        assertNotNull(found);\n+        assertTrue(definitions.size() > 0);\n+\n+        WorkflowDef workflowDefinition = new WorkflowDef();\n+        workflowDefinition.setName(\"testEphemeralWorkflow\");\n+\n+        WorkflowTask workflowTask1 = createWorkflowTask(\"storedTaskDef1\");\n+        WorkflowTask workflowTask2 = createWorkflowTask(\"storedTaskDef2\");\n+\n+        workflowDefinition.getTasks().add(workflowTask1);\n+        workflowDefinition.getTasks().add(workflowTask2);\n+\n+        String workflowExecutionName = \"ephemeralWorkflow\";\n+        StartWorkflowRequest workflowRequest = new StartWorkflowRequest()\n+                .withName(workflowExecutionName)\n+                .withWorkflowDef(workflowDefinition);\n+\n+        String workflowId = workflowClient.startWorkflow(workflowRequest);\n+        assertNotNull(workflowId);\n+\n+        Workflow workflow = workflowClient.getWorkflow(workflowId, true);\n+        WorkflowDef ephemeralWorkflow = workflow.getWorkflowDefinition();\n+        assertNotNull(ephemeralWorkflow);\n+        assertEquals(workflowDefinition.getName(), ephemeralWorkflow.getName());\n+    }\n+\n+    @Test\n+    public void testEphemeralWorkflowsWithEphemeralTasks() throws Exception {\n+        WorkflowDef workflowDefinition = new WorkflowDef();\n+        workflowDefinition.setName(\"testEphemeralWorkflowWithEphemeralTasks\");\n+\n+        WorkflowTask workflowTask1 = createWorkflowTask(\"ephemeralTask1\");\n+        TaskDef taskDefinition1 = createTaskDefinition(\"ephemeralTaskDef1\");\n+        workflowTask1.setTaskDefinition(taskDefinition1);\n+\n+        WorkflowTask workflowTask2 = createWorkflowTask(\"ephemeralTask2\");\n+        TaskDef taskDefinition2 = createTaskDefinition(\"ephemeralTaskDef2\");\n+        workflowTask2.setTaskDefinition(taskDefinition2);\n+\n+        workflowDefinition.getTasks().add(workflowTask1);\n+        workflowDefinition.getTasks().add(workflowTask2);\n+\n+        String workflowExecutionName = \"ephemeralWorkflowWithEphemeralTasks\";\n+        StartWorkflowRequest workflowRequest = new StartWorkflowRequest()\n+                .withName(workflowExecutionName)\n+                .withWorkflowDef(workflowDefinition);\n+\n+        String workflowId = workflowClient.startWorkflow(workflowRequest);\n+        assertNotNull(workflowId);\n+\n+        Workflow workflow = workflowClient.getWorkflow(workflowId, true);\n+        WorkflowDef ephemeralWorkflow = workflow.getWorkflowDefinition();\n+        assertNotNull(ephemeralWorkflow);\n+        assertEquals(workflowDefinition.getName(), ephemeralWorkflow.getName());\n+\n+        List<WorkflowTask> ephemeralTasks = ephemeralWorkflow.getTasks();\n+        assertEquals(2, ephemeralTasks.size());\n+        for (WorkflowTask ephemeralTask : ephemeralTasks) {\n+            assertNotNull(ephemeralTask.getTaskDefinition());\n+        }\n+\n+    }\n+\n+    private WorkflowTask createWorkflowTask(String name) {\n+        WorkflowTask workflowTask = new WorkflowTask();\n+        workflowTask.setName(name);\n+        workflowTask.setWorkflowTaskType(Type.SIMPLE);\n+        workflowTask.setTaskReferenceName(name);\n+        return workflowTask;\n+    }\n \n+    private TaskDef createTaskDefinition(String name) {\n+        TaskDef taskDefinition = new TaskDef();\n+        taskDefinition.setName(name);\n+        return taskDefinition;\n+    }\n+\n+    // Helper method for creating task definitions on the server\n+    private List<TaskDef> createAndRegisterTaskDefinitions(String prefixTaskDefinition, int numberOfTaskDefinitions) {\n+        assertNotNull(taskClient);\n+        String prefix = Optional.ofNullable(prefixTaskDefinition).orElse(TASK_DEFINITION_PREFIX);\n+        List<TaskDef> definitions = new LinkedList<>();\n+        for (int i = 0; i < numberOfTaskDefinitions; i++) {\n+            TaskDef def = new TaskDef(prefix + i, \"task \" + i + \"description\");\n+            def.setTimeoutPolicy(TimeoutPolicy.RETRY);\n+            definitions.add(def);\n+        }\n+        taskClient.registerTaskDefs(definitions);\n+        return definitions;\n     }\n \n }",
      "parent_sha": "ebed6ffb3edecbf24ed31ebbf8141668c91bf60c"
    }
  },
  {
    "oid": "fe449174834cccc67f4b45accec420862ca546c0",
    "message": "grpc-server: Return DebugInfo as metadata\n\nGoogle provides a set of standard error objects that can be returned as\nmetadata. In this case we're using DebugInfo for all internal\nexceptions, as we can attach a stack trace and a detailed error message\nto the payload. Note that returning this in a compatible way with Java\nand Go is rather tricky, as the required API is missing.\n\nThe comment in the implementation gives details on how we're working\naround the issue.",
    "date": "2018-06-08T14:37:41Z",
    "url": "https://github.com/conductor-oss/conductor/commit/fe449174834cccc67f4b45accec420862ca546c0",
    "details": {
      "sha": "a2b19c9cb6aa98c70508896fd0ed5ab7994f4154",
      "filename": "grpc-server/src/main/java/com/netflix/conductor/grpc/server/GRPCHelper.java",
      "status": "modified",
      "additions": 74,
      "deletions": 5,
      "changes": 79,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/fe449174834cccc67f4b45accec420862ca546c0/grpc-server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fgrpc%2Fserver%2FGRPCHelper.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/fe449174834cccc67f4b45accec420862ca546c0/grpc-server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fgrpc%2Fserver%2FGRPCHelper.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/grpc-server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fgrpc%2Fserver%2FGRPCHelper.java?ref=fe449174834cccc67f4b45accec420862ca546c0",
      "patch": "@@ -1,23 +1,92 @@\n package com.netflix.conductor.grpc.server;\n \n+import com.google.rpc.DebugInfo;\n+import io.grpc.Metadata;\n import io.grpc.Status;\n+import io.grpc.StatusException;\n+import io.grpc.protobuf.lite.ProtoLiteUtils;\n import io.grpc.stub.StreamObserver;\n import org.apache.commons.lang3.exception.ExceptionUtils;\n import org.slf4j.Logger;\n \n+import java.util.Arrays;\n+\n public class GRPCHelper {\n     private final Logger logger;\n \n+    private static final Metadata.Key<DebugInfo> STATUS_DETAILS_KEY =\n+            Metadata.Key.of(\n+                    \"grpc-status-details-bin\",\n+                    ProtoLiteUtils.metadataMarshaller(DebugInfo.getDefaultInstance()));\n+\n     public GRPCHelper(Logger log) {\n         this.logger = log;\n     }\n \n-    void onError(StreamObserver<?> response, Throwable t) {\n-        logger.error(\"error during GRPC request\", t);\n-        response.onError(Status.INTERNAL\n+    /**\n+     * Converts an internal exception thrown by Conductor into an StatusException\n+     * that uses modern \"Status\" metadata for GRPC.\n+     *\n+     * Note that this is trickier than it ought to be because the GRPC APIs have\n+     * not been upgraded yet. Here's a quick breakdown of how this works in practice:\n+     *\n+     * Reporting a \"status\" result back to a client with GRPC is pretty straightforward.\n+     * GRPC implementations simply serialize the status into several HTTP/2 trailer headers that\n+     * are sent back to the client before shutting down the HTTP/2 stream.\n+     *\n+     * - 'grpc-status', which is a string representation of a {@link com.google.rpc.Code}\n+     * - 'grpc-message', which is the description of the returned status\n+     * - 'grpc-status-details-bin' (optional), which is an arbitrary payload with a serialized\n+     *  ProtoBuf object, containing an accurate description of the error in case the status is not\n+     *  successful.\n+     *\n+     *  By convention, Google provides a default set of ProtoBuf messages for the most common\n+     *  error cases. Here, we'll be using {@link DebugInfo}, as we're reporting an internal\n+     *  Java exception which we couldn't properly handle.\n+     *\n+     *  Now, how do we go about sending all those headers _and_ the {@link DebugInfo} payload\n+     *  using the Java GRPC API?\n+     *\n+     *  The only way we can return an error with the Java API is by passing an instance of\n+     *  {@link io.grpc.StatusException} or {@link io.grpc.StatusRuntimeException} to\n+     *  {@link StreamObserver#onError(Throwable)}. The easiest way to create either of these\n+     *  exceptions is by using the {@link Status} class and one of its predefined code\n+     *  identifiers (in this case, {@link Status#INTERNAL} because we're reporting an internal\n+     *  exception). The {@link Status} class has setters to set its most relevant attributes,\n+     *  namely those that will be automatically serialized into the 'grpc-status' and 'grpc-message'\n+     *  trailers in the response. There is, however, no setter to pass an arbitrary ProtoBuf message\n+     *  to be serialized into a `grpc-status-details-bin` trailer. This feature exists in the other\n+     *  language implementations but it hasn't been brought to Java yet.\n+     *\n+     *  Fortunately, {@link Status#asException(Metadata)} exists, allowing us to pass any amount\n+     *  of arbitrary trailers before we close the response. So we're using this API to manually\n+     *  craft the 'grpc-status-detail-bin' trailer, in the same way that the GRPC server implementations\n+     *  for Go and C++ craft and serialize the header. This will allow us to access the metadata\n+     *  cleanly from Go and C++ clients by using the 'details' method which _has_ been implemented\n+     *  in those two clients.\n+     *\n+     * @param t The exception to convert\n+     * @return an instance of {@link StatusException} which will properly serialize all its\n+     * headers into the response.\n+     */\n+    private StatusException throwableToStatusException(Throwable t) {\n+        String[] frames = ExceptionUtils.getStackFrames(t);\n+        Metadata metadata = new Metadata();\n+        metadata.put(STATUS_DETAILS_KEY,\n+                DebugInfo.newBuilder()\n+                        .addAllStackEntries(Arrays.asList(frames))\n+                        .setDetail(ExceptionUtils.getMessage(t))\n+                        .build()\n+        );\n+\n+        return Status.INTERNAL\n                 .withDescription(t.getMessage())\n-                .augmentDescription(ExceptionUtils.getStackTrace(t))\n                 .withCause(t)\n-                .asException());\n+                .asException(metadata);\n+    }\n+\n+    void onError(StreamObserver<?> response, Throwable t) {\n+        logger.error(\"internal exception during GRPC request\", t);\n+        response.onError(throwableToStatusException(t));\n     }\n }",
      "parent_sha": "a9248252ace9cde92129601433a000cd9e5fd42b"
    }
  },
  {
    "oid": "8658880e2b06b0ff03983b0ade218e13d86a74ff",
    "message": "make httpCall protected access",
    "date": "2018-08-07T03:56:54Z",
    "url": "https://github.com/conductor-oss/conductor/commit/8658880e2b06b0ff03983b0ade218e13d86a74ff",
    "details": {
      "sha": "7cf594297741bf9f9db31c0088bd9589bc2ced7d",
      "filename": "contribs/src/main/java/com/netflix/conductor/contribs/http/HttpTask.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/8658880e2b06b0ff03983b0ade218e13d86a74ff/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FHttpTask.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/8658880e2b06b0ff03983b0ade218e13d86a74ff/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FHttpTask.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FHttpTask.java?ref=8658880e2b06b0ff03983b0ade218e13d86a74ff",
      "patch": "@@ -140,12 +140,12 @@ public void start(Workflow workflow, Task task, WorkflowExecutor executor) {\n \t}\n \n \t/**\n-\t * \n \t * @param input HTTP Request\n \t * @return Response of the http call\n \t * @throws Exception If there was an error making http call\n+\t * Note: protected access is so that tasks extended from this task can re-use this to make http calls\n \t */\n-\tprivate HttpResponse httpCall(Input input) throws Exception {\n+\tprotected HttpResponse httpCall(Input input) throws Exception {\n \t\tClient client = rcm.getClient(input);\n \n \t\tif(input.oauthConsumerKey != null) {",
      "parent_sha": "0a0c8079ebacfdeaab7e780db5c6d9f6160ebbe5"
    }
  },
  {
    "oid": "0af0f19faf6b1d06342b2b42eb34210b34db57f7",
    "message": "spotless",
    "date": "2023-09-15T10:06:20Z",
    "url": "https://github.com/conductor-oss/conductor/commit/0af0f19faf6b1d06342b2b42eb34210b34db57f7",
    "details": {
      "sha": "fc4abefa0e717f11c5098655a7d119480b24c06e",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/mapper/WaitTaskMapper.java",
      "status": "modified",
      "additions": 10,
      "deletions": 8,
      "changes": 18,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/0af0f19faf6b1d06342b2b42eb34210b34db57f7/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Fmapper%2FWaitTaskMapper.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/0af0f19faf6b1d06342b2b42eb34210b34db57f7/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Fmapper%2FWaitTaskMapper.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Fmapper%2FWaitTaskMapper.java?ref=0af0f19faf6b1d06342b2b42eb34210b34db57f7",
      "patch": "@@ -19,7 +19,6 @@\n import java.util.Map;\n import java.util.Optional;\n \n-import com.netflix.conductor.annotations.VisibleForTesting;\n import org.apache.commons.lang3.StringUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -84,13 +83,15 @@ public List<TaskModel> getMappedTasks(TaskMapperContext taskMapperContext) {\n         return List.of(waitTask);\n     }\n \n-\n     void setCallbackAfter(TaskModel task) {\n-        String duration = Optional.ofNullable(task.getInputData().get(DURATION_INPUT)).orElse(\"\").toString();\n-        String until = Optional.ofNullable(task.getInputData().get(UNTIL_INPUT)).orElse(\"\").toString();\n+        String duration =\n+                Optional.ofNullable(task.getInputData().get(DURATION_INPUT)).orElse(\"\").toString();\n+        String until =\n+                Optional.ofNullable(task.getInputData().get(UNTIL_INPUT)).orElse(\"\").toString();\n \n         if (StringUtils.isNotBlank(duration) && StringUtils.isNotBlank(until)) {\n-            task.setReasonForIncompletion(\"Both 'duration' and 'until' specified. Please provide only one input\");\n+            task.setReasonForIncompletion(\n+                    \"Both 'duration' and 'until' specified. Please provide only one input\");\n             task.setStatus(FAILED_WITH_TERMINAL_ERROR);\n             return;\n         }\n@@ -110,18 +111,19 @@ void setCallbackAfter(TaskModel task) {\n                 long timeInMS = expiryDate.getTime();\n                 long now = System.currentTimeMillis();\n                 long seconds = ((timeInMS - now) / 1000) + 1;\n-                if(seconds < 0) {\n+                if (seconds < 0) {\n                     seconds = 0;\n                 }\n                 task.setCallbackAfterSeconds(seconds);\n                 task.setWaitTimeout(timeInMS);\n \n             } catch (ParseException parseException) {\n-                task.setReasonForIncompletion(\"Invalid/Unsupported Wait Until format.  Provided: \" + until);\n+                task.setReasonForIncompletion(\n+                        \"Invalid/Unsupported Wait Until format.  Provided: \" + until);\n                 task.setStatus(FAILED_WITH_TERMINAL_ERROR);\n             }\n         } else {\n-            //If there is no time duration specified then the WAIT task should wait forever\n+            // If there is no time duration specified then the WAIT task should wait forever\n             task.setCallbackAfterSeconds(Integer.MAX_VALUE);\n         }\n     }",
      "parent_sha": "841624139bfebf2e908c9fe455b02d0b55d63401"
    }
  },
  {
    "oid": "70cbc788b2ca410dc2e68c3c1523af202e17f1e4",
    "message": "order of the headers",
    "date": "2017-01-12T20:29:28Z",
    "url": "https://github.com/conductor-oss/conductor/commit/70cbc788b2ca410dc2e68c3c1523af202e17f1e4",
    "details": {
      "sha": "c089d28cb632905e6d54d74704bdc6da99e8554d",
      "filename": "client/src/main/java/com/netflix/conductor/client/http/ClientBase.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/70cbc788b2ca410dc2e68c3c1523af202e17f1e4/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Fhttp%2FClientBase.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/70cbc788b2ca410dc2e68c3c1523af202e17f1e4/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Fhttp%2FClientBase.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Fhttp%2FClientBase.java?ref=70cbc788b2ca410dc2e68c3c1523af202e17f1e4",
      "patch": "@@ -182,7 +182,7 @@ protected <T> T getForEntity(String url, Object[] queryParams, GenericType<T> re\n \t}\n \t\n \tprivate Builder resource(URI URI, Object entity) {\n-\t\treturn client.resource(URI).type(MediaType.APPLICATION_JSON).entity(entity).accept(MediaType.APPLICATION_JSON, MediaType.TEXT_PLAIN);\n+\t\treturn client.resource(URI).type(MediaType.APPLICATION_JSON).entity(entity).accept(MediaType.TEXT_PLAIN, MediaType.APPLICATION_JSON);\n \t}\n \n \tprivate void handleException(Exception e) {",
      "parent_sha": "b2f2940eec37ba1edebb082fd31d890cc105d5bb"
    }
  },
  {
    "oid": "0eb8c4246c37c31404ac3cc3744e84019c15698b",
    "message": "Monitor task queue depth for system tasks",
    "date": "2020-05-24T17:18:58Z",
    "url": "https://github.com/conductor-oss/conductor/commit/0eb8c4246c37c31404ac3cc3744e84019c15698b",
    "details": {
      "sha": "1e30c8bede681016d0709f98d328e514a407873f",
      "filename": "core/src/main/java/com/netflix/conductor/service/WorkflowMonitor.java",
      "status": "modified",
      "additions": 13,
      "deletions": 4,
      "changes": 17,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/0eb8c4246c37c31404ac3cc3744e84019c15698b/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fservice%2FWorkflowMonitor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/0eb8c4246c37c31404ac3cc3744e84019c15698b/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fservice%2FWorkflowMonitor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fservice%2FWorkflowMonitor.java?ref=0eb8c4246c37c31404ac3cc3744e84019c15698b",
      "patch": "@@ -18,6 +18,7 @@\n import com.netflix.conductor.common.metadata.tasks.TaskDef;\n import com.netflix.conductor.common.metadata.workflow.WorkflowDef;\n import com.netflix.conductor.core.config.Configuration;\n+import com.netflix.conductor.core.execution.tasks.WorkflowSystemTask;\n import com.netflix.conductor.core.orchestration.ExecutionDAOFacade;\n import com.netflix.conductor.dao.MetadataDAO;\n import com.netflix.conductor.dao.QueueDAO;\n@@ -45,8 +46,6 @@ public class WorkflowMonitor {\n \tprivate final QueueDAO queueDAO;\n \tprivate final ExecutionDAOFacade executionDAOFacade;\n \n-\tprivate ScheduledExecutorService scheduledExecutorService;\n-\n \tprivate List<TaskDef> taskDefs;\n \tprivate List<WorkflowDef> workflowDefs;\n \n@@ -65,8 +64,8 @@ public WorkflowMonitor(MetadataDAO metadataDAO, QueueDAO queueDAO, ExecutionDAOF\n \t}\n \n \tpublic void init() {\n-\t\tthis.scheduledExecutorService = Executors.newScheduledThreadPool(1);\n-\t\tthis.scheduledExecutorService.scheduleWithFixedDelay(() -> {\n+\t\tScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1);\n+\t\tscheduledExecutorService.scheduleWithFixedDelay(() -> {\n \t\t\ttry {\n \t\t\t\tif (refreshCounter <= 0) {\n \t\t\t\t\tworkflowDefs = metadataDAO.getAllWorkflowDefs();\n@@ -91,6 +90,16 @@ public void init() {\n \t\t\t\t\t}\n \t\t\t\t});\n \n+\t\t\t\tWorkflowSystemTask.all()\n+\t\t\t\t\t\t.stream()\n+\t\t\t\t\t\t.filter(WorkflowSystemTask::isAsync)\n+\t\t\t\t\t\t.forEach(workflowSystemTask -> {\n+\t\t\t\t\t\t\tlong size = queueDAO.getSize(workflowSystemTask.getName());\n+\t\t\t\t\t\t\tlong inProgressCount = executionDAOFacade.getInProgressTaskCount(workflowSystemTask.getName());\n+\t\t\t\t\t\t\tMonitors.recordQueueDepth(workflowSystemTask.getName(), size, \"system\");\n+\t\t\t\t\t\t\tMonitors.recordTaskInProgress(workflowSystemTask.getName(), inProgressCount, \"system\");\n+\t\t\t\t\t\t});\n+\n \t\t\t\trefreshCounter--;\n \t\t\t} catch (Exception e) {\n \t\t\t\tLOGGER.error(\"Error while publishing scheduled metrics\", e);",
      "parent_sha": "9871639ba4e6c454141db8d571bc528f05883fbc"
    }
  },
  {
    "oid": "b0a65f29f4916471893a5b9a40153581caa313bd",
    "message": "fix flag in system task worker coordinator",
    "date": "2021-03-16T17:42:28Z",
    "url": "https://github.com/conductor-oss/conductor/commit/b0a65f29f4916471893a5b9a40153581caa313bd",
    "details": {
      "sha": "efa5c75a2607c90a1905cba1a6bb74df727ceb9e",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/tasks/SystemTaskWorkerCoordinator.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/b0a65f29f4916471893a5b9a40153581caa313bd/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FSystemTaskWorkerCoordinator.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/b0a65f29f4916471893a5b9a40153581caa313bd/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FSystemTaskWorkerCoordinator.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FSystemTaskWorkerCoordinator.java?ref=b0a65f29f4916471893a5b9a40153581caa313bd",
      "patch": "@@ -40,7 +40,7 @@\n \n @SuppressWarnings(\"SpringJavaInjectionPointsAutowiringInspection\")\n @Component\n-@ConditionalOnProperty(name=\"conductor.system-task-workers.enabled\", havingValue = \"true\", matchIfMissing = true)\n+@ConditionalOnProperty(name = \"conductor.system-task-workers.enabled\", havingValue = \"true\", matchIfMissing = true)\n public class SystemTaskWorkerCoordinator extends LifecycleAwareComponent {\n \n     private static final Logger LOGGER = LoggerFactory.getLogger(SystemTaskWorkerCoordinator.class);\n@@ -117,7 +117,7 @@ private void listen(String queueName) {\n     }\n \n     private void pollAndExecute(String queueName) {\n-        if (isRunning()) {\n+        if (!isRunning()) {\n             LOGGER.debug(\"Component stopped. Not polling for system task in queue : {}\", queueName);\n             return;\n         }",
      "parent_sha": "1285af7a1c35d67d9bd71c363bfd1b25ecd01958"
    }
  },
  {
    "oid": "fc5326b8f250d6b116e59e55b59cfd8ef5488079",
    "message": "1.7.0 (#129)\n\ndb=redis should use same configuration as dynomite for connection pooling.",
    "date": "2017-03-20T23:46:40Z",
    "url": "https://github.com/conductor-oss/conductor/commit/fc5326b8f250d6b116e59e55b59cfd8ef5488079",
    "details": {
      "sha": "d922c58fd7599299179c5cf7f7c4739767ac5f47",
      "filename": "server/src/main/java/com/netflix/conductor/server/ConductorServer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 10,
      "changes": 11,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/fc5326b8f250d6b116e59e55b59cfd8ef5488079/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorServer.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/fc5326b8f250d6b116e59e55b59cfd8ef5488079/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorServer.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fserver%2FConductorServer.java?ref=fc5326b8f250d6b116e59e55b59cfd8ef5488079",
      "patch": "@@ -51,7 +51,6 @@\n import com.netflix.dyno.jedis.DynoJedisClient;\n import com.sun.jersey.api.client.Client;\n \n-import redis.clients.jedis.Jedis;\n import redis.clients.jedis.JedisCommands;\n \n /**\n@@ -124,14 +123,7 @@ public Collection<Host> getHosts() {\n \t\t\n \t\tJedisCommands jedis = null;\n \t\tswitch(db) {\n-\t\tcase redis:\n-\t\t\t\n-\t\t\tString host = dynoHosts.get(0).getHostName();\n-\t\t\tint port = dynoHosts.get(0).getPort();\n-\t\t\tjedis = new Jedis(host, port);\n-\t\t\tlogger.info(\"Starting conductor server using standalone redis on \" + host + \":\" + port);\n-\t\t\tbreak;\n-\t\t\t\n+\t\tcase redis:\t\n \t\tcase dynomite:\n \t\t\t\n \t\t\tConnectionPoolConfigurationImpl cp = new ConnectionPoolConfigurationImpl(dynoClusterName).withTokenSupplier(new TokenMapSupplier() {\n@@ -161,7 +153,6 @@ public HostToken getTokenForHost(Host host, Set<Host> activeHosts) {\n \t\t\tbreak;\n \t\t\t\n \t\tcase memory:\n-\t\t\t\n \t\t\tjedis = new JedisMock();\n \t\t\ttry {\n \t\t\t\tEmbeddedElasticSearch.start();",
      "parent_sha": "2ae02585087f8808d2da52188d854b503339031a"
    }
  },
  {
    "oid": "8377ed738e975ba8030971d1cba7e1c25a20d310",
    "message": "Deleted logic related to terminateWorkflow\n- Assumed wrong execution flow, moving back to existing behaviour",
    "date": "2018-08-08T19:11:12Z",
    "url": "https://github.com/conductor-oss/conductor/commit/8377ed738e975ba8030971d1cba7e1c25a20d310",
    "details": {
      "sha": "cb075ce2c7d411dbcb45408abf8fb941f977f8f5",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/WorkflowExecutor.java",
      "status": "modified",
      "additions": 3,
      "deletions": 6,
      "changes": 9,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/8377ed738e975ba8030971d1cba7e1c25a20d310/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/8377ed738e975ba8030971d1cba7e1c25a20d310/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java?ref=8377ed738e975ba8030971d1cba7e1c25a20d310",
      "patch": "@@ -578,12 +578,9 @@ public void terminateWorkflow(Workflow workflow, String reason, String failureWo\n \n             try {\n \n-                WorkflowDef latestFailureWorkflow =\n-                        metadataDAO.getLatest(failureWorkflow).orElse(\n-                                Optional.ofNullable(workflow.getWorkflowDefinition())\n-                                        .orElseThrow(() ->\n-                                                new RuntimeException(\"Failure Workflow Definition not found for: \" + failureWorkflow)\n-                                        )\n+                WorkflowDef latestFailureWorkflow = metadataDAO.getLatest(failureWorkflow)\n+                        .orElseThrow(() ->\n+                                new RuntimeException(\"Failure Workflow Definition not found for: \" + failureWorkflow)\n                         );\n \n                 String failureWFId = startWorkflow(",
      "parent_sha": "c8f28aff5fe3aaca43eab595626d4ccd64f14892"
    }
  },
  {
    "oid": "5005a16eee51d4ed9d7d2ac47b3da93fe5b7c60a",
    "message": "Add unit test",
    "date": "2017-06-16T21:05:50Z",
    "url": "https://github.com/conductor-oss/conductor/commit/5005a16eee51d4ed9d7d2ac47b3da93fe5b7c60a",
    "details": {
      "sha": "a50d6228769212b8ffd274cc4126d58637647404",
      "filename": "contribs/src/test/java/com/netflix/conductor/contribs/http/TestHttpTask.java",
      "status": "modified",
      "additions": 43,
      "deletions": 4,
      "changes": 47,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/5005a16eee51d4ed9d7d2ac47b3da93fe5b7c60a/contribs%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FTestHttpTask.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/5005a16eee51d4ed9d7d2ac47b3da93fe5b7c60a/contribs%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FTestHttpTask.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/contribs%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fhttp%2FTestHttpTask.java?ref=5005a16eee51d4ed9d7d2ac47b3da93fe5b7c60a",
      "patch": "@@ -28,9 +28,11 @@\n import java.io.BufferedReader;\n import java.io.IOException;\n import java.io.PrintWriter;\n+import java.util.Arrays;\n import java.util.HashMap;\n import java.util.Map;\n import java.util.Set;\n+import java.util.stream.Collectors;\n \n import javax.servlet.ServletException;\n import javax.servlet.http.HttpServletRequest;\n@@ -57,7 +59,6 @@\n import com.netflix.conductor.core.config.Configuration;\n import com.netflix.conductor.core.execution.DeciderService;\n import com.netflix.conductor.core.execution.WorkflowExecutor;\n-import com.netflix.conductor.dao.ExecutionDAO;\n import com.netflix.conductor.dao.MetadataDAO;\n \n /**\n@@ -289,6 +290,31 @@ public void testOptional() throws Exception {\n  \t\tSystem.out.println(workflow.getTasks());\n  \t\tSystem.out.println(workflow.getStatus());\n  \t}\n+\n+ \t@Test\n+\tpublic void testOAuth() throws Exception {\n+\t\tTask task = new Task();\n+\t\tInput input = new Input();\n+\t\tinput.setUri(\"http://localhost:7009/oauth\");\n+\t\tinput.setMethod(\"POST\");\n+\t\tinput.setOauthConsumerKey(\"someKey\");\n+\t\tinput.setOauthConsumerSecret(\"someSecret\");\n+\t\ttask.getInputData().put(HttpTask.REQUEST_PARAMETER_NAME, input);\n+\n+\t\thttpTask.start(workflow, task, executor);\n+\n+\t\tMap<String, Object> response = (Map<String, Object>) task.getOutputData().get(\"response\");\n+\t\tMap<String, String> body = (Map<String, String>) response.get(\"body\");\n+\n+\t\tassertEquals(\"someKey\", body.get(\"oauth_consumer_key\"));\n+\t\tassertTrue(\"Should have OAuth nonce\", body.containsKey(\"oauth_nonce\"));\n+\t\tassertTrue(\"Should have OAuth signature\", body.containsKey(\"oauth_signature\"));\n+\t\tassertTrue(\"Should have OAuth signature method\", body.containsKey(\"oauth_signature_method\"));\n+\t\tassertTrue(\"Should have OAuth oauth_timestamp\", body.containsKey(\"oauth_timestamp\"));\n+\t\tassertTrue(\"Should have OAuth oauth_version\", body.containsKey(\"oauth_version\"));\n+\t\t\n+\t\tassertEquals(\"Task output: \" + task.getOutputData(), Status.COMPLETED, task.getStatus());\n+\t}\n \t\n \tprivate static class EchoHandler extends AbstractHandler {\n \n@@ -317,7 +343,6 @@ public void handle(String target, Request baseRequest, HttpServletRequest reques\n \t\t\t\twriter.close();\n \t\t\t} else if(request.getMethod().equals(\"POST\") && request.getRequestURI().equals(\"/post\")) {\n \t\t\t\tresponse.addHeader(\"Content-Type\", \"application/json\");\n-\t\t\t\t\n \t\t\t\tBufferedReader reader = request.getReader();\n \t\t\t\tMap<String, Object> input = om.readValue(reader, mapOfObj);\n \t\t\t\tSet<String> keys = input.keySet();\n@@ -342,8 +367,22 @@ public void handle(String target, Request baseRequest, HttpServletRequest reques\n \t\t\t\twriter.print(NUM_RESPONSE);\n \t\t\t\twriter.flush();\n \t\t\t\twriter.close();\n-\t\t\t} \n+\t\t\t} else if(request.getMethod().equals(\"POST\") && request.getRequestURI().equals(\"/oauth\")) {\n+\t\t\t\t//echo back oauth parameters generated in the Authorization header in the response\n+\t\t\t\tMap<String, String> params = parseOauthParameters(request);\n+\t\t\t\tresponse.addHeader(\"Content-Type\", \"application/json\");\n+\t\t\t\tPrintWriter writer = response.getWriter();\n+\t\t\t\twriter.print(om.writeValueAsString(params));\n+\t\t\t\twriter.flush();\n+\t\t\t\twriter.close();\n+\t\t\t}\n+\t\t}\n+\n+\t\tprivate Map<String, String> parseOauthParameters(HttpServletRequest request) {\n+\t\t\tString paramString = request.getHeader(\"Authorization\").replaceAll(\"^OAuth (.*)\", \"$1\");\n+\t\t\treturn Arrays.stream(paramString.split(\"\\\\s*,\\\\s*\"))\n+\t\t\t\t.map(pair -> pair.split(\"=\"))\n+\t\t\t\t.collect(Collectors.toMap(o -> o[0], o -> o[1].replaceAll(\"\\\"\",\"\")));\n \t\t}\n-\t\t\n \t}\n }",
      "parent_sha": "10d3fed923f8bfe4b6e839e893ca495d0facc98f"
    }
  },
  {
    "oid": "7f1f1e0984231586309eab0b2125c627164daead",
    "message": "update the constraints for the wait task",
    "date": "2023-04-23T19:39:05Z",
    "url": "https://github.com/conductor-oss/conductor/commit/7f1f1e0984231586309eab0b2125c627164daead",
    "details": {
      "sha": "888dd7a49e1aa0fc26f9211dcb18300de6a4c1cd",
      "filename": "core/src/main/java/com/netflix/conductor/validations/WorkflowTaskTypeConstraint.java",
      "status": "modified",
      "additions": 2,
      "deletions": 4,
      "changes": 6,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/7f1f1e0984231586309eab0b2125c627164daead/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fvalidations%2FWorkflowTaskTypeConstraint.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/7f1f1e0984231586309eab0b2125c627164daead/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fvalidations%2FWorkflowTaskTypeConstraint.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fvalidations%2FWorkflowTaskTypeConstraint.java?ref=7f1f1e0984231586309eab0b2125c627164daead",
      "patch": "@@ -275,11 +275,9 @@ private boolean isWaitTaskValid(\n             }\n \n             try {\n-                if (StringUtils.isNotBlank(duration)\n-                        && !(duration.startsWith(\"${\") && duration.endsWith(\"}\"))) {\n+                if (StringUtils.isNotBlank(duration) && !(duration.startsWith(\"${\"))) {\n                     DateTimeUtils.parseDuration(duration);\n-                } else if (StringUtils.isNotBlank(until)\n-                        && !(until.startsWith(\"${\") && until.endsWith(\"}\"))) {\n+                } else if (StringUtils.isNotBlank(until) && !(until.startsWith(\"${\"))) {\n                     DateTimeUtils.parseDate(until);\n                 }\n             } catch (DateTimeParseException e) {",
      "parent_sha": "31f9a7e1675f78b9533aa2c6ba7fc40b40793231"
    }
  },
  {
    "oid": "3de9aabf5221f1e04219d2e192b4c7892931e2c8",
    "message": "fix pg query",
    "date": "2024-03-18T10:22:42Z",
    "url": "https://github.com/conductor-oss/conductor/commit/3de9aabf5221f1e04219d2e192b4c7892931e2c8",
    "details": {
      "sha": "a06299599f6cf6d24657263a61570cdda3d0fbd4",
      "filename": "postgres-persistence/src/main/java/com/netflix/conductor/postgres/dao/PostgresLockDAO.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/3de9aabf5221f1e04219d2e192b4c7892931e2c8/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresLockDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/3de9aabf5221f1e04219d2e192b4c7892931e2c8/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresLockDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresLockDAO.java?ref=3de9aabf5221f1e04219d2e192b4c7892931e2c8",
      "patch": "@@ -44,15 +44,15 @@ public boolean acquireLock(String lockId, long timeToTry, long leaseTime, TimeUn\n         long endTime = System.currentTimeMillis() + unit.toMillis(timeToTry);\n         while (System.currentTimeMillis() < endTime) {\n             var sql =\n-                    \"INSERT INTO locks(lock_id, lease_expiration) VALUES (?, now() + (?::text || ' milliseconds')::interval) ON CONFLICT (lock_id) DO UPDATE SET lease_expiration = now() + (?::text || ' milliseconds')::interval WHERE lock_id = ? AND lease_expiration <= now()\";\n+                    \"INSERT INTO locks(lock_id, lease_expiration) VALUES (?, now() + (?::text || ' milliseconds')::interval) ON CONFLICT (lock_id) DO UPDATE SET lease_expiration = now() + (?::text || ' milliseconds')::interval WHERE locks.lock_id = EXCLUDED.lock_id AND lease_expiration <= now()\";\n+\n             int rowsAffected =\n                     queryWithTransaction(\n                             sql,\n                             q ->\n                                     q.addParameter(lockId)\n                                             .addParameter(unit.toMillis(leaseTime))\n                                             .addParameter(unit.toMillis(leaseTime))\n-                                            .addParameter(lockId)\n                                             .executeUpdate());\n \n             if (rowsAffected > 0) {",
      "parent_sha": "101d6811479b1d89ed641bbaff102a8ba64fa70a"
    }
  },
  {
    "oid": "c29b2b409388482feffc94622d5d451cd0be402d",
    "message": "code changes for BulkResponse in client",
    "date": "2024-11-18T09:40:55Z",
    "url": "https://github.com/conductor-oss/conductor/commit/c29b2b409388482feffc94622d5d451cd0be402d",
    "details": {
      "sha": "d27d9dc9ed149d259659d38a4b4e0750128438b4",
      "filename": "conductor-clients/java/conductor-java-sdk/conductor-client/src/main/java/com/netflix/conductor/common/model/BulkResponse.java",
      "status": "modified",
      "additions": 7,
      "deletions": 5,
      "changes": 12,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/c29b2b409388482feffc94622d5d451cd0be402d/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmodel%2FBulkResponse.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/c29b2b409388482feffc94622d5d451cd0be402d/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmodel%2FBulkResponse.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmodel%2FBulkResponse.java?ref=c29b2b409388482feffc94622d5d451cd0be402d",
      "patch": "@@ -21,15 +21,17 @@\n /**\n  * Response object to return a list of succeeded entities and a map of failed ones, including error\n  * message, for the bulk request.\n+ *\n+ * @param <T> the type of entities included in the successful results\n  */\n-public class BulkResponse {\n+public class BulkResponse<T> {\n \n     /**\n      * Key - entityId Value - error message processing this entity\n      */\n     private final Map<String, String> bulkErrorResults;\n \n-    private final List<String> bulkSuccessfulResults;\n+    private final List<T> bulkSuccessfulResults;\n \n     private final String message = \"Bulk Request has been processed.\";\n \n@@ -38,16 +40,16 @@ public BulkResponse() {\n         this.bulkErrorResults = new HashMap<>();\n     }\n \n-    public List<String> getBulkSuccessfulResults() {\n+    public List<T> getBulkSuccessfulResults() {\n         return bulkSuccessfulResults;\n     }\n \n     public Map<String, String> getBulkErrorResults() {\n         return bulkErrorResults;\n     }\n \n-    public void appendSuccessResponse(String id) {\n-        bulkSuccessfulResults.add(id);\n+    public void appendSuccessResponse(T result) {\n+        bulkSuccessfulResults.add(result);\n     }\n \n     public void appendFailedResponse(String id, String errorMessage) {",
      "parent_sha": "f878d5c0dbbdb0a9b163e4a1b56c373d2d183cd9"
    }
  },
  {
    "oid": "52f06cd6bace2d5221c4d678dd5200ce17235b30",
    "message": "timeunit ms",
    "date": "2024-03-25T12:43:51Z",
    "url": "https://github.com/conductor-oss/conductor/commit/52f06cd6bace2d5221c4d678dd5200ce17235b30",
    "details": {
      "sha": "072ec1524de305b2c28cb816786bb5f85719e3f3",
      "filename": "postgres-persistence/src/main/java/com/netflix/conductor/postgres/dao/PostgresLockDAO.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/52f06cd6bace2d5221c4d678dd5200ce17235b30/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresLockDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/52f06cd6bace2d5221c4d678dd5200ce17235b30/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresLockDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresLockDAO.java?ref=52f06cd6bace2d5221c4d678dd5200ce17235b30",
      "patch": "@@ -32,7 +32,7 @@ public PostgresLockDAO(\n \n     @Override\n     public void acquireLock(String lockId) {\n-        acquireLock(lockId, DAY_MS, DAY_MS, TimeUnit.SECONDS);\n+        acquireLock(lockId, DAY_MS, DAY_MS, TimeUnit.MILLISECONDS);\n     }\n \n     @Override",
      "parent_sha": "9b918530864d240c201337ab48e9b2f7b82c4d66"
    }
  },
  {
    "oid": "cff6aac33192cd0df1d5a7eba5fc2385b70efc08",
    "message": "Ignored test since it always fails in CI",
    "date": "2022-06-30T23:22:54Z",
    "url": "https://github.com/conductor-oss/conductor/commit/cff6aac33192cd0df1d5a7eba5fc2385b70efc08",
    "details": {
      "sha": "9ce38addc3947f10020c6ef949397f85f8621464",
      "filename": "core/src/test/java/com/netflix/conductor/core/sync/local/LocalOnlyLockTest.java",
      "status": "modified",
      "additions": 6,
      "deletions": 3,
      "changes": 9,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/cff6aac33192cd0df1d5a7eba5fc2385b70efc08/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fsync%2Flocal%2FLocalOnlyLockTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/cff6aac33192cd0df1d5a7eba5fc2385b70efc08/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fsync%2Flocal%2FLocalOnlyLockTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fsync%2Flocal%2FLocalOnlyLockTest.java?ref=cff6aac33192cd0df1d5a7eba5fc2385b70efc08",
      "patch": "@@ -14,6 +14,7 @@\n \n import java.util.concurrent.TimeUnit;\n \n+import org.junit.Ignore;\n import org.junit.Test;\n import org.springframework.boot.test.context.runner.ApplicationContextRunner;\n \n@@ -22,6 +23,8 @@\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n \n+@Ignore\n+// Test always times out in CI environment\n public class LocalOnlyLockTest {\n \n     // Lock can be global since it uses global cache internally\n@@ -40,7 +43,7 @@ public void testLockUnlock() {\n         assertEquals(localOnlyLock.cache().estimatedSize(), 0);\n     }\n \n-    @Test(timeout = 10 * 1000)\n+    @Test(timeout = 10 * 10_000)\n     public void testLockTimeout() {\n         localOnlyLock.acquireLock(\"c\", 100, 1000, TimeUnit.MILLISECONDS);\n         assertTrue(localOnlyLock.acquireLock(\"d\", 100, 1000, TimeUnit.MILLISECONDS));\n@@ -51,7 +54,7 @@ public void testLockTimeout() {\n         assertEquals(localOnlyLock.scheduledFutures().size(), 0);\n     }\n \n-    @Test(timeout = 10 * 1000)\n+    @Test(timeout = 10 * 10_000)\n     public void testLockLeaseTime() {\n         for (int i = 0; i < 10; i++) {\n             localOnlyLock.acquireLock(\"a\", 1000, 100, TimeUnit.MILLISECONDS);\n@@ -61,7 +64,7 @@ public void testLockLeaseTime() {\n         localOnlyLock.releaseLock(\"a\");\n     }\n \n-    @Test(timeout = 10 * 1000)\n+    @Test(timeout = 10 * 10_000)\n     public void testLockLeaseWithRelease() throws Exception {\n         localOnlyLock.acquireLock(\"b\", 1000, 1000, TimeUnit.MILLISECONDS);\n         localOnlyLock.releaseLock(\"b\");",
      "parent_sha": "6c496a6ddee91bb61ea188f6fc5f1381fda3a384"
    }
  },
  {
    "oid": "7a19487a7a0f778e116f29fe5a93170d26ba9a0c",
    "message": "fix ordering of params in isolated task queue producer",
    "date": "2020-11-17T00:14:50Z",
    "url": "https://github.com/conductor-oss/conductor/commit/7a19487a7a0f778e116f29fe5a93170d26ba9a0c",
    "details": {
      "sha": "46dab7916a8091a8f1d456d324f8548a9505c9a1",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/tasks/IsolatedTaskQueueProducer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/7a19487a7a0f778e116f29fe5a93170d26ba9a0c/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FIsolatedTaskQueueProducer.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/7a19487a7a0f778e116f29fe5a93170d26ba9a0c/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FIsolatedTaskQueueProducer.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FIsolatedTaskQueueProducer.java?ref=7a19487a7a0f778e116f29fe5a93170d26ba9a0c",
      "patch": "@@ -79,7 +79,7 @@ void addTaskQueues() {\n \t\tfor (TaskDef isolatedTaskDef : isolationTaskDefs) {\n \t\t\tfor (String taskType : taskTypes) {\n \t\t\t\tString taskQueue = QueueUtils.getQueueName(taskType, null,\n-\t\t\t\t\tisolatedTaskDef.getExecutionNameSpace(), isolatedTaskDef.getIsolationGroupId());\n+\t\t\t\t\tisolatedTaskDef.getIsolationGroupId(), isolatedTaskDef.getExecutionNameSpace());\n \t\t\t\tlogger.debug(\"Adding taskQueue:'{}' to system task worker coordinator\", taskQueue);\n \t\t\t\tSystemTaskWorkerCoordinator.queue.add(taskQueue);\n \t\t\t}",
      "parent_sha": "6bce3912d3ce7a39ca4010b0537a7c37d0f27091"
    }
  },
  {
    "oid": "dec0f4d467ddee27391697bb80f4f52e46ac945d",
    "message": "Handle duplicate event inserts gracefully when using Postgresql",
    "date": "2024-09-13T02:14:49Z",
    "url": "https://github.com/conductor-oss/conductor/commit/dec0f4d467ddee27391697bb80f4f52e46ac945d",
    "details": {
      "sha": "597c3c7ff5cdd03c95af375d8d8876962d7a90f9",
      "filename": "postgres-persistence/src/main/java/com/netflix/conductor/postgres/dao/PostgresExecutionDAO.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/dec0f4d467ddee27391697bb80f4f52e46ac945d/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresExecutionDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/dec0f4d467ddee27391697bb80f4f52e46ac945d/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresExecutionDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresExecutionDAO.java?ref=dec0f4d467ddee27391697bb80f4f52e46ac945d",
      "patch": "@@ -903,7 +903,8 @@ private boolean insertEventExecution(Connection connection, EventExecution event\n \n         String INSERT_EVENT_EXECUTION =\n                 \"INSERT INTO event_execution (event_handler_name, event_name, message_id, execution_id, json_data) \"\n-                        + \"VALUES (?, ?, ?, ?, ?)\";\n+                        + \"VALUES (?, ?, ?, ?, ?) \"\n+                        + \"ON CONFLICT DO NOTHING\";\n         int count =\n                 query(\n                         connection,",
      "parent_sha": "927e93d586642dcb41ad6b46b9cfe1ee08ad45ec"
    }
  },
  {
    "oid": "f2e13ddb5ade9a66aa547baa84d7699041bfb827",
    "message": "exclude empty tags from publishing",
    "date": "2016-12-20T20:47:03Z",
    "url": "https://github.com/conductor-oss/conductor/commit/f2e13ddb5ade9a66aa547baa84d7699041bfb827",
    "details": {
      "sha": "316325baf7ad74f884c25fa4bd802a28e9adb885",
      "filename": "core/src/main/java/com/netflix/conductor/metrics/Monitors.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/f2e13ddb5ade9a66aa547baa84d7699041bfb827/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmetrics%2FMonitors.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/f2e13ddb5ade9a66aa547baa84d7699041bfb827/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmetrics%2FMonitors.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmetrics%2FMonitors.java?ref=f2e13ddb5ade9a66aa547baa84d7699041bfb827",
      "patch": "@@ -131,7 +131,9 @@ private static Map<String, String> toMap(String className, String... additionalT\n \t\tfor (int j = 0; j < additionalTags.length - 1; j++) {\n \t\t\tString tk = additionalTags[j];\n \t\t\tString tv = \"\" + additionalTags[j + 1];\n-\t\t\ttags.put(tk, tv);\n+\t\t\tif(!tv.isEmpty()) {\n+\t\t\t\ttags.put(tk, tv);\t\n+\t\t\t}\t\t\t\n \t\t\tj++;\n \t\t}\n \t\treturn tags;",
      "parent_sha": "ed7c8c10dacc4239ec59d1f96b63beb4ee43f239"
    }
  },
  {
    "oid": "3d02f227d1cf556b182f8ee8e502627d4d3fbaed",
    "message": "Assign the value in seconds to postponeDurationSeconds in WAIT",
    "date": "2023-06-04T16:08:33Z",
    "url": "https://github.com/conductor-oss/conductor/commit/3d02f227d1cf556b182f8ee8e502627d4d3fbaed",
    "details": {
      "sha": "008e0f66f1bb11dc7dc9969a693b47b5e936697d",
      "filename": "core/src/main/java/com/netflix/conductor/core/reconciliation/WorkflowSweeper.java",
      "status": "modified",
      "additions": 6,
      "deletions": 6,
      "changes": 12,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/3d02f227d1cf556b182f8ee8e502627d4d3fbaed/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Freconciliation%2FWorkflowSweeper.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/3d02f227d1cf556b182f8ee8e502627d4d3fbaed/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Freconciliation%2FWorkflowSweeper.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Freconciliation%2FWorkflowSweeper.java?ref=3d02f227d1cf556b182f8ee8e502627d4d3fbaed",
      "patch": "@@ -117,12 +117,12 @@ void unack(WorkflowModel workflowModel, long workflowOffsetTimeout) {\n         long postponeDurationSeconds = 0;\n         for (TaskModel taskModel : workflowModel.getTasks()) {\n             if (taskModel.getStatus() == Status.IN_PROGRESS) {\n-                if (taskModel.getTaskType().equals(TaskType.TASK_TYPE_WAIT)\n-                        || taskModel.getTaskType().equals(TaskType.TASK_TYPE_HUMAN)) {\n-                    postponeDurationSeconds =\n-                            (taskModel.getWaitTimeout() != 0)\n-                                    ? taskModel.getWaitTimeout() + 1\n-                                    : workflowOffsetTimeout;\n+                if (taskModel.getTaskType().equals(TaskType.TASK_TYPE_WAIT)) {\n+                    long deltaInSeconds =\n+                            (System.currentTimeMillis() - taskModel.getWaitTimeout()) / 1000;\n+                    postponeDurationSeconds = (deltaInSeconds > 0) ? deltaInSeconds + 1 : 1;\n+                } else if (taskModel.getTaskType().equals(TaskType.TASK_TYPE_HUMAN)) {\n+                    postponeDurationSeconds = properties.getWorkflowOffsetTimeout().getSeconds();\n                 } else {\n                     postponeDurationSeconds =\n                             (taskModel.getResponseTimeoutSeconds() != 0)",
      "parent_sha": "4fb9806d4a955597c520f9cbeac0bad337e9f322"
    }
  },
  {
    "oid": "06bc5fd4c9a57e9f532a6ad8eab39667ce92678d",
    "message": "Optimise the postgres queries for popping messages (#76)",
    "date": "2024-02-21T12:43:23Z",
    "url": "https://github.com/conductor-oss/conductor/commit/06bc5fd4c9a57e9f532a6ad8eab39667ce92678d",
    "details": {
      "sha": "71af60838768af4b6df42422eaf265902f44961d",
      "filename": "postgres-persistence/src/main/java/com/netflix/conductor/postgres/dao/PostgresQueueDAO.java",
      "status": "modified",
      "additions": 9,
      "deletions": 35,
      "changes": 44,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/06bc5fd4c9a57e9f532a6ad8eab39667ce92678d/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresQueueDAO.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/06bc5fd4c9a57e9f532a6ad8eab39667ce92678d/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresQueueDAO.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/postgres-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresQueueDAO.java?ref=06bc5fd4c9a57e9f532a6ad8eab39667ce92678d",
      "patch": "@@ -422,17 +422,19 @@ private boolean removeMessage(Connection connection, String queueName, String me\n                 q -> q.addParameter(queueName).addParameter(messageId).executeDelete());\n     }\n \n-    private List<Message> peekMessages(Connection connection, String queueName, int count) {\n-        if (count < 1) {\n-            return Collections.emptyList();\n-        }\n+    private List<Message> popMessages(\n+            Connection connection, String queueName, int count, int timeout) {\n \n-        final String PEEK_MESSAGES =\n-                \"SELECT message_id, priority, payload FROM queue_message WHERE queue_name = ? AND popped = false AND deliver_on <= (current_timestamp + (1000 ||' microseconds')::interval) ORDER BY priority DESC, deliver_on, created_on LIMIT ? FOR UPDATE SKIP LOCKED\";\n+        String POP_QUERY =\n+                \"UPDATE queue_message SET popped = true WHERE message_id IN (\"\n+                        + \"SELECT message_id FROM queue_message WHERE queue_name = ? AND popped = false AND \"\n+                        + \"deliver_on <= (current_timestamp + (1000 ||' microseconds')::interval) \"\n+                        + \"ORDER BY priority DESC, deliver_on, created_on LIMIT ? FOR UPDATE SKIP LOCKED\"\n+                        + \") RETURNING message_id, priority, payload\";\n \n         return query(\n                 connection,\n-                PEEK_MESSAGES,\n+                POP_QUERY,\n                 p ->\n                         p.addParameter(queueName)\n                                 .addParameter(count)\n@@ -450,34 +452,6 @@ private List<Message> peekMessages(Connection connection, String queueName, int\n                                         }));\n     }\n \n-    private List<Message> popMessages(\n-            Connection connection, String queueName, int count, int timeout) {\n-        List<Message> messages = peekMessages(connection, queueName, count);\n-\n-        if (messages.isEmpty()) {\n-            return messages;\n-        }\n-\n-        List<Message> poppedMessages = new ArrayList<>();\n-        for (Message message : messages) {\n-            final String POP_MESSAGE =\n-                    \"UPDATE queue_message SET popped = true WHERE queue_name = ? AND message_id = ? AND popped = false\";\n-            int result =\n-                    query(\n-                            connection,\n-                            POP_MESSAGE,\n-                            q ->\n-                                    q.addParameter(queueName)\n-                                            .addParameter(message.getId())\n-                                            .executeUpdate());\n-\n-            if (result == 1) {\n-                poppedMessages.add(message);\n-            }\n-        }\n-        return poppedMessages;\n-    }\n-\n     @Override\n     public boolean containsMessage(String queueName, String messageId) {\n         return getWithRetriedTransactions(tx -> existsMessage(tx, queueName, messageId));",
      "parent_sha": "e04c9bf129c887d5bf227df604211b9ab51706dc"
    }
  },
  {
    "oid": "bfdf2b885be048c321a4a1db76e748382527a14d",
    "message": "Check when the channel is closed after being created",
    "date": "2021-08-09T21:14:54Z",
    "url": "https://github.com/conductor-oss/conductor/commit/bfdf2b885be048c321a4a1db76e748382527a14d",
    "details": {
      "sha": "4fee0fbfb06b157bbbc2def885b73fd7f3343847",
      "filename": "contribs/src/main/java/com/netflix/conductor/contribs/queue/amqp/AMQPObservableQueue.java",
      "status": "modified",
      "additions": 5,
      "deletions": 2,
      "changes": 7,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/bfdf2b885be048c321a4a1db76e748382527a14d/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fqueue%2Famqp%2FAMQPObservableQueue.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/bfdf2b885be048c321a4a1db76e748382527a14d/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fqueue%2Famqp%2FAMQPObservableQueue.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Fqueue%2Famqp%2FAMQPObservableQueue.java?ref=bfdf2b885be048c321a4a1db76e748382527a14d",
      "patch": "@@ -358,10 +358,13 @@ public AMQPObservableQueue build(final boolean useExchange, final String queueUR\n         }\n     }\n \n-    private Channel getOrCreateChannel() {\n+    private Channel getOrCreateChannel() throws IOException {\n         // Return the existing channel if it was created\n         if (channel != null) {\n-            return channel;\n+            if (channel.isOpen()) {\n+                return channel;\n+            }\n+            throw new IOException(\"Channel was created but is currently closed\");\n         }\n         // Channel creation is required\n         try {",
      "parent_sha": "7d85a240817347c366814f9dbd0cae6838a11c14"
    }
  },
  {
    "oid": "68b9da52ba4c32f8072d0979d912c570b0eedd8f",
    "message": "Put domain in debug logs",
    "date": "2017-05-13T00:00:01Z",
    "url": "https://github.com/conductor-oss/conductor/commit/68b9da52ba4c32f8072d0979d912c570b0eedd8f",
    "details": {
      "sha": "b82f380209bf6776539f8d5d550fcc73cca0b0db",
      "filename": "client/src/main/java/com/netflix/conductor/client/task/WorkflowTaskCoordinator.java",
      "status": "modified",
      "additions": 3,
      "deletions": 4,
      "changes": 7,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/68b9da52ba4c32f8072d0979d912c570b0eedd8f/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Ftask%2FWorkflowTaskCoordinator.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/68b9da52ba4c32f8072d0979d912c570b0eedd8f/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Ftask%2FWorkflowTaskCoordinator.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fclient%2Ftask%2FWorkflowTaskCoordinator.java?ref=68b9da52ba4c32f8072d0979d912c570b0eedd8f",
      "patch": "@@ -270,17 +270,16 @@ private void pollForTask(Worker worker) {\n \t\t\tlogger.debug(\"Worker {} has been paused. Not polling anymore!\", worker.getClass());\n \t\t\treturn;\n \t\t}\n-\t\t\n-\t\tlogger.debug(\"Polling {}, count = {} timeout = {} ms\", worker.getTaskDefName(), worker.getPollCount(), worker.getLongPollTimeoutInMS());\n+\t\tString domain = PropertyFactory.getString(worker.getTaskDefName(), DOMAIN, null);\t\t\n+\t\tlogger.debug(\"Polling {}, domain={}, count = {} timeout = {} ms\", worker.getTaskDefName(), domain, worker.getPollCount(), worker.getLongPollTimeoutInMS());\n \t\t\n \t\ttry{\n \t\t\t\n \t\t\tString taskType = worker.getTaskDefName();\n \t\t\tStopwatch sw = WorkflowTaskMetrics.pollTimer(worker.getTaskDefName());\n-\t\t\tString domain = PropertyFactory.getString(worker.getTaskDefName(), DOMAIN, null);\n \t\t\tList<Task> tasks = client.poll(taskType, domain, worker.getIdentity(), worker.getPollCount(), worker.getLongPollTimeoutInMS());\n \t\t\tsw.stop();\n-\t\t\tlogger.debug(\"Polled {} and receivd {} tasks\", worker.getTaskDefName(), tasks.size());\n+\t\t\tlogger.debug(\"Polled {}, for domain {} and receivd {} tasks\", worker.getTaskDefName(), domain, tasks.size());\n \t\t\tfor(Task task : tasks) {\n \t\t\t\tes.submit(() -> {\n \t\t\t\t\ttry {",
      "parent_sha": "211079eec4895b6cb5aa7ac1c9a691fa17f89e65"
    }
  },
  {
    "oid": "e2ea36ebd3f9660278e29d7632ddb9d5daedd050",
    "message": "Added missing tests for MetadataMapperService\n- Tests related to version population for a subworkflow task",
    "date": "2018-08-07T01:51:32Z",
    "url": "https://github.com/conductor-oss/conductor/commit/e2ea36ebd3f9660278e29d7632ddb9d5daedd050",
    "details": {
      "sha": "daa93994f495521c5e06d64537666e03b9782b7d",
      "filename": "core/src/test/java/com/netflix/conductor/metadata/MetadataMapperServiceTest.java",
      "status": "modified",
      "additions": 75,
      "deletions": 6,
      "changes": 81,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/e2ea36ebd3f9660278e29d7632ddb9d5daedd050/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmetadata%2FMetadataMapperServiceTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/e2ea36ebd3f9660278e29d7632ddb9d5daedd050/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmetadata%2FMetadataMapperServiceTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmetadata%2FMetadataMapperServiceTest.java?ref=e2ea36ebd3f9660278e29d7632ddb9d5daedd050",
      "patch": "@@ -2,9 +2,11 @@\n \n import com.google.common.collect.ImmutableList;\n import com.netflix.conductor.common.metadata.tasks.TaskDef;\n+import com.netflix.conductor.common.metadata.workflow.SubWorkflowParams;\n import com.netflix.conductor.common.metadata.workflow.TaskType;\n import com.netflix.conductor.common.metadata.workflow.WorkflowDef;\n import com.netflix.conductor.common.metadata.workflow.WorkflowTask;\n+import com.netflix.conductor.core.execution.TerminateWorkflowException;\n import com.netflix.conductor.core.metadata.MetadataMapperService;\n import com.netflix.conductor.dao.MetadataDAO;\n import org.junit.Test;\n@@ -14,6 +16,7 @@\n import org.mockito.runners.MockitoJUnitRunner;\n \n import java.util.List;\n+import java.util.Optional;\n \n import static junit.framework.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n@@ -95,19 +98,86 @@ public void testMetadataPopulationOnlyOnNecessaryWorkflowTasks() {\n     }\n \n     @Test\n-    public void testVersionPopulationForSubworkflowTaskIfNotAvailable() {\n-        // TODO\n+    public void testVersionPopulationForSubworkflowTaskIfVersionIsNotAvailable() {\n+        String nameTaskDefinition = \"taskSubworkflow6\";\n+        String workflowDefinitionName = \"subworkflow\";\n+        Integer version = 3;\n+\n+        WorkflowDef subWorkflowDefinition = createWorkflowDefinition(\"workflowDefinitionName\");\n+        subWorkflowDefinition.setVersion(version);\n+\n+        WorkflowTask workflowTask = createWorkflowTask(nameTaskDefinition);\n+        workflowTask.setWorkflowTaskType(TaskType.SUB_WORKFLOW);\n+        SubWorkflowParams subWorkflowParams = new SubWorkflowParams();\n+        subWorkflowParams.setName(workflowDefinitionName);\n+        workflowTask.setSubWorkflowParam(subWorkflowParams);\n+\n+        WorkflowDef workflowDefinition = createWorkflowDefinition(\"testMetadataPopulation\");\n+        workflowDefinition.setTasks(ImmutableList.of(workflowTask));\n+\n+        when(metadataDAO.getLatest(workflowDefinitionName)).thenReturn(Optional.of(subWorkflowDefinition));\n+\n+        metadataMapperService.populateTaskDefinitions(workflowDefinition);\n+\n+        assertEquals(1, workflowDefinition.getTasks().size());\n+        List<WorkflowTask> workflowTasks = workflowDefinition.getTasks();\n+        SubWorkflowParams params = workflowTasks.get(0).getSubWorkflowParam();\n+\n+        assertEquals(workflowDefinitionName, params.getName());\n+        assertEquals(version, params.getVersion());\n+\n+        verify(metadataDAO).getLatest(workflowDefinitionName);\n+        verifyNoMoreInteractions(metadataDAO);\n     }\n \n     @Test\n     public void testNoVersionPopulationForSubworkflowTaskIfAvailable() {\n-        // TODO\n+        String nameTaskDefinition = \"taskSubworkflow7\";\n+        String workflowDefinitionName = \"subworkflow\";\n+        Integer version = 2;\n+\n+        WorkflowTask workflowTask = createWorkflowTask(nameTaskDefinition);\n+        workflowTask.setWorkflowTaskType(TaskType.SUB_WORKFLOW);\n+        SubWorkflowParams subWorkflowParams = new SubWorkflowParams();\n+        subWorkflowParams.setName(workflowDefinitionName);\n+        subWorkflowParams.setVersion(version);\n+        workflowTask.setSubWorkflowParam(subWorkflowParams);\n+\n+        WorkflowDef workflowDefinition = createWorkflowDefinition(\"testMetadataPopulation\");\n+        workflowDefinition.setTasks(ImmutableList.of(workflowTask));\n+\n+        metadataMapperService.populateTaskDefinitions(workflowDefinition);\n+\n+        assertEquals(1, workflowDefinition.getTasks().size());\n+        List<WorkflowTask> workflowTasks = workflowDefinition.getTasks();\n+        SubWorkflowParams params = workflowTasks.get(0).getSubWorkflowParam();\n+\n+        assertEquals(workflowDefinitionName, params.getName());\n+        assertEquals(version, params.getVersion());\n+\n+        verifyZeroInteractions(metadataDAO);\n     }\n \n \n-    @Test\n+    @Test(expected = TerminateWorkflowException.class)\n     public void testExceptionWhenWorkflowDefinitionNotAvailable() {\n-        // TODO\n+        String nameTaskDefinition = \"taskSubworkflow8\";\n+        String workflowDefinitionName = \"subworkflow\";\n+\n+        WorkflowTask workflowTask = createWorkflowTask(nameTaskDefinition);\n+        workflowTask.setWorkflowTaskType(TaskType.SUB_WORKFLOW);\n+        SubWorkflowParams subWorkflowParams = new SubWorkflowParams();\n+        subWorkflowParams.setName(workflowDefinitionName);\n+        workflowTask.setSubWorkflowParam(subWorkflowParams);\n+\n+        WorkflowDef workflowDefinition = createWorkflowDefinition(\"testMetadataPopulation\");\n+        workflowDefinition.setTasks(ImmutableList.of(workflowTask));\n+\n+        when(metadataDAO.getLatest(workflowDefinitionName)).thenReturn(Optional.empty());\n+\n+        metadataMapperService.populateTaskDefinitions(workflowDefinition);\n+\n+        verify(metadataDAO).getLatest(workflowDefinitionName);\n     }\n \n \n@@ -128,5 +198,4 @@ private TaskDef createTaskDefinition(String name) {\n         TaskDef taskDefinition = new TaskDef(name);\n         return taskDefinition;\n     }\n-\n }",
      "parent_sha": "d9677335c2c5acc01f38a80b3756bdf8b0e539a3"
    }
  },
  {
    "oid": "cec44669e2c5f43bd16a6966e047bee0ae809749",
    "message": "Let do while task decided what will be the next task.",
    "date": "2019-07-12T15:14:49Z",
    "url": "https://github.com/conductor-oss/conductor/commit/cec44669e2c5f43bd16a6966e047bee0ae809749",
    "details": {
      "sha": "aed76b646aeef9ea3368f7a547c666ecd0157e11",
      "filename": "common/src/main/java/com/netflix/conductor/common/metadata/workflow/WorkflowDef.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/cec44669e2c5f43bd16a6966e047bee0ae809749/common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FWorkflowDef.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/cec44669e2c5f43bd16a6966e047bee0ae809749/common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FWorkflowDef.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FWorkflowDef.java?ref=cec44669e2c5f43bd16a6966e047bee0ae809749",
      "patch": "@@ -245,7 +245,8 @@ public WorkflowTask getNextTask(String taskReferenceName){\n \t\t\t\t return nextTask;\n \t\t\t }\n \n-\t\t\t if(task.getTaskReferenceName().equals(taskReferenceName) || task.has(taskReferenceName)){\n+\t\t\t if(task.getTaskReferenceName().equals(taskReferenceName) || (\n+\t\t\t \t\ttask.has(taskReferenceName) && !task.getType().equals(TaskType.TASK_TYPE_DO_WHILE))) {\n \t\t\t\t break;\n \t\t\t }\n \t\t}",
      "parent_sha": "062d05fb796a1bc986cc8fbc8f20c68148ccf11b"
    }
  },
  {
    "oid": "63197c9b188be498f62f3bb31aea04f1fd1e2b93",
    "message": "add a generic counter for the poller",
    "date": "2017-09-16T18:40:47Z",
    "url": "https://github.com/conductor-oss/conductor/commit/63197c9b188be498f62f3bb31aea04f1fd1e2b93",
    "details": {
      "sha": "827d7efa0b17ef97aa011b4afc8592c25bf3a2e1",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/tasks/SystemTaskWorkerCoordinator.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/63197c9b188be498f62f3bb31aea04f1fd1e2b93/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FSystemTaskWorkerCoordinator.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/63197c9b188be498f62f3bb31aea04f1fd1e2b93/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FSystemTaskWorkerCoordinator.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FSystemTaskWorkerCoordinator.java?ref=63197c9b188be498f62f3bb31aea04f1fd1e2b93",
      "patch": "@@ -136,14 +136,15 @@ private void pollAndExecute(WorkflowSystemTask systemTask) {\n \n \t\t\t// get the remaining capacity of worker queue to prevent queue full exception\n \t\t\tint realPollCount = Math.min(workerQueue.remainingCapacity(), pollCount);\n-\t\t\tif (realPollCount <= 0) {\n+\t\t\tif (realPollCount <= 0) {\t\t\t\t\n                 logger.warn(\"All workers are busy, not polling.  queue size {}, max {}\", workerQueue.size(), workerQueueSize);\n                 return;\n \t\t\t}\n \n \t\t\tString name = systemTask.getName();\n \t\t\tList<String> polled = taskQueues.pop(name, realPollCount, 200);\n \t\t\tMonitors.recordTaskPoll(name);\n+\t\t\tMonitors.recordTaskPoll(className);\n \t\t\tlogger.debug(\"Polling for {}, got {}\", name, polled.size());\n \t\t\tfor(String task : polled) {\n \t\t\t\ttry {",
      "parent_sha": "01488d76e319bf76d5513550cd93baf2926944ac"
    }
  },
  {
    "oid": "28f7cfa5ce06786a65a0ab44833acfb804cf850e",
    "message": "modified constraints",
    "date": "2024-11-25T08:40:06Z",
    "url": "https://github.com/conductor-oss/conductor/commit/28f7cfa5ce06786a65a0ab44833acfb804cf850e",
    "details": {
      "sha": "ed7c93d8614918c8e316f410299f01f221ae5beb",
      "filename": "common/src/main/java/com/netflix/conductor/common/constraints/TaskTimeoutConstraint.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/28f7cfa5ce06786a65a0ab44833acfb804cf850e/common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fconstraints%2FTaskTimeoutConstraint.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/28f7cfa5ce06786a65a0ab44833acfb804cf850e/common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fconstraints%2FTaskTimeoutConstraint.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fconstraints%2FTaskTimeoutConstraint.java?ref=28f7cfa5ce06786a65a0ab44833acfb804cf850e",
      "patch": "@@ -67,7 +67,7 @@ public boolean isValid(TaskDef taskDef, ConstraintValidatorContext context) {\n             }\n \n             // Check if timeoutSeconds is greater than totalTimeoutSeconds\n-            if (taskDef.getTimeoutSeconds() > 0\n+            if (taskDef.getTimeoutSeconds() > 0 && taskDef.getTotalTimeoutSeconds() > 0\n                     && taskDef.getTimeoutSeconds() > taskDef.getTotalTimeoutSeconds()) {\n                 valid = false;\n                 String message =",
      "parent_sha": "3f099e474848ed6445a392b799efcfeeb4c020eb"
    }
  },
  {
    "oid": "99f22b2d7f73c9c33b196a348d2a628ba8f1fe7b",
    "message": "When retrying a Workflow only one of the failed task in the fork is retried - retry all of them",
    "date": "2018-08-30T21:34:43Z",
    "url": "https://github.com/conductor-oss/conductor/commit/99f22b2d7f73c9c33b196a348d2a628ba8f1fe7b",
    "details": {
      "sha": "ae00c47c539ac831a768c7033a0ce12c763e0bf9",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/WorkflowExecutor.java",
      "status": "modified",
      "additions": 33,
      "deletions": 37,
      "changes": 70,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/99f22b2d7f73c9c33b196a348d2a628ba8f1fe7b/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/99f22b2d7f73c9c33b196a348d2a628ba8f1fe7b/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java?ref=99f22b2d7f73c9c33b196a348d2a628ba8f1fe7b",
      "patch": "@@ -253,50 +253,28 @@ public void retry(String workflowId) {\n             throw new ApplicationException(CONFLICT, \"Workflow has not started yet\");\n         }\n \n-        // First get the failed task and the cancelled task\n-        Task failedTask = null;\n-        List<Task> cancelledTasks = new ArrayList<>();\n-        for (Task t : workflow.getTasks()) {\n-            if (t.getStatus().equals(FAILED)) {\n-                failedTask = t;\n-            } else if (t.getStatus().equals(CANCELED)) {\n-                cancelledTasks.add(t);\n-            }\n-        }\n-\n-        if (failedTask != null && failedTask.getStatus().isSuccessful()) {\n+        List<Task> failedTasks = workflow.getTasks().stream()\n+                .filter(x->FAILED.equals(x.getStatus())).collect(Collectors.toList());\n+        if (failedTasks.isEmpty()) {\n             throw new ApplicationException(CONFLICT,\n-                    \"The last task has not failed!  I can only retry the last failed task.  Use restart if you want to attempt entire workflow execution again.\");\n+                    \"There are no failed tasks! Use restart if you want to attempt entire workflow execution again.\");\n         }\n-\n         List<Task> rescheduledTasks = new ArrayList<>();\n-        // Now reschedule the failed task\n-        Task taskToBeRetried = failedTask.copy();\n-        taskToBeRetried.setTaskId(IDGenerator.generate());\n-        taskToBeRetried.setRetriedTaskId(failedTask.getTaskId());\n-        taskToBeRetried.setStatus(SCHEDULED);\n-        taskToBeRetried.setRetryCount(failedTask.getRetryCount() + 1);\n-        rescheduledTasks.add(taskToBeRetried);\n+        failedTasks.forEach(failedTask -> {\n+            rescheduledTasks.add(rescheduleTask(failedTask));\n+        });\n \n-        // update the failed task in the DAO\n-        failedTask.setRetried(true);\n-        executionDAO.updateTask(failedTask);\n+\n+        List<Task> cancelledTasks = workflow.getTasks().stream()\n+                .filter(x->FAILED.equals(x.getStatus())).collect(Collectors.toList());\n \n         // Reschedule the cancelled task but if the join is cancelled set that to in progress\n-        cancelledTasks.forEach(task -> {\n-            if (task.getTaskType().equalsIgnoreCase(WorkflowTask.Type.JOIN.toString())) {\n-                task.setStatus(IN_PROGRESS);\n-                executionDAO.updateTask(task);\n+        cancelledTasks.forEach(cancelledTask -> {\n+            if (cancelledTask.getTaskType().equalsIgnoreCase(WorkflowTask.Type.JOIN.toString())) {\n+                cancelledTask.setStatus(IN_PROGRESS);\n+                executionDAO.updateTask(cancelledTask);\n             } else {\n-                Task taskToBeRescheduled = task.copy();\n-                taskToBeRescheduled.setTaskId(IDGenerator.generate());\n-                taskToBeRescheduled.setRetriedTaskId(task.getTaskId());\n-                taskToBeRescheduled.setStatus(SCHEDULED);\n-                taskToBeRescheduled.setRetryCount(task.getRetryCount() + 1);\n-                rescheduledTasks.add(taskToBeRescheduled);\n-                // since the canceled task is being retried, update this\n-                task.setRetried(true);\n-                executionDAO.updateTask(task);\n+                rescheduledTasks.add(rescheduleTask(cancelledTask));\n             }\n         });\n \n@@ -309,6 +287,24 @@ public void retry(String workflowId) {\n         decide(workflowId);\n     }\n \n+    /**\n+     * Reschedule a task\n+     * @param task failed or cancelled task\n+     * @return new instance of a task with \"SCHEDULED\" status\n+     */\n+    private Task rescheduleTask(Task task) {\n+        Task taskToBeRetried = task.copy();\n+        taskToBeRetried.setTaskId(IDGenerator.generate());\n+        taskToBeRetried.setRetriedTaskId(task.getTaskId());\n+        taskToBeRetried.setStatus(SCHEDULED);\n+        taskToBeRetried.setRetryCount(task.getRetryCount() + 1);\n+\n+        // update the failed task in the DAO\n+        task.setRetried(true);\n+        executionDAO.updateTask(task);\n+        return taskToBeRetried;\n+    }\n+\n     public Task getPendingTaskByWorkflow(String taskReferenceName, String workflowId) {\n         return executionDAO.getTasksForWorkflow(workflowId).stream()\n                 .filter(isNonTerminalTask)",
      "parent_sha": "b8fa64ba8a0735192e56c06a790dbf193d600171"
    }
  },
  {
    "oid": "62638df1a9e99f4846e661ead47c88992ffce71e",
    "message": "Rename dao event request meter name to prevent duplicates",
    "date": "2021-07-28T08:04:01Z",
    "url": "https://github.com/conductor-oss/conductor/commit/62638df1a9e99f4846e661ead47c88992ffce71e",
    "details": {
      "sha": "c01e99f521755e355d7259147cb06471e2e54f78",
      "filename": "core/src/main/java/com/netflix/conductor/metrics/Monitors.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/62638df1a9e99f4846e661ead47c88992ffce71e/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmetrics%2FMonitors.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/62638df1a9e99f4846e661ead47c88992ffce71e/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmetrics%2FMonitors.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fmetrics%2FMonitors.java?ref=62638df1a9e99f4846e661ead47c88992ffce71e",
      "patch": "@@ -295,7 +295,7 @@ public static void recordDaoRequests(String dao, String action, String taskType,\n     }\n \n     public static void recordDaoEventRequests(String dao, String action, String event) {\n-        counter(classQualifier, \"dao_requests\", \"dao\", dao, \"action\", action, \"event\", event);\n+        counter(classQualifier, \"dao_event_requests\", \"dao\", dao, \"action\", action, \"event\", event);\n     }\n \n     public static void recordDaoPayloadSize(String dao, String action, int size) {",
      "parent_sha": "5545e31843113c795dbe267765a6e9b9f4173a85"
    }
  },
  {
    "oid": "4f007b986a67aac96d0fe3ca6dfef14a16426603",
    "message": "changing logger variable name and queues property names",
    "date": "2019-03-08T21:14:51Z",
    "url": "https://github.com/conductor-oss/conductor/commit/4f007b986a67aac96d0fe3ca6dfef14a16426603",
    "details": {
      "sha": "9b9cad00266e3f52b8a1244c98bcba16b42a8acd",
      "filename": "contribs/src/main/java/com/netflix/conductor/contribs/listener/DynoQueueStatusPublisher.java",
      "status": "modified",
      "additions": 6,
      "deletions": 6,
      "changes": 12,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/4f007b986a67aac96d0fe3ca6dfef14a16426603/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Flistener%2FDynoQueueStatusPublisher.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/4f007b986a67aac96d0fe3ca6dfef14a16426603/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Flistener%2FDynoQueueStatusPublisher.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/contribs%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcontribs%2Flistener%2FDynoQueueStatusPublisher.java?ref=4f007b986a67aac96d0fe3ca6dfef14a16426603",
      "patch": "@@ -37,7 +37,7 @@\n  */\n public class DynoQueueStatusPublisher implements WorkflowStatusListener {\n \n-    private static final Logger LOG = LoggerFactory.getLogger(DynoQueueStatusPublisher.class);\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DynoQueueStatusPublisher.class);\n     private final QueueDAO queueDAO;\n     private final ObjectMapper objectMapper;\n     private final Configuration config;\n@@ -49,19 +49,19 @@ public DynoQueueStatusPublisher(QueueDAO queueDAO, ObjectMapper objectMapper, Co\n         this.queueDAO = queueDAO;\n         this.objectMapper = objectMapper;\n         this.config = config;\n-        this.successStatusQueue = config.getProperty(\"status.publisher.success.queue\", \"_callbackSuccessQueue\");\n-        this.failureStatusQueue = config.getProperty(\"status.publisher.failure.queue\", \"_callbackFailureQueue\");\n+        this.successStatusQueue = config.getProperty(\"workflowstatuslistener.publisher.success.queue\", \"_callbackSuccessQueue\");\n+        this.failureStatusQueue = config.getProperty(\"workflowstatuslistener.publisher.failure.queue\", \"_callbackFailureQueue\");\n     }\n \n     @Override\n     public void onWorkflowCompleted(Workflow workflow) {\n-        LOG.info(\"Publishing callback of workflow {} on completion \", workflow.getWorkflowId());\n+        LOGGER.info(\"Publishing callback of workflow {} on completion \", workflow.getWorkflowId());\n         queueDAO.push(successStatusQueue, Collections.singletonList(workflowToMessage(workflow)));\n     }\n \n     @Override\n     public void onWorkflowTerminated(Workflow workflow) {\n-        LOG.info(\"Publishing callback of workflow {} on termination\", workflow.getWorkflowId());\n+        LOGGER.info(\"Publishing callback of workflow {} on termination\", workflow.getWorkflowId());\n         queueDAO.push(failureStatusQueue, Collections.singletonList(workflowToMessage(workflow)));\n     }\n \n@@ -71,7 +71,7 @@ private Message workflowToMessage(Workflow workflow) {\n         try {\n             jsonWfSummary = objectMapper.writeValueAsString(summary);\n         } catch (JsonProcessingException e) {\n-            LOG.error(\"Failed to convert WorkflowSummary: {} to String. Exception: {}\", summary, e);\n+            LOGGER.error(\"Failed to convert WorkflowSummary: {} to String. Exception: {}\", summary, e);\n             throw new RuntimeException(e);\n         }\n         return new Message(workflow.getWorkflowId(), jsonWfSummary, null);",
      "parent_sha": "0031d2128d46769ccc90770c613774588ce81cb9"
    }
  },
  {
    "oid": "6965b85259282c232d52ddd71f17a23f0e2d7775",
    "message": "initial working cut.\n\nDo while along with another do while tested. Yet to write integration test.",
    "date": "2020-01-07T16:49:51Z",
    "url": "https://github.com/conductor-oss/conductor/commit/6965b85259282c232d52ddd71f17a23f0e2d7775",
    "details": {
      "sha": "f929315741b0ca7d711205af51dfd321c3cd52b6",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/DeciderService.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/6965b85259282c232d52ddd71f17a23f0e2d7775/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FDeciderService.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/6965b85259282c232d52ddd71f17a23f0e2d7775/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FDeciderService.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FDeciderService.java?ref=6965b85259282c232d52ddd71f17a23f0e2d7775",
      "patch": "@@ -190,7 +190,7 @@ private DeciderOutcome decide(final Workflow workflow, List<Task> preScheduledTa\n             if (!pendingTask.isExecuted() && !pendingTask.isRetried() && pendingTask.getStatus().isTerminal()) {\n                 pendingTask.setExecuted(true);\n                 List<Task> nextTasks = getNextTask(workflow, pendingTask);\n-                if (pendingTask.isLoopOverTask() && !nextTasks.isEmpty()) {\n+                if (pendingTask.isLoopOverTask() && nextTasks.stream().allMatch(task -> task.isLoopOverTask() && !task.getTaskType().equals(TaskType.DO_WHILE.name()))) {\n                     nextTasks = filterNextLoopOverTasks(nextTasks, pendingTask, workflow);\n                 }\n                 nextTasks.forEach(nextTask -> tasksToBeScheduled.putIfAbsent(nextTask.getReferenceTaskName(), nextTask));",
      "parent_sha": "938371aaa372a2ba0231cdc0eca490f207e3d38a"
    }
  },
  {
    "oid": "531c4e97e33c45111db26cb1b5eaf7a74d80a4a8",
    "message": "Changing the data type of priority to handle expressions.",
    "date": "2025-03-25T10:34:30Z",
    "url": "https://github.com/conductor-oss/conductor/commit/531c4e97e33c45111db26cb1b5eaf7a74d80a4a8",
    "details": {
      "sha": "4e7e23b034339218237a6d145ad48df5c51577cd",
      "filename": "conductor-clients/java/conductor-java-sdk/conductor-client/src/main/java/com/netflix/conductor/common/metadata/workflow/SubWorkflowParams.java",
      "status": "modified",
      "additions": 3,
      "deletions": 4,
      "changes": 7,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/531c4e97e33c45111db26cb1b5eaf7a74d80a4a8/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FSubWorkflowParams.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/531c4e97e33c45111db26cb1b5eaf7a74d80a4a8/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FSubWorkflowParams.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/conductor-clients%2Fjava%2Fconductor-java-sdk%2Fconductor-client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcommon%2Fmetadata%2Fworkflow%2FSubWorkflowParams.java?ref=531c4e97e33c45111db26cb1b5eaf7a74d80a4a8",
      "patch": "@@ -35,7 +35,7 @@ public class SubWorkflowParams {\n     private IdempotencyStrategy idempotencyStrategy;\n \n     // Priority of the sub workflow, not set inherits from the parent\n-    private Integer priority;\n+    private Object priority;\n \n     public String getIdempotencyKey() {\n         return idempotencyKey;\n@@ -154,11 +154,10 @@ public boolean equals(Object o) {\n         return Objects.equals(getName(), that.getName()) && Objects.equals(getVersion(), that.getVersion()) && Objects.equals(getTaskToDomain(), that.getTaskToDomain()) && Objects.equals(getWorkflowDefinition(), that.getWorkflowDefinition());\n     }\n \n-    public Integer getPriority() {\n+    public Object getPriority() {\n         return priority;\n     }\n-\n-    public void setPriority(Integer priority) {\n+    public void setPriority(Object priority) {\n         this.priority = priority;\n     }\n }",
      "parent_sha": "3a4770486c24893218ed2be64c04db69f19e4edd"
    }
  },
  {
    "oid": "81ec76fd9064f571577c2f4897693dd98c1a388e",
    "message": "#2019 updated property name to match with the new convention",
    "date": "2020-12-31T07:33:42Z",
    "url": "https://github.com/conductor-oss/conductor/commit/81ec76fd9064f571577c2f4897693dd98c1a388e",
    "details": {
      "sha": "e495d071fcb018aa09beb48576db9af152925267",
      "filename": "redis-persistence/src/main/java/com/netflix/conductor/redis/config/RedisStandaloneConfiguration.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/81ec76fd9064f571577c2f4897693dd98c1a388e/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Fconfig%2FRedisStandaloneConfiguration.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/81ec76fd9064f571577c2f4897693dd98c1a388e/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Fconfig%2FRedisStandaloneConfiguration.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/redis-persistence%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fredis%2Fconfig%2FRedisStandaloneConfiguration.java?ref=81ec76fd9064f571577c2f4897693dd98c1a388e",
      "patch": "@@ -26,7 +26,7 @@\n import redis.clients.jedis.commands.JedisCommands;\n \n @Configuration(proxyBeanMethods = false)\n-@ConditionalOnProperty(name = \"db\", havingValue = \"redis_standalone\")\n+@ConditionalOnProperty(name = \"conductor.db.type\", havingValue = \"redis_standalone\")\n public class RedisStandaloneConfiguration extends JedisCommandsConfigurer {\n \n     private static final Logger log = LoggerFactory.getLogger(RedisSentinelConfiguration.class);",
      "parent_sha": "06e90000091925bc1cdd64173b029c2fdbc3ab7f"
    }
  },
  {
    "oid": "3f0c0875e2275403aa15ee9c8b2897f84324fb46",
    "message": "test(api): added new test cases for integration tests",
    "date": "2018-08-21T01:45:49Z",
    "url": "https://github.com/conductor-oss/conductor/commit/3f0c0875e2275403aa15ee9c8b2897f84324fb46",
    "details": {
      "sha": "95be64e7ea630f6ca9342a20df6377f20a5b9c97",
      "filename": "test-harness/src/test/java/com/netflix/conductor/tests/integration/End2EndTests.java",
      "status": "modified",
      "additions": 81,
      "deletions": 1,
      "changes": 82,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/3f0c0875e2275403aa15ee9c8b2897f84324fb46/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FEnd2EndTests.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/3f0c0875e2275403aa15ee9c8b2897f84324fb46/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FEnd2EndTests.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/test-harness%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Ftests%2Fintegration%2FEnd2EndTests.java?ref=3f0c0875e2275403aa15ee9c8b2897f84324fb46",
      "patch": "@@ -40,6 +40,7 @@\n import org.junit.BeforeClass;\n import org.junit.Test;\n \n+import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n@@ -267,6 +268,31 @@ public void testInvalidResource() {\n         }\n \t}\n \n+\t@Test\n+\tpublic void testUpdateWorkflow() {\n+\t\tWorkflowDef def = new WorkflowDef();\n+\t\tdef.setName(\"testWorkflowDel\");\n+\t\tdef.setVersion(1);\n+\t\tmc.registerWorkflowDef(def);\n+\t\tdef.setVersion(2);\n+\t\tList<WorkflowDef> workflowList = new ArrayList<>();\n+\t\tworkflowList.add(def);\n+\t\tmc.updateWorkflowDefs(workflowList);\n+\t\tWorkflowDef def1 = mc.getWorkflowDef(def.getName(), 2);\n+\t\tassertNotNull(def1);\n+\n+\t\ttry{\n+\t\t\tmc.getTaskDef(\"test\");\n+\t\t} catch (ConductorClientException e) {\n+\t\t\tint statuCode = e.getStatus();\n+\t\t\tassertEquals(404, statuCode);\n+\t\t\tassertEquals(\"No such taskType found by name: test\", e.getMessage());\n+\t\t\tassertFalse(e.isRetryable());\n+\t\t}\n+\t}\n+\n+\n+\n \t@Test\n \tpublic void testStartWorkflow() {\n         StartWorkflowRequest startWorkflowRequest = new StartWorkflowRequest();\n@@ -303,5 +329,59 @@ public void testGetWorfklowNotFound() {\n             assertFalse(e.isRetryable());\n         }\n     }\n-\t\n+\n+    @Test\n+    public void testEmptyCreateWorkflowDef() {\n+        try{\n+            WorkflowDef workflowDef = new WorkflowDef();\n+            mc.registerWorkflowDef(workflowDef);\n+        } catch (ConductorClientException e){\n+            assertEquals(400, e.getStatus());\n+            assertEquals(\"Workflow name cannot be null or empty\", e.getMessage());\n+            assertFalse(e.isRetryable());\n+        }\n+    }\n+\n+    @Test\n+    public void testUpdateWorkflowDef() {\n+        try{\n+            WorkflowDef workflowDef = new WorkflowDef();\n+            List<WorkflowDef> workflowDefList = new ArrayList<>();\n+            workflowDefList.add(workflowDef);\n+            mc.updateWorkflowDefs(workflowDefList);\n+        } catch (ConductorClientException e){\n+            assertEquals(400, e.getStatus());\n+            assertEquals(\"WorkflowDef name cannot be null\", e.getMessage());\n+            assertFalse(e.isRetryable());\n+        }\n+    }\n+\n+    @Test(expected = Test.None.class /* no exception expected */)\n+    public void testGetTaskInProgress() {\n+\t    tc.getPendingTaskForWorkflow(\"test\", \"t1\");\n+    }\n+\n+    @Test\n+    public void testRemoveTaskFromTaskQueue() {\n+\t    try {\n+\t        tc.removeTaskFromQueue(\"test\", \"fakeQueue\");\n+        } catch (ConductorClientException e) {\n+\t        assertEquals(404, e.getStatus());\n+        }\n+    }\n+\n+    @Test\n+    public void testTaskByTaskId() {\n+        try {\n+            tc.getTaskDetails(\"test123\");\n+        } catch (ConductorClientException e) {\n+            assertEquals(404, e.getStatus());\n+            assertEquals(\"No such task found by taskId: test123\", e.getMessage());\n+        }\n+    }\n+\n+    @Test\n+    public void testListworkflowsByCorrelationId() {\n+\t    wc.getWorkflows(\"test\", \"test12\", false, false);\n+    }\n }",
      "parent_sha": "907de072a7de3c6144adadee1694d175eb68add3"
    }
  },
  {
    "oid": "2ab7ff290267acfad506a2569546832e19c55afb",
    "message": "Issue 1148 : Reset start time when rerunning a workflow from that task (#1156)\n\n* Issue 1148 : Reset start time when rerunning a workflow from that task\r\n\r\n* Reset starttime only for type SUBWORKFLOW\r\n\r\n* Reset task externalOutputPayloadStoragePath",
    "date": "2019-05-23T00:48:18Z",
    "url": "https://github.com/conductor-oss/conductor/commit/2ab7ff290267acfad506a2569546832e19c55afb",
    "details": {
      "sha": "5ff7fe20a4c95ddf14aa4252db44b867b5fe5a94",
      "filename": "core/src/main/java/com/netflix/conductor/core/execution/WorkflowExecutor.java",
      "status": "modified",
      "additions": 9,
      "deletions": 1,
      "changes": 10,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/2ab7ff290267acfad506a2569546832e19c55afb/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/2ab7ff290267acfad506a2569546832e19c55afb/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Fmain%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2FWorkflowExecutor.java?ref=2ab7ff290267acfad506a2569546832e19c55afb",
      "patch": "@@ -1300,9 +1300,17 @@ private boolean rerunWF(String workflowId, String taskId, Map<String, Object> ta\n                     executionDAOFacade.removeTask(task.getTaskId());\n                 }\n             }\n+            //reset fields before restarting the task\n+            rerunFromTask.setScheduledTime(System.currentTimeMillis());\n+            rerunFromTask.setStartTime(0);\n+            rerunFromTask.setUpdateTime(0);\n+            rerunFromTask.setEndTime(0);\n+            rerunFromTask.setOutputData(null);\n+            rerunFromTask.setExternalOutputPayloadStoragePath(null);\n             if (rerunFromTask.getTaskType().equalsIgnoreCase(SubWorkflow.NAME)) {\n-                // if task is sub workflow set task as IN_PROGRESS\n+                // if task is sub workflow set task as IN_PROGRESS and reset start time\n                 rerunFromTask.setStatus(IN_PROGRESS);\n+                rerunFromTask.setStartTime(System.currentTimeMillis());\n             } else {\n                 // Set the task to rerun as SCHEDULED\n                 rerunFromTask.setStatus(SCHEDULED);",
      "parent_sha": "874d56bde1a42103d8154999e99b2608b573fce9"
    }
  },
  {
    "oid": "101d6811479b1d89ed641bbaff102a8ba64fa70a",
    "message": "fix test",
    "date": "2024-03-13T09:46:43Z",
    "url": "https://github.com/conductor-oss/conductor/commit/101d6811479b1d89ed641bbaff102a8ba64fa70a",
    "details": {
      "sha": "a77a92b19f5e03bb19cce360c830aac14091d4a9",
      "filename": "postgres-persistence/src/test/java/com/netflix/conductor/postgres/dao/PostgresLockDAOTest.java",
      "status": "modified",
      "additions": 7,
      "deletions": 1,
      "changes": 8,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/101d6811479b1d89ed641bbaff102a8ba64fa70a/postgres-persistence%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresLockDAOTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/101d6811479b1d89ed641bbaff102a8ba64fa70a/postgres-persistence%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresLockDAOTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/postgres-persistence%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fpostgres%2Fdao%2FPostgresLockDAOTest.java?ref=101d6811479b1d89ed641bbaff102a8ba64fa70a",
      "patch": "@@ -29,6 +29,7 @@\n import org.springframework.boot.autoconfigure.flyway.FlywayAutoConfiguration;\n import org.springframework.boot.test.context.SpringBootTest;\n import org.springframework.test.context.ContextConfiguration;\n+import org.springframework.test.context.TestPropertySource;\n import org.springframework.test.context.junit4.SpringRunner;\n \n import com.netflix.conductor.common.config.TestObjectMapperConfiguration;\n@@ -37,14 +38,19 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertTrue;\n \n-@SpringBootTest(properties = \"spring.flyway.clean-disabled=false\")\n @RunWith(SpringRunner.class)\n @ContextConfiguration(\n         classes = {\n             TestObjectMapperConfiguration.class,\n             PostgresConfiguration.class,\n             FlywayAutoConfiguration.class\n         })\n+@TestPropertySource(\n+        properties = {\n+            \"conductor.workflow-execution-lock.type=postgres\",\n+            \"spring.flyway.clean-disabled=false\"\n+        })\n+@SpringBootTest\n public class PostgresLockDAOTest {\n \n     @Autowired private PostgresLockDAO postgresLock;",
      "parent_sha": "845e7efd6e5b7ef32cb0195a9f6a944b0f6597b0"
    }
  },
  {
    "oid": "95272563ea867243689ba0893076e96395a48d4b",
    "message": "Unit test fix.",
    "date": "2019-07-12T15:17:04Z",
    "url": "https://github.com/conductor-oss/conductor/commit/95272563ea867243689ba0893076e96395a48d4b",
    "details": {
      "sha": "a73f590de29142ee30833736479fecca6fef0803",
      "filename": "core/src/test/java/com/netflix/conductor/core/execution/tasks/DoWhileTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/conductor-oss/conductor/blob/95272563ea867243689ba0893076e96395a48d4b/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhileTest.java",
      "raw_url": "https://github.com/conductor-oss/conductor/raw/95272563ea867243689ba0893076e96395a48d4b/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhileTest.java",
      "contents_url": "https://api.github.com/repos/conductor-oss/conductor/contents/core%2Fsrc%2Ftest%2Fjava%2Fcom%2Fnetflix%2Fconductor%2Fcore%2Fexecution%2Ftasks%2FDoWhileTest.java?ref=95272563ea867243689ba0893076e96395a48d4b",
      "patch": "@@ -147,7 +147,7 @@ public void testConditionException() {\n         Mockito.doNothing().when(provider).scheduleLoopTasks(loopTask, workflow);\n         boolean success = doWhile.execute(workflow, loopTask, provider);\n         Assert.assertTrue(success);\n-        Assert.assertTrue(loopTask.getStatus() == Task.Status.FAILED);\n+        Assert.assertTrue(loopTask.getStatus() == Task.Status.FAILED_WITH_TERMINAL_ERROR);\n     }\n \n ",
      "parent_sha": "cec44669e2c5f43bd16a6966e047bee0ae809749"
    }
  }
]