[
  {
    "oid": "f8a72b987a869a2969b36c06f8a5b01f6f61ce72",
    "message": "read metadata in SimpleQueryableIndex if available to compute segment ordering (#17181)",
    "date": "2024-09-28T02:39:03Z",
    "url": "https://github.com/apache/druid/commit/f8a72b987a869a2969b36c06f8a5b01f6f61ce72",
    "details": {
      "sha": "9518876cb75ccc7f9130b3bebc03a9d996e4c960",
      "filename": "processing/src/main/java/org/apache/druid/segment/SimpleQueryableIndex.java",
      "status": "modified",
      "additions": 10,
      "deletions": 7,
      "changes": 17,
      "blob_url": "https://github.com/apache/druid/blob/f8a72b987a869a2969b36c06f8a5b01f6f61ce72/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2FSimpleQueryableIndex.java",
      "raw_url": "https://github.com/apache/druid/raw/f8a72b987a869a2969b36c06f8a5b01f6f61ce72/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2FSimpleQueryableIndex.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2FSimpleQueryableIndex.java?ref=f8a72b987a869a2969b36c06f8a5b01f6f61ce72",
      "patch": "@@ -49,6 +49,7 @@ public abstract class SimpleQueryableIndex implements QueryableIndex\n   private final Map<String, Supplier<ColumnHolder>> columns;\n   private final SmooshedFileMapper fileMapper;\n   private final Supplier<Map<String, DimensionHandler>> dimensionHandlers;\n+  private final List<OrderBy> ordering;\n \n   public SimpleQueryableIndex(\n       Interval dataInterval,\n@@ -83,6 +84,14 @@ public SimpleQueryableIndex(\n     } else {\n       this.dimensionHandlers = () -> initDimensionHandlers(availableDimensions);\n     }\n+\n+    final Metadata metadata = getMetadata();\n+    if (metadata != null && metadata.getOrdering() != null) {\n+      this.ordering = metadata.getOrdering();\n+    } else {\n+      // When sort order isn't set in metadata.drd, assume the segment is sorted by __time.\n+      this.ordering = Cursors.ascendingTimeOrder();\n+    }\n   }\n \n   @Override\n@@ -112,13 +121,7 @@ public Indexed<String> getAvailableDimensions()\n   @Override\n   public List<OrderBy> getOrdering()\n   {\n-    final Metadata metadata = getMetadata();\n-    if (metadata != null && metadata.getOrdering() != null) {\n-      return metadata.getOrdering();\n-    } else {\n-      // When sort order isn't set in metadata.drd, assume the segment is sorted by __time.\n-      return Cursors.ascendingTimeOrder();\n-    }\n+    return ordering;\n   }\n \n   @Override",
      "parent_sha": "7417ead3afab3097da2d3630d3808525fc79ae0c"
    }
  },
  {
    "oid": "26b87c9f8ef3d241a93e665fa7a53ea76fa5b443",
    "message": "Fix CachingCostBalancerStrategyFactory's constructor (#4974)\n\n* Fix CachingCostBalancerStrategyFactory's constructor\r\n\r\n* Fix CachingCostBalancerStrategyFactory not registered in Lifecycle",
    "date": "2017-10-18T21:21:54Z",
    "url": "https://github.com/apache/druid/commit/26b87c9f8ef3d241a93e665fa7a53ea76fa5b443",
    "details": {
      "sha": "999c503723ad3c9e92206839eac5523d723d0523",
      "filename": "server/src/main/java/io/druid/server/coordinator/CachingCostBalancerStrategyFactory.java",
      "status": "modified",
      "additions": 12,
      "deletions": 6,
      "changes": 18,
      "blob_url": "https://github.com/apache/druid/blob/26b87c9f8ef3d241a93e665fa7a53ea76fa5b443/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fcoordinator%2FCachingCostBalancerStrategyFactory.java",
      "raw_url": "https://github.com/apache/druid/raw/26b87c9f8ef3d241a93e665fa7a53ea76fa5b443/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fcoordinator%2FCachingCostBalancerStrategyFactory.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fcoordinator%2FCachingCostBalancerStrategyFactory.java?ref=26b87c9f8ef3d241a93e665fa7a53ea76fa5b443",
      "patch": "@@ -19,16 +19,17 @@\n \n package io.druid.server.coordinator;\n \n+import com.fasterxml.jackson.annotation.JacksonInject;\n+import com.fasterxml.jackson.annotation.JsonCreator;\n import com.google.common.base.Preconditions;\n import com.google.common.util.concurrent.ListeningExecutorService;\n-import com.google.inject.Inject;\n import com.metamx.emitter.EmittingLogger;\n import io.druid.client.ServerInventoryView;\n import io.druid.client.ServerView;\n import io.druid.java.util.common.concurrent.Execs;\n import io.druid.concurrent.LifecycleLock;\n-import io.druid.guice.ManageLifecycle;\n import io.druid.java.util.common.ISE;\n+import io.druid.java.util.common.lifecycle.Lifecycle;\n import io.druid.java.util.common.lifecycle.LifecycleStart;\n import io.druid.java.util.common.lifecycle.LifecycleStop;\n import io.druid.server.coordination.DruidServerMetadata;\n@@ -43,7 +44,6 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.TimeoutException;\n \n-@ManageLifecycle\n public class CachingCostBalancerStrategyFactory implements BalancerStrategyFactory\n {\n   private static final EmittingLogger LOG = new EmittingLogger(CachingCostBalancerStrategyFactory.class);\n@@ -55,10 +55,16 @@ public class CachingCostBalancerStrategyFactory implements BalancerStrategyFacto\n   private final ClusterCostCache.Builder clusterCostCacheBuilder = ClusterCostCache.builder();\n   private volatile boolean initialized = false;\n \n-  @Inject\n-  public CachingCostBalancerStrategyFactory(ServerInventoryView serverInventoryView)\n+  @JsonCreator\n+  public CachingCostBalancerStrategyFactory(\n+      @JacksonInject ServerInventoryView serverInventoryView,\n+      @JacksonInject Lifecycle lifecycle\n+  ) throws Exception\n   {\n     this.serverInventoryView = Preconditions.checkNotNull(serverInventoryView);\n+    // Adding to lifecycle dynamically because couldn't use @ManageLifecycle on the class,\n+    // see https://github.com/druid-io/druid/issues/4980\n+    lifecycle.addMaybeStartManagedInstance(this);\n   }\n \n   @LifecycleStart\n@@ -123,7 +129,7 @@ public void stop()\n   @Override\n   public BalancerStrategy createBalancerStrategy(final ListeningExecutorService exec)\n   {\n-    if (!lifecycleLock.awaitStarted()) {\n+    if (!lifecycleLock.awaitStarted(1, TimeUnit.MINUTES)) {\n       throw new ISE(\"CachingCostBalancerStrategyFactory is not started\");\n     }\n     if (initialized) {",
      "parent_sha": "5fc68914046c9fcc16a26e15fe798f4c9ebb330a"
    }
  },
  {
    "oid": "08a95103bdb9d1192186ff08f6192e31f65fafef",
    "message": "More NumberedShardSpec serde tests",
    "date": "2013-08-21T21:45:24Z",
    "url": "https://github.com/apache/druid/commit/08a95103bdb9d1192186ff08f6192e31f65fafef",
    "details": {
      "sha": "7f3680de8a08ccda749301390613e09830104b5c",
      "filename": "client/src/test/java/com/metamx/druid/shard/NumberedShardSpecTest.java",
      "status": "modified",
      "additions": 18,
      "deletions": 4,
      "changes": 22,
      "blob_url": "https://github.com/apache/druid/blob/08a95103bdb9d1192186ff08f6192e31f65fafef/client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fshard%2FNumberedShardSpecTest.java",
      "raw_url": "https://github.com/apache/druid/raw/08a95103bdb9d1192186ff08f6192e31f65fafef/client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fshard%2FNumberedShardSpecTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fshard%2FNumberedShardSpecTest.java?ref=08a95103bdb9d1192186ff08f6192e31f65fafef",
      "patch": "@@ -33,12 +33,26 @@\n public class NumberedShardSpecTest\n {\n   @Test\n-  public void testSerde() throws Exception\n+  public void testSerdeRoundTrip() throws Exception\n   {\n     final ObjectMapper jsonMapper = new DefaultObjectMapper();\n-    final ShardSpec spec = new NumberedShardSpec(1, 2);\n-    final ShardSpec spec2 = jsonMapper.readValue(jsonMapper.writeValueAsBytes(spec), ShardSpec.class);\n-    Assert.assertEquals(1, spec2.getPartitionNum());\n+    final ShardSpec spec = jsonMapper.readValue(\n+        jsonMapper.writeValueAsBytes(new NumberedShardSpec(1, 2)),\n+        ShardSpec.class\n+    );\n+    Assert.assertEquals(1, spec.getPartitionNum());\n+    Assert.assertEquals(2, ((NumberedShardSpec) spec).getPartitions());\n+  }\n+\n+  @Test\n+  public void testSerdeBackwardsCompat() throws Exception\n+  {\n+    final ObjectMapper jsonMapper = new DefaultObjectMapper();\n+    final ShardSpec spec = jsonMapper.readValue(\n+        \"{\\\"type\\\": \\\"numbered\\\", \\\"partition\\\": 1, \\\"partitionNum\\\": 2}\",\n+        ShardSpec.class\n+    );\n+    Assert.assertEquals(1, spec.getPartitionNum());\n     Assert.assertEquals(2, ((NumberedShardSpec) spec).getPartitions());\n   }\n ",
      "parent_sha": "9cc75804274de6f04031ceb9a7e59fdf1c5b46e9"
    }
  },
  {
    "oid": "a110eafff74af742012e658e71ac476ffef4ad18",
    "message": "build at Tue Dec 22 17:30:00 CST 2015",
    "date": "2015-12-22T23:30:00Z",
    "url": "https://github.com/apache/druid/commit/a110eafff74af742012e658e71ac476ffef4ad18",
    "details": {
      "sha": "8500e14796b593e8d7a62ff135ef0375ba4f07b9",
      "filename": "server/src/test/java/io/druid/curator/CuratorTestBase.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/a110eafff74af742012e658e71ac476ffef4ad18/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fcurator%2FCuratorTestBase.java",
      "raw_url": "https://github.com/apache/druid/raw/a110eafff74af742012e658e71ac476ffef4ad18/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fcurator%2FCuratorTestBase.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fcurator%2FCuratorTestBase.java?ref=a110eafff74af742012e658e71ac476ffef4ad18",
      "patch": "@@ -158,4 +158,4 @@ protected void tearDownServerAndCurator()\n \n }\n \n-//Build at Tue Dec 22 17:00:01 CST 2015\n+//Build at Tue Dec 22 17:30:00 CST 2015",
      "parent_sha": "4b93528334f0ddfe9c23eeee25d178be02ddb203"
    }
  },
  {
    "oid": "3134affac9e1f6fc628289f345c1f0ebfdbf6307",
    "message": "fix NPE in DirectDruidClient",
    "date": "2013-11-21T01:15:26Z",
    "url": "https://github.com/apache/druid/commit/3134affac9e1f6fc628289f345c1f0ebfdbf6307",
    "details": {
      "sha": "8befef5cabb3f0fe4ebc0994ea442e7677c40660",
      "filename": "server/src/main/java/io/druid/client/DirectDruidClient.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/3134affac9e1f6fc628289f345c1f0ebfdbf6307/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2FDirectDruidClient.java",
      "raw_url": "https://github.com/apache/druid/raw/3134affac9e1f6fc628289f345c1f0ebfdbf6307/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2FDirectDruidClient.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2FDirectDruidClient.java?ref=3134affac9e1f6fc628289f345c1f0ebfdbf6307",
      "patch": "@@ -292,7 +292,9 @@ private void init()\n     @Override\n     public void close() throws IOException\n     {\n-      jp.close();\n+      if(jp != null) {\n+        jp.close();\n+      }\n     }\n   }\n }",
      "parent_sha": "d0fe70a21f716a26bbaf6dc0e38c50e535a539e7"
    }
  },
  {
    "oid": "fe0233adf2d163f0d6afcd1c96e2ba3880d028c9",
    "message": "removing unused imports from HadoopIndexTask",
    "date": "2015-09-09T16:12:01Z",
    "url": "https://github.com/apache/druid/commit/fe0233adf2d163f0d6afcd1c96e2ba3880d028c9",
    "details": {
      "sha": "ceacc83167df6739d854fd6de0131ff7f7802b59",
      "filename": "indexing-service/src/main/java/io/druid/indexing/common/task/HadoopIndexTask.java",
      "status": "modified",
      "additions": 3,
      "deletions": 6,
      "changes": 9,
      "blob_url": "https://github.com/apache/druid/blob/fe0233adf2d163f0d6afcd1c96e2ba3880d028c9/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Fcommon%2Ftask%2FHadoopIndexTask.java",
      "raw_url": "https://github.com/apache/druid/raw/fe0233adf2d163f0d6afcd1c96e2ba3880d028c9/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Fcommon%2Ftask%2FHadoopIndexTask.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Fcommon%2Ftask%2FHadoopIndexTask.java?ref=fe0233adf2d163f0d6afcd1c96e2ba3880d028c9",
      "patch": "@@ -35,23 +35,19 @@\n import io.druid.indexer.HadoopIngestionSpec;\n import io.druid.indexer.Jobby;\n import io.druid.indexer.MetadataStorageUpdaterJobHandler;\n-import io.druid.indexer.hadoop.DatasourceIngestionSpec;\n import io.druid.indexing.common.TaskLock;\n import io.druid.indexing.common.TaskStatus;\n import io.druid.indexing.common.TaskToolbox;\n import io.druid.indexing.common.actions.LockAcquireAction;\n import io.druid.indexing.common.actions.LockTryAcquireAction;\n-import io.druid.indexing.common.actions.SegmentListUsedAction;\n import io.druid.indexing.common.actions.TaskActionClient;\n import io.druid.indexing.hadoop.OverlordActionBasedUsedSegmentLister;\n import io.druid.timeline.DataSegment;\n import java.util.Map;\n import org.joda.time.DateTime;\n import org.joda.time.Interval;\n \n-import java.io.IOException;\n import java.util.List;\n-import java.util.Map;\n import java.util.SortedSet;\n \n public class HadoopIndexTask extends HadoopTask\n@@ -75,7 +71,7 @@ private static String getTheDataSource(HadoopIngestionSpec spec)\n   /**\n    * @param spec is used by the HadoopDruidIndexerJob to set up the appropriate parameters\n    *             for creating Druid index segments. It may be modified.\n-   *             <p/>\n+   *             <p>\n    *             Here, we will ensure that the DbConnectorConfig field of the spec is set to null, such that the\n    *             job does not push a list of published segments the database. Instead, we will use the method\n    *             IndexGeneratorJob.getPublishedSegments() to simply return a list of the published\n@@ -171,7 +167,8 @@ public TaskStatus run(TaskToolbox toolbox) throws Exception\n     spec = HadoopIngestionSpec.updateSegmentListIfDatasourcePathSpecIsUsed(\n         spec,\n         jsonMapper,\n-        new OverlordActionBasedUsedSegmentLister(toolbox));\n+        new OverlordActionBasedUsedSegmentLister(toolbox)\n+    );\n \n     final String config = invokeForeignLoader(\n         \"io.druid.indexing.common.task.HadoopIndexTask$HadoopDetermineConfigInnerProcessing\",",
      "parent_sha": "07266d682a4456012f048593e14d702779404588"
    }
  },
  {
    "oid": "9fb93a3a794723d6a5aa9fbc1edd2cdcd8c33507",
    "message": "fix smile exception on error + content-type",
    "date": "2014-11-25T19:30:17Z",
    "url": "https://github.com/apache/druid/commit/9fb93a3a794723d6a5aa9fbc1edd2cdcd8c33507",
    "details": {
      "sha": "6075d628db950b25ca4f789c26bb7e3dc31e5ddc",
      "filename": "server/src/main/java/io/druid/server/QueryResource.java",
      "status": "modified",
      "additions": 7,
      "deletions": 6,
      "changes": 13,
      "blob_url": "https://github.com/apache/druid/blob/9fb93a3a794723d6a5aa9fbc1edd2cdcd8c33507/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2FQueryResource.java",
      "raw_url": "https://github.com/apache/druid/raw/9fb93a3a794723d6a5aa9fbc1edd2cdcd8c33507/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2FQueryResource.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2FQueryResource.java?ref=9fb93a3a794723d6a5aa9fbc1edd2cdcd8c33507",
      "patch": "@@ -126,6 +126,7 @@ public Response doPost(\n     String queryId = null;\n \n     final boolean isSmile = APPLICATION_SMILE.equals(req.getContentType());\n+    final String contentType = isSmile ? APPLICATION_SMILE : APPLICATION_JSON;\n \n     ObjectMapper objectMapper = isSmile ? smileMapper : jsonMapper;\n     final ObjectWriter jsonWriter = req.getParameter(\"pretty\") == null\n@@ -209,7 +210,7 @@ public void write(OutputStream outputStream) throws IOException, WebApplicationE\n                     outputStream.close();\n                   }\n                 },\n-                isSmile ? APPLICATION_SMILE : APPLICATION_JSON\n+                contentType\n         )\n             .header(\"X-Druid-Query-Id\", queryId)\n             .header(\"X-Druid-Response-Context\", jsonMapper.writeValueAsString(responseContext))\n@@ -249,10 +250,10 @@ public void write(OutputStream outputStream) throws IOException, WebApplicationE\n       catch (Exception e2) {\n         log.error(e2, \"Unable to log query [%s]!\", query);\n       }\n-      return Response.serverError().entity(\n-          jsonWriter.writeValueAsString(\n+      return Response.serverError().type(contentType).entity(\n+          jsonWriter.writeValueAsBytes(\n               ImmutableMap.of(\n-                  \"error\", e.getMessage()\n+                  \"error\", e.getMessage() == null ? \"null exception\" : e.getMessage()\n               )\n           )\n       ).build();\n@@ -285,8 +286,8 @@ public void write(OutputStream outputStream) throws IOException, WebApplicationE\n          .addData(\"peer\", req.getRemoteAddr())\n          .emit();\n \n-      return Response.serverError().entity(\n-          jsonWriter.writeValueAsString(\n+      return Response.serverError().type(contentType).entity(\n+          jsonWriter.writeValueAsBytes(\n               ImmutableMap.of(\n                   \"error\", e.getMessage() == null ? \"null exception\" : e.getMessage()\n               )",
      "parent_sha": "763967c62c2958c988cb9f0d9c4e7edc78258314"
    }
  },
  {
    "oid": "454bb034f1d0d13defda8de9e6b564ce64bf2ec6",
    "message": "Nicer toString on ListneingAnnouncerConfig (#2936)\n\n* Helps with debugging",
    "date": "2016-05-10T06:51:06Z",
    "url": "https://github.com/apache/druid/commit/454bb034f1d0d13defda8de9e6b564ce64bf2ec6",
    "details": {
      "sha": "ea964a455cac86c5a463171db33b686680b43a35",
      "filename": "server/src/main/java/io/druid/server/listener/announcer/ListeningAnnouncerConfig.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/454bb034f1d0d13defda8de9e6b564ce64bf2ec6/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Flistener%2Fannouncer%2FListeningAnnouncerConfig.java",
      "raw_url": "https://github.com/apache/druid/raw/454bb034f1d0d13defda8de9e6b564ce64bf2ec6/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Flistener%2Fannouncer%2FListeningAnnouncerConfig.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Flistener%2Fannouncer%2FListeningAnnouncerConfig.java?ref=454bb034f1d0d13defda8de9e6b564ce64bf2ec6",
      "patch": "@@ -80,7 +80,7 @@ public int hashCode()\n   public String toString()\n   {\n     return \"ListeningAnnouncerConfig{\" +\n-           \"listenersPath='\" + listenersPath + '\\'' +\n+           \"listenersPath='\" + getListenersPath() + '\\'' +\n            '}';\n   }\n ",
      "parent_sha": "a31348450fb558aa0c962ce2b503c959057a9936"
    }
  },
  {
    "oid": "f3a36621335e6205c58e37ee1a5f3fb3f0e50742",
    "message": "Fix compile error in SearchBinaryFnTest (#3201)",
    "date": "2016-06-29T14:44:45Z",
    "url": "https://github.com/apache/druid/commit/f3a36621335e6205c58e37ee1a5f3fb3f0e50742",
    "details": {
      "sha": "1ab58720bc06b51b41fefa4e04c624214dbbc3f3",
      "filename": "processing/src/test/java/io/druid/query/search/SearchBinaryFnTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/f3a36621335e6205c58e37ee1a5f3fb3f0e50742/processing%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fquery%2Fsearch%2FSearchBinaryFnTest.java",
      "raw_url": "https://github.com/apache/druid/raw/f3a36621335e6205c58e37ee1a5f3fb3f0e50742/processing%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fquery%2Fsearch%2FSearchBinaryFnTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fquery%2Fsearch%2FSearchBinaryFnTest.java?ref=f3a36621335e6205c58e37ee1a5f3fb3f0e50742",
      "patch": "@@ -297,7 +297,7 @@ public void testAlphanumericMerge()\n     );\n \n     Result<SearchResultValue> actual = new SearchBinaryFn(\n-        searchSortSpec, QueryGranularity.ALL, Integer.MAX_VALUE).apply(r1, r2);\n+        searchSortSpec, QueryGranularities.ALL, Integer.MAX_VALUE).apply(r1, r2);\n     Assert.assertEquals(expected.getTimestamp(), actual.getTimestamp());\n     assertSearchMergeResult(expected.getValue(), actual.getValue());\n   }  ",
      "parent_sha": "b24425a2805f2bf5819df3e94f1f88a1dda32ead"
    }
  },
  {
    "oid": "34d2f9ebfe27b1fa8546c18f725486682b405453",
    "message": "Queries: Restore old prepareAggregations method. (#4432)\n\nFor backwards compatibility, post-#4394.",
    "date": "2017-06-21T12:36:32Z",
    "url": "https://github.com/apache/druid/commit/34d2f9ebfe27b1fa8546c18f725486682b405453",
    "details": {
      "sha": "519ef2acc65ccec1fd8bd1db79e7cdb6f3e46ba0",
      "filename": "processing/src/main/java/io/druid/query/Queries.java",
      "status": "modified",
      "additions": 24,
      "deletions": 2,
      "changes": 26,
      "blob_url": "https://github.com/apache/druid/blob/34d2f9ebfe27b1fa8546c18f725486682b405453/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2FQueries.java",
      "raw_url": "https://github.com/apache/druid/raw/34d2f9ebfe27b1fa8546c18f725486682b405453/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2FQueries.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2FQueries.java?ref=34d2f9ebfe27b1fa8546c18f725486682b405453",
      "patch": "@@ -25,6 +25,7 @@\n import io.druid.query.aggregation.AggregatorFactory;\n import io.druid.query.aggregation.PostAggregator;\n \n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n@@ -47,14 +48,35 @@ public static List<PostAggregator> decoratePostAggregators(\n     return decorated;\n   }\n \n+  /**\n+   * Like {@link #prepareAggregations(List, List, List)} but with otherOutputNames as an empty list. Deprecated\n+   * because it makes it easy to forget to include dimensions, etc. in \"otherOutputNames\".\n+   *\n+   * @param aggFactories aggregator factories for this query\n+   * @param postAggs     post-aggregators for this query\n+   *\n+   * @return decorated post-aggregators\n+   *\n+   * @throws NullPointerException     if aggFactories is null\n+   * @throws IllegalArgumentException if there are any output name collisions or missing post-aggregator inputs\n+   */\n+  @Deprecated\n+  public static List<PostAggregator> prepareAggregations(\n+      List<AggregatorFactory> aggFactories,\n+      List<PostAggregator> postAggs\n+  )\n+  {\n+    return prepareAggregations(Collections.emptyList(), aggFactories, postAggs);\n+  }\n+\n   /**\n    * Returns decorated post-aggregators, based on original un-decorated post-aggregators. In addition, this method\n    * also verifies that there are no output name collisions, and that all of the post-aggregators' required input\n    * fields are present.\n    *\n    * @param otherOutputNames names of fields that will appear in the same output namespace as aggregators and\n-   *                         post-aggregators. For most built-in query types, this is either empty, or the list of\n-   *                         dimension output names.\n+   *                         post-aggregators, and are also assumed to be valid inputs to post-aggregators. For most\n+   *                         built-in query types, this is either empty, or the list of dimension output names.\n    * @param aggFactories     aggregator factories for this query\n    * @param postAggs         post-aggregators for this query\n    *",
      "parent_sha": "679cf277c07f9df84ea6b2fec11afc44b798341f"
    }
  },
  {
    "oid": "fbba30eb604b7354da936cc50ffa14adba238450",
    "message": "cache max data timestamp in QueryableIndexStorageAdapter so that TimestampCheckingOffset\ndoes not have to get it per cursor.",
    "date": "2015-10-12T20:34:22Z",
    "url": "https://github.com/apache/druid/commit/fbba30eb604b7354da936cc50ffa14adba238450",
    "details": {
      "sha": "dc25f21e1d3a9deac5fc1a66159c7e725df09d13",
      "filename": "processing/src/main/java/io/druid/segment/QueryableIndexStorageAdapter.java",
      "status": "modified",
      "additions": 15,
      "deletions": 8,
      "changes": 23,
      "blob_url": "https://github.com/apache/druid/blob/fbba30eb604b7354da936cc50ffa14adba238450/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2FQueryableIndexStorageAdapter.java",
      "raw_url": "https://github.com/apache/druid/raw/fbba30eb604b7354da936cc50ffa14adba238450/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2FQueryableIndexStorageAdapter.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2FQueryableIndexStorageAdapter.java?ref=fbba30eb604b7354da936cc50ffa14adba238450",
      "patch": "@@ -160,9 +160,11 @@ public Sequence<Cursor> makeCursors(Filter filter, Interval interval, QueryGranu\n   {\n     Interval actualInterval = interval;\n \n+    long minDataTimestamp = getMinTime().getMillis();\n+    long maxDataTimestamp = getMaxTime().getMillis();\n     final Interval dataInterval = new Interval(\n-        getMinTime().getMillis(),\n-        gran.next(gran.truncate(getMaxTime().getMillis()))\n+        minDataTimestamp,\n+        gran.next(gran.truncate(maxDataTimestamp))\n     );\n \n     if (!actualInterval.overlaps(dataInterval)) {\n@@ -189,7 +191,7 @@ public Sequence<Cursor> makeCursors(Filter filter, Interval interval, QueryGranu\n     }\n \n     return Sequences.filter(\n-        new CursorSequenceBuilder(index, actualInterval, gran, offset).build(),\n+        new CursorSequenceBuilder(index, actualInterval, gran, offset, maxDataTimestamp).build(),\n         Predicates.<Cursor>notNull()\n     );\n   }\n@@ -200,18 +202,21 @@ private static class CursorSequenceBuilder\n     private final Interval interval;\n     private final QueryGranularity gran;\n     private final Offset offset;\n+    private final long maxDataTimestamp;\n \n     public CursorSequenceBuilder(\n         ColumnSelector index,\n         Interval interval,\n         QueryGranularity gran,\n-        Offset offset\n+        Offset offset,\n+        long maxDataTimestamp\n     )\n     {\n       this.index = index;\n       this.interval = interval;\n       this.gran = gran;\n       this.offset = offset;\n+      this.maxDataTimestamp = maxDataTimestamp;\n     }\n \n     public Sequence<Cursor> build()\n@@ -239,8 +244,9 @@ public Cursor apply(final Long input)\n                     baseOffset.increment();\n                   }\n \n+                  long threshold = Math.min(interval.getEndMillis(), gran.next(input));\n                   final Offset offset = new TimestampCheckingOffset(\n-                      baseOffset, timestamps, Math.min(interval.getEndMillis(), gran.next(input))\n+                      baseOffset, timestamps, threshold, maxDataTimestamp < threshold\n                   );\n \n                   return new Cursor()\n@@ -677,14 +683,15 @@ private static class TimestampCheckingOffset implements Offset\n     public TimestampCheckingOffset(\n         Offset baseOffset,\n         GenericColumn timestamps,\n-        long threshold\n+        long threshold,\n+        boolean allWithinThreshold\n     )\n     {\n       this.baseOffset = baseOffset;\n       this.timestamps = timestamps;\n       this.threshold = threshold;\n       // checks if all the values are within the Threshold specified, skips timestamp lookups and checks if all values are within threshold.\n-      this.allWithinThreshold = timestamps.getLongSingleValueRow(timestamps.length() - 1) < threshold;\n+      this.allWithinThreshold = allWithinThreshold;\n     }\n \n     @Override\n@@ -696,7 +703,7 @@ public int getOffset()\n     @Override\n     public Offset clone()\n     {\n-      return new TimestampCheckingOffset(baseOffset.clone(), timestamps, threshold);\n+      return new TimestampCheckingOffset(baseOffset.clone(), timestamps, threshold, allWithinThreshold);\n     }\n \n     @Override",
      "parent_sha": "f4e4dac43433cf262e14a1a0bc816961b1f3e223"
    }
  },
  {
    "oid": "7bb7489afc7a2cc496be93ae69681b6ab13a7c66",
    "message": "Fix UnknownTypeComplexColumn#makeVectorObjectSelector",
    "date": "2020-07-01T19:02:23Z",
    "url": "https://github.com/apache/druid/commit/7bb7489afc7a2cc496be93ae69681b6ab13a7c66",
    "details": {
      "sha": "ce4db70438efebdb9f702aff6db7631cbae7d2cd",
      "filename": "processing/src/main/java/org/apache/druid/segment/column/UnknownTypeComplexColumn.java",
      "status": "modified",
      "additions": 1,
      "deletions": 18,
      "changes": 19,
      "blob_url": "https://github.com/apache/druid/blob/7bb7489afc7a2cc496be93ae69681b6ab13a7c66/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2Fcolumn%2FUnknownTypeComplexColumn.java",
      "raw_url": "https://github.com/apache/druid/raw/7bb7489afc7a2cc496be93ae69681b6ab13a7c66/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2Fcolumn%2FUnknownTypeComplexColumn.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2Fcolumn%2FUnknownTypeComplexColumn.java?ref=7bb7489afc7a2cc496be93ae69681b6ab13a7c66",
      "patch": "@@ -25,7 +25,6 @@\n import org.apache.druid.segment.vector.NilVectorSelector;\n import org.apache.druid.segment.vector.ReadableVectorOffset;\n import org.apache.druid.segment.vector.VectorObjectSelector;\n-import org.apache.druid.segment.vector.VectorSizeInspector;\n \n import javax.annotation.Nullable;\n \n@@ -38,22 +37,6 @@ public static UnknownTypeComplexColumn instance()\n     return INSTANCE;\n   }\n \n-  private static final NilVectorSelector NIL_VECTOR_SELECTOR_INSTANCE =\n-      NilVectorSelector.create(new VectorSizeInspector()\n-      {\n-        @Override\n-        public int getMaxVectorSize()\n-        {\n-          return 0;\n-        }\n-\n-        @Override\n-        public int getCurrentVectorSize()\n-        {\n-          return 0;\n-        }\n-      });\n-\n   @Override\n   public Class<?> getClazz()\n   {\n@@ -94,6 +77,6 @@ public ColumnValueSelector<?> makeColumnValueSelector(ReadableOffset offset)\n   @Override\n   public VectorObjectSelector makeVectorObjectSelector(ReadableVectorOffset offset)\n   {\n-    return NIL_VECTOR_SELECTOR_INSTANCE;\n+    return NilVectorSelector.create(offset);\n   }\n }",
      "parent_sha": "d3497a6581c69f810090f5180f1f18328b06c781"
    }
  },
  {
    "oid": "b113a34355a6fdbf115b737299aced1815138cde",
    "message": "In CPUTimeMetricQueryRunner, account CPU consumed in baseSequence.toYielder() (#3587)",
    "date": "2016-10-18T14:06:42Z",
    "url": "https://github.com/apache/druid/commit/b113a34355a6fdbf115b737299aced1815138cde",
    "details": {
      "sha": "909d89dce0ed647b746193ccd4d5feb1fc9d82be",
      "filename": "processing/src/main/java/io/druid/query/CPUTimeMetricQueryRunner.java",
      "status": "modified",
      "additions": 11,
      "deletions": 10,
      "changes": 21,
      "blob_url": "https://github.com/apache/druid/blob/b113a34355a6fdbf115b737299aced1815138cde/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2FCPUTimeMetricQueryRunner.java",
      "raw_url": "https://github.com/apache/druid/raw/b113a34355a6fdbf115b737299aced1815138cde/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2FCPUTimeMetricQueryRunner.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2FCPUTimeMetricQueryRunner.java?ref=b113a34355a6fdbf115b737299aced1815138cde",
      "patch": "@@ -85,11 +85,16 @@ public <OutType> OutType accumulate(OutType initValue, Accumulator<OutType, T> a\n           }\n \n           @Override\n-          public <OutType> Yielder<OutType> toYielder(\n-              OutType initValue, YieldingAccumulator<OutType, T> accumulator\n-          )\n+          public <OutType> Yielder<OutType> toYielder(OutType initValue, YieldingAccumulator<OutType, T> accumulator)\n           {\n-            final Yielder<OutType> delegateYielder = baseSequence.toYielder(initValue, accumulator);\n+            final long start = VMUtils.getCurrentThreadCpuTime();\n+            final Yielder<OutType> delegateYielder;\n+            try {\n+              delegateYielder = baseSequence.toYielder(initValue, accumulator);\n+            }\n+            finally {\n+              cpuTimeAccumulator.addAndGet(VMUtils.getCurrentThreadCpuTime() - start);\n+            }\n             return new Yielder<OutType>()\n             {\n               @Override\n@@ -100,9 +105,7 @@ public OutType get()\n                   return delegateYielder.get();\n                 }\n                 finally {\n-                  cpuTimeAccumulator.addAndGet(\n-                      VMUtils.getCurrentThreadCpuTime() - start\n-                  );\n+                  cpuTimeAccumulator.addAndGet(VMUtils.getCurrentThreadCpuTime() - start);\n                 }\n               }\n \n@@ -114,9 +117,7 @@ public Yielder<OutType> next(OutType initValue)\n                   return delegateYielder.next(initValue);\n                 }\n                 finally {\n-                  cpuTimeAccumulator.addAndGet(\n-                      VMUtils.getCurrentThreadCpuTime() - start\n-                  );\n+                  cpuTimeAccumulator.addAndGet(VMUtils.getCurrentThreadCpuTime() - start);\n                 }\n               }\n ",
      "parent_sha": "2c5c8198db5193aa02af76bc2b6946bdd9b6e606"
    }
  },
  {
    "oid": "58882752068b841b6737f8e4a8c9734b82089cd1",
    "message": "Repeat less repeatedly.",
    "date": "2014-08-26T20:39:09Z",
    "url": "https://github.com/apache/druid/commit/58882752068b841b6737f8e4a8c9734b82089cd1",
    "details": {
      "sha": "d41711852b3ac79b0fd7656b4ed704859cb31347",
      "filename": "services/src/main/java/io/druid/cli/CliHadoopIndexer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/58882752068b841b6737f8e4a8c9734b82089cd1/services%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fcli%2FCliHadoopIndexer.java",
      "raw_url": "https://github.com/apache/druid/raw/58882752068b841b6737f8e4a8c9734b82089cd1/services%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fcli%2FCliHadoopIndexer.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/services%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fcli%2FCliHadoopIndexer.java?ref=58882752068b841b6737f8e4a8c9734b82089cd1",
      "patch": "@@ -58,7 +58,7 @@ public class CliHadoopIndexer implements Runnable\n   private List<String> coordinates;\n \n   @Option(name = \"--no-default-hadoop\",\n-          description = \"don't pull down the default hadoop version (currently org.apache.hadoop:hadoop-client:2.3.0)\",\n+          description = \"don't pull down the default hadoop version (currently \" + DEFAULT_HADOOP_COORDINATES + \")\",\n           required = false)\n   public boolean noDefaultHadoop;\n ",
      "parent_sha": "890ba5a4caba42b6f1bafd56993da89791b0ea3e"
    }
  },
  {
    "oid": "e725ff4146e84a3f3c61b8bea10ed23d7a9e9f9b",
    "message": "1-based counts in ZkCoordinator (#4917)",
    "date": "2017-10-10T20:00:51Z",
    "url": "https://github.com/apache/druid/commit/e725ff4146e84a3f3c61b8bea10ed23d7a9e9f9b",
    "details": {
      "sha": "5747cd46fc9e76a53a7cc50d17197da77ddb02eb",
      "filename": "server/src/main/java/io/druid/server/coordination/ZkCoordinator.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/e725ff4146e84a3f3c61b8bea10ed23d7a9e9f9b/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fcoordination%2FZkCoordinator.java",
      "raw_url": "https://github.com/apache/druid/raw/e725ff4146e84a3f3c61b8bea10ed23d7a9e9f9b/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fcoordination%2FZkCoordinator.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fcoordination%2FZkCoordinator.java?ref=e725ff4146e84a3f3c61b8bea10ed23d7a9e9f9b",
      "patch": "@@ -261,7 +261,7 @@ public void loadLocalCache()\n     int ignored = 0;\n     for (int i = 0; i < segmentsToLoad.length; i++) {\n       File file = segmentsToLoad[i];\n-      log.info(\"Loading segment cache file [%d/%d][%s].\", i, segmentsToLoad.length, file);\n+      log.info(\"Loading segment cache file [%d/%d][%s].\", i + 1, segmentsToLoad.length, file);\n       try {\n         final DataSegment segment = jsonMapper.readValue(file, DataSegment.class);\n \n@@ -407,7 +407,7 @@ public void run()\n                 try {\n                   log.info(\n                       \"Loading segment[%d/%d][%s]\",\n-                      counter.getAndIncrement(),\n+                      counter.incrementAndGet(),\n                       numSegments,\n                       segment.getIdentifier()\n                   );",
      "parent_sha": "b20e3038b6a14332185f538f11fe1146f75b44d1"
    }
  },
  {
    "oid": "eb0bae49eca5013c7121994e20f9cd8146df8424",
    "message": "Update PostAggregator to be backwards compat (#12138)\n\nThis change mimics what was done in PR #11917 to\r\nfix the incompatibilities produced by #11713. #11917\r\nfixed it with AggregatorFactory by creating default\r\nmethods to allow for extensions built against old\r\njars to still work.  This does the same for PostAggregator",
    "date": "2022-01-11T10:18:14Z",
    "url": "https://github.com/apache/druid/commit/eb0bae49eca5013c7121994e20f9cd8146df8424",
    "details": {
      "sha": "399947afb344de87f19c8f29a16558004f4569f9",
      "filename": "processing/src/main/java/org/apache/druid/query/aggregation/PostAggregator.java",
      "status": "modified",
      "additions": 20,
      "deletions": 1,
      "changes": 21,
      "blob_url": "https://github.com/apache/druid/blob/eb0bae49eca5013c7121994e20f9cd8146df8424/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fquery%2Faggregation%2FPostAggregator.java",
      "raw_url": "https://github.com/apache/druid/raw/eb0bae49eca5013c7121994e20f9cd8146df8424/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fquery%2Faggregation%2FPostAggregator.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fquery%2Faggregation%2FPostAggregator.java?ref=eb0bae49eca5013c7121994e20f9cd8146df8424",
      "patch": "@@ -23,6 +23,8 @@\n import org.apache.druid.java.util.common.Cacheable;\n import org.apache.druid.segment.ColumnInspector;\n import org.apache.druid.segment.column.ColumnType;\n+import org.apache.druid.segment.column.ColumnTypeFactory;\n+import org.apache.druid.segment.column.ValueType;\n \n import javax.annotation.Nullable;\n import java.util.Comparator;\n@@ -48,10 +50,27 @@ public interface PostAggregator extends Cacheable\n   /**\n    * Return the output type of a row processed with this post aggregator. Refer to the {@link ColumnType} javadocs\n    * for details on the implications of choosing a type.\n+   *\n    * @param signature\n    */\n   @Nullable\n-  ColumnType getType(ColumnInspector signature);\n+  default ColumnType getType(ColumnInspector signature)\n+  {\n+    return ColumnTypeFactory.ofValueType(getType());\n+  }\n+\n+  /**\n+   * This method is deprecated and will be removed soon. Use {@link #getType(ColumnInspector)} instead. Do not call this\n+   * method, it will likely produce incorrect results, it exists for backwards compatibility.\n+   */\n+  @Deprecated\n+  default ValueType getType()\n+  {\n+    throw new UnsupportedOperationException(\n+        \"Do not call or implement this method, it is deprecated, use 'getType(ColumnInspector)' instead\"\n+    );\n+  }\n+\n \n   /**\n    * Allows returning an enriched post aggregator, built from contextual information available from the given map of",
      "parent_sha": "7cf9192765a47d9a63cdd87c12e964de78a08d2f"
    }
  },
  {
    "oid": "12f4147df57593736fb69c17cfad3089eee7f543",
    "message": "switch index gen job to use logging indicator",
    "date": "2014-08-21T20:28:15Z",
    "url": "https://github.com/apache/druid/commit/12f4147df57593736fb69c17cfad3089eee7f543",
    "details": {
      "sha": "bdd25f8e78dbace9a60294c910fb36cfe44e1220",
      "filename": "indexing-hadoop/src/main/java/io/druid/indexer/IndexGeneratorJob.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/druid/blob/12f4147df57593736fb69c17cfad3089eee7f543/indexing-hadoop%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexer%2FIndexGeneratorJob.java",
      "raw_url": "https://github.com/apache/druid/raw/12f4147df57593736fb69c17cfad3089eee7f543/indexing-hadoop%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexer%2FIndexGeneratorJob.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-hadoop%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexer%2FIndexGeneratorJob.java?ref=12f4147df57593736fb69c17cfad3089eee7f543",
      "patch": "@@ -40,6 +40,7 @@\n import io.druid.segment.BaseProgressIndicator;\n import io.druid.segment.IndexIO;\n import io.druid.segment.IndexMaker;\n+import io.druid.segment.LoggingProgressIndicator;\n import io.druid.segment.ProgressIndicator;\n import io.druid.segment.QueryableIndex;\n import io.druid.segment.SegmentUtils;\n@@ -296,7 +297,7 @@ protected void reduce(\n         long startTime = System.currentTimeMillis();\n \n         Set<String> allDimensionNames = Sets.newHashSet();\n-        final ProgressIndicator progressIndicator = new BaseProgressIndicator()\n+        final ProgressIndicator progressIndicator = new LoggingProgressIndicator(\"IndexGeneratorJob\")\n         {\n           @Override\n           public void progress()",
      "parent_sha": "64ff5f0880525110d2357b5851fb7cbeaae39c91"
    }
  },
  {
    "oid": "cd3b62e1238d1974598ee5ce5a0f31f296c78c33",
    "message": "fix to runtime params",
    "date": "2013-08-02T00:01:50Z",
    "url": "https://github.com/apache/druid/commit/cd3b62e1238d1974598ee5ce5a0f31f296c78c33",
    "details": {
      "sha": "7e6229374e9db6b252673c7604d4f007eaba20c6",
      "filename": "server/src/main/java/com/metamx/druid/master/DruidMasterRuntimeParams.java",
      "status": "modified",
      "additions": 11,
      "deletions": 2,
      "changes": 13,
      "blob_url": "https://github.com/apache/druid/blob/cd3b62e1238d1974598ee5ce5a0f31f296c78c33/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmaster%2FDruidMasterRuntimeParams.java",
      "raw_url": "https://github.com/apache/druid/raw/cd3b62e1238d1974598ee5ce5a0f31f296c78c33/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmaster%2FDruidMasterRuntimeParams.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmaster%2FDruidMasterRuntimeParams.java?ref=cd3b62e1238d1974598ee5ce5a0f31f296c78c33",
      "patch": "@@ -207,7 +207,8 @@ public Builder buildFromExisting()\n         mergeSegmentsLimit,\n         maxSegmentsToMove,\n         balancerReferenceTimestamp,\n-        emitStats\n+        emitStats,\n+        strategyFactory\n     );\n   }\n \n@@ -268,7 +269,8 @@ public static class Builder\n         int mergeSegmentsLimit,\n         int maxSegmentsToMove,\n         DateTime balancerReferenceTimestamp,\n-        boolean emitBalancingCostParams\n+        boolean emitBalancingCostParams,\n+        BalancerStrategyFactory strategyFactory\n     )\n     {\n       this.startTime = startTime;\n@@ -287,6 +289,7 @@ public static class Builder\n       this.maxSegmentsToMove = maxSegmentsToMove;\n       this.balancerReferenceTimestamp = balancerReferenceTimestamp;\n       this.emitBalancingCostParams = emitBalancingCostParams;\n+      this.strategyFactory=strategyFactory;\n     }\n \n     public DruidMasterRuntimeParams build()\n@@ -413,5 +416,11 @@ public Builder withBalancerReferenceTimestamp(DateTime balancerReferenceTimestam\n       this.balancerReferenceTimestamp = balancerReferenceTimestamp;\n       return this;\n     }\n+\n+    public Builder withBalancerStrategyFactory(BalancerStrategyFactory strategyFactory)\n+    {\n+      this.strategyFactory=strategyFactory;\n+      return this;\n+    }\n   }\n }",
      "parent_sha": "cf3e6da79929ae3294747fddf04d3ea4ce0337dd"
    }
  },
  {
    "oid": "e5d5050c3f93d66ee550044ad1f2b8363d5cf341",
    "message": "RemoteTaskActionClient: Log retry timer on errors",
    "date": "2013-03-14T06:06:55Z",
    "url": "https://github.com/apache/druid/commit/e5d5050c3f93d66ee550044ad1f2b8363d5cf341",
    "details": {
      "sha": "d2c761f27703bb40e0923b530b8653dbd0bfe150",
      "filename": "merger/src/main/java/com/metamx/druid/merger/common/actions/RemoteTaskActionClient.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/apache/druid/blob/e5d5050c3f93d66ee550044ad1f2b8363d5cf341/merger%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmerger%2Fcommon%2Factions%2FRemoteTaskActionClient.java",
      "raw_url": "https://github.com/apache/druid/raw/e5d5050c3f93d66ee550044ad1f2b8363d5cf341/merger%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmerger%2Fcommon%2Factions%2FRemoteTaskActionClient.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/merger%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmerger%2Fcommon%2Factions%2FRemoteTaskActionClient.java?ref=e5d5050c3f93d66ee550044ad1f2b8363d5cf341",
      "patch": "@@ -13,6 +13,7 @@\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.netflix.curator.x.discovery.ServiceInstance;\n import com.netflix.curator.x.discovery.ServiceProvider;\n+import org.joda.time.Duration;\n \n import java.io.IOException;\n import java.net.URI;\n@@ -89,7 +90,9 @@ public <RetType> RetType submit(TaskAction<RetType> taskAction) throws IOExcepti\n           throw e;\n         } else {\n           try {\n-            Thread.sleep(retryPolicy.getAndIncrementRetryDelay().getMillis());\n+            final long sleepTime = retryPolicy.getAndIncrementRetryDelay().getMillis();\n+            log.info(\"Will try again in %s.\", new Duration(sleepTime).toString());\n+            Thread.sleep(sleepTime);\n           }\n           catch (InterruptedException e2) {\n             throw Throwables.propagate(e2);",
      "parent_sha": "df8e4d40615234eb4ba6635a4f19757a3678de9f"
    }
  },
  {
    "oid": "48538d045409f3d996bc06fe3a191ca7ccfb05af",
    "message": "1) Fix bug in index converter when column has cardinality 0",
    "date": "2013-02-15T04:59:17Z",
    "url": "https://github.com/apache/druid/commit/48538d045409f3d996bc06fe3a191ca7ccfb05af",
    "details": {
      "sha": "5a2c7307ddd79f0e47ea68d719d55346d423fe1e",
      "filename": "index-common/src/main/java/com/metamx/druid/index/v1/IndexIO.java",
      "status": "modified",
      "additions": 34,
      "deletions": 25,
      "changes": 59,
      "blob_url": "https://github.com/apache/druid/blob/48538d045409f3d996bc06fe3a191ca7ccfb05af/index-common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Findex%2Fv1%2FIndexIO.java",
      "raw_url": "https://github.com/apache/druid/raw/48538d045409f3d996bc06fe3a191ca7ccfb05af/index-common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Findex%2Fv1%2FIndexIO.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/index-common%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Findex%2Fv1%2FIndexIO.java?ref=48538d045409f3d996bc06fe3a191ca7ccfb05af",
      "patch": "@@ -370,6 +370,7 @@ public static void convertV8toV9(File v8Dir, File v9Dir) throws IOException\n       }\n \n       LinkedHashSet<String> skippedFiles = Sets.newLinkedHashSet();\n+      Set<String> skippedDimensions = Sets.newLinkedHashSet();\n       for (String filename : v8SmooshedFiles.getInternalFilenames()) {\n         log.info(\"Processing file[%s]\", filename);\n         if (filename.startsWith(\"dim_\")) {\n@@ -392,6 +393,12 @@ public static void convertV8toV9(File v8Dir, File v9Dir) throws IOException\n               dimBuffer, GenericIndexed.stringStrategy\n           );\n \n+          if (dictionary.size() == 0) {\n+            log.info(\"Dimension[%s] had cardinality 0, equivalent to no column, so skipping.\", dimension);\n+            skippedDimensions.add(dimension);\n+            continue;\n+          }\n+\n           VSizeIndexedInts singleValCol = null;\n           VSizeIndexed multiValCol = VSizeIndexed.readFromByteBuffer(dimBuffer.asReadOnlyBuffer());\n           GenericIndexed<ImmutableConciseSet> bitmaps = bitmapIndexes.get(dimension);\n@@ -555,35 +562,37 @@ public int size()\n           channel.write(ByteBuffer.wrap(specBytes));\n           serdeficator.write(channel);\n           channel.close();\n-        } else if (\"index.drd\".equals(filename)) {\n-          final ByteBuffer indexBuffer = v8SmooshedFiles.mapFile(filename);\n-\n-          indexBuffer.get(); // Skip the version byte\n-          final GenericIndexed<String> dims = GenericIndexed.read(\n-              indexBuffer, GenericIndexed.stringStrategy\n-          );\n-          final GenericIndexed<String> availableMetrics = GenericIndexed.read(\n-              indexBuffer, GenericIndexed.stringStrategy\n-          );\n-          final Interval dataInterval = new Interval(serializerUtils.readString(indexBuffer));\n-\n-          Set<String> columns = Sets.newTreeSet();\n-          columns.addAll(Lists.newArrayList(dims));\n-          columns.addAll(Lists.newArrayList(availableMetrics));\n-\n-          GenericIndexed<String> cols = GenericIndexed.fromIterable(columns, GenericIndexed.stringStrategy);\n-\n-          final int numBytes = cols.getSerializedSize() + dims.getSerializedSize() + 16;\n-          final SmooshedWriter writer = v9Smoosher.addWithSmooshedWriter(\"index.drd\", numBytes);\n-          cols.writeToChannel(writer);\n-          dims.writeToChannel(writer);\n-          serializerUtils.writeLong(writer, dataInterval.getStartMillis());\n-          serializerUtils.writeLong(writer, dataInterval.getEndMillis());\n-          writer.close();\n         } else {\n           skippedFiles.add(filename);\n         }\n       }\n+\n+      final ByteBuffer indexBuffer = v8SmooshedFiles.mapFile(\"index.drd\");\n+\n+      indexBuffer.get(); // Skip the version byte\n+      final GenericIndexed<String> dims = GenericIndexed.read(\n+          indexBuffer, GenericIndexed.stringStrategy\n+      );\n+      final GenericIndexed<String> availableMetrics = GenericIndexed.read(\n+          indexBuffer, GenericIndexed.stringStrategy\n+      );\n+      final Interval dataInterval = new Interval(serializerUtils.readString(indexBuffer));\n+\n+      Set<String> columns = Sets.newTreeSet();\n+      columns.addAll(Lists.newArrayList(dims));\n+      columns.addAll(Lists.newArrayList(availableMetrics));\n+      columns.removeAll(skippedDimensions);\n+\n+      GenericIndexed<String> cols = GenericIndexed.fromIterable(columns, GenericIndexed.stringStrategy);\n+\n+      final int numBytes = cols.getSerializedSize() + dims.getSerializedSize() + 16;\n+      final SmooshedWriter writer = v9Smoosher.addWithSmooshedWriter(\"index.drd\", numBytes);\n+      cols.writeToChannel(writer);\n+      dims.writeToChannel(writer);\n+      serializerUtils.writeLong(writer, dataInterval.getStartMillis());\n+      serializerUtils.writeLong(writer, dataInterval.getEndMillis());\n+      writer.close();\n+\n       log.info(\"Skipped files[%s]\", skippedFiles);\n \n       v9Smoosher.close();",
      "parent_sha": "4f11eb5209a98e909fc3aa0b6b08cc9ed6463c32"
    }
  },
  {
    "oid": "54f00479cc8e950faddd295288749e762c037da7",
    "message": "add explicit null check for moving tasks from pending to running",
    "date": "2013-08-21T20:02:35Z",
    "url": "https://github.com/apache/druid/commit/54f00479cc8e950faddd295288749e762c037da7",
    "details": {
      "sha": "b7774a9b419e7841f5c7b8219fa7b43c7dfd195d",
      "filename": "indexing-service/src/main/java/com/metamx/druid/indexing/coordinator/RemoteTaskRunner.java",
      "status": "modified",
      "additions": 16,
      "deletions": 8,
      "changes": 24,
      "blob_url": "https://github.com/apache/druid/blob/54f00479cc8e950faddd295288749e762c037da7/indexing-service%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Findexing%2Fcoordinator%2FRemoteTaskRunner.java",
      "raw_url": "https://github.com/apache/druid/raw/54f00479cc8e950faddd295288749e762c037da7/indexing-service%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Findexing%2Fcoordinator%2FRemoteTaskRunner.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Findexing%2Fcoordinator%2FRemoteTaskRunner.java?ref=54f00479cc8e950faddd295288749e762c037da7",
      "patch": "@@ -521,7 +521,15 @@ private void announceTask(Worker theWorker, RemoteTaskRunnerWorkItem taskRunnerW\n         );\n     }\n \n-    runningTasks.put(task.getId(), pendingTasks.remove(task.getId()).withWorker(theWorker));\n+    RemoteTaskRunnerWorkItem workItem = pendingTasks.remove(task.getId());\n+    if (workItem == null) {\n+      log.makeAlert(\"WTF?! Got a null work item from pending tasks?! How can this be?!\")\n+         .addData(\"taskId\", task.getId())\n+         .emit();\n+      return;\n+    }\n+\n+    runningTasks.put(task.getId(), workItem.withWorker(theWorker));\n     log.info(\"Task %s switched from pending to running\", task.getId());\n \n     // Syncing state with Zookeeper - don't assign new tasks until the task we just assigned is actually running\n@@ -613,10 +621,10 @@ public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) th\n                       taskId = ZKPaths.getNodeFromPath(event.getData().getPath());\n                       taskRunnerWorkItem = runningTasks.get(taskId);\n                       if (taskRunnerWorkItem != null) {\n-                        log.info(\"Task %s just disappeared!\", taskId);\n+                        log.info(\"Task[%s] just disappeared!\", taskId);\n                         taskRunnerWorkItem.setResult(TaskStatus.failure(taskRunnerWorkItem.getTask().getId()));\n                       } else {\n-                        log.warn(\"Task %s just disappeared but I didn't know about it?!\", taskId);\n+                        log.warn(\"Task[%s] just disappeared but I didn't know about it?!\", taskId);\n                       }\n                       break;\n                   }\n@@ -661,11 +669,11 @@ private void removeWorker(final Worker worker)\n         List<String> tasksToFail = Lists.newArrayList(\n             cf.getChildren().forPath(JOINER.join(config.getIndexerTaskPath(), worker.getHost()))\n         );\n-        log.info(\"%s: Found %d tasks assigned\", worker.getHost(), tasksToFail.size());\n+        log.info(\"[%s]: Found %d tasks assigned\", worker.getHost(), tasksToFail.size());\n \n         for (Map.Entry<String, RemoteTaskRunnerWorkItem> entry : runningTasks.entrySet()) {\n           if (entry.getValue().getWorker().getHost().equalsIgnoreCase(worker.getHost())) {\n-            log.info(\"%s: Found %s running\", worker.getHost(), entry.getKey());\n+            log.info(\"[%s]: Found [%s] running\", worker.getHost(), entry.getKey());\n             tasksToFail.add(entry.getKey());\n           }\n         }\n@@ -678,10 +686,10 @@ private void removeWorker(final Worker worker)\n               cf.delete().guaranteed().forPath(taskPath);\n             }\n \n-            log.info(\"Failing task %s\", assignedTask);\n+            log.info(\"Failing task[%s]\", assignedTask);\n             taskRunnerWorkItem.setResult(TaskStatus.failure(taskRunnerWorkItem.getTask().getId()));\n           } else {\n-            log.warn(\"RemoteTaskRunner has no knowledge of task %s\", assignedTask);\n+            log.warn(\"RemoteTaskRunner has no knowledge of task[%s]\", assignedTask);\n           }\n         }\n       }\n@@ -693,7 +701,7 @@ private void removeWorker(final Worker worker)\n           zkWorker.close();\n         }\n         catch (Exception e) {\n-          log.error(e, \"Exception closing worker %s!\", worker.getHost());\n+          log.error(e, \"Exception closing worker[%s]!\", worker.getHost());\n         }\n         zkWorkers.remove(worker.getHost());\n       }",
      "parent_sha": "88661b26a04dd89fb914dc3f780d592576d0d634"
    }
  },
  {
    "oid": "b7426cd0b299f107fdeb51a194f7c3847a7d1b23",
    "message": "Make RabbitMQ Firehose resilient to broker deconnections",
    "date": "2014-05-22T15:39:50Z",
    "url": "https://github.com/apache/druid/commit/b7426cd0b299f107fdeb51a194f7c3847a7d1b23",
    "details": {
      "sha": "71e53b5089c55a712a847262f5cf29fbd967ae1d",
      "filename": "rabbitmq/src/main/java/io/druid/firehose/rabbitmq/RabbitMQFirehoseFactory.java",
      "status": "modified",
      "additions": 45,
      "deletions": 2,
      "changes": 47,
      "blob_url": "https://github.com/apache/druid/blob/b7426cd0b299f107fdeb51a194f7c3847a7d1b23/rabbitmq%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Ffirehose%2Frabbitmq%2FRabbitMQFirehoseFactory.java",
      "raw_url": "https://github.com/apache/druid/raw/b7426cd0b299f107fdeb51a194f7c3847a7d1b23/rabbitmq%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Ffirehose%2Frabbitmq%2FRabbitMQFirehoseFactory.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/rabbitmq%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Ffirehose%2Frabbitmq%2FRabbitMQFirehoseFactory.java?ref=b7426cd0b299f107fdeb51a194f7c3847a7d1b23",
      "patch": "@@ -25,9 +25,13 @@\n import com.rabbitmq.client.Channel;\n import com.rabbitmq.client.Connection;\n import com.rabbitmq.client.ConnectionFactory;\n-import com.rabbitmq.client.QueueingConsumer;\n+import com.rabbitmq.client.DefaultConsumer;\n+import com.rabbitmq.client.Envelope;\n+import com.rabbitmq.client.AMQP;\n+import com.rabbitmq.client.QueueingConsumer.Delivery;\n import com.rabbitmq.client.ShutdownListener;\n import com.rabbitmq.client.ShutdownSignalException;\n+import com.rabbitmq.client.ConsumerCancelledException;\n import io.druid.data.input.ByteBufferInputRowParser;\n import io.druid.data.input.Firehose;\n import io.druid.data.input.FirehoseFactory;\n@@ -40,6 +44,8 @@\n import net.jodah.lyra.util.Duration;\n \n import java.io.IOException;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n \n /**\n  * A FirehoseFactory for RabbitMQ.\n@@ -179,7 +185,7 @@ public void shutdownCompleted(ShutdownSignalException cause)\n        * Storing the latest delivery as a member variable should be safe since this will only be run\n        * by a single thread.\n        */\n-      private QueueingConsumer.Delivery delivery;\n+      private Delivery delivery;\n \n       /**\n        * Store the latest delivery tag to be able to commit (acknowledge) the message delivery up to\n@@ -268,4 +274,41 @@ public ByteBufferInputRowParser getParser()\n   {\n     return parser;\n   }\n+\n+  private static class QueueingConsumer extends DefaultConsumer\n+  {\n+    private final BlockingQueue<Delivery> _queue;\n+\n+    public QueueingConsumer(Channel ch) {\n+      this(ch, new LinkedBlockingQueue<Delivery>());\n+    }\n+\n+    public QueueingConsumer(Channel ch, BlockingQueue<Delivery> q) {\n+      super(ch);\n+      this._queue = q;\n+    }\n+\n+    @Override public void handleShutdownSignal(String consumerTag, ShutdownSignalException sig) {\n+      _queue.clear();\n+    }\n+\n+    @Override public void handleCancel(String consumerTag) throws IOException {\n+      _queue.clear();\n+    }\n+\n+    @Override public void handleDelivery(String consumerTag,\n+                                         Envelope envelope,\n+                                         AMQP.BasicProperties properties,\n+                                         byte[] body)\n+      throws IOException\n+    {\n+      this._queue.add(new Delivery(envelope, properties, body));\n+    }\n+\n+    public Delivery nextDelivery()\n+      throws InterruptedException, ShutdownSignalException, ConsumerCancelledException\n+    {\n+      return _queue.take();\n+    }\n+  }\n }",
      "parent_sha": "5ce80068d2a6be68278b495e72af838c5fae4969"
    }
  },
  {
    "oid": "874a0a4bdd50308ddab5c2ef5c95d3be7fd244dd",
    "message": "MetadataResource: Fix handling of includeDisabled. (#3042)",
    "date": "2016-06-01T18:56:37Z",
    "url": "https://github.com/apache/druid/commit/874a0a4bdd50308ddab5c2ef5c95d3be7fd244dd",
    "details": {
      "sha": "bdc66da012d4625c123fd535360dc7dcd33562ee",
      "filename": "server/src/main/java/io/druid/server/http/MetadataResource.java",
      "status": "modified",
      "additions": 56,
      "deletions": 78,
      "changes": 134,
      "blob_url": "https://github.com/apache/druid/blob/874a0a4bdd50308ddab5c2ef5c95d3be7fd244dd/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FMetadataResource.java",
      "raw_url": "https://github.com/apache/druid/raw/874a0a4bdd50308ddab5c2ef5c95d3be7fd244dd/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FMetadataResource.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FMetadataResource.java?ref=874a0a4bdd50308ddab5c2ef5c95d3be7fd244dd",
      "patch": "@@ -22,8 +22,9 @@\n import com.google.common.base.Function;\n import com.google.common.base.Predicate;\n import com.google.common.collect.Collections2;\n+import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Iterables;\n-import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n import com.google.inject.Inject;\n import com.metamx.common.Pair;\n import com.sun.jersey.spi.container.ResourceFilters;\n@@ -51,11 +52,10 @@\n import javax.ws.rs.core.MediaType;\n import javax.ws.rs.core.Response;\n import java.io.IOException;\n-import java.util.Collection;\n-import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Set;\n \n /**\n  */\n@@ -82,103 +82,81 @@ public MetadataResource(\n   @Path(\"/datasources\")\n   @Produces(MediaType.APPLICATION_JSON)\n   public Response getDatabaseDataSources(\n-      @QueryParam(\"full\") String full,\n-      @QueryParam(\"includeDisabled\") String includeDisabled,\n+      @QueryParam(\"full\") final String full,\n+      @QueryParam(\"includeDisabled\") final String includeDisabled,\n       @Context final HttpServletRequest req\n   )\n   {\n-    Response.ResponseBuilder builder = Response.status(Response.Status.OK);\n+    final Set<String> dataSourceNamesPreAuth;\n+    if (includeDisabled != null) {\n+      dataSourceNamesPreAuth = Sets.newTreeSet(metadataSegmentManager.getAllDatasourceNames());\n+    } else {\n+      dataSourceNamesPreAuth = Sets.newTreeSet(\n+          Iterables.transform(\n+              metadataSegmentManager.getInventory(),\n+              new Function<DruidDataSource, String>()\n+              {\n+                @Override\n+                public String apply(DruidDataSource input)\n+                {\n+                  return input.getName();\n+                }\n+              }\n+          )\n+      );\n+    }\n+\n+    final Set<String> dataSourceNamesPostAuth;\n \n-    final Collection<DruidDataSource> druidDataSources;\n     if (authConfig.isEnabled()) {\n       // This is an experimental feature, see - https://github.com/druid-io/druid/pull/2424\n       final Map<Pair<Resource, Action>, Access> resourceAccessMap = new HashMap<>();\n       final AuthorizationInfo authorizationInfo = (AuthorizationInfo) req.getAttribute(AuthConfig.DRUID_AUTH_TOKEN);\n-      if (includeDisabled != null) {\n-        return builder.entity(\n-            Collections2.filter(\n-                metadataSegmentManager.getAllDatasourceNames(),\n-                new Predicate<String>()\n-                {\n-                  @Override\n-                  public boolean apply(String input)\n-                  {\n-                    Resource resource = new Resource(input, ResourceType.DATASOURCE);\n-                    Action action = Action.READ;\n-                    Pair<Resource, Action> key = new Pair<>(resource, action);\n-                    if (resourceAccessMap.containsKey(key)) {\n-                      return resourceAccessMap.get(key).isAllowed();\n-                    } else {\n-                      Access access = authorizationInfo.isAuthorized(key.lhs, key.rhs);\n-                      resourceAccessMap.put(key, access);\n-                      return access.isAllowed();\n-                    }\n-                  }\n-                }\n-            )).build();\n-      } else {\n-        druidDataSources =\n-            Collections2.filter(\n-                metadataSegmentManager.getInventory(),\n-                new Predicate<DruidDataSource>()\n+      dataSourceNamesPostAuth = ImmutableSet.copyOf(\n+          Sets.filter(\n+              dataSourceNamesPreAuth,\n+              new Predicate<String>()\n+              {\n+                @Override\n+                public boolean apply(String input)\n                 {\n-                  @Override\n-                  public boolean apply(DruidDataSource input)\n-                  {\n-                    Resource resource = new Resource(input.getName(), ResourceType.DATASOURCE);\n-                    Action action = Action.READ;\n-                    Pair<Resource, Action> key = new Pair<>(resource, action);\n-                    if (resourceAccessMap.containsKey(key)) {\n-                      return resourceAccessMap.get(key).isAllowed();\n-                    } else {\n-                      Access access = authorizationInfo.isAuthorized(key.lhs, key.rhs);\n-                      resourceAccessMap.put(key, access);\n-                      return access.isAllowed();\n-                    }\n+                  Resource resource = new Resource(input, ResourceType.DATASOURCE);\n+                  Action action = Action.READ;\n+                  Pair<Resource, Action> key = new Pair<>(resource, action);\n+                  if (resourceAccessMap.containsKey(key)) {\n+                    return resourceAccessMap.get(key).isAllowed();\n+                  } else {\n+                    Access access = authorizationInfo.isAuthorized(key.lhs, key.rhs);\n+                    resourceAccessMap.put(key, access);\n+                    return access.isAllowed();\n                   }\n                 }\n-            );\n-      }\n+              }\n+          )\n+      );\n     } else {\n-      druidDataSources = metadataSegmentManager.getInventory();\n+      dataSourceNamesPostAuth = dataSourceNamesPreAuth;\n     }\n \n-    if (includeDisabled != null) {\n-      return builder.entity(\n-          Collections2.transform(\n-              druidDataSources,\n-              new Function<DruidDataSource, String>()\n+    // Cannot do both includeDisabled and full, let includeDisabled take priority\n+    // Always use dataSourceNamesPostAuth to determine the set of returned dataSources\n+    if (full != null && includeDisabled == null) {\n+      return Response.ok().entity(\n+          Collections2.filter(\n+              metadataSegmentManager.getInventory(),\n+              new Predicate<DruidDataSource>()\n               {\n                 @Override\n-                public String apply(DruidDataSource input)\n+                public boolean apply(DruidDataSource input)\n                 {\n-                  return input.getName();\n+                  return dataSourceNamesPostAuth.contains(input.getName());\n                 }\n               }\n           )\n       ).build();\n+    } else {\n+      return Response.ok().entity(dataSourceNamesPostAuth).build();\n     }\n-    if (full != null) {\n-      return builder.entity(druidDataSources).build();\n-    }\n-\n-    List<String> dataSourceNames = Lists.newArrayList(\n-        Iterables.transform(\n-            druidDataSources,\n-            new Function<DruidDataSource, String>()\n-            {\n-              @Override\n-              public String apply(DruidDataSource dataSource)\n-              {\n-                return dataSource.getName();\n-              }\n-            }\n-        )\n-    );\n-\n-    Collections.sort(dataSourceNames);\n-\n-    return builder.entity(dataSourceNames).build();\n   }\n \n   @GET",
      "parent_sha": "5e44ed71322101717f72d276c2196c2c392fc8a0"
    }
  },
  {
    "oid": "0fa8b48fb73d2489fb89cf2ce500532462b7d565",
    "message": "fix selector dim filter check for value null",
    "date": "2013-08-22T01:51:44Z",
    "url": "https://github.com/apache/druid/commit/0fa8b48fb73d2489fb89cf2ce500532462b7d565",
    "details": {
      "sha": "dd2f39dfe13c122270d67ca0ef44be00ecd75649",
      "filename": "client/src/main/java/com/metamx/druid/query/filter/SelectorDimFilter.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/0fa8b48fb73d2489fb89cf2ce500532462b7d565/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fquery%2Ffilter%2FSelectorDimFilter.java",
      "raw_url": "https://github.com/apache/druid/raw/0fa8b48fb73d2489fb89cf2ce500532462b7d565/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fquery%2Ffilter%2FSelectorDimFilter.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fquery%2Ffilter%2FSelectorDimFilter.java?ref=0fa8b48fb73d2489fb89cf2ce500532462b7d565",
      "patch": "@@ -39,7 +39,7 @@ public SelectorDimFilter(\n   )\n   {\n     Preconditions.checkArgument(dimension != null, \"dimension must not be null\");\n-    Preconditions.checkArgument(value != null, \"value must not be null\");\n+\n     this.dimension = dimension;\n     this.value = value;\n   }\n@@ -48,7 +48,7 @@ public SelectorDimFilter(\n   public byte[] getCacheKey()\n   {\n     byte[] dimensionBytes = dimension.getBytes();\n-    byte[] valueBytes = value.getBytes();\n+    byte[] valueBytes = (value == null) ? new byte[]{} : value.getBytes();\n \n     return ByteBuffer.allocate(1 + dimensionBytes.length + valueBytes.length)\n                      .put(DimFilterCacheHelper.SELECTOR_CACHE_ID)",
      "parent_sha": "ef12439df9b6ddb906a8b04259cff1878e84de4a"
    }
  },
  {
    "oid": "e26f7fca360edae7bd8c7b65653637d801f2270d",
    "message": "cleanup cache interface",
    "date": "2015-11-11T20:25:22Z",
    "url": "https://github.com/apache/druid/commit/e26f7fca360edae7bd8c7b65653637d801f2270d",
    "details": {
      "sha": "70181b1ab58781a52bf8828b4d52243aa0c5d0e4",
      "filename": "server/src/main/java/io/druid/client/cache/Cache.java",
      "status": "modified",
      "additions": 8,
      "deletions": 8,
      "changes": 16,
      "blob_url": "https://github.com/apache/druid/blob/e26f7fca360edae7bd8c7b65653637d801f2270d/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2Fcache%2FCache.java",
      "raw_url": "https://github.com/apache/druid/raw/e26f7fca360edae7bd8c7b65653637d801f2270d/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2Fcache%2FCache.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2Fcache%2FCache.java?ref=e26f7fca360edae7bd8c7b65653637d801f2270d",
      "patch": "@@ -30,30 +30,30 @@\n  */\n public interface Cache\n {\n-  public byte[] get(NamedKey key);\n-  public void put(NamedKey key, byte[] value);\n+  byte[] get(NamedKey key);\n+  void put(NamedKey key, byte[] value);\n \n   /**\n    * Resulting map should not contain any null values (i.e. cache misses should not be included)\n    *\n    * @param keys\n    * @return\n    */\n-  public Map<NamedKey, byte[]> getBulk(Iterable<NamedKey> keys);\n+  Map<NamedKey, byte[]> getBulk(Iterable<NamedKey> keys);\n \n-  public void close(String namespace);\n+  void close(String namespace);\n \n-  public CacheStats getStats();\n+  CacheStats getStats();\n \n-  public boolean isLocal();\n+  boolean isLocal();\n \n   /**\n    * Custom metrics not covered by CacheStats may be emitted by this method.\n    * @param emitter The service emitter to emit on.\n    */\n-  public void doMonitor(ServiceEmitter emitter);\n+  void doMonitor(ServiceEmitter emitter);\n \n-  public class NamedKey\n+  class NamedKey\n   {\n     final public String namespace;\n     final public byte[] key;",
      "parent_sha": "fa6142e217e3b6e22ba21218bcfd86ec422cdf6c"
    }
  },
  {
    "oid": "ca90530085ae4e2306e566eb2a0ab449f153dc81",
    "message": "simplify things a bit",
    "date": "2014-01-09T22:45:13Z",
    "url": "https://github.com/apache/druid/commit/ca90530085ae4e2306e566eb2a0ab449f153dc81",
    "details": {
      "sha": "25e27ce50df335844191c688aa70230d07114ef1",
      "filename": "server/src/main/java/io/druid/client/selector/RandomServerSelectorStrategy.java",
      "status": "modified",
      "additions": 1,
      "deletions": 5,
      "changes": 6,
      "blob_url": "https://github.com/apache/druid/blob/ca90530085ae4e2306e566eb2a0ab449f153dc81/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2Fselector%2FRandomServerSelectorStrategy.java",
      "raw_url": "https://github.com/apache/druid/raw/ca90530085ae4e2306e566eb2a0ab449f153dc81/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2Fselector%2FRandomServerSelectorStrategy.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2Fselector%2FRandomServerSelectorStrategy.java?ref=ca90530085ae4e2306e566eb2a0ab449f153dc81",
      "patch": "@@ -26,11 +26,7 @@\n \n public class RandomServerSelectorStrategy implements ServerSelectorStrategy\n {\n-  Random random;\n-\n-  public RandomServerSelectorStrategy() {\n-    this.random = new Random();\n-  }\n+  private static final Random random = new Random();\n \n   @Override\n   public QueryableDruidServer pick(Set<QueryableDruidServer> servers)",
      "parent_sha": "632d6b0a7847054fe81d38242afe8a1dc233710c"
    }
  },
  {
    "oid": "f7c4d6a24af24fcbc04e9e8e58924ef061d920f3",
    "message": "fix bug with loadqueue endpoint",
    "date": "2014-06-03T17:28:22Z",
    "url": "https://github.com/apache/druid/commit/f7c4d6a24af24fcbc04e9e8e58924ef061d920f3",
    "details": {
      "sha": "6fbff039707b976074234ec35588e8798a6cfd8c",
      "filename": "server/src/main/java/io/druid/server/http/CoordinatorResource.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/f7c4d6a24af24fcbc04e9e8e58924ef061d920f3/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FCoordinatorResource.java",
      "raw_url": "https://github.com/apache/druid/raw/f7c4d6a24af24fcbc04e9e8e58924ef061d920f3/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FCoordinatorResource.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FCoordinatorResource.java?ref=f7c4d6a24af24fcbc04e9e8e58924ef061d920f3",
      "patch": "@@ -80,7 +80,7 @@ public Response getLoadStatus(\n   @Produces(\"application/json\")\n   public Response getLoadQueue(\n       @QueryParam(\"simple\") String simple,\n-      @QueryParam(\"simple\") String full\n+      @QueryParam(\"full\") String full\n   )\n   {\n     if (simple != null) {",
      "parent_sha": "dd76fc49e247ef6e45b3614def8abc5d82d3080d"
    }
  },
  {
    "oid": "eabddffd6ea182a7a8417d68e5982b9bbf5bdf1c",
    "message": "fix http firehose factory leaky connection in constructor (#8576)\n\n* fix http firehose factory leaky connection in constructor\r\n\r\n* stylin",
    "date": "2019-09-24T23:08:43Z",
    "url": "https://github.com/apache/druid/commit/eabddffd6ea182a7a8417d68e5982b9bbf5bdf1c",
    "details": {
      "sha": "860f6cc11061a8497fef69e71350d3d07f179aef",
      "filename": "server/src/main/java/org/apache/druid/segment/realtime/firehose/HttpFirehoseFactory.java",
      "status": "modified",
      "additions": 22,
      "deletions": 30,
      "changes": 52,
      "blob_url": "https://github.com/apache/druid/blob/eabddffd6ea182a7a8417d68e5982b9bbf5bdf1c/server%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2Frealtime%2Ffirehose%2FHttpFirehoseFactory.java",
      "raw_url": "https://github.com/apache/druid/raw/eabddffd6ea182a7a8417d68e5982b9bbf5bdf1c/server%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2Frealtime%2Ffirehose%2FHttpFirehoseFactory.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2Frealtime%2Ffirehose%2FHttpFirehoseFactory.java?ref=eabddffd6ea182a7a8417d68e5982b9bbf5bdf1c",
      "patch": "@@ -49,7 +49,6 @@ public class HttpFirehoseFactory extends PrefetchableTextFilesFirehoseFactory<UR\n {\n   private static final Logger log = new Logger(HttpFirehoseFactory.class);\n   private final List<URI> uris;\n-  private final boolean supportContentRange;\n   @Nullable\n   private final String httpAuthenticationUsername;\n   @Nullable\n@@ -63,29 +62,25 @@ public HttpFirehoseFactory(\n       @JsonProperty(\"prefetchTriggerBytes\") Long prefetchTriggerBytes,\n       @JsonProperty(\"fetchTimeout\") Long fetchTimeout,\n       @JsonProperty(\"maxFetchRetry\") Integer maxFetchRetry,\n-      @Nullable\n-      @JsonProperty(\"httpAuthenticationUsername\") String httpAuthenticationUsername,\n-      @Nullable\n-      @JsonProperty(\"httpAuthenticationPassword\") PasswordProvider httpAuthenticationPasswordProvider\n-  ) throws IOException\n+      @JsonProperty(\"httpAuthenticationUsername\") @Nullable String httpAuthenticationUsername,\n+      @JsonProperty(\"httpAuthenticationPassword\") @Nullable PasswordProvider httpAuthenticationPasswordProvider\n+  )\n   {\n     super(maxCacheCapacityBytes, maxFetchCapacityBytes, prefetchTriggerBytes, fetchTimeout, maxFetchRetry);\n-    this.uris = uris;\n-\n     Preconditions.checkArgument(uris.size() > 0, \"Empty URIs\");\n-    final URLConnection connection = uris.get(0).toURL().openConnection();\n-    final String acceptRanges = connection.getHeaderField(HttpHeaders.ACCEPT_RANGES);\n-    this.supportContentRange = acceptRanges != null && \"bytes\".equalsIgnoreCase(acceptRanges);\n+    this.uris = uris;\n     this.httpAuthenticationUsername = httpAuthenticationUsername;\n     this.httpAuthenticationPasswordProvider = httpAuthenticationPasswordProvider;\n   }\n \n+  @Nullable\n   @JsonProperty\n   public String getHttpAuthenticationUsername()\n   {\n     return httpAuthenticationUsername;\n   }\n \n+  @Nullable\n   @JsonProperty(\"httpAuthenticationPassword\")\n   public PasswordProvider getHttpAuthenticationPasswordProvider()\n   {\n@@ -115,14 +110,16 @@ protected InputStream openObjectStream(URI object) throws IOException\n   protected InputStream openObjectStream(URI object, long start) throws IOException\n   {\n     URLConnection urlConnection = openURLConnection(object);\n-    if (supportContentRange && start > 0) {\n+    final String acceptRanges = urlConnection.getHeaderField(HttpHeaders.ACCEPT_RANGES);\n+    final boolean withRanges = \"bytes\".equalsIgnoreCase(acceptRanges);\n+    if (withRanges && start > 0) {\n       // Set header for range request.\n       // Since we need to set only the start offset, the header is \"bytes=<range-start>-\".\n       // See https://tools.ietf.org/html/rfc7233#section-2.1\n       urlConnection.addRequestProperty(HttpHeaders.RANGE, StringUtils.format(\"bytes=%d-\", start));\n       return urlConnection.getInputStream();\n     } else {\n-      if (!supportContentRange && start > 0) {\n+      if (!withRanges && start > 0) {\n         log.warn(\n                 \"Since the input source doesn't support range requests, the object input stream is opened from the start and \"\n                         + \"then skipped. This may make the ingestion speed slower. Consider enabling prefetch if you see this message\"\n@@ -159,8 +156,8 @@ public boolean equals(Object o)\n            getPrefetchTriggerBytes() == that.getPrefetchTriggerBytes() &&\n            getFetchTimeout() == that.getFetchTimeout() &&\n            getMaxFetchRetry() == that.getMaxFetchRetry() &&\n-           httpAuthenticationUsername.equals(that.getHttpAuthenticationUsername()) &&\n-           httpAuthenticationPasswordProvider.equals(that.getHttpAuthenticationPasswordProvider());\n+           Objects.equals(httpAuthenticationUsername, that.getHttpAuthenticationUsername()) &&\n+           Objects.equals(httpAuthenticationPasswordProvider, that.getHttpAuthenticationPasswordProvider());\n   }\n \n   @Override\n@@ -187,21 +184,16 @@ protected Predicate<Throwable> getRetryCondition()\n   @Override\n   public FiniteFirehoseFactory<StringInputRowParser, URI> withSplit(InputSplit<URI> split)\n   {\n-    try {\n-      return new HttpFirehoseFactory(\n-          Collections.singletonList(split.get()),\n-          getMaxCacheCapacityBytes(),\n-          getMaxFetchCapacityBytes(),\n-          getPrefetchTriggerBytes(),\n-          getFetchTimeout(),\n-          getMaxFetchRetry(),\n-          getHttpAuthenticationUsername(),\n-          httpAuthenticationPasswordProvider\n-      );\n-    }\n-    catch (IOException e) {\n-      throw new RuntimeException(e);\n-    }\n+    return new HttpFirehoseFactory(\n+        Collections.singletonList(split.get()),\n+        getMaxCacheCapacityBytes(),\n+        getMaxFetchCapacityBytes(),\n+        getPrefetchTriggerBytes(),\n+        getFetchTimeout(),\n+        getMaxFetchRetry(),\n+        getHttpAuthenticationUsername(),\n+        httpAuthenticationPasswordProvider\n+    );\n   }\n \n   private URLConnection openURLConnection(URI object) throws IOException",
      "parent_sha": "7c14fa08f8198385075f223bcd5994b1f47ae7d0"
    }
  },
  {
    "oid": "0f4746450bf079593858e01fa7894cc61dbfde5a",
    "message": "RemoteTaskRunner: null check on workerData",
    "date": "2013-01-25T21:40:20Z",
    "url": "https://github.com/apache/druid/commit/0f4746450bf079593858e01fa7894cc61dbfde5a",
    "details": {
      "sha": "067356ceddc32a16df97aca50b0f226c41a25df1",
      "filename": "merger/src/main/java/com/metamx/druid/merger/coordinator/RemoteTaskRunner.java",
      "status": "modified",
      "additions": 17,
      "deletions": 9,
      "changes": 26,
      "blob_url": "https://github.com/apache/druid/blob/0f4746450bf079593858e01fa7894cc61dbfde5a/merger%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmerger%2Fcoordinator%2FRemoteTaskRunner.java",
      "raw_url": "https://github.com/apache/druid/raw/0f4746450bf079593858e01fa7894cc61dbfde5a/merger%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmerger%2Fcoordinator%2FRemoteTaskRunner.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/merger%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmerger%2Fcoordinator%2FRemoteTaskRunner.java?ref=0f4746450bf079593858e01fa7894cc61dbfde5a",
      "patch": "@@ -44,6 +44,7 @@\n import com.metamx.druid.merger.worker.Worker;\n import com.metamx.emitter.EmittingLogger;\n import com.netflix.curator.framework.CuratorFramework;\n+import com.netflix.curator.framework.recipes.cache.ChildData;\n import com.netflix.curator.framework.recipes.cache.PathChildrenCache;\n import com.netflix.curator.framework.recipes.cache.PathChildrenCacheEvent;\n import com.netflix.curator.framework.recipes.cache.PathChildrenCacheListener;\n@@ -286,21 +287,28 @@ private void assignTask(TaskWrapper taskWrapper)\n       try {\n         log.info(\"Worker[%s] is already running task[%s].\", worker.getHost(), taskWrapper.getTask().getId());\n \n-        TaskStatus taskStatus = jsonMapper.readValue(\n-            workerWrapper.getStatusCache()\n-                         .getCurrentData(\n-                             JOINER.join(config.getStatusPath(), worker.getHost(), taskWrapper.getTask().getId())\n-                         )\n-                         .getData(),\n-            TaskStatus.class\n-        );\n+        final ChildData workerData = workerWrapper.getStatusCache()\n+                                                  .getCurrentData(\n+                                                      JOINER.join(\n+                                                          config.getStatusPath(),\n+                                                          worker.getHost(),\n+                                                          taskWrapper.getTask().getId()\n+                                                      )\n+                                                  );\n+\n+        if (workerData != null && workerData.getData() != null) {\n+          final TaskStatus taskStatus = jsonMapper.readValue(\n+              workerData.getData(),\n+              TaskStatus.class\n+          );\n \n-        if (taskStatus.isComplete()) {\n           TaskCallback callback = taskWrapper.getCallback();\n           if (callback != null) {\n             callback.notify(taskStatus);\n           }\n           new CleanupPaths(worker.getHost(), taskWrapper.getTask().getId()).run();\n+        } else {\n+          log.warn(\"Worker data was null for worker: %s\", worker.getHost());\n         }\n       }\n       catch (Exception e) {",
      "parent_sha": "bb68091cef9d49eefcb3934bb939a1d557dbe422"
    }
  },
  {
    "oid": "8c7fd3bc490e3f7a68f13a093836d8d7edf139df",
    "message": "more specific checks in rule test",
    "date": "2012-12-14T17:12:02Z",
    "url": "https://github.com/apache/druid/commit/8c7fd3bc490e3f7a68f13a093836d8d7edf139df",
    "details": {
      "sha": "c31dc670b82f2bbd0a92daea9ce47fdcdb21e986",
      "filename": "server/src/test/java/com/metamx/druid/master/DruidMasterRuleRunnerTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/8c7fd3bc490e3f7a68f13a093836d8d7edf139df/server%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmaster%2FDruidMasterRuleRunnerTest.java",
      "raw_url": "https://github.com/apache/druid/raw/8c7fd3bc490e3f7a68f13a093836d8d7edf139df/server%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmaster%2FDruidMasterRuleRunnerTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmaster%2FDruidMasterRuleRunnerTest.java?ref=8c7fd3bc490e3f7a68f13a093836d8d7edf139df",
      "patch": "@@ -357,7 +357,7 @@ public void testRunTwoTiersWithExistingSegments() throws Exception\n   public void testRunTwoTiersTierDoesNotExist() throws Exception\n   {\n     emitter.emit(EasyMock.<ServiceEventBuilder>anyObject());\n-    EasyMock.expectLastCall().atLeastOnce();\n+    EasyMock.expectLastCall().times(12);\n     EasyMock.replay(emitter);\n \n     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.<String>anyObject())).andReturn(\n@@ -408,7 +408,7 @@ public void testRunTwoTiersTierDoesNotExist() throws Exception\n   public void testRunRuleDoesNotExist() throws Exception\n   {\n     emitter.emit(EasyMock.<ServiceEventBuilder>anyObject());\n-    EasyMock.expectLastCall().atLeastOnce();\n+    EasyMock.expectLastCall().times(availableSegments.size());\n     EasyMock.replay(emitter);\n \n     EasyMock.expect(databaseRuleManager.getRulesWithDefault(EasyMock.<String>anyObject())).andReturn(",
      "parent_sha": "aa69ed52511e9913231247ae71bdc56694eb0c92"
    }
  },
  {
    "oid": "f293794710b08ebc5a7c88f48b7f98adbc76ad9e",
    "message": "WebJsonSupplierTest: Use fqdn to avoid search domain wonkiness (fixes #275)",
    "date": "2014-01-27T11:49:12Z",
    "url": "https://github.com/apache/druid/commit/f293794710b08ebc5a7c88f48b7f98adbc76ad9e",
    "details": {
      "sha": "ca181427c8266b6389b1eae8d81ad6681597b10b",
      "filename": "examples/src/test/java/io/druid/examples/web/WebJsonSupplierTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/f293794710b08ebc5a7c88f48b7f98adbc76ad9e/examples%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fexamples%2Fweb%2FWebJsonSupplierTest.java",
      "raw_url": "https://github.com/apache/druid/raw/f293794710b08ebc5a7c88f48b7f98adbc76ad9e/examples%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fexamples%2Fweb%2FWebJsonSupplierTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/examples%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fexamples%2Fweb%2FWebJsonSupplierTest.java?ref=f293794710b08ebc5a7c88f48b7f98adbc76ad9e",
      "patch": "@@ -29,7 +29,7 @@ public class WebJsonSupplierTest\n   public void checkInvalidUrl() throws Exception\n   {\n \n-    String invalidURL = \"http://invalid.url\";\n+    String invalidURL = \"http://invalid.url.\";\n     WebJsonSupplier supplier = new WebJsonSupplier(invalidURL);\n     supplier.getInput();\n   }",
      "parent_sha": "8edf0cf13641f6bdb1a5bc8cbef56f7608352941"
    }
  },
  {
    "oid": "179170224d6f8b547a7b6ba2c2fe3d1b17bde65b",
    "message": "Fix conditional.",
    "date": "2014-08-08T18:20:36Z",
    "url": "https://github.com/apache/druid/commit/179170224d6f8b547a7b6ba2c2fe3d1b17bde65b",
    "details": {
      "sha": "bf1b575ff4199d6a87227eb6ddc8a1577b93ca84",
      "filename": "server/src/main/java/io/druid/segment/realtime/plumber/RealtimePlumber.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/179170224d6f8b547a7b6ba2c2fe3d1b17bde65b/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Frealtime%2Fplumber%2FRealtimePlumber.java",
      "raw_url": "https://github.com/apache/druid/raw/179170224d6f8b547a7b6ba2c2fe3d1b17bde65b/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Frealtime%2Fplumber%2FRealtimePlumber.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Frealtime%2Fplumber%2FRealtimePlumber.java?ref=179170224d6f8b547a7b6ba2c2fe3d1b17bde65b",
      "patch": "@@ -356,7 +356,7 @@ public void doRun()\n               log.makeAlert(e, \"Failed to persist merged index[%s]\", schema.getDataSource())\n                  .addData(\"interval\", interval)\n                  .emit();\n-              if (!shuttingDown) {\n+              if (shuttingDown) {\n                 // We're trying to shut down, and this segment failed to push. Let's just get rid of it.\n                 // This call will also delete possibly-partially-written files, so we don't need to do it explicitly.\n                 abandonSegment(truncatedTime, sink);",
      "parent_sha": "f547049929ff97decdad6fbb5730aac8f41463a2"
    }
  },
  {
    "oid": "8819a1b9d5c14c47c7714688e773f25ec2aa0483",
    "message": "Add Smile annotation to BrokerServerView",
    "date": "2014-11-18T18:18:10Z",
    "url": "https://github.com/apache/druid/commit/8819a1b9d5c14c47c7714688e773f25ec2aa0483",
    "details": {
      "sha": "543951df1c39204cad6cfb6b4c8d4d04475940de",
      "filename": "server/src/main/java/io/druid/client/BrokerServerView.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/druid/blob/8819a1b9d5c14c47c7714688e773f25ec2aa0483/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2FBrokerServerView.java",
      "raw_url": "https://github.com/apache/druid/raw/8819a1b9d5c14c47c7714688e773f25ec2aa0483/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2FBrokerServerView.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2FBrokerServerView.java?ref=8819a1b9d5c14c47c7714688e773f25ec2aa0483",
      "patch": "@@ -31,6 +31,7 @@\n import io.druid.client.selector.TierSelectorStrategy;\n import io.druid.concurrent.Execs;\n import io.druid.guice.annotations.Client;\n+import io.druid.guice.annotations.Smile;\n import io.druid.query.DataSource;\n import io.druid.query.QueryRunner;\n import io.druid.query.QueryToolChestWarehouse;\n@@ -69,7 +70,7 @@ public class BrokerServerView implements TimelineServerView\n   public BrokerServerView(\n       QueryToolChestWarehouse warehouse,\n       QueryWatcher queryWatcher,\n-      ObjectMapper smileMapper,\n+      @Smile ObjectMapper smileMapper,\n       @Client HttpClient httpClient,\n       ServerInventoryView baseView,\n       TierSelectorStrategy tierSelectorStrategy",
      "parent_sha": "00806ba8362322bde064152af5bbedef5f0d323f"
    }
  },
  {
    "oid": "2f0a43790c22318b9c48ed0f38db7350fef267ef",
    "message": "Make GuavaUtilsTest use less CPU (#14487)",
    "date": "2023-06-27T04:45:29Z",
    "url": "https://github.com/apache/druid/commit/2f0a43790c22318b9c48ed0f38db7350fef267ef",
    "details": {
      "sha": "317cb350f970b90a559751ba0997fb91749bf52d",
      "filename": "processing/src/test/java/org/apache/druid/common/guava/GuavaUtilsTest.java",
      "status": "modified",
      "additions": 24,
      "deletions": 19,
      "changes": 43,
      "blob_url": "https://github.com/apache/druid/blob/2f0a43790c22318b9c48ed0f38db7350fef267ef/processing%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fcommon%2Fguava%2FGuavaUtilsTest.java",
      "raw_url": "https://github.com/apache/druid/raw/2f0a43790c22318b9c48ed0f38db7350fef267ef/processing%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fcommon%2Fguava%2FGuavaUtilsTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fcommon%2Fguava%2FGuavaUtilsTest.java?ref=2f0a43790c22318b9c48ed0f38db7350fef267ef",
      "patch": "@@ -31,9 +31,11 @@\n import java.util.ArrayList;\n import java.util.List;\n import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.atomic.AtomicInteger;\n import java.util.function.Function;\n \n public class GuavaUtilsTest\n@@ -73,23 +75,23 @@ public void testCancelAll()\n     int tasks = 3;\n     ExecutorService service = Execs.multiThreaded(tasks, \"GuavaUtilsTest-%d\");\n     ListeningExecutorService exc = MoreExecutors.listeningDecorator(service);\n-    AtomicInteger index = new AtomicInteger(0);\n     //a flag what time to throw exception.\n-    AtomicBoolean active = new AtomicBoolean(false);\n+    AtomicBoolean someoneFailed = new AtomicBoolean(false);\n+    List<CountDownLatch> latches = new ArrayList<>(tasks);\n     Function<Integer, List<ListenableFuture<Object>>> function = (taskCount) -> {\n       List<ListenableFuture<Object>> futures = new ArrayList<>();\n       for (int i = 0; i < taskCount; i++) {\n+        final CountDownLatch latch = new CountDownLatch(1);\n+        latches.add(latch);\n         ListenableFuture<Object> future = exc.submit(new Callable<Object>() {\n           @Override\n-          public Object call() throws RuntimeException\n+          public Object call() throws RuntimeException, InterruptedException\n           {\n-            int internalIndex = index.incrementAndGet();\n-            while (true) {\n-              if (internalIndex == taskCount && active.get()) {\n-                //here we simulate occurs exception in some one future.\n-                throw new RuntimeException(\"A big bug\");\n-              }\n+            latch.await(60, TimeUnit.SECONDS);\n+            if (someoneFailed.compareAndSet(false, true)) {\n+              throw new RuntimeException(\"This exception simulates an error\");\n             }\n+            return null;\n           }\n         });\n         futures.add(future);\n@@ -99,17 +101,20 @@ public Object call() throws RuntimeException\n \n     List<ListenableFuture<Object>> futures = function.apply(tasks);\n     Assert.assertEquals(tasks, futures.stream().filter(f -> !f.isDone()).count());\n-    //here we make one of task throw exception.\n-    active.set(true);\n+    // \"release\" the last tasks, which will cause it to fail as someoneFailed will still be false\n+    latches.get(tasks - 1).countDown();\n \n     ListenableFuture<List<Object>> future = Futures.allAsList(futures);\n-    try {\n-      future.get();\n-    }\n-    catch (Exception e) {\n-      Assert.assertEquals(\"java.lang.RuntimeException: A big bug\", e.getMessage());\n-      GuavaUtils.cancelAll(true, future, futures);\n-      Assert.assertEquals(0, futures.stream().filter(f -> !f.isDone()).count());\n+\n+    ExecutionException thrown = Assert.assertThrows(\n+        ExecutionException.class,\n+        future::get\n+    );\n+    Assert.assertEquals(\"This exception simulates an error\", thrown.getCause().getMessage());\n+    GuavaUtils.cancelAll(true, future, futures);\n+    Assert.assertEquals(0, futures.stream().filter(f -> !f.isDone()).count());\n+    for (CountDownLatch latch : latches) {\n+      latch.countDown();\n     }\n   }\n }",
      "parent_sha": "6ba10c8b6caf1de5afd782ba70edda406466c892"
    }
  },
  {
    "oid": "e13ed7b8787f60021c8e9e261f97e7f4663819f8",
    "message": "Improve javadoc for ReadableFieldPointer. (#16043)\n\nSince #15175, the javadoc for ReadableFieldPointer is somewhat out of date. It says that\r\nthe pointer only points to the beginning of the field, but this is no longer true. This\r\npatch updates the javadoc to be more accurate.",
    "date": "2024-03-05T19:52:49Z",
    "url": "https://github.com/apache/druid/commit/e13ed7b8787f60021c8e9e261f97e7f4663819f8",
    "details": {
      "sha": "5a005a8e7adc9e0f7fb88bc58e31f58292780a80",
      "filename": "processing/src/main/java/org/apache/druid/frame/field/ReadableFieldPointer.java",
      "status": "modified",
      "additions": 4,
      "deletions": 5,
      "changes": 9,
      "blob_url": "https://github.com/apache/druid/blob/e13ed7b8787f60021c8e9e261f97e7f4663819f8/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fframe%2Ffield%2FReadableFieldPointer.java",
      "raw_url": "https://github.com/apache/druid/raw/e13ed7b8787f60021c8e9e261f97e7f4663819f8/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fframe%2Ffield%2FReadableFieldPointer.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fframe%2Ffield%2FReadableFieldPointer.java?ref=e13ed7b8787f60021c8e9e261f97e7f4663819f8",
      "patch": "@@ -20,10 +20,8 @@\n package org.apache.druid.frame.field;\n \n /**\n- * Pointer to a field position in some memory. Only points to the beginning of the field, since all fields\n- * can be read without knowing their entire length.\n- *\n- * See {@link org.apache.druid.frame.write.RowBasedFrameWriter} for details about the format.\n+ * Pointer to a field position in some memory. See {@link org.apache.druid.frame.write.RowBasedFrameWriter} for details\n+ * about the format.\n  */\n public interface ReadableFieldPointer\n {\n@@ -33,7 +31,8 @@ public interface ReadableFieldPointer\n   long position();\n \n   /**\n-   * Length of the field.\n+   * Length of the field. Never necessary to read a field, since all fields can be read without knowing their\n+   * entire length. Provided because it may be useful for reading in a more optimal manner.\n    */\n   long length();\n }",
      "parent_sha": "bb882727c03e2a386a895d394e7e807d45a7826f"
    }
  },
  {
    "oid": "9bc6436176a39e3d339b088036483c4295a7491d",
    "message": "Add more endpoints, fixes #517",
    "date": "2014-04-28T22:55:03Z",
    "url": "https://github.com/apache/druid/commit/9bc6436176a39e3d339b088036483c4295a7491d",
    "details": {
      "sha": "f99586c10560a7f73d80d2b6ae4290860f8f7ad5",
      "filename": "server/src/main/java/io/druid/server/http/DatasourcesResource.java",
      "status": "modified",
      "additions": 65,
      "deletions": 61,
      "changes": 126,
      "blob_url": "https://github.com/apache/druid/blob/9bc6436176a39e3d339b088036483c4295a7491d/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FDatasourcesResource.java",
      "raw_url": "https://github.com/apache/druid/raw/9bc6436176a39e3d339b088036483c4295a7491d/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FDatasourcesResource.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FDatasourcesResource.java?ref=9bc6436176a39e3d339b088036483c4295a7491d",
      "patch": "@@ -59,14 +59,6 @@\n @Path(\"/druid/coordinator/v1/datasources\")\n public class DatasourcesResource\n {\n-  private static Map<String, Object> makeSimpleDatasource(DruidDataSource input)\n-  {\n-    return new ImmutableMap.Builder<String, Object>()\n-        .put(\"name\", input.getName())\n-        .put(\"properties\", input.getProperties())\n-        .build();\n-  }\n-\n   private final InventoryView serverInventoryView;\n   private final DatabaseSegmentManager databaseSegmentManager;\n   private final IndexingServiceClient indexingServiceClient;\n@@ -145,58 +137,7 @@ public Response getTheDataSource(\n       return Response.ok(dataSource).build();\n     }\n \n-    Map<String, Object> tiers = Maps.newHashMap();\n-    Map<String, Object> segments = Maps.newHashMap();\n-    Map<String, Map<String, Object>> retVal = ImmutableMap.of(\n-        \"tiers\", tiers,\n-        \"segments\", segments\n-    );\n-\n-    int totalSegmentCount = 0;\n-    long totalSegmentSize = 0;\n-    long minTime = Long.MAX_VALUE;\n-    long maxTime = Long.MIN_VALUE;\n-    for (DruidServer druidServer : serverInventoryView.getInventory()) {\n-      DruidDataSource druidDataSource = druidServer.getDataSource(dataSourceName);\n-\n-      if (druidDataSource == null) {\n-        continue;\n-      }\n-\n-      long dataSourceSegmentSize = 0;\n-      for (DataSegment dataSegment : druidDataSource.getSegments()) {\n-        dataSourceSegmentSize += dataSegment.getSize();\n-        if (dataSegment.getInterval().getStartMillis() < minTime) {\n-          minTime = dataSegment.getInterval().getStartMillis();\n-        }\n-        if (dataSegment.getInterval().getEndMillis() > maxTime) {\n-          maxTime = dataSegment.getInterval().getEndMillis();\n-        }\n-      }\n-\n-      // segment stats\n-      totalSegmentCount += druidDataSource.getSegments().size();\n-      totalSegmentSize += dataSourceSegmentSize;\n-\n-      // tier stats\n-      Map<String, Object> tierStats = (Map) tiers.get(druidServer.getTier());\n-      if (tierStats == null) {\n-        tierStats = Maps.newHashMap();\n-        tiers.put(druidServer.getTier(), tierStats);\n-      }\n-      int segmentCount = MapUtils.getInt(tierStats, \"segmentCount\", 0);\n-      tierStats.put(\"segmentCount\", segmentCount + druidDataSource.getSegments().size());\n-\n-      long segmentSize = MapUtils.getLong(tierStats, \"size\", 0L);\n-      tierStats.put(\"size\", segmentSize + dataSourceSegmentSize);\n-    }\n-\n-    segments.put(\"count\", totalSegmentCount);\n-    segments.put(\"size\", totalSegmentSize);\n-    segments.put(\"minTime\", new DateTime(minTime));\n-    segments.put(\"maxTime\", new DateTime(maxTime));\n-\n-    return Response.ok(retVal).build();\n+    return Response.ok(getSimpleDatasource(dataSourceName)).build();\n   }\n \n   @POST\n@@ -281,7 +222,7 @@ public Response getSegmentDataSourceIntervals(\n     }\n \n     if (simple != null) {\n-      final Map<Interval, Map<String, Object>> retVal = Maps.newHashMap();\n+      final Map<Interval, Map<String, Object>> retVal = Maps.newTreeMap(comparator);\n       for (DataSegment dataSegment : dataSource.getSegments()) {\n         Map<String, Object> properties = retVal.get(dataSegment.getInterval());\n         if (properties == null) {\n@@ -570,4 +511,67 @@ private Pair<DataSegment, Set<String>> getSegment(String segmentId)\n \n     return new Pair<>(theSegment, servers);\n   }\n+\n+  private Map<String, Object> makeSimpleDatasource(DruidDataSource input)\n+  {\n+    return new ImmutableMap.Builder<String, Object>()\n+        .put(\"name\", input.getName())\n+        .put(\"properties\", getSimpleDatasource(input.getName()))\n+        .build();\n+  }\n+\n+  private Map<String, Map<String, Object>> getSimpleDatasource(String dataSourceName)\n+  {\n+    Map<String, Object> tiers = Maps.newHashMap();\n+    Map<String, Object> segments = Maps.newHashMap();\n+    Map<String, Map<String, Object>> retVal = ImmutableMap.of(\n+        \"tiers\", tiers,\n+        \"segments\", segments\n+    );\n+\n+    int totalSegmentCount = 0;\n+    long totalSegmentSize = 0;\n+    long minTime = Long.MAX_VALUE;\n+    long maxTime = Long.MIN_VALUE;\n+    for (DruidServer druidServer : serverInventoryView.getInventory()) {\n+      DruidDataSource druidDataSource = druidServer.getDataSource(dataSourceName);\n+\n+      if (druidDataSource == null) {\n+        continue;\n+      }\n+\n+      long dataSourceSegmentSize = 0;\n+      for (DataSegment dataSegment : druidDataSource.getSegments()) {\n+        dataSourceSegmentSize += dataSegment.getSize();\n+        if (dataSegment.getInterval().getStartMillis() < minTime) {\n+          minTime = dataSegment.getInterval().getStartMillis();\n+        }\n+        if (dataSegment.getInterval().getEndMillis() > maxTime) {\n+          maxTime = dataSegment.getInterval().getEndMillis();\n+        }\n+      }\n+\n+      // segment stats\n+      totalSegmentCount += druidDataSource.getSegments().size();\n+      totalSegmentSize += dataSourceSegmentSize;\n+\n+      // tier stats\n+      Map<String, Object> tierStats = (Map) tiers.get(druidServer.getTier());\n+      if (tierStats == null) {\n+        tierStats = Maps.newHashMap();\n+        tiers.put(druidServer.getTier(), tierStats);\n+      }\n+      int segmentCount = MapUtils.getInt(tierStats, \"segmentCount\", 0);\n+      tierStats.put(\"segmentCount\", segmentCount + druidDataSource.getSegments().size());\n+\n+      long segmentSize = MapUtils.getLong(tierStats, \"size\", 0L);\n+      tierStats.put(\"size\", segmentSize + dataSourceSegmentSize);\n+    }\n+\n+    segments.put(\"count\", totalSegmentCount);\n+    segments.put(\"size\", totalSegmentSize);\n+    segments.put(\"minTime\", new DateTime(minTime));\n+    segments.put(\"maxTime\", new DateTime(maxTime));\n+    return retVal;\n+  }\n }",
      "parent_sha": "f88cb13ccbd5c040f2713a47cda499a258f22651"
    }
  },
  {
    "oid": "fa5f46057c0d19e7dbdb842db00f51d8356dcd8e",
    "message": "fix spacing issue",
    "date": "2013-07-23T23:25:07Z",
    "url": "https://github.com/apache/druid/commit/fa5f46057c0d19e7dbdb842db00f51d8356dcd8e",
    "details": {
      "sha": "ca638f44727ff0fcf801cf7e24ea431eaa1f8d86",
      "filename": "client/src/main/java/com/metamx/druid/QueryableNode.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/druid/blob/fa5f46057c0d19e7dbdb842db00f51d8356dcd8e/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2FQueryableNode.java",
      "raw_url": "https://github.com/apache/druid/raw/fa5f46057c0d19e7dbdb842db00f51d8356dcd8e/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2FQueryableNode.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2FQueryableNode.java?ref=fa5f46057c0d19e7dbdb842db00f51d8356dcd8e",
      "patch": "@@ -346,8 +346,7 @@ private void initializeServerView()\n     }\n   }\n \n-  private void\n-  initializeInventoryView()\n+  private void initializeInventoryView()\n   {\n     if (inventoryView == null) {\n       initializeServerInventoryView();",
      "parent_sha": "e4c2a2c705d3ec4ca954eda46233c5111a94f570"
    }
  },
  {
    "oid": "2c53af4d663683c8096334169e29c455c8cca16f",
    "message": "ForkingTaskRunner: Upload task logs even when job fails",
    "date": "2014-01-08T22:46:18Z",
    "url": "https://github.com/apache/druid/commit/2c53af4d663683c8096334169e29c455c8cca16f",
    "details": {
      "sha": "2be8a6a6a3b6e699b786870c57078f7b38bfbb6f",
      "filename": "indexing-service/src/main/java/io/druid/indexing/overlord/ForkingTaskRunner.java",
      "status": "modified",
      "additions": 14,
      "deletions": 23,
      "changes": 37,
      "blob_url": "https://github.com/apache/druid/blob/2c53af4d663683c8096334169e29c455c8cca16f/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Foverlord%2FForkingTaskRunner.java",
      "raw_url": "https://github.com/apache/druid/raw/2c53af4d663683c8096334169e29c455c8cca16f/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Foverlord%2FForkingTaskRunner.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Foverlord%2FForkingTaskRunner.java?ref=2c53af4d663683c8096334169e29c455c8cca16f",
      "patch": "@@ -218,29 +218,20 @@ public TaskStatus call()\n                             }\n \n                             log.info(\"Logging task %s output to: %s\", task.getId(), logFile);\n-\n-                            final InputStream fromProc = processHolder.process.getInputStream();\n-                            final OutputStream toLogfile = closer.register(\n-                                Files.newOutputStreamSupplier(logFile).getOutput()\n-                            );\n-\n                             boolean runFailed = true;\n \n-                            ByteStreams.copy(fromProc, toLogfile);\n-                            final int statusCode = processHolder.process.waitFor();\n-                            log.info(\"Process exited with status[%d] for task: %s\", statusCode, task.getId());\n-\n-                            if (statusCode == 0) {\n-                              runFailed = false;\n+                            try (final OutputStream toLogfile = Files.newOutputStreamSupplier(logFile).getOutput()) {\n+                              ByteStreams.copy(processHolder.process.getInputStream(), toLogfile);\n+                              final int statusCode = processHolder.process.waitFor();\n+                              log.info(\"Process exited with status[%d] for task: %s\", statusCode, task.getId());\n+                              if (statusCode == 0) {\n+                                runFailed = false;\n+                              }\n+                            }\n+                            finally {\n+                              // Upload task logs\n+                              taskLogPusher.pushTaskLog(task.getId(), logFile);\n                             }\n-\n-                            // Upload task logs\n-\n-                            // XXX: Consider uploading periodically for very long-lived tasks to prevent\n-                            // XXX: bottlenecks at the end or the possibility of losing a lot of logs all\n-                            // XXX: at once.\n-\n-                            taskLogPusher.pushTaskLog(task.getId(), logFile);\n \n                             if (!runFailed) {\n                               // Process exited successfully\n@@ -255,9 +246,9 @@ public TaskStatus call()\n                             closer.close();\n                           }\n                         }\n-                        catch (Exception e) {\n-                          log.info(e, \"Exception caught during execution\");\n-                          throw Throwables.propagate(e);\n+                        catch (Throwable t) {\n+                          log.info(t, \"Exception caught during execution\");\n+                          throw Throwables.propagate(t);\n                         }\n                         finally {\n                           try {",
      "parent_sha": "7f430d9fdef013ad494b393159bd251c48727866"
    }
  },
  {
    "oid": "cf841b8e67ae9840fa8942cf060aaa20abae6543",
    "message": "Fix incorrect class in BaseMacroFunctionExpr.equals. (#16294)\n\nThe equals method cast to the wrong class, potentially leading to\r\nClassCastException.",
    "date": "2024-04-16T16:40:46Z",
    "url": "https://github.com/apache/druid/commit/cf841b8e67ae9840fa8942cf060aaa20abae6543",
    "details": {
      "sha": "4910baafefba7f09aa362a3d0fcc6a734fab4302",
      "filename": "processing/src/main/java/org/apache/druid/math/expr/ExprMacroTable.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/cf841b8e67ae9840fa8942cf060aaa20abae6543/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fmath%2Fexpr%2FExprMacroTable.java",
      "raw_url": "https://github.com/apache/druid/raw/cf841b8e67ae9840fa8942cf060aaa20abae6543/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fmath%2Fexpr%2FExprMacroTable.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fmath%2Fexpr%2FExprMacroTable.java?ref=cf841b8e67ae9840fa8942cf060aaa20abae6543",
      "patch": "@@ -172,7 +172,7 @@ public boolean equals(Object o)\n       if (o == null || getClass() != o.getClass()) {\n         return false;\n       }\n-      BaseScalarMacroFunctionExpr that = (BaseScalarMacroFunctionExpr) o;\n+      BaseMacroFunctionExpr that = (BaseMacroFunctionExpr) o;\n       return Objects.equals(macro, that.macro) &&\n              Objects.equals(args, that.args);\n     }",
      "parent_sha": "ad6bd6214057f379b1f31968e9a41180b0210804"
    }
  },
  {
    "oid": "1e79a1be82218260f950d8e309dfab23ded732de",
    "message": "fix useExplicitVersion (#3559)",
    "date": "2016-10-10T19:28:06Z",
    "url": "https://github.com/apache/druid/commit/1e79a1be82218260f950d8e309dfab23ded732de",
    "details": {
      "sha": "94e2e52e9491674084a20a5ac0d1ac09074ba211",
      "filename": "indexing-service/src/main/java/io/druid/indexing/common/task/HadoopIndexTask.java",
      "status": "modified",
      "additions": 12,
      "deletions": 10,
      "changes": 22,
      "blob_url": "https://github.com/apache/druid/blob/1e79a1be82218260f950d8e309dfab23ded732de/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Fcommon%2Ftask%2FHadoopIndexTask.java",
      "raw_url": "https://github.com/apache/druid/raw/1e79a1be82218260f950d8e309dfab23ded732de/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Fcommon%2Ftask%2FHadoopIndexTask.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Fcommon%2Ftask%2FHadoopIndexTask.java?ref=1e79a1be82218260f950d8e309dfab23ded732de",
      "patch": "@@ -73,7 +73,7 @@ private static String getTheDataSource(HadoopIngestionSpec spec)\n   /**\n    * @param spec is used by the HadoopDruidIndexerJob to set up the appropriate parameters\n    *             for creating Druid index segments. It may be modified.\n-   *             <p>\n+   *             <p/>\n    *             Here, we will ensure that the DbConnectorConfig field of the spec is set to null, such that the\n    *             job does not push a list of published segments the database. Instead, we will use the method\n    *             IndexGeneratorJob.getPublishedSegments() to simply return a list of the published\n@@ -204,15 +204,17 @@ public TaskStatus run(TaskToolbox toolbox) throws Exception\n     }\n \n     final String specVersion = indexerSchema.getTuningConfig().getVersion();\n-    if (indexerSchema.getTuningConfig().isUseExplicitVersion() && version.compareTo(specVersion) > 0) {\n-      version = specVersion;\n-    } else {\n-      log.error(\n-          \"Spec version can not be greater than lock version, Spec version: [%s] Lock version: [%s].\",\n-          specVersion,\n-          version\n-      );\n-      return TaskStatus.failure(getId());\n+    if (indexerSchema.getTuningConfig().isUseExplicitVersion()) {\n+      if (specVersion.compareTo(version) < 0) {\n+        version = specVersion;\n+      } else {\n+        log.error(\n+            \"Spec version can not be greater than or equal to the lock version, Spec version: [%s] Lock version: [%s].\",\n+            specVersion,\n+            version\n+        );\n+        return TaskStatus.failure(getId());\n+      }\n     }\n \n     log.info(\"Setting version to: %s\", version);",
      "parent_sha": "3a83e0513e1b1b7dfb43b285afc5cf89ae418a53"
    }
  },
  {
    "oid": "36e67655966815a79151bf94a4bb912362564946",
    "message": "Fix flaky test (#13603)",
    "date": "2023-01-03T08:22:05Z",
    "url": "https://github.com/apache/druid/commit/36e67655966815a79151bf94a4bb912362564946",
    "details": {
      "sha": "3bd25f4c197e487e27442944caad815fe75a71b2",
      "filename": "server/src/test/java/org/apache/druid/server/coordinator/duty/KillUnusedSegmentsTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/druid/blob/36e67655966815a79151bf94a4bb912362564946/server%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fserver%2Fcoordinator%2Fduty%2FKillUnusedSegmentsTest.java",
      "raw_url": "https://github.com/apache/druid/raw/36e67655966815a79151bf94a4bb912362564946/server%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fserver%2Fcoordinator%2Fduty%2FKillUnusedSegmentsTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fserver%2Fcoordinator%2Fduty%2FKillUnusedSegmentsTest.java?ref=36e67655966815a79151bf94a4bb912362564946",
      "patch": "@@ -120,10 +120,11 @@ public void setup()\n         )\n     ).thenAnswer(invocation -> {\n       DateTime maxEndTime = invocation.getArgument(1);\n+      long maxEndMillis = maxEndTime.getMillis();\n       List<Interval> unusedIntervals =\n           unusedSegments.stream()\n                         .map(DataSegment::getInterval)\n-                        .filter(i -> i.getEnd().isBefore(maxEndTime))\n+                        .filter(i -> i.getEnd().getMillis() <= maxEndMillis)\n                         .collect(Collectors.toList());\n \n       int limit = invocation.getArgument(2);",
      "parent_sha": "313d937236efa242bc7b3ae144b50405a0499761"
    }
  },
  {
    "oid": "a4b95af839a16dacc217ef2733fa2ea0391c1be1",
    "message": "Fix grouper closing in GroupByMergingQueryRunnerV2. (#3316)\n\nThe grouperHolder should be closed on failure, not the grouper.",
    "date": "2016-08-03T04:02:30Z",
    "url": "https://github.com/apache/druid/commit/a4b95af839a16dacc217ef2733fa2ea0391c1be1",
    "details": {
      "sha": "3b467e5917b0a3817c0b7672e2a28da17ed2bf89",
      "filename": "processing/src/main/java/io/druid/query/groupby/epinephelinae/GroupByMergingQueryRunnerV2.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/druid/blob/a4b95af839a16dacc217ef2733fa2ea0391c1be1/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Fgroupby%2Fepinephelinae%2FGroupByMergingQueryRunnerV2.java",
      "raw_url": "https://github.com/apache/druid/raw/a4b95af839a16dacc217ef2733fa2ea0391c1be1/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Fgroupby%2Fepinephelinae%2FGroupByMergingQueryRunnerV2.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Fgroupby%2Fepinephelinae%2FGroupByMergingQueryRunnerV2.java?ref=a4b95af839a16dacc217ef2733fa2ea0391c1be1",
      "patch": "@@ -133,6 +133,8 @@ public Sequence<Row> run(final Query queryParam, final Map responseContext)\n         String.format(\"druid-groupBy-%s_%s\", UUID.randomUUID(), query.getId())\n     );\n \n+    final int priority = BaseQuery.getContextPriority(query, 0);\n+\n     // Figure out timeoutAt time now, so we can apply the timeout to both the mergeBufferPool.take and the actual\n     // query processing together.\n     final Number queryTimeout = query.getContextValue(QueryContextKeys.TIMEOUT, null);\n@@ -181,9 +183,6 @@ public CloseableGrouperIterator<RowBasedKey, Row> make()\n               );\n               final Grouper<RowBasedKey> grouper = pair.lhs;\n               final Accumulator<Grouper<RowBasedKey>, Row> accumulator = pair.rhs;\n-              closeOnFailure.add(grouper);\n-\n-              final int priority = BaseQuery.getContextPriority(query, 0);\n \n               final ReferenceCountingResourceHolder<Grouper<RowBasedKey>> grouperHolder = new ReferenceCountingResourceHolder<>(\n                   grouper,\n@@ -196,6 +195,7 @@ public void close() throws IOException\n                     }\n                   }\n               );\n+              closeOnFailure.add(grouperHolder);\n \n               ListenableFuture<List<Boolean>> futures = Futures.allAsList(\n                   Lists.newArrayList(",
      "parent_sha": "0299ac73b816a6ac1bfeb953d2a39a3c8277d2c2"
    }
  },
  {
    "oid": "bd6dcd397386c4f915335e6c1468a2340a034513",
    "message": "interruption!",
    "date": "2013-07-10T00:39:19Z",
    "url": "https://github.com/apache/druid/commit/bd6dcd397386c4f915335e6c1468a2340a034513",
    "details": {
      "sha": "dc846261f9e496a334f836b6ee161deb77c637da",
      "filename": "examples/src/main/java/druid/examples/webStream/InputSupplierUpdateStream.java",
      "status": "modified",
      "additions": 15,
      "deletions": 2,
      "changes": 17,
      "blob_url": "https://github.com/apache/druid/blob/bd6dcd397386c4f915335e6c1468a2340a034513/examples%2Fsrc%2Fmain%2Fjava%2Fdruid%2Fexamples%2FwebStream%2FInputSupplierUpdateStream.java",
      "raw_url": "https://github.com/apache/druid/raw/bd6dcd397386c4f915335e6c1468a2340a034513/examples%2Fsrc%2Fmain%2Fjava%2Fdruid%2Fexamples%2FwebStream%2FInputSupplierUpdateStream.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/examples%2Fsrc%2Fmain%2Fjava%2Fdruid%2Fexamples%2FwebStream%2FInputSupplierUpdateStream.java?ref=bd6dcd397386c4f915335e6c1468a2340a034513",
      "patch": "@@ -19,13 +19,16 @@\n \n package druid.examples.webStream;\n \n+import com.fasterxml.jackson.core.JsonParseException;\n import com.fasterxml.jackson.core.type.TypeReference;\n+import com.fasterxml.jackson.databind.JsonMappingException;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.google.common.io.InputSupplier;\n import com.metamx.druid.jackson.DefaultObjectMapper;\n import com.metamx.emitter.EmittingLogger;\n \n import java.io.BufferedReader;\n+import java.io.IOException;\n import java.util.HashMap;\n import java.util.Map;\n import java.util.concurrent.ArrayBlockingQueue;\n@@ -70,8 +73,18 @@ public void run()\n             }\n           }\n \n-          catch (Exception e) {\n-            log.info(e,e.getMessage());\n+          catch (InterruptedException e){\n+            log.info(e, \"Thread adding events to the queue interrupted\");\n+            return;\n+          }\n+          catch (JsonMappingException e) {\n+            log.info(e, \"Error in converting json to map\");\n+          }\n+          catch (JsonParseException e) {\n+            log.info(e, \"Error in parsing json\");\n+          }\n+          catch (IOException e) {\n+            log.info(e, \"Error in connecting to InputStream\");\n           }\n         }\n       }",
      "parent_sha": "ba484fca5cb794f0e770ae5a8a8850d517a0e18b"
    }
  },
  {
    "oid": "ffdb9407e5d6052e563c433568776dcccee35af9",
    "message": "build at Tue Dec 22 15:30:00 CST 2015",
    "date": "2015-12-22T21:30:00Z",
    "url": "https://github.com/apache/druid/commit/ffdb9407e5d6052e563c433568776dcccee35af9",
    "details": {
      "sha": "6ed35c4f428bd3dc3cc534e397afcdcd4ba666fc",
      "filename": "server/src/test/java/io/druid/curator/CuratorTestBase.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/ffdb9407e5d6052e563c433568776dcccee35af9/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fcurator%2FCuratorTestBase.java",
      "raw_url": "https://github.com/apache/druid/raw/ffdb9407e5d6052e563c433568776dcccee35af9/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fcurator%2FCuratorTestBase.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fcurator%2FCuratorTestBase.java?ref=ffdb9407e5d6052e563c433568776dcccee35af9",
      "patch": "@@ -158,4 +158,4 @@ protected void tearDownServerAndCurator()\n \n }\n \n-//Build at Tue Dec 22 15:00:00 CST 2015\n+//Build at Tue Dec 22 15:30:00 CST 2015",
      "parent_sha": "6f54c1b60d940a55e93236c41386419bf2bf5dee"
    }
  },
  {
    "oid": "a5651ea4747e05b0982cb2ba3915b95ae44a47ca",
    "message": "Remove primitive value instead of CoordinatorStats from LoadRule.assign method (#4469)",
    "date": "2017-06-29T17:48:49Z",
    "url": "https://github.com/apache/druid/commit/a5651ea4747e05b0982cb2ba3915b95ae44a47ca",
    "details": {
      "sha": "f14a4f3581e03a0c6ede3518bf46e53cfaf1663d",
      "filename": "server/src/main/java/io/druid/server/coordinator/rules/LoadRule.java",
      "status": "modified",
      "additions": 7,
      "deletions": 9,
      "changes": 16,
      "blob_url": "https://github.com/apache/druid/blob/a5651ea4747e05b0982cb2ba3915b95ae44a47ca/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fcoordinator%2Frules%2FLoadRule.java",
      "raw_url": "https://github.com/apache/druid/raw/a5651ea4747e05b0982cb2ba3915b95ae44a47ca/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fcoordinator%2Frules%2FLoadRule.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fcoordinator%2Frules%2FLoadRule.java?ref=a5651ea4747e05b0982cb2ba3915b95ae44a47ca",
      "patch": "@@ -89,7 +89,7 @@ public CoordinatorStats run(DruidCoordinator coordinator, DruidCoordinatorRuntim\n \n       final BalancerStrategy strategy = params.getBalancerStrategy();\n       if (availableSegments.contains(segment)) {\n-        CoordinatorStats assignStats = assign(\n+        int assignedCount = assign(\n             params.getReplicationManager(),\n             tier,\n             totalReplicantsInCluster,\n@@ -99,8 +99,8 @@ public CoordinatorStats run(DruidCoordinator coordinator, DruidCoordinatorRuntim\n             serverHolderList,\n             segment\n         );\n-        stats.accumulate(assignStats);\n-        totalReplicantsInCluster += assignStats.getTieredStat(ASSIGNED_COUNT, tier);\n+        stats.addToTieredStat(ASSIGNED_COUNT, tier, assignedCount);\n+        totalReplicantsInCluster += assignedCount;\n       }\n \n       loadStatus.put(tier, expectedReplicantsInTier - loadedReplicantsInTier);\n@@ -112,7 +112,7 @@ public CoordinatorStats run(DruidCoordinator coordinator, DruidCoordinatorRuntim\n     return stats;\n   }\n \n-  private CoordinatorStats assign(\n+  private int assign(\n       final ReplicationThrottler replicationManager,\n       final String tier,\n       final int totalReplicantsInCluster,\n@@ -123,9 +123,7 @@ private CoordinatorStats assign(\n       final DataSegment segment\n   )\n   {\n-    final CoordinatorStats stats = new CoordinatorStats();\n-    stats.addToTieredStat(ASSIGNED_COUNT, tier, 0);\n-\n+    int assignedCount = 0;\n     int currReplicantsInTier = totalReplicantsInTier;\n     int currTotalReplicantsInCluster = totalReplicantsInCluster;\n     while (currReplicantsInTier < expectedReplicantsInTier) {\n@@ -169,12 +167,12 @@ public void execute()\n           }\n       );\n \n-      stats.addToTieredStat(ASSIGNED_COUNT, tier, 1);\n+      ++assignedCount;\n       ++currReplicantsInTier;\n       ++currTotalReplicantsInCluster;\n     }\n \n-    return stats;\n+    return assignedCount;\n   }\n \n   private CoordinatorStats drop(",
      "parent_sha": "ae900a493479bf0be2cef5ce598a6822caa3def1"
    }
  },
  {
    "oid": "54b3f363c4fc73fcc47b757bda8071a877b967f9",
    "message": "Remove unnecessary principal handling in KerberosAuthenticator (#7685)",
    "date": "2019-05-23T20:15:44Z",
    "url": "https://github.com/apache/druid/commit/54b3f363c4fc73fcc47b757bda8071a877b967f9",
    "details": {
      "sha": "801d3947211175281c1aad20f27d1b8804e6537a",
      "filename": "extensions-core/druid-kerberos/src/main/java/org/apache/druid/security/kerberos/KerberosAuthenticator.java",
      "status": "modified",
      "additions": 13,
      "deletions": 112,
      "changes": 125,
      "blob_url": "https://github.com/apache/druid/blob/54b3f363c4fc73fcc47b757bda8071a877b967f9/extensions-core%2Fdruid-kerberos%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsecurity%2Fkerberos%2FKerberosAuthenticator.java",
      "raw_url": "https://github.com/apache/druid/raw/54b3f363c4fc73fcc47b757bda8071a877b967f9/extensions-core%2Fdruid-kerberos%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsecurity%2Fkerberos%2FKerberosAuthenticator.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/extensions-core%2Fdruid-kerberos%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsecurity%2Fkerberos%2FKerberosAuthenticator.java?ref=54b3f363c4fc73fcc47b757bda8071a877b967f9",
      "patch": "@@ -43,20 +43,9 @@\n import org.apache.hadoop.security.authentication.util.SignerSecretProvider;\n import org.eclipse.jetty.client.api.Request;\n import org.eclipse.jetty.http.HttpHeader;\n-import sun.security.krb5.EncryptedData;\n-import sun.security.krb5.EncryptionKey;\n-import sun.security.krb5.internal.APReq;\n-import sun.security.krb5.internal.EncTicketPart;\n-import sun.security.krb5.internal.Krb5;\n-import sun.security.krb5.internal.Ticket;\n-import sun.security.krb5.internal.crypto.KeyUsage;\n-import sun.security.util.DerInputStream;\n-import sun.security.util.DerValue;\n \n import javax.security.auth.Subject;\n-import javax.security.auth.kerberos.KerberosKey;\n import javax.security.auth.kerberos.KerberosPrincipal;\n-import javax.security.auth.kerberos.KeyTab;\n import javax.security.auth.login.AppConfigurationEntry;\n import javax.security.auth.login.Configuration;\n import javax.security.auth.login.LoginContext;\n@@ -89,7 +78,6 @@\n import java.util.Set;\n import java.util.TimeZone;\n import java.util.concurrent.ThreadLocalRandom;\n-import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n \n@@ -230,56 +218,36 @@ protected AuthenticationToken getToken(HttpServletRequest request) throws Authen\n       public void doFilter(ServletRequest request, ServletResponse response, FilterChain filterChain)\n           throws IOException, ServletException\n       {\n-        HttpServletRequest httpReq = (HttpServletRequest) request;\n-\n         // If there's already an auth result, then we have authenticated already, skip this.\n         if (request.getAttribute(AuthConfig.DRUID_AUTHENTICATION_RESULT) != null) {\n           filterChain.doFilter(request, response);\n           return;\n         }\n \n+        // In the hadoop-auth 2.7.3 code that this was adapted from, the login would've occurred during init() of\n+        // the AuthenticationFilter via `initializeAuthHandler(authHandlerClassName, filterConfig)`.\n+        // Since we co-exist with other authentication schemes, don't login until we've checked that\n+        // some other Authenticator didn't already validate this request.\n         if (loginContext == null) {\n           initializeKerberosLogin();\n         }\n \n+        // Checking for excluded paths is Druid-specific, not from hadoop-auth\n         String path = ((HttpServletRequest) request).getRequestURI();\n         if (isExcluded(path)) {\n           filterChain.doFilter(request, response);\n         } else {\n-          String clientPrincipal;\n-          try {\n-            Cookie[] cookies = httpReq.getCookies();\n-            if (cookies == null) {\n-              clientPrincipal = getPrincipalFromRequestNew((HttpServletRequest) request);\n-            } else {\n-              clientPrincipal = null;\n-              for (Cookie cookie : cookies) {\n-                if (\"hadoop.auth\".equals(cookie.getName())) {\n-                  Matcher matcher = HADOOP_AUTH_COOKIE_REGEX.matcher(cookie.getValue());\n-                  if (matcher.matches()) {\n-                    clientPrincipal = matcher.group(1);\n-                    break;\n-                  }\n-                }\n-              }\n-            }\n-          }\n-          catch (Exception ex) {\n-            clientPrincipal = null;\n-          }\n-\n-          if (clientPrincipal != null) {\n-            request.setAttribute(\n-                AuthConfig.DRUID_AUTHENTICATION_RESULT,\n-                new AuthenticationResult(clientPrincipal, authorizerName, name, null)\n-            );\n-          }\n+          // Run the original doFilter method, but with modifications to error handling\n+          doFilterSuper(request, response, filterChain);\n         }\n-\n-        doFilterSuper(request, response, filterChain);\n       }\n \n-      // Copied from hadoop-auth's AuthenticationFilter, to allow us to change error response handling\n+\n+      /**\n+       * Copied from hadoop-auth 2.7.3 AuthenticationFilter, to allow us to change error response handling.\n+       * Specifically, we want to defer the sending of 401 Unauthorized so that other Authenticators later in the chain\n+       * can check the request.\n+       */\n       private void doFilterSuper(ServletRequest request, ServletResponse response, FilterChain filterChain)\n           throws IOException, ServletException\n       {\n@@ -550,73 +518,6 @@ public AppConfigurationEntry[] getAppConfigurationEntry(String name)\n     }\n   }\n \n-  private String getPrincipalFromRequestNew(HttpServletRequest req)\n-  {\n-    String authorization = req.getHeader(org.apache.hadoop.security.authentication.client.KerberosAuthenticator.AUTHORIZATION);\n-    if (authorization == null\n-        || !authorization.startsWith(org.apache.hadoop.security.authentication.client.KerberosAuthenticator.NEGOTIATE)) {\n-      return null;\n-    } else {\n-      authorization = authorization.substring(org.apache.hadoop.security.authentication.client.KerberosAuthenticator.NEGOTIATE\n-                                                  .length()).trim();\n-      final byte[] clientToken = StringUtils.decodeBase64String(authorization);\n-      try {\n-        DerInputStream ticketStream = new DerInputStream(clientToken);\n-        DerValue[] values = ticketStream.getSet(clientToken.length, true);\n-\n-        // see this link for AP-REQ format: https://tools.ietf.org/html/rfc1510#section-5.5.1\n-        for (DerValue value : values) {\n-          if (isValueAPReq(value)) {\n-            APReq apReq = new APReq(value);\n-            Ticket ticket = apReq.ticket;\n-            EncryptedData encData = ticket.encPart;\n-            int eType = encData.getEType();\n-\n-            // find the server's key\n-            EncryptionKey finalKey = null;\n-            Subject serverSubj = loginContext.getSubject();\n-            Set<Object> serverCreds = serverSubj.getPrivateCredentials(Object.class);\n-            for (Object cred : serverCreds) {\n-              if (cred instanceof KeyTab) {\n-                KeyTab serverKeyTab = (KeyTab) cred;\n-                KerberosPrincipal kerberosPrincipal = new KerberosPrincipal(serverPrincipal);\n-                KerberosKey[] serverKeys = serverKeyTab.getKeys(kerberosPrincipal);\n-                for (KerberosKey key : serverKeys) {\n-                  if (key.getKeyType() == eType) {\n-                    finalKey = new EncryptionKey(key.getKeyType(), key.getEncoded());\n-                    break;\n-                  }\n-                }\n-              }\n-            }\n-\n-            if (finalKey == null) {\n-              log.error(\"Could not find matching key from server creds.\");\n-              return null;\n-            }\n-\n-            // decrypt the ticket with the server's key\n-            byte[] decryptedBytes = encData.decrypt(finalKey, KeyUsage.KU_TICKET);\n-            decryptedBytes = encData.reset(decryptedBytes);\n-            EncTicketPart decrypted = new EncTicketPart(decryptedBytes);\n-            String clientPrincipal = decrypted.cname.toString();\n-            return clientPrincipal;\n-          }\n-        }\n-      }\n-      catch (Exception ex) {\n-        throw new RuntimeException(ex);\n-      }\n-    }\n-\n-    return null;\n-  }\n-\n-  private boolean isValueAPReq(DerValue value)\n-  {\n-    return value.isConstructed((byte) Krb5.KRB_AP_REQ);\n-  }\n-\n   private void initializeKerberosLogin() throws ServletException\n   {\n     String keytab;",
      "parent_sha": "23e96d15d41fef0cfe4d1815421b19fca3005334"
    }
  },
  {
    "oid": "2118252ef236489dbc580f217328b7d304278b2b",
    "message": "make memcached host configurable for benchmarking",
    "date": "2012-12-18T00:44:39Z",
    "url": "https://github.com/apache/druid/commit/2118252ef236489dbc580f217328b7d304278b2b",
    "details": {
      "sha": "72a1d0da932dd9d90eb0109a4fa58611fac6b4a9",
      "filename": "client/src/test/java/com/metamx/druid/client/cache/MemcachedCacheBrokerBenchmark.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/2118252ef236489dbc580f217328b7d304278b2b/client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fclient%2Fcache%2FMemcachedCacheBrokerBenchmark.java",
      "raw_url": "https://github.com/apache/druid/raw/2118252ef236489dbc580f217328b7d304278b2b/client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fclient%2Fcache%2FMemcachedCacheBrokerBenchmark.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fclient%2Fcache%2FMemcachedCacheBrokerBenchmark.java?ref=2118252ef236489dbc580f217328b7d304278b2b",
      "patch": "@@ -46,7 +46,7 @@ protected void setUp() throws Exception\n                                       .setTranscoder(transcoder)\n                                       .setShouldOptimize(true)\n                                       .build(),\n-        AddrUtil.getAddresses(\"localhost:11211\")\n+        AddrUtil.getAddresses(System.getProperty(\"druid.bard.cache.hosts\", \"localhost:11211\"))\n     );\n \n     broker = new MemcachedCacheBroker(",
      "parent_sha": "e34944b1c525a48126b3f5d5520f7dec0b4ef923"
    }
  },
  {
    "oid": "c4de26ef099175b22dd3637338b9fe54d08e6516",
    "message": "fix typo in InitializationTest.java",
    "date": "2015-12-09T06:11:03Z",
    "url": "https://github.com/apache/druid/commit/c4de26ef099175b22dd3637338b9fe54d08e6516",
    "details": {
      "sha": "335d51e1daac25e2a33707c40e1996752eb65d5d",
      "filename": "server/src/test/java/io/druid/initialization/InitializationTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/c4de26ef099175b22dd3637338b9fe54d08e6516/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Finitialization%2FInitializationTest.java",
      "raw_url": "https://github.com/apache/druid/raw/c4de26ef099175b22dd3637338b9fe54d08e6516/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Finitialization%2FInitializationTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Finitialization%2FInitializationTest.java?ref=c4de26ef099175b22dd3637338b9fe54d08e6516",
      "patch": "@@ -192,7 +192,7 @@ public void testGetExtensionFilesToLoad_non_exist_extensions_dir() throws IOExce\n     final File tmpDir = temporaryFolder.newFolder();\n     Assert.assertTrue(\"could not create missing folder\", !tmpDir.exists() || tmpDir.delete());\n     Assert.assertArrayEquals(\n-        \"Non-exist root extensionsDir should return emply array of File\",\n+        \"Non-exist root extensionsDir should return an empty array of File\",\n         new File[]{},\n         Initialization.getExtensionFilesToLoad(new ExtensionsConfig(){\n           @Override\n@@ -233,7 +233,7 @@ public String getDirectory()\n     };\n \n     Assert.assertArrayEquals(\n-        \"Empty root extensionsDir should return emply array of File\",\n+        \"Empty root extensionsDir should return an empty array of File\",\n         new File[]{},\n         Initialization.getExtensionFilesToLoad(config)\n     );",
      "parent_sha": "788acdbd1cb2f29bfba8625f38f1f358c0e74a73"
    }
  },
  {
    "oid": "1a972ef6420e5469dc6826ad023b6f43748d885e",
    "message": "Fix Hadoop Indexing CLI for versions JDK v9+ (#17569)\n\nFor JDK v9+ this job will fail as URLClassloader is no longer the default classloader. Additionally, with the introduction of the jmod system, the job failed to find the correct JDK-included modules because the new classloader constructed for the job didn't reference the system classloader. Adding ClassLoader.getSystemClassLoader() as the parent classloader fixes this.",
    "date": "2025-01-07T21:59:34Z",
    "url": "https://github.com/apache/druid/commit/1a972ef6420e5469dc6826ad023b6f43748d885e",
    "details": {
      "sha": "a1a382318112842bfd2510a135423b4b34890cda",
      "filename": "services/src/main/java/org/apache/druid/cli/CliHadoopIndexer.java",
      "status": "modified",
      "additions": 9,
      "deletions": 6,
      "changes": 15,
      "blob_url": "https://github.com/apache/druid/blob/1a972ef6420e5469dc6826ad023b6f43748d885e/services%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fcli%2FCliHadoopIndexer.java",
      "raw_url": "https://github.com/apache/druid/raw/1a972ef6420e5469dc6826ad023b6f43748d885e/services%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fcli%2FCliHadoopIndexer.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/services%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fcli%2FCliHadoopIndexer.java?ref=1a972ef6420e5469dc6826ad023b6f43748d885e",
      "patch": "@@ -29,6 +29,7 @@\n import org.apache.druid.indexing.common.config.TaskConfig;\n import org.apache.druid.indexing.common.task.Initialization;\n import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.utils.JvmUtils;\n \n import java.io.File;\n import java.lang.reflect.Method;\n@@ -39,6 +40,7 @@\n import java.util.List;\n \n /**\n+ *\n  */\n @Command(\n     name = \"hadoop\",\n@@ -55,12 +57,12 @@ public class CliHadoopIndexer implements Runnable\n   private String argumentSpec;\n \n   @Option(name = {\"-c\", \"--coordinate\", \"hadoopDependencies\"},\n-          description = \"extra dependencies to pull down (e.g. non-default hadoop coordinates or extra hadoop jars)\")\n+      description = \"extra dependencies to pull down (e.g. non-default hadoop coordinates or extra hadoop jars)\")\n   @SuppressWarnings(\"MismatchedQueryAndUpdateOfCollection\")\n   private List<String> coordinates;\n \n   @Option(name = \"--no-default-hadoop\",\n-          description = \"don't pull down the default hadoop version\")\n+      description = \"don't pull down the default hadoop version\")\n   public boolean noDefaultHadoop;\n \n   @Inject\n@@ -84,9 +86,7 @@ public void run()\n         extensionURLs.addAll(Arrays.asList(extensionLoader.getURLs()));\n       }\n \n-      final List<URL> nonHadoopURLs = Arrays.asList(\n-          ((URLClassLoader) CliHadoopIndexer.class.getClassLoader()).getURLs()\n-      );\n+      final List<URL> nonHadoopURLs = JvmUtils.systemClassPath();\n \n       final List<URL> driverURLs = new ArrayList<>(nonHadoopURLs);\n       // put hadoop dependencies last to avoid jets3t & apache.httpcore version conflicts\n@@ -95,7 +95,10 @@ public void run()\n         driverURLs.addAll(Arrays.asList(hadoopLoader.getURLs()));\n       }\n \n-      final URLClassLoader loader = new URLClassLoader(driverURLs.toArray(new URL[0]), null);\n+      final URLClassLoader loader = new URLClassLoader(\n+          driverURLs.toArray(new URL[0]),\n+          ClassLoader.getSystemClassLoader()\n+      );\n       Thread.currentThread().setContextClassLoader(loader);\n \n       final List<URL> jobUrls = new ArrayList<>();",
      "parent_sha": "951792a52e20cd1ddd885a144b23a0f00dd1e96a"
    }
  },
  {
    "oid": "6e18ff7ed4c46f4eecd34aa13138f931fb864e53",
    "message": "use prepared statement instead of string interpolation",
    "date": "2014-03-27T19:41:30Z",
    "url": "https://github.com/apache/druid/commit/6e18ff7ed4c46f4eecd34aa13138f931fb864e53",
    "details": {
      "sha": "54045782f468ba4bb03e6eadf542c18b6d268d1a",
      "filename": "server/src/main/java/io/druid/db/DatabaseRuleManager.java",
      "status": "modified",
      "additions": 8,
      "deletions": 6,
      "changes": 14,
      "blob_url": "https://github.com/apache/druid/blob/6e18ff7ed4c46f4eecd34aa13138f931fb864e53/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fdb%2FDatabaseRuleManager.java",
      "raw_url": "https://github.com/apache/druid/raw/6e18ff7ed4c46f4eecd34aa13138f931fb864e53/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fdb%2FDatabaseRuleManager.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fdb%2FDatabaseRuleManager.java?ref=6e18ff7ed4c46f4eecd34aa13138f931fb864e53",
      "patch": "@@ -74,13 +74,15 @@ public static void createDefaultRule(\n             @Override\n             public Void withHandle(Handle handle) throws Exception\n             {\n-              List<Map<String, Object>> existing = handle.select(\n-                  String.format(\n-                      \"SELECT id from %s where datasource='%s';\",\n-                      ruleTable,\n-                      defaultTier\n+              List<Map<String, Object>> existing = handle\n+                  .createQuery(\n+                      String.format(\n+                          \"SELECT id from %s where datasource=:dataSource;\",\n+                          ruleTable\n+                      )\n                   )\n-              );\n+                  .bind(\"dataSource\", defaultTier)\n+                  .list();\n \n               if (!existing.isEmpty()) {\n                 return null;",
      "parent_sha": "e0ff2aa0d6308bcbcf8d04b75de8c6ae064db373"
    }
  },
  {
    "oid": "10ec53dc1cf74e8a8271e344f913cbab8b8e2e3b",
    "message": "fix calculation of expected timestamp for query in ITRealtimeIndexTaskTest",
    "date": "2016-04-01T15:47:17Z",
    "url": "https://github.com/apache/druid/commit/10ec53dc1cf74e8a8271e344f913cbab8b8e2e3b",
    "details": {
      "sha": "7b31818cbc9f9fc1c217aa51782940c4e76a5a6a",
      "filename": "integration-tests/src/test/java/io/druid/tests/indexer/ITRealtimeIndexTaskTest.java",
      "status": "modified",
      "additions": 4,
      "deletions": 3,
      "changes": 7,
      "blob_url": "https://github.com/apache/druid/blob/10ec53dc1cf74e8a8271e344f913cbab8b8e2e3b/integration-tests%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Ftests%2Findexer%2FITRealtimeIndexTaskTest.java",
      "raw_url": "https://github.com/apache/druid/raw/10ec53dc1cf74e8a8271e344f913cbab8b8e2e3b/integration-tests%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Ftests%2Findexer%2FITRealtimeIndexTaskTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/integration-tests%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Ftests%2Findexer%2FITRealtimeIndexTaskTest.java?ref=10ec53dc1cf74e8a8271e344f913cbab8b8e2e3b",
      "patch": "@@ -81,6 +81,7 @@ public class ITRealtimeIndexTaskTest extends AbstractIndexerTest\n   private final DateTimeFormatter TIMESTAMP_FMT = DateTimeFormat.forPattern(\"yyyy-MM-dd'T'HH:mm:ss'.000Z'\");\n   private DateTime dtFirst;            // timestamp of 1st event\n   private DateTime dtLast;             // timestamp of last event\n+  private DateTime dtGroupBy;          // timestamp for expected response for groupBy query\n \n   @Inject\n   ServerDiscoveryFactory factory;\n@@ -130,9 +131,7 @@ public void testRealtimeIndexTask() throws Exception\n           .replace(\"%%TIMESERIES_RESPONSE_TIMESTAMP%%\", TIMESTAMP_FMT.print(dtFirst))\n           .replace(\"%%POST_AG_REQUEST_START%%\", INTERVAL_FMT.print(dtFirst))\n           .replace(\"%%POST_AG_REQUEST_END%%\", INTERVAL_FMT.print(dtLast.plusMinutes(2)))\n-          .replace(\n-              \"%%POST_AG_RESPONSE_TIMESTAMP%%\",\n-              TIMESTAMP_FMT.print(dtLast.minusSeconds(24).withSecondOfMinute(0))\n+          .replace(\"%%POST_AG_RESPONSE_TIMESTAMP%%\", TIMESTAMP_FMT.print(dtGroupBy.withSecondOfMinute(0))\n           );\n \n       // should hit the queries all on realtime task or some on realtime task\n@@ -215,6 +214,8 @@ public void postEvents() throws Exception\n       while ((line = reader.readLine()) != null) {\n         if (i == 15) { // for the 15th line, use a time before the window\n           dt = dt.minusMinutes(10);\n+        } else if (i == 16) { // remember this time to use in the expected response from the groupBy query\n+          dtGroupBy = dt;\n         } else if (i == 18) { // use a time 6 seconds ago so it will be out of order\n           dt = dt.minusSeconds(6);\n         }",
      "parent_sha": "2fc5918e69b13896b9a6da48a08c94dac08a0cd3"
    }
  },
  {
    "oid": "2c5c8198db5193aa02af76bc2b6946bdd9b6e606",
    "message": "Make query/cpu/time still report on error (#3535)",
    "date": "2016-10-18T13:26:21Z",
    "url": "https://github.com/apache/druid/commit/2c5c8198db5193aa02af76bc2b6946bdd9b6e606",
    "details": {
      "sha": "1a5dbd191adedf694d7a8e2ab181bcfdc8ebdb93",
      "filename": "processing/src/main/java/io/druid/query/CPUTimeMetricQueryRunner.java",
      "status": "modified",
      "additions": 5,
      "deletions": 7,
      "changes": 12,
      "blob_url": "https://github.com/apache/druid/blob/2c5c8198db5193aa02af76bc2b6946bdd9b6e606/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2FCPUTimeMetricQueryRunner.java",
      "raw_url": "https://github.com/apache/druid/raw/2c5c8198db5193aa02af76bc2b6946bdd9b6e606/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2FCPUTimeMetricQueryRunner.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2FCPUTimeMetricQueryRunner.java?ref=2c5c8198db5193aa02af76bc2b6946bdd9b6e606",
      "patch": "@@ -21,8 +21,6 @@\n \n import com.google.common.base.Function;\n import com.google.common.base.Preconditions;\n-import com.google.common.base.Strings;\n-import com.google.common.util.concurrent.MoreExecutors;\n import com.metamx.common.ISE;\n import com.metamx.common.guava.Accumulator;\n import com.metamx.common.guava.Sequence;\n@@ -33,6 +31,7 @@\n import com.metamx.emitter.service.ServiceMetricEvent;\n import io.druid.common.utils.VMUtils;\n \n+import java.io.Closeable;\n import java.io.IOException;\n import java.util.Map;\n import java.util.concurrent.atomic.AtomicLong;\n@@ -70,7 +69,7 @@ public Sequence<T> run(\n   )\n   {\n     final Sequence<T> baseSequence = delegate.run(query, responseContext);\n-    return Sequences.withEffect(\n+    return Sequences.withBaggage(\n         new Sequence<T>()\n         {\n           @Override\n@@ -135,10 +134,10 @@ public void close() throws IOException\n             };\n           }\n         },\n-        new Runnable()\n+        new Closeable()\n         {\n           @Override\n-          public void run()\n+          public void close()\n           {\n             if (report) {\n               final long cpuTime = cpuTimeAccumulator.get();\n@@ -148,8 +147,7 @@ public void run()\n               }\n             }\n           }\n-        },\n-        MoreExecutors.sameThreadExecutor()\n+        }\n     );\n   }\n ",
      "parent_sha": "8ea5f9324dc4cb12b455be097fe1d341a98a7660"
    }
  },
  {
    "oid": "c6d555e1f3be6f3579e1c5bd9dd16d744ce595c5",
    "message": "better default",
    "date": "2014-11-04T18:18:27Z",
    "url": "https://github.com/apache/druid/commit/c6d555e1f3be6f3579e1c5bd9dd16d744ce595c5",
    "details": {
      "sha": "a906ed92abf86c6d43335740f36d3e092ea153b1",
      "filename": "server/src/main/java/io/druid/server/initialization/ServerConfig.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/c6d555e1f3be6f3579e1c5bd9dd16d744ce595c5/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Finitialization%2FServerConfig.java",
      "raw_url": "https://github.com/apache/druid/raw/c6d555e1f3be6f3579e1c5bd9dd16d744ce595c5/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Finitialization%2FServerConfig.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Finitialization%2FServerConfig.java?ref=c6d555e1f3be6f3579e1c5bd9dd16d744ce595c5",
      "patch": "@@ -31,7 +31,7 @@ public class ServerConfig\n {\n   @JsonProperty\n   @Min(1)\n-  private int numThreads = 30;\n+  private int numThreads = Math.max(10, (Runtime.getRuntime().availableProcessors() * 17) / 16 + 2) + 30;\n \n   @JsonProperty\n   @NotNull",
      "parent_sha": "42cf20c9f1842cc9257fe271b7b8bc4367795b57"
    }
  },
  {
    "oid": "28d85702aded503aa49e708771b4cbbbbd73688e",
    "message": "Fix rolling of request log files. (#3916)\n\n* Use common date format for request log files.\r\n\r\n* Remove code duplication in creating logging FileWriter.",
    "date": "2017-02-14T17:33:43Z",
    "url": "https://github.com/apache/druid/commit/28d85702aded503aa49e708771b4cbbbbd73688e",
    "details": {
      "sha": "88a61bd3122421b583b3dacdf422ca355e556166",
      "filename": "server/src/main/java/io/druid/server/log/FileRequestLogger.java",
      "status": "modified",
      "additions": 10,
      "deletions": 8,
      "changes": 18,
      "blob_url": "https://github.com/apache/druid/blob/28d85702aded503aa49e708771b4cbbbbd73688e/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Flog%2FFileRequestLogger.java",
      "raw_url": "https://github.com/apache/druid/raw/28d85702aded503aa49e708771b4cbbbbd73688e/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Flog%2FFileRequestLogger.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Flog%2FFileRequestLogger.java?ref=28d85702aded503aa49e708771b4cbbbbd73688e",
      "patch": "@@ -32,6 +32,7 @@\n import org.joda.time.MutableDateTime;\n \n import java.io.File;\n+import java.io.FileNotFoundException;\n import java.io.FileOutputStream;\n import java.io.IOException;\n import java.io.OutputStreamWriter;\n@@ -69,10 +70,7 @@ public void start()\n       synchronized (lock) {\n         currentDay = mutableDateTime.toDateTime();\n \n-        fileWriter = new OutputStreamWriter(\n-            new FileOutputStream(new File(baseDir, currentDay.toString(\"yyyy-MM-dd'.log'\")), true),\n-            Charsets.UTF_8\n-        );\n+        fileWriter = getFileWriter();\n       }\n       long nextDay = currentDay.plusDays(1).getMillis();\n       Duration delay = new Duration(nextDay - new DateTime().getMillis());\n@@ -90,10 +88,7 @@ public ScheduledExecutors.Signal call()\n                 synchronized (lock) {\n                   currentDay = currentDay.plusDays(1);\n                   CloseQuietly.close(fileWriter);\n-                  fileWriter = new OutputStreamWriter(\n-                      new FileOutputStream(new File(baseDir, currentDay.toString()), true),\n-                      Charsets.UTF_8\n-                  );\n+                  fileWriter = getFileWriter();\n                 }\n               }\n               catch (Exception e) {\n@@ -110,6 +105,13 @@ public ScheduledExecutors.Signal call()\n     }\n   }\n \n+  private OutputStreamWriter getFileWriter() throws FileNotFoundException {\n+    return new OutputStreamWriter(\n+            new FileOutputStream(new File(baseDir, currentDay.toString(\"yyyy-MM-dd'.log'\")), true),\n+            Charsets.UTF_8\n+    );\n+  }\n+\n   @LifecycleStop\n   public void stop()\n   {",
      "parent_sha": "af67e8904e2a98f5a5b991c876899c96295d55ec"
    }
  },
  {
    "oid": "5f39ee21ff9bbe15f0e2bb022cecb905bec2d8d2",
    "message": "Add diags for flaky CuratorModuleTest (#8528)\n\nAdd diagnostic messages to debug\r\nCuratorModuleTest.exitsJvmWhenMaxRetriesExceeded() intermittent test\r\nfailures.",
    "date": "2019-09-14T18:55:26Z",
    "url": "https://github.com/apache/druid/commit/5f39ee21ff9bbe15f0e2bb022cecb905bec2d8d2",
    "details": {
      "sha": "d17d76c99a2472ddb651698aaa9d57324dfb6336",
      "filename": "server/src/test/java/org/apache/druid/curator/CuratorModuleTest.java",
      "status": "modified",
      "additions": 4,
      "deletions": 3,
      "changes": 7,
      "blob_url": "https://github.com/apache/druid/blob/5f39ee21ff9bbe15f0e2bb022cecb905bec2d8d2/server%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fcurator%2FCuratorModuleTest.java",
      "raw_url": "https://github.com/apache/druid/raw/5f39ee21ff9bbe15f0e2bb022cecb905bec2d8d2/server%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fcurator%2FCuratorModuleTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fcurator%2FCuratorModuleTest.java?ref=5f39ee21ff9bbe15f0e2bb022cecb905bec2d8d2",
      "patch": "@@ -149,8 +149,9 @@ public void exitsJvmWhenMaxRetriesExceeded() throws Exception\n     List<LogEvent> loggingEvents = logger.getLogEvents();\n     Assert.assertFalse(loggingEvents.isEmpty());\n     LogEvent logEvent = loggingEvents.get(0);\n-    Assert.assertEquals(Level.ERROR, logEvent.getLevel());\n-    Assert.assertEquals(\"Unhandled error in Curator Framework\", logEvent.getMessage().getFormattedMessage());\n+    String message = \"Logging events: \" + loggingEvents;\n+    Assert.assertEquals(message, Level.ERROR, logEvent.getLevel());\n+    Assert.assertEquals(message, \"Unhandled error in Curator Framework\", logEvent.getMessage().getFormattedMessage());\n   }\n \n   @Ignore(\"Verifies changes in https://github.com/apache/incubator-druid/pull/8458, but overkill for regular testing\")\n@@ -187,7 +188,7 @@ private static CuratorFramework createCuratorFramework(Injector injector, int ma\n     CuratorFramework curatorFramework = injector.getInstance(CuratorFramework.class);\n     RetryPolicy retryPolicy = curatorFramework.getZookeeperClient().getRetryPolicy();\n     Assert.assertThat(retryPolicy, CoreMatchers.instanceOf(ExponentialBackoffRetry.class));\n-    RetryPolicy adjustedRetryPolicy = adjustRetryPolicy((BoundedExponentialBackoffRetry) retryPolicy, 0);\n+    RetryPolicy adjustedRetryPolicy = adjustRetryPolicy((BoundedExponentialBackoffRetry) retryPolicy, maxRetries);\n     curatorFramework.getZookeeperClient().setRetryPolicy(adjustedRetryPolicy);\n     return curatorFramework;\n   }",
      "parent_sha": "df14e5d696da4f3d3bc19edda40d4c11ee3e97b4"
    }
  },
  {
    "oid": "ada3ae08dfc432f34e4ef1ec87f9800c91ba2c54",
    "message": "Retain order in TaskReport. (#12005)",
    "date": "2022-03-04T16:06:20Z",
    "url": "https://github.com/apache/druid/commit/ada3ae08dfc432f34e4ef1ec87f9800c91ba2c54",
    "details": {
      "sha": "5e2f5b6c740944b05243c465ba6157177b317ee1",
      "filename": "indexing-service/src/main/java/org/apache/druid/indexing/common/TaskReport.java",
      "status": "modified",
      "additions": 6,
      "deletions": 2,
      "changes": 8,
      "blob_url": "https://github.com/apache/druid/blob/ada3ae08dfc432f34e4ef1ec87f9800c91ba2c54/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fcommon%2FTaskReport.java",
      "raw_url": "https://github.com/apache/druid/raw/ada3ae08dfc432f34e4ef1ec87f9800c91ba2c54/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fcommon%2FTaskReport.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fcommon%2FTaskReport.java?ref=ada3ae08dfc432f34e4ef1ec87f9800c91ba2c54",
      "patch": "@@ -22,7 +22,7 @@\n import com.fasterxml.jackson.annotation.JsonSubTypes;\n import com.fasterxml.jackson.annotation.JsonTypeInfo;\n \n-import java.util.HashMap;\n+import java.util.LinkedHashMap;\n import java.util.Map;\n \n /**\n@@ -44,9 +44,13 @@ public interface TaskReport\n    */\n   Object getPayload();\n \n+  /**\n+   * Returns an order-preserving map that is suitable for passing into {@link TaskReportFileWriter#write}.\n+   */\n   static Map<String, TaskReport> buildTaskReports(TaskReport... taskReports)\n   {\n-    Map<String, TaskReport> taskReportMap = new HashMap<>();\n+    // Use LinkedHashMap to preserve order of the reports.\n+    Map<String, TaskReport> taskReportMap = new LinkedHashMap<>();\n     for (TaskReport taskReport : taskReports) {\n       taskReportMap.put(taskReport.getReportKey(), taskReport);\n     }",
      "parent_sha": "61e1ffc7f782ab5d83b4cbb2fcca45c34b8c6bb2"
    }
  },
  {
    "oid": "0eb8d733d42bb644dad65f74b8d8adba5a0fef8c",
    "message": "Adding leader and not being leader logging on the overlord. (#17519)",
    "date": "2024-12-03T17:06:53Z",
    "url": "https://github.com/apache/druid/commit/0eb8d733d42bb644dad65f74b8d8adba5a0fef8c",
    "details": {
      "sha": "547b63a70f0705981e77dba58be0bd31279201aa",
      "filename": "indexing-service/src/main/java/org/apache/druid/indexing/overlord/DruidOverlord.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/druid/blob/0eb8d733d42bb644dad65f74b8d8adba5a0fef8c/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Foverlord%2FDruidOverlord.java",
      "raw_url": "https://github.com/apache/druid/raw/0eb8d733d42bb644dad65f74b8d8adba5a0fef8c/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Foverlord%2FDruidOverlord.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Foverlord%2FDruidOverlord.java?ref=0eb8d733d42bb644dad65f74b8d8adba5a0fef8c",
      "patch": "@@ -106,7 +106,7 @@ public void becomeLeader()\n         giant.lock();\n \n         // I AM THE MASTER OF THE UNIVERSE.\n-        log.info(\"By the power of Grayskull, I have the power!\");\n+        log.info(\"By the power of Grayskull, I have the power. I am the leader\");\n \n         try {\n           final TaskRunner taskRunner = runnerFactory.build();\n@@ -193,6 +193,7 @@ public void stopBeingLeader()\n         giant.lock();\n         try {\n           initialized = false;\n+          log.info(\"I am no longer the leader...\");\n           final Lifecycle leaderLifecycle = leaderLifecycleRef.getAndSet(null);\n \n           if (leaderLifecycle != null) {",
      "parent_sha": "9ef46fc92d7df8bcaf1f904d7a601cd3f6765a4d"
    }
  },
  {
    "oid": "951b36e2bc7caed992a263a5e1289414b12b39da",
    "message": "BytesFullResponseHandler should only consume readableBytes of ChannelBuffer (#6270)",
    "date": "2018-08-31T03:22:08Z",
    "url": "https://github.com/apache/druid/commit/951b36e2bc7caed992a263a5e1289414b12b39da",
    "details": {
      "sha": "0f75ae2f3726ed819e758356eadcb731c78d0875",
      "filename": "extensions-core/druid-basic-security/src/main/java/org/apache/druid/security/basic/authentication/BytesFullResponseHandler.java",
      "status": "modified",
      "additions": 10,
      "deletions": 2,
      "changes": 12,
      "blob_url": "https://github.com/apache/druid/blob/951b36e2bc7caed992a263a5e1289414b12b39da/extensions-core%2Fdruid-basic-security%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsecurity%2Fbasic%2Fauthentication%2FBytesFullResponseHandler.java",
      "raw_url": "https://github.com/apache/druid/raw/951b36e2bc7caed992a263a5e1289414b12b39da/extensions-core%2Fdruid-basic-security%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsecurity%2Fbasic%2Fauthentication%2FBytesFullResponseHandler.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/extensions-core%2Fdruid-basic-security%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsecurity%2Fbasic%2Fauthentication%2FBytesFullResponseHandler.java?ref=951b36e2bc7caed992a263a5e1289414b12b39da",
      "patch": "@@ -22,6 +22,7 @@\n import org.apache.druid.java.util.http.client.response.ClientResponse;\n import org.apache.druid.java.util.http.client.response.FullResponseHolder;\n import org.apache.druid.java.util.http.client.response.HttpResponseHandler;\n+import org.jboss.netty.buffer.ChannelBuffer;\n import org.jboss.netty.handler.codec.http.HttpChunk;\n import org.jboss.netty.handler.codec.http.HttpResponse;\n \n@@ -36,7 +37,7 @@ public ClientResponse<FullResponseHolder> handleResponse(HttpResponse response)\n         null\n     );\n \n-    holder.addChunk(response.getContent().array());\n+    holder.addChunk(getContentBytes(response.getContent()));\n \n     return ClientResponse.unfinished(\n         holder\n@@ -55,7 +56,7 @@ public ClientResponse<FullResponseHolder> handleChunk(\n       return ClientResponse.finished(null);\n     }\n \n-    holder.addChunk(chunk.getContent().array());\n+    holder.addChunk(getContentBytes(chunk.getContent()));\n     return response;\n   }\n \n@@ -72,4 +73,11 @@ public void exceptionCaught(\n   {\n     // Its safe to Ignore as the ClientResponse returned in handleChunk were unfinished\n   }\n+\n+  private byte[] getContentBytes(ChannelBuffer content)\n+  {\n+    byte[] contentBytes = new byte[content.readableBytes()];\n+    content.readBytes(contentBytes);\n+    return contentBytes;\n+  }\n }",
      "parent_sha": "9b04846e6b6b7320ad93024c8c0c10c2dbbbbd81"
    }
  },
  {
    "oid": "cbb9f7d21485549184a2c2d5434ad43fa1ebfe8b",
    "message": "Log used flag (#4614)",
    "date": "2017-07-31T21:58:36Z",
    "url": "https://github.com/apache/druid/commit/cbb9f7d21485549184a2c2d5434ad43fa1ebfe8b",
    "details": {
      "sha": "c1750101c1ed90f85af5d4079e16f03f39a90b9f",
      "filename": "server/src/main/java/io/druid/metadata/IndexerSQLMetadataStorageCoordinator.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/cbb9f7d21485549184a2c2d5434ad43fa1ebfe8b/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fmetadata%2FIndexerSQLMetadataStorageCoordinator.java",
      "raw_url": "https://github.com/apache/druid/raw/cbb9f7d21485549184a2c2d5434ad43fa1ebfe8b/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fmetadata%2FIndexerSQLMetadataStorageCoordinator.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fmetadata%2FIndexerSQLMetadataStorageCoordinator.java?ref=cbb9f7d21485549184a2c2d5434ad43fa1ebfe8b",
      "patch": "@@ -631,10 +631,10 @@ private boolean announceHistoricalSegment(\n             .bind(\"payload\", jsonMapper.writeValueAsBytes(segment))\n             .execute();\n \n-      log.info(\"Published segment [%s] to DB\", segment.getIdentifier());\n+      log.info(\"Published segment [%s] to DB with used flag [%s]\", segment.getIdentifier(), used);\n     }\n     catch (Exception e) {\n-      log.error(e, \"Exception inserting segment [%s] into DB\", segment.getIdentifier());\n+      log.error(e, \"Exception inserting segment [%s] with used flag [%s] into DB\", segment.getIdentifier(), used);\n       throw e;\n     }\n ",
      "parent_sha": "89215382510a6eceac898d14022b76a065fd7cdf"
    }
  },
  {
    "oid": "4fd547955952b6768b240bec1c1e2705b0e922e9",
    "message": "fix typo",
    "date": "2014-08-19T19:34:10Z",
    "url": "https://github.com/apache/druid/commit/4fd547955952b6768b240bec1c1e2705b0e922e9",
    "details": {
      "sha": "7e8dd86f5a0a88970ebb6e6355e44eec9dc45fe6",
      "filename": "indexing-service/src/test/java/io/druid/indexing/common/task/MergeTaskBaseTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/4fd547955952b6768b240bec1c1e2705b0e922e9/indexing-service%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Findexing%2Fcommon%2Ftask%2FMergeTaskBaseTest.java",
      "raw_url": "https://github.com/apache/druid/raw/4fd547955952b6768b240bec1c1e2705b0e922e9/indexing-service%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Findexing%2Fcommon%2Ftask%2FMergeTaskBaseTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Findexing%2Fcommon%2Ftask%2FMergeTaskBaseTest.java?ref=4fd547955952b6768b240bec1c1e2705b0e922e9",
      "patch": "@@ -46,7 +46,7 @@ public class MergeTaskBaseTest\n   final MergeTaskBase testMergeTaskBase = new MergeTaskBase(null, \"foo\", segments)\n   {\n     @Override\n-    protected File merge( Map<DataSegment, File> segments, File outDir) throws Exception\n+    protected File merge(Map<DataSegment, File> segments, File outDir) throws Exception\n     {\n       return null;\n     }",
      "parent_sha": "0facb4202db43850f7643a905c2898b5c4bf89de"
    }
  },
  {
    "oid": "7dcbea464f259b4bf5e38660dd13e036953f929e",
    "message": "fix bug with out of order events on broker",
    "date": "2013-05-31T22:50:53Z",
    "url": "https://github.com/apache/druid/commit/7dcbea464f259b4bf5e38660dd13e036953f929e",
    "details": {
      "sha": "a61b6b0c00f8e8202e17a4ed7552e8396bd59d56",
      "filename": "client/src/main/java/com/metamx/druid/client/BrokerServerView.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/druid/blob/7dcbea464f259b4bf5e38660dd13e036953f929e/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fclient%2FBrokerServerView.java",
      "raw_url": "https://github.com/apache/druid/raw/7dcbea464f259b4bf5e38660dd13e036953f929e/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fclient%2FBrokerServerView.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fclient%2FBrokerServerView.java?ref=7dcbea464f259b4bf5e38660dd13e036953f929e",
      "patch": "@@ -145,11 +145,10 @@ private DirectDruidClient makeDirectClient(DruidServer server)\n \n   private QueryableDruidServer removeServer(DruidServer server)\n   {\n-    QueryableDruidServer retVal = clients.remove(server.getName());\n     for (DataSegment segment : server.getSegments().values()) {\n       serverRemovedSegment(server, segment);\n     }\n-    return retVal;\n+    return clients.remove(server.getName());\n   }\n \n   private void serverAddedSegment(final DruidServer server, final DataSegment segment)",
      "parent_sha": "2138f9fdf2d7fb0986286f0118b7195c5a0ad658"
    }
  },
  {
    "oid": "34ae426ffaef3a24d3974735f0e9a264be918d2c",
    "message": "address cr",
    "date": "2014-09-16T00:08:02Z",
    "url": "https://github.com/apache/druid/commit/34ae426ffaef3a24d3974735f0e9a264be918d2c",
    "details": {
      "sha": "726112de62fbbb2225aeec348804a0dae2f1efed",
      "filename": "server/src/main/java/io/druid/segment/indexing/DataSchema.java",
      "status": "modified",
      "additions": 14,
      "deletions": 13,
      "changes": 27,
      "blob_url": "https://github.com/apache/druid/blob/34ae426ffaef3a24d3974735f0e9a264be918d2c/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Findexing%2FDataSchema.java",
      "raw_url": "https://github.com/apache/druid/raw/34ae426ffaef3a24d3974735f0e9a264be918d2c/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Findexing%2FDataSchema.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Findexing%2FDataSchema.java?ref=34ae426ffaef3a24d3974735f0e9a264be918d2c",
      "patch": "@@ -49,22 +49,23 @@ public DataSchema(\n     this.dataSource = dataSource;\n \n     final Set<String> dimensionExclusions = Sets.newHashSet();\n+    for (AggregatorFactory aggregator : aggregators) {\n+      dimensionExclusions.add(aggregator.getName());\n+    }\n     if (parser != null && parser.getParseSpec() != null && parser.getParseSpec().getTimestampSpec() != null) {\n       dimensionExclusions.add(parser.getParseSpec().getTimestampSpec().getTimestampColumn());\n-      for (AggregatorFactory aggregator : aggregators) {\n-        dimensionExclusions.add(aggregator.getName());\n+      if (parser.getParseSpec().getDimensionsSpec() != null) {\n+        this.parser = parser.withParseSpec(\n+            parser.getParseSpec()\n+                  .withDimensionsSpec(\n+                      parser.getParseSpec()\n+                            .getDimensionsSpec()\n+                            .withDimensionExclusions(dimensionExclusions)\n+                  )\n+        );\n+      } else {\n+        this.parser = parser;\n       }\n-    }\n-\n-    if (parser != null && parser.getParseSpec() != null && parser.getParseSpec().getDimensionsSpec() != null) {\n-      this.parser = parser.withParseSpec(\n-          parser.getParseSpec()\n-                .withDimensionsSpec(\n-                    parser.getParseSpec()\n-                          .getDimensionsSpec()\n-                          .withDimensionExclusions(dimensionExclusions)\n-                )\n-      );\n     } else {\n       this.parser = parser;\n     }",
      "parent_sha": "1ab266277288bed015e0673fa5e58f1bc7d43b2d"
    }
  },
  {
    "oid": "ffded61f5eba8665e1e00b0d35a26fb7e9750aa7",
    "message": "fix build (#6897)",
    "date": "2019-01-22T01:18:14Z",
    "url": "https://github.com/apache/druid/commit/ffded61f5eba8665e1e00b0d35a26fb7e9750aa7",
    "details": {
      "sha": "42fc6487f3e9db84795341de8727e141c1f84860",
      "filename": "extensions-core/histogram/src/test/java/org/apache/druid/query/aggregation/histogram/FixedBucketsHistogramTopNQueryTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/ffded61f5eba8665e1e00b0d35a26fb7e9750aa7/extensions-core%2Fhistogram%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fquery%2Faggregation%2Fhistogram%2FFixedBucketsHistogramTopNQueryTest.java",
      "raw_url": "https://github.com/apache/druid/raw/ffded61f5eba8665e1e00b0d35a26fb7e9750aa7/extensions-core%2Fhistogram%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fquery%2Faggregation%2Fhistogram%2FFixedBucketsHistogramTopNQueryTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/extensions-core%2Fhistogram%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fquery%2Faggregation%2Fhistogram%2FFixedBucketsHistogramTopNQueryTest.java?ref=ffded61f5eba8665e1e00b0d35a26fb7e9750aa7",
      "patch": "@@ -127,7 +127,7 @@ public void testTopNWithFixedHistogramAgg()\n         .dimension(QueryRunnerTestHelper.marketDimension)\n         .metric(QueryRunnerTestHelper.dependentPostAggMetric)\n         .threshold(4)\n-        .intervals(QueryRunnerTestHelper.fullOnInterval)\n+        .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n         .aggregators(\n             Lists.newArrayList(\n                 Iterables.concat(",
      "parent_sha": "8eae26fd4e7572060d112864dd3d5f6a865b9c89"
    }
  },
  {
    "oid": "7913b27066051edaa6e3b0139b26cb1ff0cada42",
    "message": "reuse tmp file for geoip db",
    "date": "2013-08-21T21:12:39Z",
    "url": "https://github.com/apache/druid/commit/7913b27066051edaa6e3b0139b26cb1ff0cada42",
    "details": {
      "sha": "b1ecf91554ef32e2dae545f35e55617682e96c27",
      "filename": "realtime/src/main/java/com/metamx/druid/realtime/firehose/WikipediaIrcDecoder.java",
      "status": "modified",
      "additions": 12,
      "deletions": 11,
      "changes": 23,
      "blob_url": "https://github.com/apache/druid/blob/7913b27066051edaa6e3b0139b26cb1ff0cada42/realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Frealtime%2Ffirehose%2FWikipediaIrcDecoder.java",
      "raw_url": "https://github.com/apache/druid/raw/7913b27066051edaa6e3b0139b26cb1ff0cada42/realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Frealtime%2Ffirehose%2FWikipediaIrcDecoder.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Frealtime%2Ffirehose%2FWikipediaIrcDecoder.java?ref=7913b27066051edaa6e3b0139b26cb1ff0cada42",
      "patch": "@@ -91,17 +91,18 @@ public WikipediaIrcDecoder(@JsonProperty(\"namespaces\") Map<String, Map<String, S\n       geoDb = new File(geoIpDatabase);\n     } else {\n       try {\n-        geoDb = File.createTempFile(\"geoip\", null);\n-        geoDb.deleteOnExit();\n-\n-        log.info(\"Downloading geo ip database to [%s]\", geoDb);\n-\n-        FileUtils.copyInputStreamToFile(\n-            new GZIPInputStream(\n-                new URL(\"http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.mmdb.gz\").openStream()\n-            ),\n-            geoDb\n-        );\n+        String tmpDir = System.getProperty(\"java.io.tmpdir\");\n+        geoDb = new File(tmpDir, this.getClass().getCanonicalName() + \".GeoLite2-City.mmdb\");\n+        if(!geoDb.exists()) {\n+          log.info(\"Downloading geo ip database to [%s]\", geoDb);\n+\n+          FileUtils.copyInputStreamToFile(\n+              new GZIPInputStream(\n+                  new URL(\"http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.mmdb.gz\").openStream()\n+              ),\n+              geoDb\n+          );\n+        }\n       } catch(IOException e) {\n         throw new RuntimeException(\"Unable to download geo ip database [%s]\", e);\n       }",
      "parent_sha": "0f4a1328071b61c643d626191ef37b78fcb9c5f1"
    }
  },
  {
    "oid": "4b902aed232e9ad30cc3ecad518491f97cac2ab4",
    "message": "fix typo",
    "date": "2014-02-14T07:25:11Z",
    "url": "https://github.com/apache/druid/commit/4b902aed232e9ad30cc3ecad518491f97cac2ab4",
    "details": {
      "sha": "d09a6d11ec62f3d990e269722a12f00534bee7d8",
      "filename": "server/src/main/java/io/druid/client/BrokerServerView.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/apache/druid/blob/4b902aed232e9ad30cc3ecad518491f97cac2ab4/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2FBrokerServerView.java",
      "raw_url": "https://github.com/apache/druid/raw/4b902aed232e9ad30cc3ecad518491f97cac2ab4/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2FBrokerServerView.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fclient%2FBrokerServerView.java?ref=4b902aed232e9ad30cc3ecad518491f97cac2ab4",
      "patch": "@@ -54,22 +54,22 @@ public class BrokerServerView implements TimelineServerView\n   private final Map<String, ServerSelector> selectors;\n   private final Map<String, VersionedIntervalTimeline<String, ServerSelector>> timelines;\n \n-  private final QueryToolChestWarehouse warehose;\n+  private final QueryToolChestWarehouse warehouse;\n   private final ObjectMapper smileMapper;\n   private final HttpClient httpClient;\n   private final ServerView baseView;\n   private final ServerSelectorStrategy serverSelectorStrategy;\n \n   @Inject\n   public BrokerServerView(\n-      QueryToolChestWarehouse warehose,\n+      QueryToolChestWarehouse warehouse,\n       ObjectMapper smileMapper,\n       @Client HttpClient httpClient,\n       ServerView baseView,\n       ServerSelectorStrategy serverSelectorStrategy\n   )\n   {\n-    this.warehose = warehose;\n+    this.warehouse = warehouse;\n     this.smileMapper = smileMapper;\n     this.httpClient = httpClient;\n     this.baseView = baseView;\n@@ -149,7 +149,7 @@ private QueryableDruidServer addServer(DruidServer server)\n \n   private DirectDruidClient makeDirectClient(DruidServer server)\n   {\n-    return new DirectDruidClient(warehose, smileMapper, httpClient, server.getHost());\n+    return new DirectDruidClient(warehouse, smileMapper, httpClient, server.getHost());\n   }\n \n   private QueryableDruidServer removeServer(DruidServer server)",
      "parent_sha": "82fd35726fcf920d047dee6229335a4a17a27a94"
    }
  },
  {
    "oid": "d459df8d6e3789e6ad234289b27e55a92667b1ea",
    "message": "Fix log syntax (#15004)",
    "date": "2023-09-18T17:40:02Z",
    "url": "https://github.com/apache/druid/commit/d459df8d6e3789e6ad234289b27e55a92667b1ea",
    "details": {
      "sha": "9fdc25fa6455775c53600bf4c00f59d206c2aa45",
      "filename": "extensions-contrib/kubernetes-overlord-extensions/src/main/java/org/apache/druid/k8s/overlord/common/KubernetesPeonClient.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/druid/blob/d459df8d6e3789e6ad234289b27e55a92667b1ea/extensions-contrib%2Fkubernetes-overlord-extensions%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fk8s%2Foverlord%2Fcommon%2FKubernetesPeonClient.java",
      "raw_url": "https://github.com/apache/druid/raw/d459df8d6e3789e6ad234289b27e55a92667b1ea/extensions-contrib%2Fkubernetes-overlord-extensions%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fk8s%2Foverlord%2Fcommon%2FKubernetesPeonClient.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/extensions-contrib%2Fkubernetes-overlord-extensions%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fk8s%2Foverlord%2Fcommon%2FKubernetesPeonClient.java?ref=d459df8d6e3789e6ad234289b27e55a92667b1ea",
      "patch": "@@ -121,13 +121,13 @@ public boolean deletePeonJob(K8sTaskId taskId)\n                                                                  .withName(taskId.getK8sJobName())\n                                                                  .delete().isEmpty());\n       if (result) {\n-        log.info(\"Cleaned up k8s task: %s\", taskId);\n+        log.info(\"Cleaned up k8s job: %s\", taskId);\n       } else {\n-        log.info(\"K8s task does not exist: %s\", taskId);\n+        log.info(\"K8s job does not exist: %s\", taskId);\n       }\n       return result;\n     } else {\n-      log.info(\"Not cleaning up task %s due to flag: debugJobs=true\", taskId);\n+      log.info(\"Not cleaning up job %s due to flag: debugJobs=true\", taskId);\n       return true;\n     }\n   }",
      "parent_sha": "973fbaf962fbf5d442c511d39ee077b4d2ff5053"
    }
  },
  {
    "oid": "0d99cee3c3fff7802009d4a3609acd10a07102e7",
    "message": "1) Whitespace ftw!",
    "date": "2013-02-15T20:25:44Z",
    "url": "https://github.com/apache/druid/commit/0d99cee3c3fff7802009d4a3609acd10a07102e7",
    "details": {
      "sha": "dcf09526a314e7f3b60017f783e83fe8c7d42349",
      "filename": "client/src/main/java/com/metamx/druid/query/timeboundary/TimeBoundaryQueryQueryToolChest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 5,
      "changes": 7,
      "blob_url": "https://github.com/apache/druid/blob/0d99cee3c3fff7802009d4a3609acd10a07102e7/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fquery%2Ftimeboundary%2FTimeBoundaryQueryQueryToolChest.java",
      "raw_url": "https://github.com/apache/druid/raw/0d99cee3c3fff7802009d4a3609acd10a07102e7/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fquery%2Ftimeboundary%2FTimeBoundaryQueryQueryToolChest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fquery%2Ftimeboundary%2FTimeBoundaryQueryQueryToolChest.java?ref=0d99cee3c3fff7802009d4a3609acd10a07102e7",
      "patch": "@@ -38,8 +38,8 @@\n import com.metamx.druid.result.Result;\n import com.metamx.druid.result.TimeBoundaryResultValue;\n import com.metamx.emitter.service.ServiceMetricEvent;\n-\n import org.joda.time.DateTime;\n+\n import javax.annotation.Nullable;\n import java.nio.ByteBuffer;\n import java.util.List;\n@@ -65,10 +65,7 @@ public <T extends LogicalSegment> List<T> filterSegments(TimeBoundaryQuery query\n       return input;\n     }\n \n-    return Lists.newArrayList(\n-        input.get(0),\n-        input.get(input.size() - 1)\n-    );\n+    return Lists.newArrayList(input.get(0), input.get(input.size() - 1));\n   }\n \n   @Override",
      "parent_sha": "06ef83db69d5244c819f342b9fa6a061cd3419a5"
    }
  },
  {
    "oid": "578625b77167d78d57112d4f4c2d8ae91c65c25d",
    "message": "Replace TestInputRowHandler with mocking object (#11529)\n\n* Replace TestInputRowHandler with mocking object\r\n\r\n* Change EasyMock object to Mockito object. Make test logic concise\r\n\r\n* correct code format",
    "date": "2021-08-04T23:45:22Z",
    "url": "https://github.com/apache/druid/commit/578625b77167d78d57112d4f4c2d8ae91c65c25d",
    "details": {
      "sha": "c70fa3977988863dc492f754435526560670d08e",
      "filename": "core/src/test/java/org/apache/druid/data/input/HandlingInputRowIteratorTest.java",
      "status": "modified",
      "additions": 29,
      "deletions": 38,
      "changes": 67,
      "blob_url": "https://github.com/apache/druid/blob/578625b77167d78d57112d4f4c2d8ae91c65c25d/core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fdata%2Finput%2FHandlingInputRowIteratorTest.java",
      "raw_url": "https://github.com/apache/druid/raw/578625b77167d78d57112d4f4c2d8ae91c65c25d/core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fdata%2Finput%2FHandlingInputRowIteratorTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fdata%2Finput%2FHandlingInputRowIteratorTest.java?ref=578625b77167d78d57112d4f4c2d8ae91c65c25d",
      "patch": "@@ -19,22 +19,28 @@\n \n package org.apache.druid.data.input;\n \n+import org.apache.druid.data.input.HandlingInputRowIterator.InputRowHandler;\n import org.apache.druid.java.util.common.CloseableIterators;\n import org.apache.druid.java.util.common.parsers.CloseableIterator;\n-import org.easymock.EasyMock;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n import org.junit.experimental.runners.Enclosed;\n import org.junit.runner.RunWith;\n+import org.mockito.Mockito;\n \n import javax.annotation.Nullable;\n+\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.Iterator;\n import java.util.List;\n import java.util.NoSuchElementException;\n \n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n @RunWith(Enclosed.class)\n public class HandlingInputRowIteratorTest\n {\n@@ -55,8 +61,7 @@ public InputRow next()\n           {\n             throw new NoSuchElementException();\n           }\n-        }\n-    );\n+        });\n \n     private HandlingInputRowIterator target;\n \n@@ -81,58 +86,64 @@ public void throwsExceptionWhenYieldingNext()\n \n   public static class PresentRowTest\n   {\n-    private static final InputRow INPUT_ROW1 = EasyMock.mock(InputRow.class);\n-    private static final InputRow INPUT_ROW2 = EasyMock.mock(InputRow.class);\n+\n+    private static final InputRow INPUT_ROW1 = mock(InputRow.class);\n+    private static final InputRow INPUT_ROW2 = mock(InputRow.class);\n     private static final List<InputRow> INPUT_ROWS = Arrays.asList(INPUT_ROW1, INPUT_ROW2);\n \n-    private TestInputRowHandler successfulHandler;\n-    private TestInputRowHandler unsuccessfulHandler;\n+    private InputRowHandler successfulHandler;\n+    private InputRowHandler unsuccessfulHandler;\n \n     @Before\n     public void setup()\n     {\n-      successfulHandler = new TestInputRowHandler(true);\n-      unsuccessfulHandler = new TestInputRowHandler(false);\n+      // Construct mock object\n+      successfulHandler = mock(HandlingInputRowIterator.InputRowHandler.class);\n+      // Method Stubs\n+      when(successfulHandler.handle(any(InputRow.class))).thenReturn(true);\n+      // Construct mock object\n+      unsuccessfulHandler = mock(HandlingInputRowIterator.InputRowHandler.class);\n+      // Method Stubs\n+      when(unsuccessfulHandler.handle(any(InputRow.class))).thenReturn(false);\n     }\n \n     @Test\n     public void hasNext()\n     {\n       HandlingInputRowIterator target = createInputRowIterator(unsuccessfulHandler, unsuccessfulHandler);\n       Assert.assertTrue(target.hasNext());\n-      Assert.assertFalse(unsuccessfulHandler.invoked);\n+      Mockito.verify(unsuccessfulHandler, Mockito.never()).handle(INPUT_ROW1);\n     }\n \n     @Test\n     public void yieldsNextIfUnhandled()\n     {\n       HandlingInputRowIterator target = createInputRowIterator(unsuccessfulHandler, unsuccessfulHandler);\n       Assert.assertEquals(INPUT_ROW1, target.next());\n-      Assert.assertTrue(unsuccessfulHandler.invoked);\n+      Mockito.verify(unsuccessfulHandler, Mockito.times(2)).handle(INPUT_ROW1);\n     }\n \n     @Test\n     public void yieldsNullIfHandledByFirst()\n     {\n       HandlingInputRowIterator target = createInputRowIterator(successfulHandler, unsuccessfulHandler);\n       Assert.assertNull(target.next());\n-      Assert.assertTrue(successfulHandler.invoked);\n-      Assert.assertFalse(unsuccessfulHandler.invoked);\n+      Mockito.verify(successfulHandler, Mockito.times(1)).handle(INPUT_ROW1);\n+      Mockito.verify(unsuccessfulHandler, Mockito.never()).handle(INPUT_ROW1);\n     }\n \n     @Test\n     public void yieldsNullIfHandledBySecond()\n     {\n       HandlingInputRowIterator target = createInputRowIterator(unsuccessfulHandler, successfulHandler);\n       Assert.assertNull(target.next());\n-      Assert.assertTrue(unsuccessfulHandler.invoked);\n-      Assert.assertTrue(successfulHandler.invoked);\n+      Mockito.verify(unsuccessfulHandler, Mockito.times(1)).handle(INPUT_ROW1);\n+      Mockito.verify(successfulHandler, Mockito.times(1)).handle(INPUT_ROW1);\n     }\n \n     private static HandlingInputRowIterator createInputRowIterator(\n         HandlingInputRowIterator.InputRowHandler firstHandler,\n-        HandlingInputRowIterator.InputRowHandler secondHandler\n-    )\n+        HandlingInputRowIterator.InputRowHandler secondHandler)\n     {\n       CloseableIterator<InputRow> iterator = CloseableIterators.withEmptyBaggage(\n           new Iterator<InputRow>()\n@@ -151,29 +162,9 @@ public InputRow next()\n             {\n               return delegate.next();\n             }\n-          }\n-      );\n+          });\n \n       return new HandlingInputRowIterator(iterator, Arrays.asList(firstHandler, secondHandler));\n     }\n-\n-    private static class TestInputRowHandler implements HandlingInputRowIterator.InputRowHandler\n-    {\n-      boolean invoked = false;\n-\n-      private final boolean successful;\n-\n-      TestInputRowHandler(boolean successful)\n-      {\n-        this.successful = successful;\n-      }\n-\n-      @Override\n-      public boolean handle(InputRow inputRow)\n-      {\n-        invoked = true;\n-        return successful;\n-      }\n-    }\n   }\n }",
      "parent_sha": "361bfdcaa5f79e531f4eba2359cf1f967606c8e6"
    }
  },
  {
    "oid": "44911039c5aa4583c2c7b86a7f2fae63be9623ec",
    "message": "update indexing in the helper to use multiple persists and final merge to\ncatch further issues in aggregator implementations",
    "date": "2015-09-09T16:10:33Z",
    "url": "https://github.com/apache/druid/commit/44911039c5aa4583c2c7b86a7f2fae63be9623ec",
    "details": {
      "sha": "a26a751930539057e781162d57f1fa236999f4c4",
      "filename": "processing/src/test/java/io/druid/query/aggregation/AggregationTestHelper.java",
      "status": "modified",
      "additions": 50,
      "deletions": 11,
      "changes": 61,
      "blob_url": "https://github.com/apache/druid/blob/44911039c5aa4583c2c7b86a7f2fae63be9623ec/processing%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fquery%2Faggregation%2FAggregationTestHelper.java",
      "raw_url": "https://github.com/apache/druid/raw/44911039c5aa4583c2c7b86a7f2fae63be9623ec/processing%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fquery%2Faggregation%2FAggregationTestHelper.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fquery%2Faggregation%2FAggregationTestHelper.java?ref=44911039c5aa4583c2c7b86a7f2fae63be9623ec",
      "patch": "@@ -28,7 +28,6 @@\n import com.google.common.base.Throwables;\n import com.google.common.collect.Lists;\n import com.google.common.collect.Maps;\n-import com.google.common.io.CharSource;\n import com.google.common.io.Closeables;\n import com.google.common.io.Files;\n import com.google.common.util.concurrent.ListenableFuture;\n@@ -58,9 +57,11 @@\n import io.druid.segment.IndexIO;\n import io.druid.segment.IndexMerger;\n import io.druid.segment.IndexSpec;\n+import io.druid.segment.QueryableIndex;\n import io.druid.segment.QueryableIndexSegment;\n import io.druid.segment.Segment;\n import io.druid.segment.incremental.IncrementalIndex;\n+import io.druid.segment.incremental.IndexSizeExceededException;\n import io.druid.segment.incremental.OnheapIncrementalIndex;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.io.IOUtils;\n@@ -71,7 +72,7 @@\n import java.io.IOException;\n import java.io.InputStream;\n import java.nio.ByteBuffer;\n-import java.nio.charset.Charset;\n+import java.util.ArrayList;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -237,19 +238,57 @@ public void createIndex(\n       int maxRowCount\n   ) throws Exception\n   {\n-    try(IncrementalIndex index = new OnheapIncrementalIndex(minTimestamp, gran, metrics, deserializeComplexMetrics, maxRowCount)) {\n-      while (rows.hasNext()) {\n+    IncrementalIndex index = null;\n+    List<File> toMerge = new ArrayList<>();\n \n+    try {\n+      index = new OnheapIncrementalIndex(minTimestamp, gran, metrics, deserializeComplexMetrics, maxRowCount);\n+      while (rows.hasNext()) {\n         Object row = rows.next();\n-        if (row instanceof String && parser instanceof StringInputRowParser) {\n-          //Note: this is required because StringInputRowParser is InputRowParser<ByteBuffer> as opposed to\n-          //InputRowsParser<String>\n-          index.add(((StringInputRowParser) parser).parse((String) row));\n-        } else {\n-          index.add(parser.parse(row));\n+        try {\n+          if (row instanceof String && parser instanceof StringInputRowParser) {\n+            //Note: this is required because StringInputRowParser is InputRowParser<ByteBuffer> as opposed to\n+            //InputRowsParser<String>\n+            index.add(((StringInputRowParser) parser).parse((String) row));\n+          } else {\n+            index.add(parser.parse(row));\n+          }\n         }\n+        catch (IndexSizeExceededException ex) {\n+          File tmp = Files.createTempDir();\n+          toMerge.add(tmp);\n+          IndexMerger.persist(index, tmp, null, new IndexSpec());\n+          index.close();\n+          index = new OnheapIncrementalIndex(minTimestamp, gran, metrics, deserializeComplexMetrics, maxRowCount);\n+        }\n+      }\n+\n+      if (toMerge.size() > 0) {\n+        File tmp = Files.createTempDir();\n+        toMerge.add(tmp);\n+        IndexMerger.persist(index, tmp, null, new IndexSpec());\n+\n+        List<QueryableIndex> indexes = new ArrayList<>(toMerge.size());\n+        for (File file : toMerge) {\n+          indexes.add(IndexIO.loadIndex(file));\n+        }\n+        IndexMerger.mergeQueryableIndex(indexes, metrics, outDir, new IndexSpec());\n+\n+        for (QueryableIndex qi : indexes) {\n+          qi.close();\n+        }\n+      } else {\n+        IndexMerger.persist(index, outDir, null, new IndexSpec());\n+      }\n+    }\n+    finally {\n+      if (index != null) {\n+        index.close();\n+      }\n+\n+      for (File file : toMerge) {\n+        FileUtils.deleteDirectory(file);\n       }\n-      IndexMerger.persist(index, outDir, null, new IndexSpec());\n     }\n   }\n ",
      "parent_sha": "07266d682a4456012f048593e14d702779404588"
    }
  },
  {
    "oid": "c97eb93e16d914e3ad54138efe9d3a3990e1a52a",
    "message": "Fix dead code in IndexTask.collectIntervalsAndShardSpecs() (#7746)",
    "date": "2019-05-25T17:12:14Z",
    "url": "https://github.com/apache/druid/commit/c97eb93e16d914e3ad54138efe9d3a3990e1a52a",
    "details": {
      "sha": "20cee242c2299814fe10a7788f077879fdbb24c8",
      "filename": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/IndexTask.java",
      "status": "modified",
      "additions": 5,
      "deletions": 6,
      "changes": 11,
      "blob_url": "https://github.com/apache/druid/blob/c97eb93e16d914e3ad54138efe9d3a3990e1a52a/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fcommon%2Ftask%2FIndexTask.java",
      "raw_url": "https://github.com/apache/druid/raw/c97eb93e16d914e3ad54138efe9d3a3990e1a52a/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fcommon%2Ftask%2FIndexTask.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fcommon%2Ftask%2FIndexTask.java?ref=c97eb93e16d914e3ad54138efe9d3a3990e1a52a",
      "patch": "@@ -731,8 +731,6 @@ private Map<Interval, Optional<HyperLogLogCollector>> collectIntervalsAndShardSp\n     final Map<Interval, Optional<HyperLogLogCollector>> hllCollectors = new TreeMap<>(\n         Comparators.intervalsByStartThenEnd()\n     );\n-    int thrownAway = 0;\n-    int unparseable = 0;\n     final Granularity queryGranularity = granularitySpec.getQueryGranularity();\n \n     try (\n@@ -807,12 +805,13 @@ private Map<Interval, Optional<HyperLogLogCollector>> collectIntervalsAndShardSp\n     }\n \n     // These metrics are reported in generateAndPublishSegments()\n-    if (thrownAway > 0) {\n-      log.warn(\"Unable to find a matching interval for [%,d] events\", thrownAway);\n+    if (determinePartitionsMeters.getThrownAway() > 0) {\n+      log.warn(\"Unable to find a matching interval for [%,d] events\", determinePartitionsMeters.getThrownAway());\n     }\n-    if (unparseable > 0) {\n-      log.warn(\"Unable to parse [%,d] events\", unparseable);\n+    if (determinePartitionsMeters.getUnparseable() > 0) {\n+      log.warn(\"Unable to parse [%,d] events\", determinePartitionsMeters.getUnparseable());\n     }\n+\n     return hllCollectors;\n   }\n ",
      "parent_sha": "7eac685e47e2eadbaf2ad0fe83ed42e399e4796f"
    }
  },
  {
    "oid": "7d2778fae62e4279a5448d1ad886b80b73aed400",
    "message": "fix TimeBoundaryQuery to properly handle timestamps prior to 1970",
    "date": "2012-11-28T21:12:38Z",
    "url": "https://github.com/apache/druid/commit/7d2778fae62e4279a5448d1ad886b80b73aed400",
    "details": {
      "sha": "b9b64fe6948fcddcf804cd9dec6e86c38ffe9b02",
      "filename": "client/src/main/java/com/metamx/druid/query/timeboundary/TimeBoundaryQuery.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/7d2778fae62e4279a5448d1ad886b80b73aed400/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fquery%2Ftimeboundary%2FTimeBoundaryQuery.java",
      "raw_url": "https://github.com/apache/druid/raw/7d2778fae62e4279a5448d1ad886b80b73aed400/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fquery%2Ftimeboundary%2FTimeBoundaryQuery.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fquery%2Ftimeboundary%2FTimeBoundaryQuery.java?ref=7d2778fae62e4279a5448d1ad886b80b73aed400",
      "patch": "@@ -42,7 +42,7 @@\n  */\n public class TimeBoundaryQuery extends BaseQuery<Result<TimeBoundaryResultValue>>\n {\n-  public static final Interval MY_Y2K_INTERVAL = new Interval(new DateTime(0), new DateTime(\"3000-01-01\"));\n+  public static final Interval MY_Y2K_INTERVAL = new Interval(new DateTime(Long.MIN_VALUE), new DateTime(Long.MAX_VALUE));\n   public static final String MAX_TIME = \"maxTime\";\n   public static final String MIN_TIME = \"minTime\";\n   private static final byte CACHE_TYPE_ID = 0x0;",
      "parent_sha": "09a31e5995302598fa1126d3585b4a77d107a665"
    }
  },
  {
    "oid": "3f5449d40a3c5cf9770494d36da2291841c95205",
    "message": "loop unrolling provides no benefit for timeseries",
    "date": "2014-11-11T18:58:37Z",
    "url": "https://github.com/apache/druid/commit/3f5449d40a3c5cf9770494d36da2291841c95205",
    "details": {
      "sha": "9489e400a6e6ffdeeca50e252087a9a8c6d7981a",
      "filename": "processing/src/main/java/io/druid/query/timeseries/TimeseriesQueryEngine.java",
      "status": "modified",
      "additions": 2,
      "deletions": 21,
      "changes": 23,
      "blob_url": "https://github.com/apache/druid/blob/3f5449d40a3c5cf9770494d36da2291841c95205/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Ftimeseries%2FTimeseriesQueryEngine.java",
      "raw_url": "https://github.com/apache/druid/raw/3f5449d40a3c5cf9770494d36da2291841c95205/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Ftimeseries%2FTimeseriesQueryEngine.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Ftimeseries%2FTimeseriesQueryEngine.java?ref=3f5449d40a3c5cf9770494d36da2291841c95205",
      "patch": "@@ -60,28 +60,9 @@ public Result<TimeseriesResultValue> apply(Cursor cursor)\n           {\n             Aggregator[] aggregators = QueryRunnerHelper.makeAggregators(cursor, aggregatorSpecs);\n             try {\n-              final int aggSize = aggregators.length;\n-              final int aggExtra = aggSize % AGG_UNROLL_COUNT;\n-\n               while (!cursor.isDone()) {\n-                switch(aggExtra) {\n-                  case 7: aggregators[6].aggregate();\n-                  case 6: aggregators[5].aggregate();\n-                  case 5: aggregators[4].aggregate();\n-                  case 4: aggregators[3].aggregate();\n-                  case 3: aggregators[2].aggregate();\n-                  case 2: aggregators[1].aggregate();\n-                  case 1: aggregators[0].aggregate();\n-                }\n-                for (int j = aggExtra; j < aggSize; j += AGG_UNROLL_COUNT) {\n-                  aggregators[j].aggregate();\n-                  aggregators[j+1].aggregate();\n-                  aggregators[j+2].aggregate();\n-                  aggregators[j+3].aggregate();\n-                  aggregators[j+4].aggregate();\n-                  aggregators[j+5].aggregate();\n-                  aggregators[j+6].aggregate();\n-                  aggregators[j+7].aggregate();\n+                for(Aggregator aggregator : aggregators) {\n+                  aggregator.aggregate();\n                 }\n                 cursor.advance();\n               }",
      "parent_sha": "e817db8b6c7b7f55680cb3f90dbf3e1174dff776"
    }
  },
  {
    "oid": "a6bfcc5bfd77df809d667da03050bdf2b8a450a8",
    "message": "Allow change minTopNThreshold per topN query",
    "date": "2016-01-07T06:51:00Z",
    "url": "https://github.com/apache/druid/commit/a6bfcc5bfd77df809d667da03050bdf2b8a450a8",
    "details": {
      "sha": "7a53817091e6161ad4c5f3c4b9b7535569083cb4",
      "filename": "processing/src/main/java/io/druid/query/topn/TopNQueryQueryToolChest.java",
      "status": "modified",
      "additions": 5,
      "deletions": 4,
      "changes": 9,
      "blob_url": "https://github.com/apache/druid/blob/a6bfcc5bfd77df809d667da03050bdf2b8a450a8/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Ftopn%2FTopNQueryQueryToolChest.java",
      "raw_url": "https://github.com/apache/druid/raw/a6bfcc5bfd77df809d667da03050bdf2b8a450a8/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Ftopn%2FTopNQueryQueryToolChest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Ftopn%2FTopNQueryQueryToolChest.java?ref=a6bfcc5bfd77df809d667da03050bdf2b8a450a8",
      "patch": "@@ -470,7 +470,7 @@ public QueryRunner<Result<TopNResultValue>> postMergeQueryDecoration(final Query\n   {\n     final ThresholdAdjustingQueryRunner thresholdRunner = new ThresholdAdjustingQueryRunner(\n         runner,\n-        config.getMinTopNThreshold()\n+        config\n     );\n     return new QueryRunner<Result<TopNResultValue>>()\n     {\n@@ -535,15 +535,15 @@ public Ordering<Result<TopNResultValue>> getOrdering()\n   private static class ThresholdAdjustingQueryRunner implements QueryRunner<Result<TopNResultValue>>\n   {\n     private final QueryRunner<Result<TopNResultValue>> runner;\n-    private final int minTopNThreshold;\n+    private final TopNQueryConfig config;\n \n     public ThresholdAdjustingQueryRunner(\n         QueryRunner<Result<TopNResultValue>> runner,\n-        int minTopNThreshold\n+        TopNQueryConfig config\n     )\n     {\n       this.runner = runner;\n-      this.minTopNThreshold = minTopNThreshold;\n+      this.config = config;\n     }\n \n     @Override\n@@ -557,6 +557,7 @@ public Sequence<Result<TopNResultValue>> run(\n       }\n \n       final TopNQuery query = (TopNQuery) input;\n+      final int minTopNThreshold = query.getContextValue(\"minTopNThreshold\", config.getMinTopNThreshold());\n       if (query.getThreshold() > minTopNThreshold) {\n         return runner.run(query, responseContext);\n       }",
      "parent_sha": "3048b1f0a5058fcd83c48015252b7a942fa63f5c"
    }
  },
  {
    "oid": "abf64a13b0564d60d78e91ef5d3074724e493c30",
    "message": "reconnect to the graphite after transient disconnect (#2952)\n\n* reconnect to the graphite after transient disconnect\r\n\r\n* catch the socket exception and retry",
    "date": "2016-05-12T18:32:36Z",
    "url": "https://github.com/apache/druid/commit/abf64a13b0564d60d78e91ef5d3074724e493c30",
    "details": {
      "sha": "52e892c0fead5fe07b3fb5a3399889f4736e957e",
      "filename": "extensions-contrib/graphite-emitter/src/main/java/io/druid/emitter/graphite/GraphiteEmitter.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/abf64a13b0564d60d78e91ef5d3074724e493c30/extensions-contrib%2Fgraphite-emitter%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Femitter%2Fgraphite%2FGraphiteEmitter.java",
      "raw_url": "https://github.com/apache/druid/raw/abf64a13b0564d60d78e91ef5d3074724e493c30/extensions-contrib%2Fgraphite-emitter%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Femitter%2Fgraphite%2FGraphiteEmitter.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/extensions-contrib%2Fgraphite-emitter%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Femitter%2Fgraphite%2FGraphiteEmitter.java?ref=abf64a13b0564d60d78e91ef5d3074724e493c30",
      "patch": "@@ -29,6 +29,7 @@\n import com.metamx.emitter.service.ServiceMetricEvent;\n \n import java.io.IOException;\n+import java.net.SocketException;\n import java.util.List;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Executors;\n@@ -139,7 +140,6 @@ private class ConsumerRunnable implements Runnable\n     public void run()\n     {\n       try {\n-\n         if (!pickledGraphite.isConnected()) {\n           log.info(\"trying to connect to graphite server\");\n           pickledGraphite.connect();\n@@ -168,6 +168,8 @@ public void run()\n             log.error(e, e.getMessage());\n             if (e instanceof InterruptedException) {\n               Thread.currentThread().interrupt();\n+            } else if (e instanceof SocketException){\n+              pickledGraphite.connect();\n             }\n           }\n         }",
      "parent_sha": "01bebf432af197198ec6972c42ba8a488e01c159"
    }
  },
  {
    "oid": "6d2747adcfdd38941b7d5e8b0cfddc8e8229e6a7",
    "message": "make loading local files easier",
    "date": "2014-09-30T21:50:48Z",
    "url": "https://github.com/apache/druid/commit/6d2747adcfdd38941b7d5e8b0cfddc8e8229e6a7",
    "details": {
      "sha": "6be8f08ec0d1d3ce74d6e69a37df77f9f91661e1",
      "filename": "server/src/main/java/io/druid/segment/realtime/firehose/LocalFirehoseFactory.java",
      "status": "modified",
      "additions": 10,
      "deletions": 15,
      "changes": 25,
      "blob_url": "https://github.com/apache/druid/blob/6d2747adcfdd38941b7d5e8b0cfddc8e8229e6a7/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Frealtime%2Ffirehose%2FLocalFirehoseFactory.java",
      "raw_url": "https://github.com/apache/druid/raw/6d2747adcfdd38941b7d5e8b0cfddc8e8229e6a7/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Frealtime%2Ffirehose%2FLocalFirehoseFactory.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Frealtime%2Ffirehose%2FLocalFirehoseFactory.java?ref=6d2747adcfdd38941b7d5e8b0cfddc8e8229e6a7",
      "patch": "@@ -30,11 +30,12 @@\n import io.druid.data.input.impl.StringInputRowParser;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.io.LineIterator;\n+import org.apache.commons.io.filefilter.RegexFileFilter;\n+import org.apache.commons.io.filefilter.TrueFileFilter;\n \n import java.io.File;\n-import java.io.FilenameFilter;\n import java.io.IOException;\n-import java.util.Arrays;\n+import java.util.Collection;\n import java.util.Iterator;\n import java.util.LinkedList;\n \n@@ -79,26 +80,20 @@ public StringInputRowParser getParser()\n   @Override\n   public Firehose connect(StringInputRowParser firehoseParser) throws IOException\n   {\n-    File[] foundFiles = baseDir.listFiles(\n-        new FilenameFilter()\n-        {\n-          @Override\n-          public boolean accept(File file, String name)\n-          {\n-            return name.contains(filter);\n-          }\n-        }\n+    Collection<File> foundFiles = FileUtils.listFiles(\n+        baseDir,\n+        new RegexFileFilter(filter),\n+        TrueFileFilter.INSTANCE\n     );\n \n-    if (foundFiles == null || foundFiles.length == 0) {\n+    if (foundFiles == null || foundFiles.isEmpty()) {\n       throw new ISE(\"Found no files to ingest! Check your schema.\");\n     }\n \n-    final LinkedList<File> files = Lists.<File>newLinkedList(\n-        Arrays.asList(foundFiles)\n+    final LinkedList<File> files = Lists.newLinkedList(\n+        foundFiles\n     );\n \n-\n     return new FileIteratingFirehose(\n         new Iterator<LineIterator>()\n         {",
      "parent_sha": "0781781b990b9425f1db7bb677aae316fa7b3c05"
    }
  },
  {
    "oid": "8ccc0b241a38838e2e4031050f08d8b82b9c8fce",
    "message": "Fix some flaws of KafkaEmitter (#9573)\n\n* fix flaws of KafkaEmitter\r\n\r\n* fix flaws of KafkaEmitter\r\n\r\n* fix flaws of KafkaEmitter\r\n\r\n* Update extensions-contrib/kafka-emitter/src/main/java/org/apache/druid/emitter/kafka/KafkaEmitter.java\r\n\r\nCo-Authored-By: Himanshu <g.himanshu@gmail.com>\r\n\r\n* Update extensions-contrib/kafka-emitter/src/main/java/org/apache/druid/emitter/kafka/KafkaEmitter.java\r\n\r\nCo-Authored-By: Himanshu <g.himanshu@gmail.com>\r\n\r\nCo-authored-by: Himanshu <g.himanshu@gmail.com>",
    "date": "2020-04-10T06:31:32Z",
    "url": "https://github.com/apache/druid/commit/8ccc0b241a38838e2e4031050f08d8b82b9c8fce",
    "details": {
      "sha": "ceb21c3975c98b565edf980e1dca66289d514da4",
      "filename": "extensions-contrib/kafka-emitter/src/main/java/org/apache/druid/emitter/kafka/KafkaEmitter.java",
      "status": "modified",
      "additions": 10,
      "deletions": 20,
      "changes": 30,
      "blob_url": "https://github.com/apache/druid/blob/8ccc0b241a38838e2e4031050f08d8b82b9c8fce/extensions-contrib%2Fkafka-emitter%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Femitter%2Fkafka%2FKafkaEmitter.java",
      "raw_url": "https://github.com/apache/druid/raw/8ccc0b241a38838e2e4031050f08d8b82b9c8fce/extensions-contrib%2Fkafka-emitter%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Femitter%2Fkafka%2FKafkaEmitter.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/extensions-contrib%2Fkafka-emitter%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Femitter%2Fkafka%2FKafkaEmitter.java?ref=8ccc0b241a38838e2e4031050f08d8b82b9c8fce",
      "patch": "@@ -24,7 +24,6 @@\n import com.google.common.collect.ImmutableMap;\n import org.apache.druid.emitter.kafka.MemoryBoundLinkedBlockingQueue.ObjectContainer;\n import org.apache.druid.java.util.common.StringUtils;\n-import org.apache.druid.java.util.common.lifecycle.LifecycleStart;\n import org.apache.druid.java.util.common.lifecycle.LifecycleStop;\n import org.apache.druid.java.util.common.logger.Logger;\n import org.apache.druid.java.util.emitter.core.Emitter;\n@@ -56,7 +55,6 @@ public class KafkaEmitter implements Emitter\n \n   private final KafkaEmitterConfig config;\n   private final Producer<String, String> producer;\n-  private final Callback producerCallback;\n   private final ObjectMapper jsonMapper;\n   private final MemoryBoundLinkedBlockingQueue<String> metricQueue;\n   private final MemoryBoundLinkedBlockingQueue<String> alertQueue;\n@@ -67,7 +65,6 @@ public KafkaEmitter(KafkaEmitterConfig config, ObjectMapper jsonMapper)\n     this.config = config;\n     this.jsonMapper = jsonMapper;\n     this.producer = setKafkaProducer();\n-    this.producerCallback = setProducerCallback();\n     // same with kafka producer's buffer.memory\n     long queueMemoryBound = Long.parseLong(this.config.getKafkaProducerConfig()\n                                                       .getOrDefault(ProducerConfig.BUFFER_MEMORY_CONFIG, \"33554432\"));\n@@ -79,18 +76,12 @@ public KafkaEmitter(KafkaEmitterConfig config, ObjectMapper jsonMapper)\n     this.invalidLost = new AtomicLong(0L);\n   }\n \n-  private Callback setProducerCallback()\n+  private Callback setProducerCallback(AtomicLong lostCouter)\n   {\n     return (recordMetadata, e) -> {\n       if (e != null) {\n         log.debug(\"Event send failed [%s]\", e.getMessage());\n-        if (recordMetadata.topic().equals(config.getMetricTopic())) {\n-          metricLost.incrementAndGet();\n-        } else if (recordMetadata.topic().equals(config.getAlertTopic())) {\n-          alertLost.incrementAndGet();\n-        } else {\n-          invalidLost.incrementAndGet();\n-        }\n+        lostCouter.incrementAndGet();\n       }\n     };\n   }\n@@ -116,11 +107,10 @@ private Producer<String, String> setKafkaProducer()\n   }\n \n   @Override\n-  @LifecycleStart\n   public void start()\n   {\n-    scheduler.scheduleWithFixedDelay(this::sendMetricToKafka, 10, 10, TimeUnit.SECONDS);\n-    scheduler.scheduleWithFixedDelay(this::sendAlertToKafka, 10, 10, TimeUnit.SECONDS);\n+    scheduler.schedule(this::sendMetricToKafka, 10, TimeUnit.SECONDS);\n+    scheduler.schedule(this::sendAlertToKafka, 10, TimeUnit.SECONDS);\n     scheduler.scheduleWithFixedDelay(() -> {\n       log.info(\"Message lost counter: metricLost=[%d], alertLost=[%d], invalidLost=[%d]\",\n                metricLost.get(), alertLost.get(), invalidLost.get());\n@@ -130,25 +120,25 @@ public void start()\n \n   private void sendMetricToKafka()\n   {\n-    sendToKafka(config.getMetricTopic(), metricQueue);\n+    sendToKafka(config.getMetricTopic(), metricQueue, setProducerCallback(metricLost));\n   }\n \n   private void sendAlertToKafka()\n   {\n-    sendToKafka(config.getAlertTopic(), alertQueue);\n+    sendToKafka(config.getAlertTopic(), alertQueue, setProducerCallback(alertLost));\n   }\n \n-  private void sendToKafka(final String topic, MemoryBoundLinkedBlockingQueue<String> recordQueue)\n+  private void sendToKafka(final String topic, MemoryBoundLinkedBlockingQueue<String> recordQueue, Callback callback)\n   {\n     ObjectContainer<String> objectToSend;\n     try {\n       while (true) {\n         objectToSend = recordQueue.take();\n-        producer.send(new ProducerRecord<>(topic, objectToSend.getData()), producerCallback);\n+        producer.send(new ProducerRecord<>(topic, objectToSend.getData()), callback);\n       }\n     }\n-    catch (InterruptedException e) {\n-      log.warn(e, \"Failed to take record from queue!\");\n+    catch (Throwable e) {\n+      log.warn(e, \"Exception while getting record from queue or producer send, Events would not be emitted anymore.\");\n     }\n   }\n ",
      "parent_sha": "65de636893a4f389eb33dba81e951e5a5aefdfe4"
    }
  },
  {
    "oid": "c5fbb67d94cdc9c47fc520e67967adfa9f423b00",
    "message": "more endpoint cleanup",
    "date": "2014-03-28T21:31:30Z",
    "url": "https://github.com/apache/druid/commit/c5fbb67d94cdc9c47fc520e67967adfa9f423b00",
    "details": {
      "sha": "e892c48d15abbc42429e835daf418796e7bf8a9d",
      "filename": "server/src/main/java/io/druid/server/http/DatasourcesResource.java",
      "status": "modified",
      "additions": 56,
      "deletions": 3,
      "changes": 59,
      "blob_url": "https://github.com/apache/druid/blob/c5fbb67d94cdc9c47fc520e67967adfa9f423b00/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FDatasourcesResource.java",
      "raw_url": "https://github.com/apache/druid/raw/c5fbb67d94cdc9c47fc520e67967adfa9f423b00/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FDatasourcesResource.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Fhttp%2FDatasourcesResource.java?ref=c5fbb67d94cdc9c47fc520e67967adfa9f423b00",
      "patch": "@@ -35,6 +35,7 @@\n import io.druid.client.indexing.IndexingServiceClient;\n import io.druid.db.DatabaseSegmentManager;\n import io.druid.timeline.DataSegment;\n+import org.joda.time.DateTime;\n import org.joda.time.Interval;\n \n import javax.annotation.Nullable;\n@@ -131,15 +132,67 @@ public String apply(DruidDataSource dataSource)\n   @Path(\"/{dataSourceName}\")\n   @Produces(\"application/json\")\n   public Response getTheDataSource(\n-      @PathParam(\"dataSourceName\") final String dataSourceName\n+      @PathParam(\"dataSourceName\") final String dataSourceName,\n+      @QueryParam(\"full\") final String full\n   )\n   {\n     DruidDataSource dataSource = getDataSource(dataSourceName.toLowerCase());\n     if (dataSource == null) {\n       return Response.noContent().build();\n     }\n \n-    return Response.ok(dataSource).build();\n+    if (full != null) {\n+      return Response.ok(dataSource).build();\n+    }\n+\n+    Map<String, Object> tiers = Maps.newHashMap();\n+    Map<String, Object> segments = Maps.newHashMap();\n+    Map<String, Map<String, Object>> retVal = ImmutableMap.of(\n+        \"tiers\", tiers,\n+        \"segments\", segments\n+    );\n+\n+    int totalSegmentCount = 0;\n+    long totalSegmentSize = 0;\n+    long minTime = Long.MAX_VALUE;\n+    long maxTime = Long.MIN_VALUE;\n+    for (DruidServer druidServer : serverInventoryView.getInventory()) {\n+      DruidDataSource druidDataSource = druidServer.getDataSource(dataSourceName);\n+\n+      long dataSourceSegmentSize = 0;\n+      for (DataSegment dataSegment : druidDataSource.getSegments()) {\n+        dataSourceSegmentSize += dataSegment.getSize();\n+        if (dataSegment.getInterval().getStartMillis() < minTime) {\n+          minTime = dataSegment.getInterval().getStartMillis();\n+        }\n+        if (dataSegment.getInterval().getEndMillis() > maxTime) {\n+          maxTime = dataSegment.getInterval().getEndMillis();\n+        }\n+      }\n+\n+      // segment stats\n+      totalSegmentCount += druidDataSource.getSegments().size();\n+      totalSegmentSize += dataSourceSegmentSize;\n+\n+      // tier stats\n+      Map<String, Object> tierStats = (Map) tiers.get(druidServer.getTier());\n+      if (tierStats == null) {\n+        tierStats = Maps.newHashMap();\n+        tiers.put(druidServer.getTier(), tierStats);\n+      }\n+      int segmentCount = MapUtils.getInt(tierStats, \"segmentCount\", 0);\n+      tierStats.put(\"segmentCount\", segmentCount + druidDataSource.getSegments().size());\n+\n+      long segmentSize = MapUtils.getLong(tierStats, \"size\", 0L);\n+      tierStats.put(\"size\", segmentSize + dataSourceSegmentSize);\n+    }\n+\n+    segments.put(\"count\", totalSegmentCount);\n+    segments.put(\"size\", totalSegmentSize);\n+    segments.put(\"minTime\", new DateTime(minTime));\n+    segments.put(\"maxTime\", new DateTime(maxTime));\n+\n+    return Response.ok(retVal).build();\n   }\n \n   @POST\n@@ -418,7 +471,7 @@ public Response getSegmentDataSourceTiers(\n   {\n     Set<String> retVal = Sets.newHashSet();\n     for (DruidServer druidServer : serverInventoryView.getInventory()) {\n-      if(druidServer.getDataSource(dataSourceName) != null) {\n+      if (druidServer.getDataSource(dataSourceName) != null) {\n         retVal.add(druidServer.getTier());\n       }\n     }",
      "parent_sha": "676671e575b404743f4f580ac0d9717b5341a30c"
    }
  },
  {
    "oid": "97ddb38d75d83ab553ba921e1c5b5cbcbce23c6e",
    "message": "DatasourceInputSplit: Serialize with write instead of writeUTF. (#4195)\n\nwriteUTF has a limit of 64KB, making it difficult to write out splits\r\nthat read a large number of descriptors for small segments.",
    "date": "2017-04-25T17:26:44Z",
    "url": "https://github.com/apache/druid/commit/97ddb38d75d83ab553ba921e1c5b5cbcbce23c6e",
    "details": {
      "sha": "b18c8eb69482bc64687a30dd6079049cd200969d",
      "filename": "indexing-hadoop/src/main/java/io/druid/indexer/hadoop/DatasourceInputSplit.java",
      "status": "modified",
      "additions": 7,
      "deletions": 2,
      "changes": 9,
      "blob_url": "https://github.com/apache/druid/blob/97ddb38d75d83ab553ba921e1c5b5cbcbce23c6e/indexing-hadoop%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexer%2Fhadoop%2FDatasourceInputSplit.java",
      "raw_url": "https://github.com/apache/druid/raw/97ddb38d75d83ab553ba921e1c5b5cbcbce23c6e/indexing-hadoop%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexer%2Fhadoop%2FDatasourceInputSplit.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-hadoop%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexer%2Fhadoop%2FDatasourceInputSplit.java?ref=97ddb38d75d83ab553ba921e1c5b5cbcbce23c6e",
      "patch": "@@ -74,7 +74,9 @@ public List<WindowedDataSegment> getSegments()\n   @Override\n   public void write(DataOutput out) throws IOException\n   {\n-    out.writeUTF(HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(segments));\n+    final byte[] segmentsBytes = HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsBytes(segments);\n+    out.writeInt(segmentsBytes.length);\n+    out.write(segmentsBytes);\n     out.writeInt(locations.length);\n     for (String location : locations) {\n       out.writeUTF(location);\n@@ -84,8 +86,11 @@ public void write(DataOutput out) throws IOException\n   @Override\n   public void readFields(DataInput in) throws IOException\n   {\n+    final int segmentsBytesLength = in.readInt();\n+    final byte[] buf = new byte[segmentsBytesLength];\n+    in.readFully(buf);\n     segments = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n-        in.readUTF(),\n+        buf,\n         new TypeReference<List<WindowedDataSegment>>()\n         {\n         }",
      "parent_sha": "e4fbc2bc5bb259f7e037c20fbac105095ce60eeb"
    }
  },
  {
    "oid": "94b5c292b93052afb32ce897baa0dd8cb3a3fb0b",
    "message": "Fix test oopsie",
    "date": "2013-08-21T22:05:29Z",
    "url": "https://github.com/apache/druid/commit/94b5c292b93052afb32ce897baa0dd8cb3a3fb0b",
    "details": {
      "sha": "8ce5641325f3e19da28d55853aadb8a458f46612",
      "filename": "client/src/test/java/com/metamx/druid/shard/NumberedShardSpecTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/94b5c292b93052afb32ce897baa0dd8cb3a3fb0b/client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fshard%2FNumberedShardSpecTest.java",
      "raw_url": "https://github.com/apache/druid/raw/94b5c292b93052afb32ce897baa0dd8cb3a3fb0b/client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fshard%2FNumberedShardSpecTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/client%2Fsrc%2Ftest%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fshard%2FNumberedShardSpecTest.java?ref=94b5c292b93052afb32ce897baa0dd8cb3a3fb0b",
      "patch": "@@ -49,7 +49,7 @@ public void testSerdeBackwardsCompat() throws Exception\n   {\n     final ObjectMapper jsonMapper = new DefaultObjectMapper();\n     final ShardSpec spec = jsonMapper.readValue(\n-        \"{\\\"type\\\": \\\"numbered\\\", \\\"partition\\\": 1, \\\"partitionNum\\\": 2}\",\n+        \"{\\\"type\\\": \\\"numbered\\\", \\\"partitions\\\": 2, \\\"partitionNum\\\": 2}\",\n         ShardSpec.class\n     );\n     Assert.assertEquals(1, spec.getPartitionNum());",
      "parent_sha": "9b9a2a9f53e4d02ca1214313eaa54855c54a1beb"
    }
  },
  {
    "oid": "4aa9381f6ad953582fd52a3f2b18d4bb4cdcfcac",
    "message": "fixing the typo for audit table (type, created_date) index creation",
    "date": "2015-04-29T22:09:35Z",
    "url": "https://github.com/apache/druid/commit/4aa9381f6ad953582fd52a3f2b18d4bb4cdcfcac",
    "details": {
      "sha": "57c6fc0d07da8901e90c565b331f062f064982f1",
      "filename": "server/src/main/java/io/druid/metadata/SQLMetadataConnector.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/4aa9381f6ad953582fd52a3f2b18d4bb4cdcfcac/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fmetadata%2FSQLMetadataConnector.java",
      "raw_url": "https://github.com/apache/druid/raw/4aa9381f6ad953582fd52a3f2b18d4bb4cdcfcac/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fmetadata%2FSQLMetadataConnector.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fmetadata%2FSQLMetadataConnector.java?ref=4aa9381f6ad953582fd52a3f2b18d4bb4cdcfcac",
      "patch": "@@ -397,7 +397,7 @@ private void createAuditTable(final IDBI dbi, final String tableName)\n                 tableName, getSerialType(), getPayloadType()\n             ),\n             String.format(\"CREATE INDEX idx_%1$s_key_time ON %1$s(audit_key, created_date)\", tableName),\n-            String.format(\"CREATE INDEX idx_%1$s_type_time ON %1$s(audit_key, created_date)\", tableName),\n+            String.format(\"CREATE INDEX idx_%1$s_type_time ON %1$s(type, created_date)\", tableName),\n             String.format(\"CREATE INDEX idx_%1$s_audit_time ON %1$s(created_date)\", tableName)\n         )\n     );",
      "parent_sha": "95e0429eb1bcd0ae657ee59cf6a17be934963753"
    }
  },
  {
    "oid": "c8b3f8cc008b58c0138f8581aff95ff8a7511917",
    "message": "avoid logging pause message multiple times (#11375)\n\nIn some instances the ingestion thread could be woken up spuriously,\r\nresulting in the \"Received pause command...\" log message getting logged\r\nmultiple times. This change ensures we only log it once the first time\r\nthe pause is requested.",
    "date": "2021-06-22T16:30:38Z",
    "url": "https://github.com/apache/druid/commit/c8b3f8cc008b58c0138f8581aff95ff8a7511917",
    "details": {
      "sha": "ffede95c1e623ee9740243719ac014608e6dfc21",
      "filename": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/SeekableStreamIndexTaskRunner.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/druid/blob/c8b3f8cc008b58c0138f8581aff95ff8a7511917/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fseekablestream%2FSeekableStreamIndexTaskRunner.java",
      "raw_url": "https://github.com/apache/druid/raw/c8b3f8cc008b58c0138f8581aff95ff8a7511917/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fseekablestream%2FSeekableStreamIndexTaskRunner.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fseekablestream%2FSeekableStreamIndexTaskRunner.java?ref=c8b3f8cc008b58c0138f8581aff95ff8a7511917",
      "patch": "@@ -1279,8 +1279,8 @@ private boolean possiblyPause() throws InterruptedException\n         status = Status.PAUSED;\n         hasPaused.signalAll();\n \n+        log.debug(\"Received pause command, pausing ingestion until resumed.\");\n         while (pauseRequested) {\n-          log.debug(\"Received pause command, pausing ingestion until resumed.\");\n           shouldResume.await();\n         }\n ",
      "parent_sha": "f0b105ec63657643c85d82686ae1dea3e588a5f5"
    }
  },
  {
    "oid": "0b91cc4db20bb2395eb7177d9e15d04353c885c8",
    "message": "Fix incorrect tests in Sting first/last serde's null handling (#15657)\n\nFixes a couple of incorrect test cases, that got merged accidentally",
    "date": "2024-01-10T13:46:12Z",
    "url": "https://github.com/apache/druid/commit/0b91cc4db20bb2395eb7177d9e15d04353c885c8",
    "details": {
      "sha": "effa47b471f6fa86a5b71e568237784240073743",
      "filename": "processing/src/test/java/org/apache/druid/query/aggregation/BackwardCompatibleSerializablePairLongStringSimpleStagedSerdeTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/0b91cc4db20bb2395eb7177d9e15d04353c885c8/processing%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fquery%2Faggregation%2FBackwardCompatibleSerializablePairLongStringSimpleStagedSerdeTest.java",
      "raw_url": "https://github.com/apache/druid/raw/0b91cc4db20bb2395eb7177d9e15d04353c885c8/processing%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fquery%2Faggregation%2FBackwardCompatibleSerializablePairLongStringSimpleStagedSerdeTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fquery%2Faggregation%2FBackwardCompatibleSerializablePairLongStringSimpleStagedSerdeTest.java?ref=0b91cc4db20bb2395eb7177d9e15d04353c885c8",
      "patch": "@@ -59,7 +59,7 @@ public void testNullString()\n     SerializablePairLongString value = new SerializablePairLongString(Long.MAX_VALUE, null);\n     // Write using the older serde, read using the newer serde\n     Assert.assertEquals(\n-        new SerializablePairLongString(Long.MAX_VALUE, \"\"),\n+        new SerializablePairLongString(Long.MAX_VALUE, null),\n         readUsingSerde(writeUsingSerde(value, OLDER_SERDE), SERDE)\n     );\n     // Write using the newer serde, read using the older serde\n@@ -77,7 +77,7 @@ public void testEmptyString()\n     SerializablePairLongString value = new SerializablePairLongString(Long.MAX_VALUE, \"\");\n     // Write using the older serde, read using the newer serde\n     Assert.assertEquals(\n-        new SerializablePairLongString(Long.MAX_VALUE, \"\"),\n+        new SerializablePairLongString(Long.MAX_VALUE, null),\n         readUsingSerde(writeUsingSerde(value, OLDER_SERDE), SERDE)\n     );\n     // Write using the newer serde, read using the older serde",
      "parent_sha": "d623756c66885029e54fe11838a51ae4d843020d"
    }
  },
  {
    "oid": "071e8cb2f33732bef17aec9ac39cadf7d4a04f9b",
    "message": "updated javadocs",
    "date": "2013-08-15T18:07:55Z",
    "url": "https://github.com/apache/druid/commit/071e8cb2f33732bef17aec9ac39cadf7d4a04f9b",
    "details": {
      "sha": "aa9f47c8c817459607d0b411526c286b70cbeaee",
      "filename": "realtime/src/main/java/com/metamx/druid/realtime/firehose/IrcFirehoseFactory.java",
      "status": "modified",
      "additions": 4,
      "deletions": 2,
      "changes": 6,
      "blob_url": "https://github.com/apache/druid/blob/071e8cb2f33732bef17aec9ac39cadf7d4a04f9b/realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Frealtime%2Ffirehose%2FIrcFirehoseFactory.java",
      "raw_url": "https://github.com/apache/druid/raw/071e8cb2f33732bef17aec9ac39cadf7d4a04f9b/realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Frealtime%2Ffirehose%2FIrcFirehoseFactory.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Frealtime%2Ffirehose%2FIrcFirehoseFactory.java?ref=071e8cb2f33732bef17aec9ac39cadf7d4a04f9b",
      "patch": "@@ -41,10 +41,11 @@\n import java.util.concurrent.LinkedBlockingQueue;\n \n /**\n- * Example usage\n+ * <p><b>Example Usage</b></p>\n  *\n- * Decoder definition <code>wikipedia-decoder.json</code>\n+ * <p>Decoder definition: <code>wikipedia-decoder.json</code></p>\n  * <pre>{@code\n+ *\n  * {\n  *   \"type\": \"wikipedia\",\n  *   \"namespaces\": {\n@@ -71,6 +72,7 @@\n  * }\n  * }</pre>\n  *\n+ * <p><b>Example code:</b></p>\n  * <pre>{@code\n  * IrcDecoder wikipediaDecoder = new ObjectMapper().readValue(\n  *   new File(\"wikipedia-decoder.json\"),",
      "parent_sha": "3f33ca15b057a5771b64ceda6e35fc7125dfa797"
    }
  },
  {
    "oid": "4716e0b5857cebaee50cf999f679941d8b2fff1a",
    "message": "Fix concurrency of ComplexMetrics.java (#9134)",
    "date": "2020-01-15T14:19:45Z",
    "url": "https://github.com/apache/druid/commit/4716e0b5857cebaee50cf999f679941d8b2fff1a",
    "details": {
      "sha": "4f40ff6bd83d4309deccf0b84b3ac64f0e640875",
      "filename": "processing/src/main/java/org/apache/druid/segment/serde/ComplexMetrics.java",
      "status": "modified",
      "additions": 29,
      "deletions": 14,
      "changes": 43,
      "blob_url": "https://github.com/apache/druid/blob/4716e0b5857cebaee50cf999f679941d8b2fff1a/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2Fserde%2FComplexMetrics.java",
      "raw_url": "https://github.com/apache/druid/raw/4716e0b5857cebaee50cf999f679941d8b2fff1a/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2Fserde%2FComplexMetrics.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fsegment%2Fserde%2FComplexMetrics.java?ref=4716e0b5857cebaee50cf999f679941d8b2fff1a",
      "patch": "@@ -22,34 +22,49 @@\n import org.apache.druid.java.util.common.ISE;\n \n import javax.annotation.Nullable;\n-import java.util.HashMap;\n-import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n \n /**\n+ *  ComplexMetrics houses a mapping of serde names to affiliated ComplexMetricSerde objects.\n  */\n public class ComplexMetrics\n {\n-  private static final Map<String, ComplexMetricSerde> COMPLEX_SERIALIZERS = new HashMap<>();\n+  private static final ConcurrentHashMap<String, ComplexMetricSerde> COMPLEX_SERIALIZERS = new ConcurrentHashMap<>();\n \n   @Nullable\n   public static ComplexMetricSerde getSerdeForType(String type)\n   {\n     return COMPLEX_SERIALIZERS.get(type);\n   }\n \n+  /**\n+   * Register a serde name -> ComplexMetricSerde mapping.\n+   *\n+   * <p>\n+   * If the specified serde key string is already used and the supplied ComplexMetricSerde is not of the same\n+   * type as the existing value in the map for said key, an ISE is thrown.\n+   * </p>\n+   *\n+   * @param type The serde name used as the key in the map.\n+   * @param serde The ComplexMetricSerde object to be associated with the 'type' in the map.\n+   */\n   public static void registerSerde(String type, ComplexMetricSerde serde)\n   {\n-    if (COMPLEX_SERIALIZERS.containsKey(type)) {\n-      if (!COMPLEX_SERIALIZERS.get(type).getClass().getName().equals(serde.getClass().getName())) {\n-        throw new ISE(\n-            \"Incompatible serializer for type[%s] already exists. Expected [%s], found [%s].\",\n-            type,\n-            serde.getClass().getName(),\n-            COMPLEX_SERIALIZERS.get(type).getClass().getName()\n-        );\n+    COMPLEX_SERIALIZERS.compute(type, (key, value) -> {\n+      if (value == null) {\n+        return serde;\n+      } else {\n+        if (!value.getClass().getName().equals(serde.getClass().getName())) {\n+          throw new ISE(\n+              \"Incompatible serializer for type[%s] already exists. Expected [%s], found [%s].\",\n+              key,\n+              serde.getClass().getName(),\n+              value.getClass().getName()\n+          );\n+        } else {\n+          return value;\n+        }\n       }\n-    } else {\n-      COMPLEX_SERIALIZERS.put(type, serde);\n-    }\n+    });\n   }\n }",
      "parent_sha": "b2877119d07a973b19aa48cb1e278779bf88e874"
    }
  },
  {
    "oid": "972c5dac31b68fc321cb2a408a516daadb6b9095",
    "message": "improve memory usage and rename firehose",
    "date": "2014-07-14T15:47:53Z",
    "url": "https://github.com/apache/druid/commit/972c5dac31b68fc321cb2a408a516daadb6b9095",
    "details": {
      "sha": "85f1c0b4c590fea56a198629344f21087cac9af5",
      "filename": "indexing-service/src/main/java/io/druid/indexing/firehose/IngestSegmentFirehoseFactory.java",
      "status": "modified",
      "additions": 63,
      "deletions": 38,
      "changes": 101,
      "blob_url": "https://github.com/apache/druid/blob/972c5dac31b68fc321cb2a408a516daadb6b9095/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Ffirehose%2FIngestSegmentFirehoseFactory.java",
      "raw_url": "https://github.com/apache/druid/raw/972c5dac31b68fc321cb2a408a516daadb6b9095/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Ffirehose%2FIngestSegmentFirehoseFactory.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Findexing%2Ffirehose%2FIngestSegmentFirehoseFactory.java?ref=972c5dac31b68fc321cb2a408a516daadb6b9095",
      "patch": "@@ -71,6 +71,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.HashSet;\n+import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n@@ -138,7 +139,7 @@ public List<String> getMetrics()\n   @Override\n   public Firehose connect(InputRowParser inputRowParser) throws IOException, ParseException\n   {\n-    log.info(\"Connecting firehose: DruidFirehose[%s,%s]\", dataSource, interval);\n+    log.info(\"Connecting firehose: IngestSegmentFirehose[%s,%s]\", dataSource, interval);\n     // better way to achieve this is to pass toolbox to Firehose, The instance is initialized Lazily on connect method.\n     final TaskToolbox toolbox = injector.getInstance(TaskToolboxFactory.class).build(\n         new IngestTask(\"Ingest-Task-Id\", dataSource)\n@@ -204,7 +205,7 @@ public StorageAdapter apply(TimelineObjectHolder<String, DataSegment> input)\n           }\n       );\n \n-      return new DruidFirehose(adapters, dims, metricsList);\n+      return new IngestSegmentFirehose(adapters, dims, metricsList);\n \n     }\n     catch (IOException e) {\n@@ -249,11 +250,11 @@ public TaskStatus run(TaskToolbox toolbox) throws Exception\n     }\n   }\n \n-  public class DruidFirehose implements Firehose\n+  public class IngestSegmentFirehose implements Firehose\n   {\n     private volatile Yielder<InputRow> rowYielder;\n \n-    public DruidFirehose(List<StorageAdapter> adapters, final List<String> dims, final List<String> metrics)\n+    public IngestSegmentFirehose(List<StorageAdapter> adapters, final List<String> dims, final List<String> metrics)\n     {\n       Sequence<InputRow> rows = Sequences.concat(\n           Iterables.transform(\n@@ -273,54 +274,78 @@ public Sequence<InputRow> apply(@Nullable StorageAdapter adapter)\n                   {\n                     @Nullable\n                     @Override\n-                    public Sequence<InputRow> apply(@Nullable Cursor cursor)\n+                    public Sequence<InputRow> apply(@Nullable final Cursor cursor)\n                     {\n-                      TimestampColumnSelector timestampColumnSelector = cursor.makeTimestampColumnSelector();\n+                      final TimestampColumnSelector timestampColumnSelector = cursor.makeTimestampColumnSelector();\n \n-                      Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n+                      final Map<String, DimensionSelector> dimSelectors = Maps.newHashMap();\n                       for (String dim : dims) {\n                         final DimensionSelector dimSelector = cursor.makeDimensionSelector(dim);\n                         dimSelectors.put(dim, dimSelector);\n                       }\n \n-                      Map<String, ObjectColumnSelector> metSelectors = Maps.newHashMap();\n+                      final Map<String, ObjectColumnSelector> metSelectors = Maps.newHashMap();\n                       for (String metric : metrics) {\n                         final ObjectColumnSelector metricSelector = cursor.makeObjectColumnSelector(metric);\n                         metSelectors.put(metric, metricSelector);\n                       }\n \n-                      List<InputRow> rowList = Lists.newArrayList();\n-                      while (!cursor.isDone()) {\n-                        final Map<String, Object> theEvent = Maps.newLinkedHashMap();\n-                        final long timestamp = timestampColumnSelector.getTimestamp();\n-                        theEvent.put(EventHolder.timestampKey, new DateTime(timestamp));\n-\n-                        for (Map.Entry<String, DimensionSelector> dimSelector : dimSelectors.entrySet()) {\n-                          final String dim = dimSelector.getKey();\n-                          final DimensionSelector selector = dimSelector.getValue();\n-                          final IndexedInts vals = selector.getRow();\n-\n-                          if (vals.size() == 1) {\n-                            final String dimVal = selector.lookupName(vals.get(0));\n-                            theEvent.put(dim, dimVal);\n-                          } else {\n-                            List<String> dimVals = Lists.newArrayList();\n-                            for (int i = 0; i < vals.size(); ++i) {\n-                              dimVals.add(selector.lookupName(vals.get(i)));\n+                      return Sequences.simple(\n+                          new Iterable<InputRow>()\n+                          {\n+                            @Override\n+                            public Iterator<InputRow> iterator()\n+                            {\n+                              return new Iterator<InputRow>()\n+                              {\n+                                @Override\n+                                public boolean hasNext()\n+                                {\n+                                  return !cursor.isDone();\n+                                }\n+\n+                                @Override\n+                                public InputRow next()\n+                                {\n+                                  final Map<String, Object> theEvent = Maps.newLinkedHashMap();\n+                                  final long timestamp = timestampColumnSelector.getTimestamp();\n+                                  theEvent.put(EventHolder.timestampKey, new DateTime(timestamp));\n+\n+                                  for (Map.Entry<String, DimensionSelector> dimSelector : dimSelectors.entrySet()) {\n+                                    final String dim = dimSelector.getKey();\n+                                    final DimensionSelector selector = dimSelector.getValue();\n+                                    final IndexedInts vals = selector.getRow();\n+\n+                                    if (vals.size() == 1) {\n+                                      final String dimVal = selector.lookupName(vals.get(0));\n+                                      theEvent.put(dim, dimVal);\n+                                    } else {\n+                                      List<String> dimVals = Lists.newArrayList();\n+                                      for (int i = 0; i < vals.size(); ++i) {\n+                                        dimVals.add(selector.lookupName(vals.get(i)));\n+                                      }\n+                                      theEvent.put(dim, dimVals);\n+                                    }\n+                                  }\n+\n+                                  for (Map.Entry<String, ObjectColumnSelector> metSelector : metSelectors.entrySet()) {\n+                                    final String metric = metSelector.getKey();\n+                                    final ObjectColumnSelector selector = metSelector.getValue();\n+                                    theEvent.put(metric, selector.get());\n+                                  }\n+                                  cursor.advance();\n+                                  return new MapBasedInputRow(timestamp, dims, theEvent);\n+                                }\n+\n+                                @Override\n+                                public void remove()\n+                                {\n+                                  throw new UnsupportedOperationException(\"Remove Not Supported\");\n+                                }\n+                              };\n                             }\n-                            theEvent.put(dim, dimVals);\n                           }\n-                        }\n-\n-                        for (Map.Entry<String, ObjectColumnSelector> metSelector : metSelectors.entrySet()) {\n-                          final String metric = metSelector.getKey();\n-                          final ObjectColumnSelector selector = metSelector.getValue();\n-                          theEvent.put(metric, selector.get());\n-                        }\n-                        rowList.add(new MapBasedInputRow(timestamp, dims, theEvent));\n-                        cursor.advance();\n-                      }\n-                      return Sequences.simple(rowList);\n+                      );\n                     }\n                   }\n                   )",
      "parent_sha": "7168adcca7be87a8d91882fbebb96b296bd7381b"
    }
  },
  {
    "oid": "4ab0b71513132e6cef38483253aeaeacb84a15e7",
    "message": "Fix missing task failure error message on Overlord caused by MessageBodyWriter not found error on Middle Manager (#15412)\n\nFixes missing task failure error message on Overlord.\r\n\r\nThe error message was missing since TaskManagementResource#assignTask API wasn't annotated with @Produces(MediaType.APPLICATION_JSON) resulting in the response being treated as application/octet-stream, that in turn lead to MessageBodyWriter not found error on the middle manager. The exception is not logged on the middle manager itself since it happens even before entering the assignTask function -- while mapping arg Task -> MSQControllerTask.",
    "date": "2023-11-24T06:05:56Z",
    "url": "https://github.com/apache/druid/commit/4ab0b71513132e6cef38483253aeaeacb84a15e7",
    "details": {
      "sha": "4c2e9237a81b25d51fa36fdaed88f8bc671727be",
      "filename": "indexing-service/src/main/java/org/apache/druid/indexing/worker/http/TaskManagementResource.java",
      "status": "modified",
      "additions": 7,
      "deletions": 1,
      "changes": 8,
      "blob_url": "https://github.com/apache/druid/blob/4ab0b71513132e6cef38483253aeaeacb84a15e7/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fworker%2Fhttp%2FTaskManagementResource.java",
      "raw_url": "https://github.com/apache/druid/raw/4ab0b71513132e6cef38483253aeaeacb84a15e7/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fworker%2Fhttp%2FTaskManagementResource.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Findexing%2Fworker%2Fhttp%2FTaskManagementResource.java?ref=4ab0b71513132e6cef38483253aeaeacb84a15e7",
      "patch": "@@ -203,13 +203,19 @@ public void onFailure(Throwable th)\n   @POST\n   @Path(\"/assignTask\")\n   @Consumes({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})\n+  @Produces({MediaType.APPLICATION_JSON, SmileMediaTypes.APPLICATION_JACKSON_SMILE})\n   public Response assignTask(Task task)\n   {\n+    // Sometimes assignTask API can fail when the supplied task arg can't be interpreted due to some missing extension(s).\n+    // In such cases, the call produces an error response without entering the function.\n+    // @Produces helps to correctly write back this error response as JSON which otherwise ends up in an empty response\n+    // message due to \"MessageBodyWriter not found for SingletonImmutableBiMap\" error.\n+    // Ref: https://github.com/apache/druid/pull/15412.\n     try {\n       workerTaskManager.assignTask(task);\n       return Response.ok().build();\n     }\n-    catch (RuntimeException ex) {\n+    catch (Exception ex) {\n       return Response.serverError().entity(ex.getMessage()).build();\n     }\n   }",
      "parent_sha": "75d6993da93a7459793048095d27c33031c463e2"
    }
  },
  {
    "oid": "7f32629429021f8c693fd77a132a1aaadbfbe850",
    "message": "fix a bug with client info resource with guice",
    "date": "2013-03-01T21:53:27Z",
    "url": "https://github.com/apache/druid/commit/7f32629429021f8c693fd77a132a1aaadbfbe850",
    "details": {
      "sha": "8bf344e88bbb643aadd6a76e1a9643a732b68f61",
      "filename": "client/src/main/java/com/metamx/druid/http/ClientInfoResource.java",
      "status": "modified",
      "additions": 15,
      "deletions": 9,
      "changes": 24,
      "blob_url": "https://github.com/apache/druid/blob/7f32629429021f8c693fd77a132a1aaadbfbe850/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fhttp%2FClientInfoResource.java",
      "raw_url": "https://github.com/apache/druid/raw/7f32629429021f8c693fd77a132a1aaadbfbe850/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fhttp%2FClientInfoResource.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/client%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fhttp%2FClientInfoResource.java?ref=7f32629429021f8c693fd77a132a1aaadbfbe850",
      "patch": "@@ -84,7 +84,7 @@ public Iterable<String> getDataSources()\n   @Produces(\"application/json\")\n   public Map<String, Object> getDatasource(\n       @PathParam(\"dataSourceName\") String dataSourceName,\n-      @QueryParam(\"interval\") Interval interval\n+      @QueryParam(\"interval\") String interval\n   )\n   {\n     return ImmutableMap.<String, Object>of(\n@@ -98,19 +98,22 @@ public Map<String, Object> getDatasource(\n   @Produces(\"application/json\")\n   public Iterable<String> getDatasourceDimensions(\n       @PathParam(\"dataSourceName\") String dataSourceName,\n-      @QueryParam(\"interval\") Interval interval\n+      @QueryParam(\"interval\") String interval\n   )\n   {\n     List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n \n-    if (interval == null) {\n+    Interval theInterval;\n+    if (interval == null || interval.isEmpty()) {\n       DateTime now = new DateTime();\n-      interval = new Interval(now.minusMillis(SEGMENT_HISTORY_MILLIS), now);\n+      theInterval = new Interval(now.minusMillis(SEGMENT_HISTORY_MILLIS), now);\n+    } else {\n+      theInterval = new Interval(interval);\n     }\n \n     Set<String> dims = Sets.newHashSet();\n     for (DataSegment segment : segments) {\n-      if (interval.overlaps(segment.getInterval())) {\n+      if (theInterval.overlaps(segment.getInterval())) {\n         dims.addAll(segment.getDimensions());\n       }\n     }\n@@ -123,19 +126,22 @@ public Iterable<String> getDatasourceDimensions(\n   @Produces(\"application/json\")\n   public Iterable<String> getDatasourceMetrics(\n       @PathParam(\"dataSourceName\") String dataSourceName,\n-      @QueryParam(\"interval\") Interval interval\n+      @QueryParam(\"interval\") String interval\n   )\n   {\n     List<DataSegment> segments = getSegmentsForDatasources().get(dataSourceName);\n \n-    if (interval == null) {\n+    Interval theInterval;\n+    if (interval == null || interval.isEmpty()) {\n       DateTime now = new DateTime();\n-      interval = new Interval(now.minusMillis(SEGMENT_HISTORY_MILLIS), now);\n+      theInterval = new Interval(now.minusMillis(SEGMENT_HISTORY_MILLIS), now);\n+    } else {\n+      theInterval = new Interval(interval);\n     }\n \n     Set<String> metrics = Sets.newHashSet();\n     for (DataSegment segment : segments) {\n-      if (interval.overlaps(segment.getInterval())) {\n+      if (theInterval.overlaps(segment.getInterval())) {\n         metrics.addAll(segment.getMetrics());\n       }\n     }",
      "parent_sha": "3264a689f1867b6fe2ed384b43c4a76a66c48831"
    }
  },
  {
    "oid": "5eaadfffa9a4b9e1fa6c7811018a1ce39bcc29ad",
    "message": "no need for volatile references\n\nwe make no guarantees wrt threadsafety in this class",
    "date": "2014-03-26T00:07:09Z",
    "url": "https://github.com/apache/druid/commit/5eaadfffa9a4b9e1fa6c7811018a1ce39bcc29ad",
    "details": {
      "sha": "a96b7a5c2b7eb1769eec552d05ecaa9bb35e5509",
      "filename": "processing/src/main/java/io/druid/query/aggregation/hyperloglog/HyperLogLogCollector.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/druid/blob/5eaadfffa9a4b9e1fa6c7811018a1ce39bcc29ad/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Faggregation%2Fhyperloglog%2FHyperLogLogCollector.java",
      "raw_url": "https://github.com/apache/druid/raw/5eaadfffa9a4b9e1fa6c7811018a1ce39bcc29ad/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Faggregation%2Fhyperloglog%2FHyperLogLogCollector.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fquery%2Faggregation%2Fhyperloglog%2FHyperLogLogCollector.java?ref=5eaadfffa9a4b9e1fa6c7811018a1ce39bcc29ad",
      "patch": "@@ -202,9 +202,9 @@ private static boolean isSparse(ByteBuffer buffer)\n     return buffer.remaining() != NUM_BYTES_FOR_BUCKETS;\n   }\n \n-  private volatile ByteBuffer storageBuffer;\n-  private volatile int initPosition;\n-  private volatile Double estimatedCardinality;\n+  private ByteBuffer storageBuffer;\n+  private int initPosition;\n+  private Double estimatedCardinality;\n \n   public HyperLogLogCollector(ByteBuffer byteBuffer)\n   {",
      "parent_sha": "7dc0e9f1a98cf094eedcc76f998ec64496b7a3ee"
    }
  },
  {
    "oid": "96d5d4a59a6c97477afd1762ecd44fd84a22c572",
    "message": "fix some rt cleanup and failure edge cases",
    "date": "2014-08-14T20:38:32Z",
    "url": "https://github.com/apache/druid/commit/96d5d4a59a6c97477afd1762ecd44fd84a22c572",
    "details": {
      "sha": "fd06b54502c9575a7fe25adde56a931a5b16614e",
      "filename": "server/src/main/java/io/druid/segment/realtime/plumber/RealtimePlumber.java",
      "status": "modified",
      "additions": 30,
      "deletions": 11,
      "changes": 41,
      "blob_url": "https://github.com/apache/druid/blob/96d5d4a59a6c97477afd1762ecd44fd84a22c572/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Frealtime%2Fplumber%2FRealtimePlumber.java",
      "raw_url": "https://github.com/apache/druid/raw/96d5d4a59a6c97477afd1762ecd44fd84a22c572/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Frealtime%2Fplumber%2FRealtimePlumber.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Frealtime%2Fplumber%2FRealtimePlumber.java?ref=96d5d4a59a6c97477afd1762ecd44fd84a22c572",
      "patch": "@@ -59,6 +59,7 @@\n import java.io.File;\n import java.io.FilenameFilter;\n import java.io.IOException;\n+import java.nio.file.Files;\n import java.util.Arrays;\n import java.util.Comparator;\n import java.util.List;\n@@ -312,6 +313,12 @@ public void doRun()\n           {\n             final Interval interval = sink.getInterval();\n \n+            // use a marker file to indicate that merging has completed\n+            final File marker = new File(computePersistDir(schema, sink.getInterval()), \"marker\");\n+            if (marker.exists()) {\n+              removeMergedSegment(sink);\n+            }\n+\n             for (FireHydrant hydrant : sink) {\n               synchronized (hydrant) {\n                 if (!hydrant.hasSwapped()) {\n@@ -342,6 +349,9 @@ public void doRun()\n                   schema.getAggregators(),\n                   mergedTarget\n               );\n+              if (!marker.createNewFile()) {\n+                log.makeAlert(\"Unable to make marker file[%s]! WTF?!\", marker).emit();\n+              }\n \n               QueryableIndex index = IndexIO.loadIndex(mergedFile);\n \n@@ -350,6 +360,10 @@ public void doRun()\n                   sink.getSegment().withDimensions(Lists.newArrayList(index.getAvailableDimensions()))\n               );\n \n+              if (!marker.delete()) {\n+                log.makeAlert(\"Unable to remove merged marker\", marker).emit();\n+              }\n+\n               segmentPublisher.publishSegment(segment);\n             }\n             catch (Exception e) {\n@@ -649,7 +663,7 @@ protected void abandonSegment(final long truncatedTime, final Sink sink)\n   {\n     try {\n       segmentAnnouncer.unannounceSegment(sink.getSegment());\n-      removeMergedSegment(sink);\n+      removeSegment(sink, computePersistDir(schema, sink.getInterval()));\n       log.info(\"Removing sinkKey %d for segment %s\", truncatedTime, sink.getSegment().getIdentifier());\n       sinks.remove(truncatedTime);\n       sinkTimeline.remove(\n@@ -785,13 +799,13 @@ public boolean apply(final DataSegment segment)\n                 && config.getShardSpec().getPartitionNum() == segment.getShardSpec().getPartitionNum()\n                 && Iterables.any(\n                     sinks.keySet(), new Predicate<Long>()\n-                    {\n-                      @Override\n-                      public boolean apply(Long sinkKey)\n-                      {\n-                        return segment.getInterval().contains(sinkKey);\n-                      }\n-                    }\n+                {\n+                  @Override\n+                  public boolean apply(Long sinkKey)\n+                  {\n+                    return segment.getInterval().contains(sinkKey);\n+                  }\n+                }\n                 );\n           }\n         }\n@@ -801,10 +815,15 @@ public boolean apply(Long sinkKey)\n   private void removeMergedSegment(final Sink sink)\n   {\n     final File mergedTarget = new File(computePersistDir(schema, sink.getInterval()), \"merged\");\n-    if (mergedTarget.exists()) {\n+    removeSegment(sink, mergedTarget);\n+  }\n+\n+  private void removeSegment(final Sink sink, final File target)\n+  {\n+    if (target.exists()) {\n       try {\n-        log.info(\"Deleting Index File[%s]\", mergedTarget);\n-        FileUtils.deleteDirectory(mergedTarget);\n+        log.info(\"Deleting Index File[%s]\", target);\n+        FileUtils.deleteDirectory(target);\n       }\n       catch (Exception e) {\n         log.makeAlert(e, \"Unable to remove merged segment for dataSource[%s]\", schema.getDataSource())",
      "parent_sha": "4e3f4fbc22ff7d1ca535089b17aa04cef19c3ae0"
    }
  },
  {
    "oid": "80c72eb4864916b54dd8be7ed239934755874796",
    "message": "minor test adjustments",
    "date": "2014-10-31T22:44:09Z",
    "url": "https://github.com/apache/druid/commit/80c72eb4864916b54dd8be7ed239934755874796",
    "details": {
      "sha": "5fb44db5e37a67dc3d29ca7283b77f39a6fb218f",
      "filename": "server/src/test/java/io/druid/metadata/SQLMetadataConnectorTest.java",
      "status": "modified",
      "additions": 9,
      "deletions": 12,
      "changes": 21,
      "blob_url": "https://github.com/apache/druid/blob/80c72eb4864916b54dd8be7ed239934755874796/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fmetadata%2FSQLMetadataConnectorTest.java",
      "raw_url": "https://github.com/apache/druid/raw/80c72eb4864916b54dd8be7ed239934755874796/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fmetadata%2FSQLMetadataConnectorTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Fmetadata%2FSQLMetadataConnectorTest.java?ref=80c72eb4864916b54dd8be7ed239934755874796",
      "patch": "@@ -22,7 +22,7 @@\n import com.google.common.base.Suppliers;\n import com.google.common.base.Throwables;\n import io.druid.jackson.DefaultObjectMapper;\n-import junit.framework.Assert;\n+import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n import org.skife.jdbi.v2.Batch;\n@@ -52,8 +52,10 @@ public void setUp() {\n         MetadataStorageConnectorConfig.class\n     );\n \n-    connector = new DerbyConnector(Suppliers.ofInstance(config),\n-                                  Suppliers.ofInstance(tablesConfig));\n+    connector = new DerbyConnector(\n+        Suppliers.ofInstance(config),\n+        Suppliers.ofInstance(tablesConfig)\n+    );\n     dbi = connector.getDBI();\n   }\n \n@@ -80,7 +82,10 @@ public void testCreateTables()\n             public Void withHandle(Handle handle) throws Exception\n             {\n               for(String table : tables) {\n-                Assert.assertTrue(String.format(\"table $s was not created!\", table), tableExists(handle, table));\n+                Assert.assertTrue(\n+                    String.format(\"table $s was not created!\", table),\n+                    connector.tableExists(handle, table)\n+                );\n               }\n \n               return null;\n@@ -166,12 +171,4 @@ private <T> T jsonReadWriteRead(String s, Class<T> klass)\n       throw Throwables.propagate(e);\n     }\n   }\n-\n-  private boolean tableExists(Handle handle, String tableName)\n-  {\n-    return !handle.createQuery(\"select * from SYS.SYSTABLES where tablename = :tableName\")\n-                  .bind(\"tableName\", tableName.toUpperCase())\n-                  .list()\n-                  .isEmpty();\n-  }\n }",
      "parent_sha": "1872b8f979855b6e13b51c41bd21a122c8fd709f"
    }
  },
  {
    "oid": "db656c5a88d3c548ce60227905e881413d8eea5f",
    "message": "fix kafka8 unparsable message halt job issue (#4164)",
    "date": "2017-04-18T18:23:02Z",
    "url": "https://github.com/apache/druid/commit/db656c5a88d3c548ce60227905e881413d8eea5f",
    "details": {
      "sha": "fd2755aff2f8b57f3c82750313e4b289b81e36a2",
      "filename": "extensions-contrib/kafka-eight-simpleConsumer/src/main/java/io/druid/firehose/kafka/KafkaEightSimpleConsumerFirehoseFactory.java",
      "status": "modified",
      "additions": 10,
      "deletions": 1,
      "changes": 11,
      "blob_url": "https://github.com/apache/druid/blob/db656c5a88d3c548ce60227905e881413d8eea5f/extensions-contrib%2Fkafka-eight-simpleConsumer%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Ffirehose%2Fkafka%2FKafkaEightSimpleConsumerFirehoseFactory.java",
      "raw_url": "https://github.com/apache/druid/raw/db656c5a88d3c548ce60227905e881413d8eea5f/extensions-contrib%2Fkafka-eight-simpleConsumer%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Ffirehose%2Fkafka%2FKafkaEightSimpleConsumerFirehoseFactory.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/extensions-contrib%2Fkafka-eight-simpleConsumer%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Ffirehose%2Fkafka%2FKafkaEightSimpleConsumerFirehoseFactory.java?ref=db656c5a88d3c548ce60227905e881413d8eea5f",
      "patch": "@@ -24,6 +24,7 @@\n import com.google.common.base.Preconditions;\n import com.google.common.collect.Maps;\n import com.google.common.io.Closeables;\n+import com.metamx.common.parsers.ParseException;\n import com.metamx.emitter.EmittingLogger;\n import io.druid.data.input.ByteBufferInputRowParser;\n import io.druid.data.input.Committer;\n@@ -182,7 +183,6 @@ public FirehoseV2 connect(final ByteBufferInputRowParser firehoseParser, Object\n       @Override\n       public void start() throws Exception\n       {\n-        nextMessage();\n       }\n \n       @Override\n@@ -224,6 +224,15 @@ public InputRow currRow()\n         if (stopped) {\n           return null;\n         }\n+        // currRow will be called before the first advance\n+        if (row == null) {\n+          try {\n+            nextMessage();\n+          }\n+          catch (ParseException e) {\n+            return null;\n+          }\n+        }\n         return row;\n       }\n ",
      "parent_sha": "0bcfd9354cb4b2f0e7699b4232b719a7851e582c"
    }
  },
  {
    "oid": "0bc18e790659830e8a9f3d5306f26d9ace8c1eaf",
    "message": "Make UpdateCounter proof to update count overflow (#4138)\n\n* Make UpdateCounter proof to update count overflow.\r\n\r\n* Fix",
    "date": "2017-05-01T16:59:49Z",
    "url": "https://github.com/apache/druid/commit/0bc18e790659830e8a9f3d5306f26d9ace8c1eaf",
    "details": {
      "sha": "ed229b45f891de995a0a8c532515fd3dd02a3f11",
      "filename": "extensions-core/lookups-cached-global/src/main/java/io/druid/server/lookup/namespace/cache/UpdateCounter.java",
      "status": "modified",
      "additions": 31,
      "deletions": 1,
      "changes": 32,
      "blob_url": "https://github.com/apache/druid/blob/0bc18e790659830e8a9f3d5306f26d9ace8c1eaf/extensions-core%2Flookups-cached-global%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Flookup%2Fnamespace%2Fcache%2FUpdateCounter.java",
      "raw_url": "https://github.com/apache/druid/raw/0bc18e790659830e8a9f3d5306f26d9ace8c1eaf/extensions-core%2Flookups-cached-global%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Flookup%2Fnamespace%2Fcache%2FUpdateCounter.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/extensions-core%2Flookups-cached-global%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fserver%2Flookup%2Fnamespace%2Fcache%2FUpdateCounter.java?ref=0bc18e790659830e8a9f3d5306f26d9ace8c1eaf",
      "patch": "@@ -25,6 +25,11 @@\n \n final class UpdateCounter\n {\n+  /**\n+   * Max {@link Phaser}'s phase, specified in it's javadoc. Then it wraps to zero.\n+   */\n+  private static final int MAX_PHASE = Integer.MAX_VALUE;\n+\n   private final Phaser phaser = new Phaser(1);\n \n   void update()\n@@ -34,14 +39,39 @@ void update()\n \n   void awaitTotalUpdates(int totalUpdates) throws InterruptedException\n   {\n+    totalUpdates &= MAX_PHASE;\n     int currentUpdates = phaser.getPhase();\n-    while (totalUpdates - currentUpdates > 0) { // overflow-aware\n+    checkNotTerminated(currentUpdates);\n+    while (comparePhases(totalUpdates, currentUpdates) > 0) {\n       currentUpdates = phaser.awaitAdvanceInterruptibly(currentUpdates);\n+      checkNotTerminated(currentUpdates);\n+    }\n+  }\n+\n+  private static int comparePhases(int phase1, int phase2)\n+  {\n+    int diff = (phase1 - phase2) & MAX_PHASE;\n+    if (diff == 0) {\n+      return 0;\n+    }\n+    return diff < MAX_PHASE / 2 ? 1 : -1;\n+  }\n+\n+  private void checkNotTerminated(int phase)\n+  {\n+    if (phase < 0) {\n+      throw new IllegalStateException(\"Phaser[\" + phaser + \"] unexpectedly terminated.\");\n     }\n   }\n \n   void awaitNextUpdates(int nextUpdates) throws InterruptedException\n   {\n+    if (nextUpdates <= 0) {\n+      throw new IllegalArgumentException(\"nextUpdates is not positive: \" + nextUpdates);\n+    }\n+    if (nextUpdates > MAX_PHASE / 4) {\n+      throw new UnsupportedOperationException(\"Couldn't wait for so many updates: \" + nextUpdates);\n+    }\n     awaitTotalUpdates(phaser.getPhase() + nextUpdates);\n   }\n ",
      "parent_sha": "0c464f4a842e26b9b6e6a32e12eee4c85854e306"
    }
  },
  {
    "oid": "d102a897608c1e9e81db4e02f1fcebdf4974baa9",
    "message": "Fix license on EC2AutoScalerSerdeTest.",
    "date": "2015-03-11T00:31:30Z",
    "url": "https://github.com/apache/druid/commit/d102a897608c1e9e81db4e02f1fcebdf4974baa9",
    "details": {
      "sha": "3b967326cfaf6ca12586ff81dd18a56b2a9aeec0",
      "filename": "indexing-service/src/test/java/io/druid/indexing/overlord/autoscaling/EC2AutoScalerSerdeTest.java",
      "status": "modified",
      "additions": 14,
      "deletions": 14,
      "changes": 28,
      "blob_url": "https://github.com/apache/druid/blob/d102a897608c1e9e81db4e02f1fcebdf4974baa9/indexing-service%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Findexing%2Foverlord%2Fautoscaling%2FEC2AutoScalerSerdeTest.java",
      "raw_url": "https://github.com/apache/druid/raw/d102a897608c1e9e81db4e02f1fcebdf4974baa9/indexing-service%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Findexing%2Foverlord%2Fautoscaling%2FEC2AutoScalerSerdeTest.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/indexing-service%2Fsrc%2Ftest%2Fjava%2Fio%2Fdruid%2Findexing%2Foverlord%2Fautoscaling%2FEC2AutoScalerSerdeTest.java?ref=d102a897608c1e9e81db4e02f1fcebdf4974baa9",
      "patch": "@@ -1,20 +1,20 @@\n /*\n- * Druid - a distributed column store.\n- * Copyright (C) 2014  Metamarkets Group Inc.\n+ * Licensed to Metamarkets Group Inc. (Metamarkets) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  Metamarkets licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n- * This program is free software; you can redistribute it and/or\n- * modify it under the terms of the GNU General Public License\n- * as published by the Free Software Foundation; either version 2\n- * of the License, or (at your option) any later version.\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n  *\n- * This program is distributed in the hope that it will be useful,\n- * but WITHOUT ANY WARRANTY; without even the implied warranty of\n- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n- * GNU General Public License for more details.\n- *\n- * You should have received a copy of the GNU General Public License\n- * along with this program; if not, write to the Free Software\n- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n  */\n \n package io.druid.indexing.overlord.autoscaling;",
      "parent_sha": "9f242ed1ba09ce1f91bf150327fad046c5ce7064"
    }
  },
  {
    "oid": "e1c649e90683a5b49648522d4c7e0ff1088f1efd",
    "message": "Add metadata indexes to help with segment allocation. (#6348)\n\nSegment allocation queries can take a long time (10s of seconds) when\r\nyou have a lot of segments. Adding these indexes helps greatly.",
    "date": "2018-09-19T22:54:13Z",
    "url": "https://github.com/apache/druid/commit/e1c649e90683a5b49648522d4c7e0ff1088f1efd",
    "details": {
      "sha": "688409e3cf9b7abeca281068d0992d235ff76028",
      "filename": "server/src/main/java/org/apache/druid/metadata/SQLMetadataConnector.java",
      "status": "modified",
      "additions": 15,
      "deletions": 3,
      "changes": 18,
      "blob_url": "https://github.com/apache/druid/blob/e1c649e90683a5b49648522d4c7e0ff1088f1efd/server%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fmetadata%2FSQLMetadataConnector.java",
      "raw_url": "https://github.com/apache/druid/raw/e1c649e90683a5b49648522d4c7e0ff1088f1efd/server%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fmetadata%2FSQLMetadataConnector.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fdruid%2Fmetadata%2FSQLMetadataConnector.java?ref=e1c649e90683a5b49648522d4c7e0ff1088f1efd",
      "patch": "@@ -23,11 +23,11 @@\n import com.google.common.base.Supplier;\n import com.google.common.base.Throwables;\n import com.google.common.collect.ImmutableList;\n+import org.apache.commons.dbcp2.BasicDataSource;\n import org.apache.druid.java.util.common.ISE;\n import org.apache.druid.java.util.common.RetryUtils;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.commons.dbcp2.BasicDataSource;\n import org.skife.jdbi.v2.Batch;\n import org.skife.jdbi.v2.DBI;\n import org.skife.jdbi.v2.Handle;\n@@ -215,6 +215,11 @@ public void createPendingSegmentsTable(final String tableName)\n                 + \"  UNIQUE (sequence_name_prev_id_sha1)\\n\"\n                 + \")\",\n                 tableName, getPayloadType(), getQuoteString()\n+            ),\n+            StringUtils.format(\n+                \"CREATE INDEX idx_%1$s_datasource_used_end ON %1$s(dataSource, used, %2$send%2$s)\",\n+                tableName,\n+                getQuoteString()\n             )\n         )\n     );\n@@ -259,8 +264,15 @@ public void createSegmentTable(final String tableName)\n                 + \")\",\n                 tableName, getPayloadType(), getQuoteString()\n             ),\n-            StringUtils.format(\"CREATE INDEX idx_%1$s_datasource ON %1$s(dataSource)\", tableName),\n-            StringUtils.format(\"CREATE INDEX idx_%1$s_used ON %1$s(used)\", tableName)\n+            StringUtils.format(\n+                \"CREATE INDEX idx_%1$s_datasource_end ON %1$s(dataSource, %2$send%2$s)\",\n+                tableName,\n+                getQuoteString()\n+            ),\n+            StringUtils.format(\n+                \"CREATE INDEX idx_%1$s_datasource_sequence ON %1$s(dataSource, sequence_name)\",\n+                tableName\n+            )\n         )\n     );\n   }",
      "parent_sha": "8972244c6817cf8e0276a7904776103e172e6be3"
    }
  },
  {
    "oid": "c47cfed0ec0568a4c758ad801d5e11f32b9e05db",
    "message": "Significantly improve LongEncodingStrategy.AUTO build performance (#4215)\n\n* Significantly improve LongEncodingStrategy.AUTO build performance\r\n\r\n* use numInserted instead of tempIn.available\r\n\r\n* fix bug",
    "date": "2017-04-27T12:11:07Z",
    "url": "https://github.com/apache/druid/commit/c47cfed0ec0568a4c758ad801d5e11f32b9e05db",
    "details": {
      "sha": "6b459f489164aa15eb55926dd80187c3efdc2b02",
      "filename": "processing/src/main/java/io/druid/segment/data/IntermediateLongSupplierSerializer.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/c47cfed0ec0568a4c758ad801d5e11f32b9e05db/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Fdata%2FIntermediateLongSupplierSerializer.java",
      "raw_url": "https://github.com/apache/druid/raw/c47cfed0ec0568a4c758ad801d5e11f32b9e05db/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Fdata%2FIntermediateLongSupplierSerializer.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Fdata%2FIntermediateLongSupplierSerializer.java?ref=c47cfed0ec0568a4c758ad801d5e11f32b9e05db",
      "patch": "@@ -131,8 +131,10 @@ private void makeDelegate() throws IOException\n \n     try (DataInputStream tempIn = new DataInputStream(new BufferedInputStream(ioPeon.makeInputStream(tempFile)))) {\n       delegate.open();\n-      while (tempIn.available() > 0) {\n+      int available = numInserted;\n+      while (available > 0) {\n         delegate.add(tempIn.readLong());\n+        available--;\n       }\n     }\n   }",
      "parent_sha": "13143f9376aefb8f55595a1de938418b1634ddbf"
    }
  },
  {
    "oid": "e1c1e8997a5c3059fc96b0b6e478d2b94c90efaa",
    "message": "address cr",
    "date": "2014-09-30T00:22:58Z",
    "url": "https://github.com/apache/druid/commit/e1c1e8997a5c3059fc96b0b6e478d2b94c90efaa",
    "details": {
      "sha": "b9b4ce9cc74539023fcdd859e2693902b8bf84f7",
      "filename": "processing/src/main/java/io/druid/segment/incremental/SpatialDimensionRowFormatter.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/druid/blob/e1c1e8997a5c3059fc96b0b6e478d2b94c90efaa/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Fincremental%2FSpatialDimensionRowFormatter.java",
      "raw_url": "https://github.com/apache/druid/raw/e1c1e8997a5c3059fc96b0b6e478d2b94c90efaa/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Fincremental%2FSpatialDimensionRowFormatter.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/processing%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fsegment%2Fincremental%2FSpatialDimensionRowFormatter.java?ref=e1c1e8997a5c3059fc96b0b6e478d2b94c90efaa",
      "patch": "@@ -56,7 +56,9 @@ public SpatialDimensionRowFormatter(List<SpatialDimensionSchema> spatialDimensio\n   {\n     this.spatialDimensionMap = Maps.newHashMap();\n     for (SpatialDimensionSchema spatialDimension : spatialDimensions) {\n-      this.spatialDimensionMap.put(spatialDimension.getDimName(), spatialDimension);\n+      if (this.spatialDimensionMap.put(spatialDimension.getDimName(), spatialDimension) != null) {\n+        throw new ISE(\"Duplicate spatial dimension names found! Check your schema yo!\");\n+      }\n     }\n     this.spatialPartialDimNames = Sets.newHashSet(\n         Iterables.concat(",
      "parent_sha": "2b5ce895543f9782399162b7ce137107adf95bc2"
    }
  },
  {
    "oid": "0b61f9696514931a035217b9314aacda4bf50cc4",
    "message": "extra documentation",
    "date": "2013-02-01T19:25:05Z",
    "url": "https://github.com/apache/druid/commit/0b61f9696514931a035217b9314aacda4bf50cc4",
    "details": {
      "sha": "6b68f426d8a2ec3a4827adb63b8923314c2e9c28",
      "filename": "server/src/main/java/com/metamx/druid/master/BalancerCostAnalyzer.java",
      "status": "modified",
      "additions": 43,
      "deletions": 30,
      "changes": 73,
      "blob_url": "https://github.com/apache/druid/blob/0b61f9696514931a035217b9314aacda4bf50cc4/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmaster%2FBalancerCostAnalyzer.java",
      "raw_url": "https://github.com/apache/druid/raw/0b61f9696514931a035217b9314aacda4bf50cc4/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmaster%2FBalancerCostAnalyzer.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/server%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Fmaster%2FBalancerCostAnalyzer.java?ref=0b61f9696514931a035217b9314aacda4bf50cc4",
      "patch": "@@ -52,13 +52,16 @@ public BalancerCostAnalyzer(DateTime referenceTimestamp)\n   /**\n    * Calculates the cost normalization.  This is such that the normalized cost is lower bounded\n    * by 1 (e.g. when each segment gets its own compute node).\n-   * @param serverHolderList\n-   * @return\n+   * @param     serverHolders\n+   *            A list of ServerHolders for a particular tier.\n+   * @return    The normalization value (the sum of the diagonal entries in the\n+   *            pairwise cost matrix).  This is the cost of a cluster if each\n+   *            segment were to get its own compute node.\n    */\n-  public double calculateNormalization(List<ServerHolder> serverHolderList)\n+  public double calculateNormalization(List<ServerHolder> serverHolders)\n   {\n     double cost = 0;\n-    for (ServerHolder server : serverHolderList) {\n+    for (ServerHolder server : serverHolders) {\n       for (DataSegment segment : server.getServer().getSegments().values()) {\n         cost += computeJointSegmentCosts(segment, segment);\n       }\n@@ -68,13 +71,14 @@ public double calculateNormalization(List<ServerHolder> serverHolderList)\n \n   /**\n    * Calculates the initial cost of the Druid segment configuration.\n-   * @param serverHolderList\n-   * @return\n+   * @param     serverHolders\n+   *            A list of ServerHolders for a particular tier.\n+   * @return    The initial cost of the Druid tier.\n    */\n-  public double calculateInitialTotalCost(List<ServerHolder> serverHolderList)\n+  public double calculateInitialTotalCost(List<ServerHolder> serverHolders)\n   {\n     double cost = 0;\n-    for (ServerHolder server : serverHolderList) {\n+    for (ServerHolder server : serverHolders) {\n       DataSegment[] segments = server.getServer().getSegments().values().toArray(new DataSegment[]{});\n       for (int i = 0; i < segments.length; ++i) {\n         for (int j = i; j < segments.length; ++j) {\n@@ -92,9 +96,11 @@ public double calculateInitialTotalCost(List<ServerHolder> serverHolderList)\n    * dataSourcePenalty: if two segments belong to the same data source, they are more likely to be involved\n    * in the same queries\n    * gapPenalty: it is more likely that segments close together in time will be queried together\n-   * @param segment1\n-   * @param segment2\n-   * @return\n+   * @param     segment1\n+   *            The first DataSegment.\n+   * @param     segment2\n+   *            The second DataSegment.\n+   * @return    The joint cost of placing the two DataSegments together on one node.\n    */\n   public double computeJointSegmentCosts(DataSegment segment1, DataSegment segment2)\n   {\n@@ -134,37 +140,42 @@ public double computeJointSegmentCosts(DataSegment segment1, DataSegment segment\n \n   /**\n    * Sample from each server with probability proportional to the number of segments on that server.\n-   * @param serverHolderList\n-   * @param numSegments\n-   * @return\n+   * @param     serverHolders\n+   *            A list of ServerHolders for a particular tier.\n+   * @param     numSegments\n+\n+   * @return    A ServerHolder sampled with probability proportional to the\n+   *            number of segments on that server\n    */\n-  private ServerHolder sampleServer(List<ServerHolder> serverHolderList, int numSegments)\n+  private ServerHolder sampleServer(List<ServerHolder> serverHolders, int numSegments)\n   {\n     final int num = rand.nextInt(numSegments);\n     int cumulativeSegments = 0;\n     int numToStopAt = 0;\n \n     while (cumulativeSegments <= num) {\n-      cumulativeSegments += serverHolderList.get(numToStopAt).getServer().getSegments().size();\n+      cumulativeSegments += serverHolders.get(numToStopAt).getServer().getSegments().size();\n       numToStopAt++;\n     }\n \n-    return serverHolderList.get(numToStopAt - 1);\n+    return serverHolders.get(numToStopAt - 1);\n   }\n \n   /**\n    * The balancing application requires us to pick a proposal segment.\n-   * @param serverHolders\n-   * @param numSegments\n-   * @return\n+   * @param     serverHolders\n+   *            A list of ServerHolders for a particular tier.\n+   * @param     numSegments\n+   *            The total number of segments on a particular tier.\n+   * @return    A BalancerSegmentHolder sampled uniformly at random.\n    */\n   public BalancerSegmentHolder pickSegmentToMove(List<ServerHolder> serverHolders, int numSegments)\n   {\n-    // We want to sample from each server w.p. numSegmentsOnServer / totalSegments\n+    /** We want to sample from each server w.p. numSegmentsOnServer / totalSegments */\n     ServerHolder fromServerHolder = sampleServer(serverHolders, numSegments);\n \n-    // and actually pick that segment uniformly at random w.p. 1 / numSegmentsOnServer\n-    // so that the probability of picking a segment is 1 / totalSegments.\n+    /** and actually pick that segment uniformly at random w.p. 1 / numSegmentsOnServer\n+    so that the probability of picking a segment is 1 / totalSegments. */\n     List<DataSegment> segments = Lists.newArrayList(fromServerHolder.getServer().getSegments().values());\n \n     DataSegment proposalSegment = segments.get(rand.nextInt(segments.size()));\n@@ -173,9 +184,11 @@ public BalancerSegmentHolder pickSegmentToMove(List<ServerHolder> serverHolders,\n \n   /**\n    * The assignment application requires us to supply a proposal segment.\n-   * @param proposalSegment\n-   * @param serverHolders\n-   * @return\n+   * @param     proposalSegment\n+   *            A DataSegment that we are proposing to move.\n+   * @param     serverHolders\n+   *            An iterable of ServerHolders for a particular tier.\n+   * @return    A ServerHolder with the new home for a segment.\n    */\n   public ServerHolder findNewSegmentHome(DataSegment proposalSegment, Iterable<ServerHolder> serverHolders)\n   {\n@@ -184,20 +197,20 @@ public ServerHolder findNewSegmentHome(DataSegment proposalSegment, Iterable<Ser\n     ServerHolder toServer = null;\n \n     for (ServerHolder server : serverHolders) {\n-      // Only calculate costs if the server has enough space.\n+      /** Only calculate costs if the server has enough space. */\n       if (proposalSegmentSize > server.getAvailableSize()) {\n         break;\n       }\n \n-      // The contribution to the total cost of a given server by proposing to move the segment to that server is...\n+      /** The contribution to the total cost of a given server by proposing to move the segment to that server is... */\n       double cost = 0f;\n-      // the sum of the costs of other (exclusive of the proposalSegment) segments on the server\n+      /**  the sum of the costs of other (exclusive of the proposalSegment) segments on the server */\n       for (DataSegment segment : server.getServer().getSegments().values()) {\n         if (!proposalSegment.equals(segment)) {\n           cost += computeJointSegmentCosts(proposalSegment, segment);\n         }\n       }\n-      // plus the costs of segments that will be loaded\n+      /**  plus the costs of segments that will be loaded */\n       for (DataSegment segment : server.getPeon().getSegmentsToLoad()) {\n         cost += computeJointSegmentCosts(proposalSegment, segment);\n       }",
      "parent_sha": "dc49cccf56a89941b2e47a1cd564accb82efea29"
    }
  },
  {
    "oid": "c05629e6a4c49c81a552c9d3cddd32767b486c8c",
    "message": "more logging",
    "date": "2013-03-20T00:08:48Z",
    "url": "https://github.com/apache/druid/commit/c05629e6a4c49c81a552c9d3cddd32767b486c8c",
    "details": {
      "sha": "5b22bbbd41a81766bab601ef9142a665a4124978",
      "filename": "realtime/src/main/java/com/metamx/druid/realtime/GracefulShutdownFirehose.java",
      "status": "modified",
      "additions": 8,
      "deletions": 1,
      "changes": 9,
      "blob_url": "https://github.com/apache/druid/blob/c05629e6a4c49c81a552c9d3cddd32767b486c8c/realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Frealtime%2FGracefulShutdownFirehose.java",
      "raw_url": "https://github.com/apache/druid/raw/c05629e6a4c49c81a552c9d3cddd32767b486c8c/realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Frealtime%2FGracefulShutdownFirehose.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/realtime%2Fsrc%2Fmain%2Fjava%2Fcom%2Fmetamx%2Fdruid%2Frealtime%2FGracefulShutdownFirehose.java?ref=c05629e6a4c49c81a552c9d3cddd32767b486c8c",
      "patch": "@@ -3,6 +3,7 @@\n import com.google.common.base.Throwables;\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import com.metamx.common.concurrent.ScheduledExecutors;\n+import com.metamx.common.logger.Logger;\n import com.metamx.druid.index.v1.IndexGranularity;\n import com.metamx.druid.input.InputRow;\n import com.metamx.druid.realtime.plumber.IntervalRejectionPolicyFactory;\n@@ -21,6 +22,8 @@\n  */\n public class GracefulShutdownFirehose implements Firehose\n {\n+  private static final Logger log = new Logger(GracefulShutdownFirehose.class);\n+\n   private final Firehose firehose;\n   private final IndexGranularity segmentGranularity;\n   private final long windowMillis;\n@@ -55,10 +58,14 @@ public GracefulShutdownFirehose(\n   public void shutdown() throws IOException\n   {\n     final long truncatedNow = segmentGranularity.truncate(new DateTime()).getMillis();\n+    final long end = segmentGranularity.increment(truncatedNow) + windowMillis;\n+    final Duration timeUntilShutdown = new Duration(System.currentTimeMillis(), end);\n+\n+    log.info(\"Shutting down in %s\", timeUntilShutdown);\n \n     ScheduledExecutors.scheduleWithFixedDelay(\n         scheduledExecutor,\n-        new Duration(System.currentTimeMillis(), segmentGranularity.increment(truncatedNow) + windowMillis),\n+        timeUntilShutdown,\n         new Callable<ScheduledExecutors.Signal>()\n         {\n           @Override",
      "parent_sha": "6324225a4f01e6f711a3efcb20128416d2e027af"
    }
  },
  {
    "oid": "d8338fc51dc5c4ba7d91006a8e46f41a8fd2179f",
    "message": "fix backwards compatibility",
    "date": "2014-06-04T11:10:06Z",
    "url": "https://github.com/apache/druid/commit/d8338fc51dc5c4ba7d91006a8e46f41a8fd2179f",
    "details": {
      "sha": "7e88cabb2a58ea55eb0bb8546d420fd08d9f5a16",
      "filename": "services/src/main/java/io/druid/cli/CliInternalHadoopIndexer.java",
      "status": "modified",
      "additions": 5,
      "deletions": 3,
      "changes": 8,
      "blob_url": "https://github.com/apache/druid/blob/d8338fc51dc5c4ba7d91006a8e46f41a8fd2179f/services%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fcli%2FCliInternalHadoopIndexer.java",
      "raw_url": "https://github.com/apache/druid/raw/d8338fc51dc5c4ba7d91006a8e46f41a8fd2179f/services%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fcli%2FCliInternalHadoopIndexer.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/services%2Fsrc%2Fmain%2Fjava%2Fio%2Fdruid%2Fcli%2FCliInternalHadoopIndexer.java?ref=d8338fc51dc5c4ba7d91006a8e46f41a8fd2179f",
      "patch": "@@ -27,11 +27,11 @@\n import io.druid.indexer.HadoopDruidDetermineConfigurationJob;\n import io.druid.indexer.HadoopDruidIndexerConfig;\n import io.druid.indexer.HadoopDruidIndexerJob;\n+import io.druid.indexer.HadoopIngestionSpec;\n import io.druid.indexer.JobHelper;\n import io.druid.indexer.Jobby;\n \n import java.io.File;\n-import java.util.ArrayList;\n import java.util.List;\n \n /**\n@@ -65,11 +65,13 @@ public void run()\n   public HadoopDruidIndexerConfig getHadoopDruidIndexerConfig()\n   {\n     try {\n+      HadoopIngestionSpec spec;\n       if (argumentSpec.startsWith(\"{\")) {\n-        return HadoopDruidIndexerConfig.fromString(argumentSpec);\n+        spec = HadoopDruidIndexerConfig.jsonMapper.readValue(argumentSpec, HadoopIngestionSpec.class);\n       } else {\n-        return HadoopDruidIndexerConfig.fromFile(new File(argumentSpec));\n+        spec = HadoopDruidIndexerConfig.jsonMapper.readValue(new File(argumentSpec), HadoopIngestionSpec.class);\n       }\n+      return HadoopDruidIndexerConfig.fromSchema(spec);\n     }\n     catch (Exception e) {\n       throw Throwables.propagate(e);",
      "parent_sha": "f7c4d6a24af24fcbc04e9e8e58924ef061d920f3"
    }
  },
  {
    "oid": "c2263a339e266dcb696b2de1d713e3b4055635aa",
    "message": "Remove direct references to unsafe for Java 9+",
    "date": "2019-04-30T21:21:45Z",
    "url": "https://github.com/apache/druid/commit/c2263a339e266dcb696b2de1d713e3b4055635aa",
    "details": {
      "sha": "66ea18e87a21a177bd6037462eba447828f517f9",
      "filename": "hll/src/test/java/org/apache/druid/hll/HyperLogLogCollectorBenchmark.java",
      "status": "modified",
      "additions": 32,
      "deletions": 9,
      "changes": 41,
      "blob_url": "https://github.com/apache/druid/blob/c2263a339e266dcb696b2de1d713e3b4055635aa/hll%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fhll%2FHyperLogLogCollectorBenchmark.java",
      "raw_url": "https://github.com/apache/druid/raw/c2263a339e266dcb696b2de1d713e3b4055635aa/hll%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fhll%2FHyperLogLogCollectorBenchmark.java",
      "contents_url": "https://api.github.com/repos/apache/druid/contents/hll%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fdruid%2Fhll%2FHyperLogLogCollectorBenchmark.java?ref=c2263a339e266dcb696b2de1d713e3b4055635aa",
      "patch": "@@ -25,8 +25,11 @@\n import com.google.common.base.Preconditions;\n import com.google.common.hash.HashFunction;\n import com.google.common.hash.Hashing;\n-import sun.misc.Unsafe;\n+import org.apache.druid.java.util.common.UnsafeUtils;\n \n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.invoke.MethodType;\n import java.lang.reflect.Field;\n import java.nio.Buffer;\n import java.nio.ByteBuffer;\n@@ -177,24 +180,44 @@ public static void main(String[] args)\n \n class ByteBuffers\n {\n-  private static final Unsafe UNSAFE;\n   private static final long ADDRESS_OFFSET;\n+  private static final MethodHandle GET_LONG;\n \n   static {\n     try {\n-      Field theUnsafe = Unsafe.class.getDeclaredField(\"theUnsafe\");\n-      theUnsafe.setAccessible(true);\n-      UNSAFE = (Unsafe) theUnsafe.get(null);\n-      ADDRESS_OFFSET = UNSAFE.objectFieldOffset(Buffer.class.getDeclaredField(\"address\"));\n+      MethodHandles.Lookup lookup = MethodHandles.lookup();\n+      ADDRESS_OFFSET = lookupAddressOffset(lookup);\n+      GET_LONG = lookupGetLong(lookup);\n     }\n-    catch (Exception e) {\n-      throw new RuntimeException(\"Cannot access Unsafe methods\", e);\n+    catch (Throwable t) {\n+      throw new RuntimeException(\"Unable to lookup Unsafe methods\", t);\n     }\n   }\n \n+  private static long lookupAddressOffset(MethodHandles.Lookup lookup) throws Throwable\n+  {\n+    MethodHandle objectFieldOffset = lookup.findVirtual(UnsafeUtils.theUnsafeClass(), \"objectFieldOffset\",\n+                                                        MethodType.methodType(long.class, Field.class)\n+    );\n+    return (long) objectFieldOffset.bindTo(UnsafeUtils.theUnsafe()).invoke(Buffer.class.getDeclaredField(\"address\"));\n+  }\n+\n+  private static MethodHandle lookupGetLong(MethodHandles.Lookup lookup) throws Throwable\n+  {\n+    MethodHandle getLong = lookup.findVirtual(UnsafeUtils.theUnsafeClass(), \"getLong\",\n+                                              MethodType.methodType(long.class, Object.class, long.class)\n+    );\n+    return getLong.bindTo(UnsafeUtils.theUnsafe());\n+  }\n+\n   public static long getAddress(ByteBuffer buf)\n   {\n-    return UNSAFE.getLong(buf, ADDRESS_OFFSET);\n+    try {\n+      return (long) GET_LONG.invoke(buf, ADDRESS_OFFSET);\n+    }\n+    catch (Throwable t) {\n+      throw new UnsupportedOperationException(\"Unsafe.getLong is unsupported\", t);\n+    }\n   }\n \n   public static ByteBuffer allocateAlignedByteBuffer(int capacity, int align)",
      "parent_sha": "f776b9408962b9006cfcfe4d6c1794751972cc8e"
    }
  }
]