[
  {
    "oid": "777cc1ab53884f12ac245e62d367c4439f7939e0",
    "message": "[FLINK-9022][state] Backend disposal in StreamTaskStateInitializer should always be performed in cleanup.\n\nThis step should be independent from the fact if the backend is still registered with the closeable registry.",
    "date": "2018-04-19T09:12:22Z",
    "url": "https://github.com/apache/flink/commit/777cc1ab53884f12ac245e62d367c4439f7939e0",
    "details": {
      "sha": "460a52b4c7bca4c347544c232c593585fe211589",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/StreamTaskStateInitializerImpl.java",
      "status": "modified",
      "additions": 8,
      "deletions": 2,
      "changes": 10,
      "blob_url": "https://github.com/apache/flink/blob/777cc1ab53884f12ac245e62d367c4439f7939e0/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Foperators%2FStreamTaskStateInitializerImpl.java",
      "raw_url": "https://github.com/apache/flink/raw/777cc1ab53884f12ac245e62d367c4439f7939e0/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Foperators%2FStreamTaskStateInitializerImpl.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Foperators%2FStreamTaskStateInitializerImpl.java?ref=777cc1ab53884f12ac245e62d367c4439f7939e0",
      "patch": "@@ -165,12 +165,18 @@ public StreamOperatorStateContext streamOperatorStateContext(\n \t\t} catch (Exception ex) {\n \n \t\t\t// cleanup if something went wrong before results got published.\n-\t\t\tif (streamTaskCloseableRegistry.unregisterCloseable(keyedStatedBackend)) {\n+\t\t\tif (keyedStatedBackend != null) {\n+\t\t\t\tif (streamTaskCloseableRegistry.unregisterCloseable(keyedStatedBackend)) {\n+\t\t\t\t\tIOUtils.closeQuietly(keyedStatedBackend);\n+\t\t\t\t}\n \t\t\t\t// release resource (e.g native resource)\n \t\t\t\tkeyedStatedBackend.dispose();\n \t\t\t}\n \n-\t\t\tif (streamTaskCloseableRegistry.unregisterCloseable(operatorStateBackend)) {\n+\t\t\tif (operatorStateBackend != null) {\n+\t\t\t\tif (streamTaskCloseableRegistry.unregisterCloseable(operatorStateBackend)) {\n+\t\t\t\t\tIOUtils.closeQuietly(operatorStateBackend);\n+\t\t\t\t}\n \t\t\t\toperatorStateBackend.dispose();\n \t\t\t}\n ",
      "parent_sha": "388a083c909d1f1b065e549ba70359358eb6e330"
    }
  },
  {
    "oid": "016d90884cc6c6d3aa52d0b1634cc945ea0f2bf0",
    "message": "[FLINK-5160] Fix SecurityContextTest#testCreateInsecureHadoopContext on Windows\n\nThis closes #2888.",
    "date": "2017-01-05T11:28:18Z",
    "url": "https://github.com/apache/flink/commit/016d90884cc6c6d3aa52d0b1634cc945ea0f2bf0",
    "details": {
      "sha": "e7da404db30881c984e76642750e568d1679765a",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/security/SecurityUtilsTest.java",
      "status": "modified",
      "additions": 5,
      "deletions": 1,
      "changes": 6,
      "blob_url": "https://github.com/apache/flink/blob/016d90884cc6c6d3aa52d0b1634cc945ea0f2bf0/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fsecurity%2FSecurityUtilsTest.java",
      "raw_url": "https://github.com/apache/flink/raw/016d90884cc6c6d3aa52d0b1634cc945ea0f2bf0/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fsecurity%2FSecurityUtilsTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fsecurity%2FSecurityUtilsTest.java?ref=016d90884cc6c6d3aa52d0b1634cc945ea0f2bf0",
      "patch": "@@ -79,20 +79,24 @@ private String getOSUserName() throws Exception {\n \t\tString userName = \"\";\n \t\tString osName = System.getProperty( \"os.name\" ).toLowerCase();\n \t\tString className = null;\n+\t\tString methodName = null;\n \n \t\tif( osName.contains( \"windows\" ) ){\n \t\t\tclassName = \"com.sun.security.auth.module.NTSystem\";\n+\t\t\tmethodName = \"getName\";\n \t\t}\n \t\telse if( osName.contains( \"linux\" ) || osName.contains( \"mac\" )  ){\n \t\t\tclassName = \"com.sun.security.auth.module.UnixSystem\";\n+\t\t\tmethodName = \"getUsername\";\n \t\t}\n \t\telse if( osName.contains( \"solaris\" ) || osName.contains( \"sunos\" ) ){\n \t\t\tclassName = \"com.sun.security.auth.module.SolarisSystem\";\n+\t\t\tmethodName = \"getUsername\";\n \t\t}\n \n \t\tif( className != null ){\n \t\t\tClass<?> c = Class.forName( className );\n-\t\t\tMethod method = c.getDeclaredMethod( \"getUsername\" );\n+\t\t\tMethod method = c.getDeclaredMethod( methodName );\n \t\t\tObject o = c.newInstance();\n \t\t\tuserName = (String) method.invoke( o );\n \t\t}",
      "parent_sha": "411fff58405804a7f7f79536f8c6885a491dbef6"
    }
  },
  {
    "oid": "a80d71f638b93efc0f5f6dd3aa04bf712b4689b1",
    "message": "[hotfix] Avoid duplicate get memory segment size (#26367)",
    "date": "2025-04-07T07:41:08Z",
    "url": "https://github.com/apache/flink/commit/a80d71f638b93efc0f5f6dd3aa04bf712b4689b1",
    "details": {
      "sha": "5b1582c6b8c627dc2437c84344974ee11c513c01",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/NettyShuffleEnvironmentConfiguration.java",
      "status": "modified",
      "additions": 7,
      "deletions": 3,
      "changes": 10,
      "blob_url": "https://github.com/apache/flink/blob/a80d71f638b93efc0f5f6dd3aa04bf712b4689b1/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Ftaskmanager%2FNettyShuffleEnvironmentConfiguration.java",
      "raw_url": "https://github.com/apache/flink/raw/a80d71f638b93efc0f5f6dd3aa04bf712b4689b1/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Ftaskmanager%2FNettyShuffleEnvironmentConfiguration.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Ftaskmanager%2FNettyShuffleEnvironmentConfiguration.java?ref=a80d71f638b93efc0f5f6dd3aa04bf712b4689b1",
      "patch": "@@ -304,7 +304,8 @@ public static NettyShuffleEnvironmentConfiguration fromConfiguration(\n                         configuration,\n                         localTaskManagerCommunication,\n                         taskManagerAddress,\n-                        dataBindPortRange);\n+                        dataBindPortRange,\n+                        pageSize);\n \n         final int numberOfNetworkBuffers =\n                 calculateNumberOfNetworkBuffers(networkMemorySize, pageSize);\n@@ -454,14 +455,17 @@ private static int calculateNumberOfNetworkBuffers(MemorySize networkMemorySize,\n      * @param taskManagerAddress identifying the IP address under which the TaskManager will be\n      *     accessible\n      * @param dataPortRange data port range for communication and data exchange\n+     * @param memorySegmentSize size of memory buffers used by the network stack and the memory\n+     *     manager\n      * @return the netty configuration or {@code null} if communication is in the same task manager\n      */\n     @Nullable\n     private static NettyConfig createNettyConfig(\n             Configuration configuration,\n             boolean localTaskManagerCommunication,\n             InetAddress taskManagerAddress,\n-            PortRange dataPortRange) {\n+            PortRange dataPortRange,\n+            int memorySegmentSize) {\n \n         final NettyConfig nettyConfig;\n         if (!localTaskManagerCommunication) {\n@@ -472,7 +476,7 @@ private static NettyConfig createNettyConfig(\n                     new NettyConfig(\n                             taskManagerInetSocketAddress.getAddress(),\n                             dataPortRange,\n-                            ConfigurationParserUtils.getPageSize(configuration),\n+                            memorySegmentSize,\n                             ConfigurationParserUtils.getSlot(configuration),\n                             configuration);\n         } else {",
      "parent_sha": "0d86f549bc3ed835f13c22262adbddebea6888c0"
    }
  },
  {
    "oid": "28d36cc921c50a32442bf6a11772a348d46d5749",
    "message": "[FLINK-2097] fix finalize() method of ExecutorReaper",
    "date": "2015-09-30T23:05:42Z",
    "url": "https://github.com/apache/flink/commit/28d36cc921c50a32442bf6a11772a348d46d5749",
    "details": {
      "sha": "816324471a1f397742fb92fac58ec013baf4e427",
      "filename": "flink-java/src/main/java/org/apache/flink/api/java/LocalEnvironment.java",
      "status": "modified",
      "additions": 7,
      "deletions": 18,
      "changes": 25,
      "blob_url": "https://github.com/apache/flink/blob/28d36cc921c50a32442bf6a11772a348d46d5749/flink-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fjava%2FLocalEnvironment.java",
      "raw_url": "https://github.com/apache/flink/raw/28d36cc921c50a32442bf6a11772a348d46d5749/flink-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fjava%2FLocalEnvironment.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fjava%2FLocalEnvironment.java?ref=28d36cc921c50a32442bf6a11772a348d46d5749",
      "patch": "@@ -152,7 +152,6 @@ private static class ShutdownThread extends Thread {\n \n \t\tprivate final PlanExecutor executor;\n \n-\t\tprivate volatile boolean running = true;\n \t\tprivate volatile boolean triggered = false;\n \n \t\tShutdownThread(PlanExecutor executor) {\n@@ -166,7 +165,7 @@ private static class ShutdownThread extends Thread {\n \t\t@Override\n \t\tpublic void run() {\n \t\t\tsynchronized (monitor) {\n-\t\t\t\twhile (running && !triggered) {\n+\t\t\t\twhile (!triggered) {\n \t\t\t\t\ttry {\n \t\t\t\t\t\tmonitor.wait();\n \t\t\t\t\t}\n@@ -176,14 +175,12 @@ public void run() {\n \t\t\t\t}\n \t\t\t}\n \n-\t\t\tif (running && triggered) {\n-\t\t\t\ttry {\n-\t\t\t\t\texecutor.stop();\n-\t\t\t\t}\n-\t\t\t\tcatch (Throwable t) {\n-\t\t\t\t\tSystem.err.println(\"Cluster reaper caught exception during shutdown\");\n-\t\t\t\t\tt.printStackTrace();\n-\t\t\t\t}\n+\t\t\ttry {\n+\t\t\t\texecutor.stop();\n+\t\t\t}\n+\t\t\tcatch (Throwable t) {\n+\t\t\t\tSystem.err.println(\"Cluster reaper caught exception during shutdown\");\n+\t\t\t\tt.printStackTrace();\n \t\t\t}\n \t\t}\n \n@@ -194,12 +191,6 @@ void trigger() {\n \t\t\t}\n \t\t}\n \n-\t\tvoid cancel() {\n-\t\t\trunning = false;\n-\t\t\tsynchronized (monitor) {\n-\t\t\t\tmonitor.notifyAll();\n-\t\t\t}\n-\t\t}\n \t}\n \n \t/**\n@@ -218,9 +209,7 @@ private static class ExecutorReaper {\n \t\t@Override\n \t\tprotected void finalize() throws Throwable {\n \t\t\tsuper.finalize();\n-\n \t\t\tshutdownThread.trigger();\n-\t\t\tshutdownThread.cancel();\n \t\t}\n \t}\n }",
      "parent_sha": "011cbbf7e4643234b1183b7ccc9c60d9f23a43ce"
    }
  },
  {
    "oid": "a26a5f0bfe5436288bdd633a0773b76d8647c5b7",
    "message": "[FLINK-21954][tests] Harden JobMasterTest.testRestoringFromSavepoint and .testRequestNextInputSplitWithGlobalFailover\n\nThe test instability has been introduced by FLINK-21602 because the ExecutionGraph\nis now created asynchronously. This commit fixes the test instabilities in JobMasterTest\nby waiting for a task submission before asserting on the state.",
    "date": "2021-04-08T08:22:22Z",
    "url": "https://github.com/apache/flink/commit/a26a5f0bfe5436288bdd633a0773b76d8647c5b7",
    "details": {
      "sha": "bd1c9117d55649f5ef617a285d53710502384f5d",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java",
      "status": "modified",
      "additions": 22,
      "deletions": 8,
      "changes": 30,
      "blob_url": "https://github.com/apache/flink/blob/a26a5f0bfe5436288bdd633a0773b76d8647c5b7/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fjobmaster%2FJobMasterTest.java",
      "raw_url": "https://github.com/apache/flink/raw/a26a5f0bfe5436288bdd633a0773b76d8647c5b7/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fjobmaster%2FJobMasterTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fjobmaster%2FJobMasterTest.java?ref=a26a5f0bfe5436288bdd633a0773b76d8647c5b7",
      "patch": "@@ -673,13 +673,24 @@ public void testRestoringFromSavepoint() throws Exception {\n             // restore from the savepoint\n             jobMaster.start();\n \n+            final OneShotLatch taskSubmitLatch = new OneShotLatch();\n+\n             registerSlotsAtJobMaster(\n                     1,\n                     jobMaster.getSelfGateway(JobMasterGateway.class),\n                     jobGraph.getJobID(),\n-                    new TestingTaskExecutorGatewayBuilder().createTestingTaskExecutorGateway(),\n+                    new TestingTaskExecutorGatewayBuilder()\n+                            .setSubmitTaskConsumer(\n+                                    (taskDeploymentDescriptor, jobMasterId) -> {\n+                                        taskSubmitLatch.trigger();\n+                                        return CompletableFuture.completedFuture(Acknowledge.get());\n+                                    })\n+                            .createTestingTaskExecutorGateway(),\n                     new LocalUnresolvedTaskManagerLocation());\n \n+            // wait until a task has submitted because this guarantees that the ExecutionGraph has\n+            // been created\n+            taskSubmitLatch.await();\n             final CompletedCheckpoint savepointCheckpoint =\n                     completedCheckpointStore.getLatestCheckpoint(false);\n \n@@ -1018,13 +1029,16 @@ private void waitUntilAllExecutionsAreScheduledOrDeployed(\n         final Deadline deadline = Deadline.fromNow(duration);\n \n         CommonTestUtils.waitUntilCondition(\n-                () ->\n-                        getExecutions(jobMasterGateway).stream()\n-                                .allMatch(\n-                                        execution ->\n-                                                execution.getState() == ExecutionState.SCHEDULED\n-                                                        || execution.getState()\n-                                                                == ExecutionState.DEPLOYING),\n+                () -> {\n+                    final Collection<AccessExecution> executions = getExecutions(jobMasterGateway);\n+                    return !executions.isEmpty()\n+                            && executions.stream()\n+                                    .allMatch(\n+                                            execution ->\n+                                                    execution.getState() == ExecutionState.SCHEDULED\n+                                                            || execution.getState()\n+                                                                    == ExecutionState.DEPLOYING);\n+                },\n                 deadline);\n     }\n ",
      "parent_sha": "0fc03e408cb80f37699e116230a5876dd7ff03e9"
    }
  },
  {
    "oid": "b21a092f29e8d1f5adb0fa7ac2250432875d2aeb",
    "message": "[hotfix] [javadoc] Minor javadoc fix in TimestampAssigner.java\n\nThis closes #5646\n\nAlso close unrelated lingering pull request:\nThis closes #5643",
    "date": "2018-03-07T17:09:23Z",
    "url": "https://github.com/apache/flink/commit/b21a092f29e8d1f5adb0fa7ac2250432875d2aeb",
    "details": {
      "sha": "ba7bdd87e16c2e8db0a9eab64f3ff90c31bbcdd3",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/TimestampAssigner.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/flink/blob/b21a092f29e8d1f5adb0fa7ac2250432875d2aeb/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Ffunctions%2FTimestampAssigner.java",
      "raw_url": "https://github.com/apache/flink/raw/b21a092f29e8d1f5adb0fa7ac2250432875d2aeb/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Ffunctions%2FTimestampAssigner.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Ffunctions%2FTimestampAssigner.java?ref=b21a092f29e8d1f5adb0fa7ac2250432875d2aeb",
      "patch": "@@ -40,9 +40,9 @@ public interface TimestampAssigner<T> extends Function {\n \t * by ingestion time. If the element did not carry a timestamp before, this value is\n \t * {@code Long.MIN_VALUE}.\n \t *\n-\t * @param element The element that the timestamp is wil be assigned to.\n+\t * @param element The element that the timestamp will be assigned to.\n \t * @param previousElementTimestamp The previous internal timestamp of the element,\n-\t *                                 or a negative value, if no timestamp has been assigned, yet.\n+\t *                                 or a negative value, if no timestamp has been assigned yet.\n \t * @return The new timestamp.\n \t */\n \tlong extractTimestamp(T element, long previousElementTimestamp);",
      "parent_sha": "75a4aaea8b051aa6dae52d421f7b4d7eeab99486"
    }
  },
  {
    "oid": "66f61348be16dd1a3638e063936a43f45cb8e9db",
    "message": "[hotfix] Do not schedule timeout when future is already  completed",
    "date": "2018-01-26T12:50:24Z",
    "url": "https://github.com/apache/flink/commit/66f61348be16dd1a3638e063936a43f45cb8e9db",
    "details": {
      "sha": "7195957bfc3a24082f6e0488b27b5805ac28b557",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/concurrent/FutureUtils.java",
      "status": "modified",
      "additions": 8,
      "deletions": 6,
      "changes": 14,
      "blob_url": "https://github.com/apache/flink/blob/66f61348be16dd1a3638e063936a43f45cb8e9db/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fconcurrent%2FFutureUtils.java",
      "raw_url": "https://github.com/apache/flink/raw/66f61348be16dd1a3638e063936a43f45cb8e9db/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fconcurrent%2FFutureUtils.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fconcurrent%2FFutureUtils.java?ref=66f61348be16dd1a3638e063936a43f45cb8e9db",
      "patch": "@@ -459,13 +459,15 @@ public void onComplete(Throwable failure, T success) throws Throwable {\n \t * @return The timeout enriched future\n \t */\n \tpublic static <T> CompletableFuture<T> orTimeout(CompletableFuture<T> future, long timeout, TimeUnit timeUnit) {\n-\t\tfinal ScheduledFuture<?> timeoutFuture = Delayer.delay(new Timeout(future), timeout, timeUnit);\n+\t\tif (!future.isDone()) {\n+\t\t\tfinal ScheduledFuture<?> timeoutFuture = Delayer.delay(new Timeout(future), timeout, timeUnit);\n \n-\t\tfuture.whenComplete((T value, Throwable throwable) -> {\n-\t\t\tif (!timeoutFuture.isDone()) {\n-\t\t\t\ttimeoutFuture.cancel(false);\n-\t\t\t}\n-\t\t});\n+\t\t\tfuture.whenComplete((T value, Throwable throwable) -> {\n+\t\t\t\tif (!timeoutFuture.isDone()) {\n+\t\t\t\t\ttimeoutFuture.cancel(false);\n+\t\t\t\t}\n+\t\t\t});\n+\t\t}\n \n \t\treturn future;\n \t}",
      "parent_sha": "1ffd77a27a4b7e2e1a1a599100c21742eb9a3d00"
    }
  },
  {
    "oid": "ca206d8d620b3a0c6c7bce884e656433b6e2dfb2",
    "message": "[hotfix][tests] Adjust to use constant BUFFER_SIZE in PipelinedSubpartitionWithReadViewTest",
    "date": "2019-09-27T15:10:40Z",
    "url": "https://github.com/apache/flink/commit/ca206d8d620b3a0c6c7bce884e656433b6e2dfb2",
    "details": {
      "sha": "56945016cad6cb1d391b8a49baf04c060c14b4e2",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartitionWithReadViewTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/ca206d8d620b3a0c6c7bce884e656433b6e2dfb2/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fio%2Fnetwork%2Fpartition%2FPipelinedSubpartitionWithReadViewTest.java",
      "raw_url": "https://github.com/apache/flink/raw/ca206d8d620b3a0c6c7bce884e656433b6e2dfb2/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fio%2Fnetwork%2Fpartition%2FPipelinedSubpartitionWithReadViewTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fio%2Fnetwork%2Fpartition%2FPipelinedSubpartitionWithReadViewTest.java?ref=ca206d8d620b3a0c6c7bce884e656433b6e2dfb2",
      "patch": "@@ -308,7 +308,7 @@ private void testBacklogConsistentWithNumberOfConsumableBuffers(boolean isFlushR\n \t\tfinal int numberOfAddedBuffers = 5;\n \n \t\tfor (int i = 1; i <= numberOfAddedBuffers; i++) {\n-\t\t\tfinal BufferBuilder bufferBuilder = createFilledBufferBuilder(1024, 10);\n+\t\t\tfinal BufferBuilder bufferBuilder = createFilledBufferBuilder(BUFFER_SIZE);\n \t\t\tsubpartition.add(bufferBuilder.createBufferConsumer());\n \n \t\t\tif (i < numberOfAddedBuffers || isFinished) {",
      "parent_sha": "4ccb0a1a1a66f00141cd534bfb3765e5d008d5c7"
    }
  },
  {
    "oid": "f6fca9540253f75c7b0f1b3e208b4d29506aab0f",
    "message": "[FLINK-18205][security] Mitigate the use of reflection in Utils",
    "date": "2020-06-19T13:13:23Z",
    "url": "https://github.com/apache/flink/commit/f6fca9540253f75c7b0f1b3e208b4d29506aab0f",
    "details": {
      "sha": "c09dabbdde282dee677092cdeede41a7e4a896c0",
      "filename": "flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java",
      "status": "modified",
      "additions": 1,
      "deletions": 9,
      "changes": 10,
      "blob_url": "https://github.com/apache/flink/blob/f6fca9540253f75c7b0f1b3e208b4d29506aab0f/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FUtils.java",
      "raw_url": "https://github.com/apache/flink/raw/f6fca9540253f75c7b0f1b3e208b4d29506aab0f/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FUtils.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FUtils.java?ref=f6fca9540253f75c7b0f1b3e208b4d29506aab0f",
      "patch": "@@ -51,7 +51,6 @@\n import java.io.File;\n import java.io.IOException;\n import java.lang.reflect.InvocationTargetException;\n-import java.lang.reflect.Method;\n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n import java.util.Collection;\n@@ -460,14 +459,7 @@ static ContainerLaunchContext createTaskExecutorContext(\n \t\t\tlog.debug(\"Adding security tokens to TaskExecutor's container launch context.\");\n \n \t\t\ttry (DataOutputBuffer dob = new DataOutputBuffer()) {\n-\t\t\t\tMethod readTokenStorageFileMethod = Credentials.class.getMethod(\n-\t\t\t\t\t\"readTokenStorageFile\", File.class, org.apache.hadoop.conf.Configuration.class);\n-\n-\t\t\t\tCredentials cred =\n-\t\t\t\t\t(Credentials) readTokenStorageFileMethod.invoke(\n-\t\t\t\t\t\tnull,\n-\t\t\t\t\t\tnew File(fileLocation),\n-\t\t\t\t\t\tHadoopUtils.getHadoopConfiguration(flinkConfig));\n+\t\t\t\tCredentials cred = Credentials.readTokenStorageFile(new File(fileLocation), HadoopUtils.getHadoopConfiguration(flinkConfig));\n \n \t\t\t\t// Filter out AMRMToken before setting the tokens to the TaskManager container context.\n \t\t\t\tCredentials taskManagerCred = new Credentials();",
      "parent_sha": "9bbe503bd15ed39be5a7a13be416e240579c1a13"
    }
  },
  {
    "oid": "1d447895760308bc3ef45d5beddb8b0bb6090101",
    "message": "[refactor] Remove unnecessary implements for InternalPriorityQueue (#26315)",
    "date": "2025-03-20T09:29:59Z",
    "url": "https://github.com/apache/flink/commit/1d447895760308bc3ef45d5beddb8b0bb6090101",
    "details": {
      "sha": "a8e038532d23675b350a45e91ec1571be030551a",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/KeyGroupPartitionedPriorityQueue.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/1d447895760308bc3ef45d5beddb8b0bb6090101/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fstate%2Fheap%2FKeyGroupPartitionedPriorityQueue.java",
      "raw_url": "https://github.com/apache/flink/raw/1d447895760308bc3ef45d5beddb8b0bb6090101/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fstate%2Fheap%2FKeyGroupPartitionedPriorityQueue.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fstate%2Fheap%2FKeyGroupPartitionedPriorityQueue.java?ref=1d447895760308bc3ef45d5beddb8b0bb6090101",
      "patch": "@@ -48,7 +48,7 @@\n  */\n public class KeyGroupPartitionedPriorityQueue<\n                 T, PQ extends InternalPriorityQueue<T> & HeapPriorityQueueElement>\n-        implements InternalPriorityQueue<T>, KeyGroupedInternalPriorityQueue<T> {\n+        implements KeyGroupedInternalPriorityQueue<T> {\n \n     /** A heap of heap sets. Each sub-heap represents the partition for a key-group. */\n     @Nonnull private final HeapPriorityQueue<PQ> heapOfKeyGroupedHeaps;",
      "parent_sha": "034d19c63857c34406e0753a1ce522f0f5cab0bb"
    }
  },
  {
    "oid": "23ff1203a0583d2859b3f0f92f4cb0c604b83baa",
    "message": "[hotfix] Rearrange TaskExecutor imports",
    "date": "2018-01-30T17:30:17Z",
    "url": "https://github.com/apache/flink/commit/23ff1203a0583d2859b3f0f92f4cb0c604b83baa",
    "details": {
      "sha": "ad7414c9686925527c0567eb485f77f91fae1f33",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskExecutor.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/23ff1203a0583d2859b3f0f92f4cb0c604b83baa/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Ftaskexecutor%2FTaskExecutor.java",
      "raw_url": "https://github.com/apache/flink/raw/23ff1203a0583d2859b3f0f92f4cb0c604b83baa/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Ftaskexecutor%2FTaskExecutor.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Ftaskexecutor%2FTaskExecutor.java?ref=23ff1203a0583d2859b3f0f92f4cb0c604b83baa",
      "patch": "@@ -66,10 +66,10 @@\n import org.apache.flink.runtime.rpc.RpcEndpoint;\n import org.apache.flink.runtime.rpc.RpcService;\n import org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils;\n+import org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager;\n import org.apache.flink.runtime.state.TaskLocalStateStore;\n import org.apache.flink.runtime.state.TaskStateManager;\n import org.apache.flink.runtime.state.TaskStateManagerImpl;\n-import org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager;\n import org.apache.flink.runtime.taskexecutor.exceptions.CheckpointException;\n import org.apache.flink.runtime.taskexecutor.exceptions.PartitionException;\n import org.apache.flink.runtime.taskexecutor.exceptions.SlotAllocationException;",
      "parent_sha": "0e20b613087e1b326e05674e3d532ea4aa444bc3"
    }
  },
  {
    "oid": "57980a132ceb120cb3c179569474493c93c29f0c",
    "message": "[hotfix] Removes invalid JavaDoc",
    "date": "2022-02-28T10:35:05Z",
    "url": "https://github.com/apache/flink/commit/57980a132ceb120cb3c179569474493c93c29f0c",
    "details": {
      "sha": "fbcc278587de660803a05c7a228a87d199bcb983",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/zookeeper/ZooKeeperStateHandleStore.java",
      "status": "modified",
      "additions": 1,
      "deletions": 4,
      "changes": 5,
      "blob_url": "https://github.com/apache/flink/blob/57980a132ceb120cb3c179569474493c93c29f0c/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fzookeeper%2FZooKeeperStateHandleStore.java",
      "raw_url": "https://github.com/apache/flink/raw/57980a132ceb120cb3c179569474493c93c29f0c/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fzookeeper%2FZooKeeperStateHandleStore.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fzookeeper%2FZooKeeperStateHandleStore.java?ref=57980a132ceb120cb3c179569474493c93c29f0c",
      "patch": "@@ -408,8 +408,7 @@ List<Tuple2<RetrievableStateHandle<T>, String>> getAllAndLock(\n \n     /**\n      * Releases the lock for the given state node and tries to remove the state node if it is no\n-     * longer locked. It returns the {@link RetrievableStateHandle} stored under the given state\n-     * node if any.\n+     * longer locked.\n      *\n      * @param pathInZooKeeper Path of state handle to remove\n      * @return True if the state handle could be released\n@@ -449,8 +448,6 @@ public boolean releaseAndTryRemove(String pathInZooKeeper) throws Exception {\n      * Releases all lock nodes of this ZooKeeperStateHandleStores and tries to remove all state\n      * nodes which are not locked anymore.\n      *\n-     * <p>The delete operation is executed asynchronously\n-     *\n      * @throws Exception if the delete operation fails\n      */\n     @Override",
      "parent_sha": "145099b9446d9b749747c2413923823748369415"
    }
  },
  {
    "oid": "a154dd5be8540d20117ccdd9462434ff6ac02b46",
    "message": "[hotfix][checkstyle] fix checkstyle in PartitionRequestClientFactory",
    "date": "2018-07-12T09:46:15Z",
    "url": "https://github.com/apache/flink/commit/a154dd5be8540d20117ccdd9462434ff6ac02b46",
    "details": {
      "sha": "2df094b06e72ae1914aad9b1f83e5ca98eeaa51a",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientFactory.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/flink/blob/a154dd5be8540d20117ccdd9462434ff6ac02b46/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fio%2Fnetwork%2Fnetty%2FPartitionRequestClientFactory.java",
      "raw_url": "https://github.com/apache/flink/raw/a154dd5be8540d20117ccdd9462434ff6ac02b46/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fio%2Fnetwork%2Fnetty%2FPartitionRequestClientFactory.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fio%2Fnetwork%2Fnetty%2FPartitionRequestClientFactory.java?ref=a154dd5be8540d20117ccdd9462434ff6ac02b46",
      "patch": "@@ -18,8 +18,8 @@\n \n package org.apache.flink.runtime.io.network.netty;\n \n-import org.apache.flink.runtime.io.network.NetworkClientHandler;\n import org.apache.flink.runtime.io.network.ConnectionID;\n+import org.apache.flink.runtime.io.network.NetworkClientHandler;\n import org.apache.flink.runtime.io.network.netty.exception.LocalTransportException;\n import org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException;\n import org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel;\n@@ -34,8 +34,8 @@\n \n /**\n  * Factory for {@link PartitionRequestClient} instances.\n- * <p>\n- * Instances of partition requests clients are shared among several {@link RemoteInputChannel}\n+ *\n+ * <p>Instances of partition requests clients are shared among several {@link RemoteInputChannel}\n  * instances.\n  */\n class PartitionRequestClientFactory {",
      "parent_sha": "9e87e2030d5a8232b80e6dbb93166fb9ac9ecb53"
    }
  },
  {
    "oid": "d2a671fe39a722e1bca51bbdadd31044d18f8022",
    "message": "Implemented keyboard control",
    "date": "2012-01-11T21:29:56Z",
    "url": "https://github.com/apache/flink/commit/d2a671fe39a722e1bca51bbdadd31044d18f8022",
    "details": {
      "sha": "cf38e4b6983aed39c24cc206de67eb8e5faa17cc",
      "filename": "nephele/nephele-visualization/src/main/java/eu/stratosphere/nephele/visualization/swt/SWTFailurePatternsEditor.java",
      "status": "modified",
      "additions": 120,
      "deletions": 1,
      "changes": 121,
      "blob_url": "https://github.com/apache/flink/blob/d2a671fe39a722e1bca51bbdadd31044d18f8022/nephele%2Fnephele-visualization%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Fvisualization%2Fswt%2FSWTFailurePatternsEditor.java",
      "raw_url": "https://github.com/apache/flink/raw/d2a671fe39a722e1bca51bbdadd31044d18f8022/nephele%2Fnephele-visualization%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Fvisualization%2Fswt%2FSWTFailurePatternsEditor.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele%2Fnephele-visualization%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Fvisualization%2Fswt%2FSWTFailurePatternsEditor.java?ref=d2a671fe39a722e1bca51bbdadd31044d18f8022",
      "patch": "@@ -28,6 +28,8 @@\n \n import org.eclipse.swt.SWT;\n import org.eclipse.swt.custom.SashForm;\n+import org.eclipse.swt.events.KeyAdapter;\n+import org.eclipse.swt.events.KeyEvent;\n import org.eclipse.swt.events.MenuAdapter;\n import org.eclipse.swt.events.MenuEvent;\n import org.eclipse.swt.events.MouseAdapter;\n@@ -176,7 +178,7 @@ public void widgetSelected(SelectionEvent arg0) {\n \n \tprivate Table createFailureEventTable(final Composite parent) {\n \n-\t\tfinal Table table = new Table(parent, SWT.BORDER | SWT.MULTI);\n+\t\tfinal Table table = new Table(parent, SWT.BORDER | SWT.SINGLE);\n \t\ttable.setHeaderVisible(true);\n \t\ttable.setLinesVisible(true);\n \n@@ -300,9 +302,55 @@ public int compare(final AbstractFailureEvent o1, AbstractFailureEvent o2) {\n \t\tnameColumn.addListener(SWT.Selection, sortListener);\n \t\tintervalColumn.addListener(SWT.Selection, sortListener);\n \n+\t\t// Implement keyboard commands\n+\t\ttable.addKeyListener(new KeyAdapter() {\n+\n+\t\t\t@Override\n+\t\t\tpublic void keyReleased(final KeyEvent arg0) {\n+\n+\t\t\t\tif (arg0.keyCode != SWT.DEL) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\tremoveSelectedTableItems();\n+\t\t\t}\n+\t\t});\n+\n+\t\t// Set the menu\n+\t\ttable.setMenu(createTableContextMenu());\n+\n \t\treturn table;\n \t}\n \n+\tprivate void removeSelectedTableItems() {\n+\n+\t\tfinal TableItem[] selectedItems = this.failureEventTable.getSelection();\n+\t\tif (selectedItems == null) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tfor (final TableItem selectedItem : selectedItems) {\n+\t\t\tremoveTableItem(selectedItem);\n+\t\t}\n+\t}\n+\n+\tprivate void removeTableItem(final TableItem ti) {\n+\n+\t\tfinal AbstractFailureEvent event = (AbstractFailureEvent) ti.getData();\n+\t\tif (event == null) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tfinal MessageBox messageBox = new MessageBox(shell, SWT.ICON_QUESTION | SWT.YES | SWT.NO);\n+\t\tmessageBox.setText(\"Confirm removal\");\n+\t\tmessageBox.setMessage(\"Do you really want to remove the event '\" + event.getName() + \"'\");\n+\t\tif (messageBox.open() == SWT.YES) {\n+\t\t\tti.dispose();\n+\t\t}\n+\n+\t\tthis.selectedFailurePattern.removeEvent(event);\n+\t}\n+\n \tprivate void updateTableItem(TableItem ti, final AbstractFailureEvent event) {\n \n \t\tboolean newItemCreated = false;\n@@ -349,6 +397,77 @@ public Map<String, JobFailurePattern> show() {\n \t\treturn this.loadedPatterns;\n \t}\n \n+\tprivate Menu createTableContextMenu() {\n+\n+\t\tfinal Menu tableContextMenu = new Menu(this.shell);\n+\n+\t\tfinal MenuItem createItem = new MenuItem(tableContextMenu, SWT.PUSH);\n+\t\tcreateItem.setText(\"Create...\");\n+\t\tcreateItem.addSelectionListener(new SelectionAdapter() {\n+\n+\t\t\t@Override\n+\t\t\tpublic void widgetSelected(final SelectionEvent arg0) {\n+\t\t\t\t// TODO: Implement me\n+\t\t\t}\n+\n+\t\t});\n+\n+\t\tfinal MenuItem editItem = new MenuItem(tableContextMenu, SWT.PUSH);\n+\t\teditItem.setText(\"Edit...\");\n+\t\teditItem.addSelectionListener(new SelectionAdapter() {\n+\n+\t\t\t@Override\n+\t\t\tpublic void widgetSelected(final SelectionEvent arg0) {\n+\t\t\t\t// TODO: Implement me\n+\t\t\t}\n+\n+\t\t});\n+\n+\t\tfinal MenuItem removeItem = new MenuItem(tableContextMenu, SWT.PUSH);\n+\t\tremoveItem.setText(\"Remove...\");\n+\t\tremoveItem.addSelectionListener(new SelectionAdapter() {\n+\n+\t\t\t@Override\n+\t\t\tpublic void widgetSelected(final SelectionEvent arg0) {\n+\t\t\t\tremoveSelectedTableItems();\n+\t\t\t}\n+\t\t});\n+\n+\t\ttableContextMenu.addMenuListener(new MenuAdapter() {\n+\n+\t\t\t@Override\n+\t\t\tpublic void menuShown(final MenuEvent arg0) {\n+\n+\t\t\t\tTableItem[] selectedItems = failureEventTable.getSelection();\n+\t\t\t\tif (selectedItems == null) {\n+\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\tif (selectedItems.length == 0) {\n+\n+\t\t\t\t\teditItem.setEnabled(false);\n+\t\t\t\t\tremoveItem.setEnabled(false);\n+\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\tif (selectedItems[0].getData() == null) {\n+\n+\t\t\t\t\teditItem.setEnabled(false);\n+\t\t\t\t\tremoveItem.setEnabled(false);\n+\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\teditItem.setEnabled(true);\n+\t\t\t\tremoveItem.setEnabled(true);\n+\t\t\t}\n+\t\t});\n+\n+\t\treturn tableContextMenu;\n+\t}\n+\n \tprivate Menu createTreeContextMenu() {\n \n \t\tfinal Menu treeContextMenu = new Menu(this.shell);",
      "parent_sha": "f8b70146bcf75f20c80507b365f2a31e7d75a3e3"
    }
  },
  {
    "oid": "d750ff01f694165f7a60125f40f1af99694e8c73",
    "message": "Minor code style improvement",
    "date": "2012-10-29T17:23:39Z",
    "url": "https://github.com/apache/flink/commit/d750ff01f694165f7a60125f40f1af99694e8c73",
    "details": {
      "sha": "bd08165ecaae5929c4452bf294fa0c80236610e8",
      "filename": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/executiongraph/ExecutionStage.java",
      "status": "modified",
      "additions": 1,
      "deletions": 3,
      "changes": 4,
      "blob_url": "https://github.com/apache/flink/blob/d750ff01f694165f7a60125f40f1af99694e8c73/nephele%2Fnephele-server%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Fexecutiongraph%2FExecutionStage.java",
      "raw_url": "https://github.com/apache/flink/raw/d750ff01f694165f7a60125f40f1af99694e8c73/nephele%2Fnephele-server%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Fexecutiongraph%2FExecutionStage.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele%2Fnephele-server%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Fexecutiongraph%2FExecutionStage.java?ref=d750ff01f694165f7a60125f40f1af99694e8c73",
      "patch": "@@ -272,10 +272,8 @@ public void collectRequiredInstanceTypes(final InstanceRequestMap instanceReques\n \t\t\t\tif (vertex.getExecutionState() == executionState) {\n \t\t\t\t\tfinal AbstractInstance instance = vertex.getAllocatedResource().getInstance();\n \n-\t\t\t\t\tif (collectedInstances.contains(instance)) {\n+\t\t\t\t\tif (!collectedInstances.add(instance)) {\n \t\t\t\t\t\tcontinue;\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tcollectedInstances.add(instance);\n \t\t\t\t\t}\n \n \t\t\t\t\tif (instance instanceof DummyInstance) {",
      "parent_sha": "db927db3a1a9c9641c51f7585c9affc8bc610a3f"
    }
  },
  {
    "oid": "58dc0e712a47c6df59ec3418edff8c25a30fa198",
    "message": "[hotfix][tests] Let PhysicalSlotProviderImplWithSpreadOutStrategyTest extend TestLogger",
    "date": "2021-07-14T14:44:32Z",
    "url": "https://github.com/apache/flink/commit/58dc0e712a47c6df59ec3418edff8c25a30fa198",
    "details": {
      "sha": "f9cfa865f754530aa9c52286ee81e655ec4cbadb",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/slotpool/PhysicalSlotProviderImplWithSpreadOutStrategyTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/flink/blob/58dc0e712a47c6df59ec3418edff8c25a30fa198/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fjobmaster%2Fslotpool%2FPhysicalSlotProviderImplWithSpreadOutStrategyTest.java",
      "raw_url": "https://github.com/apache/flink/raw/58dc0e712a47c6df59ec3418edff8c25a30fa198/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fjobmaster%2Fslotpool%2FPhysicalSlotProviderImplWithSpreadOutStrategyTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fjobmaster%2Fslotpool%2FPhysicalSlotProviderImplWithSpreadOutStrategyTest.java?ref=58dc0e712a47c6df59ec3418edff8c25a30fa198",
      "patch": "@@ -22,6 +22,7 @@\n import org.apache.flink.runtime.clusterframework.types.SlotProfile;\n import org.apache.flink.runtime.jobmaster.SlotRequestId;\n import org.apache.flink.runtime.taskmanager.TaskManagerLocation;\n+import org.apache.flink.util.TestLogger;\n \n import org.junit.Rule;\n import org.junit.Test;\n@@ -38,7 +39,7 @@\n  * Tests for {@link PhysicalSlotProviderImpl} using {@link\n  * EvenlySpreadOutLocationPreferenceSlotSelectionStrategy}.\n  */\n-public class PhysicalSlotProviderImplWithSpreadOutStrategyTest {\n+public class PhysicalSlotProviderImplWithSpreadOutStrategyTest extends TestLogger {\n \n     @Rule\n     public PhysicalSlotProviderResource physicalSlotProviderResource =",
      "parent_sha": "4b85e9bcda968733b64c072390b837aec9d0ffd5"
    }
  },
  {
    "oid": "97a7a102a1d5ce32a3cc81b481266d044feaf28b",
    "message": "[FLINK-16744][task] Split finish() in ChannelStateWriter\n\nSplit finish() in ChannelStateWriter into finishIn and finishOut to ease client usage",
    "date": "2020-04-10T09:04:42Z",
    "url": "https://github.com/apache/flink/commit/97a7a102a1d5ce32a3cc81b481266d044feaf28b",
    "details": {
      "sha": "7c6a57c9f160ae2ba7168cb8c71e29dc89e231c4",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/channel/ChannelStateWriter.java",
      "status": "modified",
      "additions": 18,
      "deletions": 4,
      "changes": 22,
      "blob_url": "https://github.com/apache/flink/blob/97a7a102a1d5ce32a3cc81b481266d044feaf28b/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2Fchannel%2FChannelStateWriter.java",
      "raw_url": "https://github.com/apache/flink/raw/97a7a102a1d5ce32a3cc81b481266d044feaf28b/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2Fchannel%2FChannelStateWriter.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2Fchannel%2FChannelStateWriter.java?ref=97a7a102a1d5ce32a3cc81b481266d044feaf28b",
      "patch": "@@ -73,10 +73,20 @@ public interface ChannelStateWriter extends AutoCloseable {\n \tvoid addOutputData(long checkpointId, ResultSubpartitionInfo info, int startSeqNum, Buffer... data);\n \n \t/**\n-\t * Finalize write of channel state for the given checkpoint id.\n-\t * Must be called after {@link #start(long)} and all of the data of the given checkpoint added.\n+\t * Finalize write of channel state data for the given checkpoint id.\n+\t * Must be called after {@link #start(long)} and all of the input data of the given checkpoint added.\n+\t * When both {@link #finishInput} and {@link #finishOutput} were called the results can be (eventually) obtained\n+\t * using {@link #getWriteCompletionFuture}\n \t */\n-\tvoid finish(long checkpointId);\n+\tvoid finishInput(long checkpointId);\n+\n+\t/**\n+\t * Finalize write of channel state data for the given checkpoint id.\n+\t * Must be called after {@link #start(long)} and all of the output data of the given checkpoint added.\n+\t * When both {@link #finishInput} and {@link #finishOutput} were called the results can be (eventually) obtained\n+\t * using {@link #getWriteCompletionFuture}\n+\t */\n+\tvoid finishOutput(long checkpointId);\n \n \t/**\n \t * Must be called after {@link #start(long)}.\n@@ -101,7 +111,11 @@ public void addOutputData(long checkpointId, ResultSubpartitionInfo info, int st\n \t\t}\n \n \t\t@Override\n-\t\tpublic void finish(long checkpointId) {\n+\t\tpublic void finishInput(long checkpointId) {\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic void finishOutput(long checkpointId) {\n \t\t}\n \n \t\t@Override",
      "parent_sha": "94ccf00764aaaa6d4ccbf23371d608b3f73d8739"
    }
  },
  {
    "oid": "35f967c0c0d91d0cc5e77000f95d1fcfdc20f04d",
    "message": "[hotfix] Fix minor IDE warnings in MemoryUtils",
    "date": "2020-02-25T09:30:49Z",
    "url": "https://github.com/apache/flink/commit/35f967c0c0d91d0cc5e77000f95d1fcfdc20f04d",
    "details": {
      "sha": "c807145825ae57083206232239169383156d3b65",
      "filename": "flink-core/src/main/java/org/apache/flink/core/memory/MemoryUtils.java",
      "status": "modified",
      "additions": 2,
      "deletions": 4,
      "changes": 6,
      "blob_url": "https://github.com/apache/flink/blob/35f967c0c0d91d0cc5e77000f95d1fcfdc20f04d/flink-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcore%2Fmemory%2FMemoryUtils.java",
      "raw_url": "https://github.com/apache/flink/raw/35f967c0c0d91d0cc5e77000f95d1fcfdc20f04d/flink-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcore%2Fmemory%2FMemoryUtils.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcore%2Fmemory%2FMemoryUtils.java?ref=35f967c0c0d91d0cc5e77000f95d1fcfdc20f04d",
      "patch": "@@ -33,7 +33,7 @@\n public class MemoryUtils {\n \n \t/** The \"unsafe\", which can be used to perform native memory accesses. */\n-\t@SuppressWarnings(\"restriction\")\n+\t@SuppressWarnings({\"restriction\", \"UseOfSunClasses\"})\n \tpublic static final sun.misc.Unsafe UNSAFE = getUnsafe();\n \n \t/** The native byte order of the platform on which the system currently runs. */\n@@ -50,7 +50,7 @@ private static sun.misc.Unsafe getUnsafe() {\n \t\t} catch (SecurityException e) {\n \t\t\tthrow new Error(\"Could not access the sun.misc.Unsafe handle, permission denied by security manager.\", e);\n \t\t} catch (NoSuchFieldException e) {\n-\t\t\tthrow new Error(\"The static handle field in sun.misc.Unsafe was not found.\");\n+\t\t\tthrow new Error(\"The static handle field in sun.misc.Unsafe was not found.\", e);\n \t\t} catch (IllegalArgumentException e) {\n \t\t\tthrow new Error(\"Bug: Illegal argument reflection access for static field.\", e);\n \t\t} catch (IllegalAccessException e) {\n@@ -64,7 +64,6 @@ private static sun.misc.Unsafe getUnsafe() {\n \tprivate MemoryUtils() {}\n \n \tprivate static Constructor<? extends ByteBuffer> getDirectBufferPrivateConstructor() {\n-\t\t//noinspection OverlyBroadCatchBlock\n \t\ttry {\n \t\t\tConstructor<? extends ByteBuffer> constructor =\n \t\t\t\tByteBuffer.allocateDirect(1).getClass().getDeclaredConstructor(long.class, int.class);\n@@ -107,7 +106,6 @@ static long allocateUnsafe(long size) {\n \t * @param address address of the unsafe memory to release\n \t * @return action to run to release the unsafe memory manually\n \t */\n-\t@SuppressWarnings(\"UseOfSunClasses\")\n \tstatic Runnable createMemoryGcCleaner(Object owner, long address) {\n \t\treturn JavaGcCleanerWrapper.create(owner, () -> releaseUnsafe(address));\n \t}",
      "parent_sha": "93da5ecf096657e22e2692da73346b5ddaf5feba"
    }
  },
  {
    "oid": "204963910186a18da17fc26d7c3d768a9b7bf4b3",
    "message": "[FLINK-20118][file connector] Introduce TaskManager and JobManager failures in FileSourceTextLinesITCase\n\nThis closes #14199",
    "date": "2020-11-30T17:41:36Z",
    "url": "https://github.com/apache/flink/commit/204963910186a18da17fc26d7c3d768a9b7bf4b3",
    "details": {
      "sha": "ce68f9331b79f6a9b6f72790d88d206f287e0db3",
      "filename": "flink-connectors/flink-connector-files/src/test/java/org/apache/flink/connector/file/src/FileSourceTextLinesITCase.java",
      "status": "modified",
      "additions": 276,
      "deletions": 23,
      "changes": 299,
      "blob_url": "https://github.com/apache/flink/blob/204963910186a18da17fc26d7c3d768a9b7bf4b3/flink-connectors%2Fflink-connector-files%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fconnector%2Ffile%2Fsrc%2FFileSourceTextLinesITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/204963910186a18da17fc26d7c3d768a9b7bf4b3/flink-connectors%2Fflink-connector-files%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fconnector%2Ffile%2Fsrc%2FFileSourceTextLinesITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors%2Fflink-connector-files%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fconnector%2Ffile%2Fsrc%2FFileSourceTextLinesITCase.java?ref=204963910186a18da17fc26d7c3d768a9b7bf4b3",
      "patch": "@@ -18,18 +18,33 @@\n \n package org.apache.flink.connector.file.src;\n \n+import org.apache.flink.api.common.JobID;\n import org.apache.flink.api.common.eventtime.WatermarkStrategy;\n+import org.apache.flink.configuration.CheckpointingOptions;\n+import org.apache.flink.configuration.Configuration;\n import org.apache.flink.connector.file.src.reader.TextLineFormat;\n import org.apache.flink.core.fs.Path;\n-import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n+import org.apache.flink.runtime.checkpoint.CheckpointIDCounter;\n+import org.apache.flink.runtime.checkpoint.CheckpointRecoveryFactory;\n+import org.apache.flink.runtime.checkpoint.CompletedCheckpointStore;\n+import org.apache.flink.runtime.checkpoint.StandaloneCheckpointIDCounter;\n+import org.apache.flink.runtime.highavailability.nonha.embedded.TestingEmbeddedHaServices;\n+import org.apache.flink.runtime.minicluster.RpcServiceSharing;\n+import org.apache.flink.runtime.minicluster.TestingMiniCluster;\n+import org.apache.flink.runtime.minicluster.TestingMiniClusterConfiguration;\n+import org.apache.flink.runtime.testingUtils.TestingUtils;\n+import org.apache.flink.runtime.testutils.RecoverableCompletedCheckpointStore;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.streaming.api.operators.collect.ClientAndIterator;\n-import org.apache.flink.test.util.MiniClusterWithClientResource;\n+import org.apache.flink.streaming.util.TestStreamEnvironment;\n import org.apache.flink.util.TestLogger;\n import org.apache.flink.util.function.FunctionWithException;\n \n+import org.junit.AfterClass;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n import org.junit.ClassRule;\n import org.junit.Test;\n import org.junit.rules.TemporaryFolder;\n@@ -42,9 +57,15 @@\n import java.io.PrintWriter;\n import java.nio.charset.StandardCharsets;\n import java.time.Duration;\n+import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Supplier;\n import java.util.zip.GZIPOutputStream;\n \n import static org.hamcrest.Matchers.equalTo;\n@@ -61,20 +82,86 @@ public class FileSourceTextLinesITCase extends TestLogger {\n \t@ClassRule\n \tpublic static final TemporaryFolder TMP_FOLDER = new TemporaryFolder();\n \n-\t@ClassRule\n-\tpublic static final MiniClusterWithClientResource MINI_CLUSTER = new MiniClusterWithClientResource(\n-\t\tnew MiniClusterResourceConfiguration.Builder()\n-\t\t\t.setNumberTaskManagers(1)\n-\t\t\t.setNumberSlotsPerTaskManager(PARALLELISM)\n-\t\t\t.build());\n+\tprivate static TestingMiniCluster miniCluster;\n+\n+\tprivate static TestingEmbeddedHaServices highAvailabilityServices;\n+\n+\tprivate static CompletedCheckpointStore checkpointStore;\n+\n+\t@BeforeClass\n+\tpublic static void setupMiniCluster() throws Exception  {\n+\t\thighAvailabilityServices = new HaServices(TestingUtils.defaultExecutor(),\n+\t\t\t() -> checkpointStore,\n+\t\t\tnew StandaloneCheckpointIDCounter());\n+\n+\t\tfinal Configuration configuration = createConfiguration();\n+\n+\t\tminiCluster = new TestingMiniCluster(\n+\t\t\tnew TestingMiniClusterConfiguration.Builder()\n+\t\t\t\t.setConfiguration(configuration)\n+\t\t\t\t.setNumTaskManagers(1)\n+\t\t\t\t.setNumSlotsPerTaskManager(PARALLELISM)\n+\t\t\t\t.setRpcServiceSharing(RpcServiceSharing.DEDICATED)\n+\t\t\t\t.build(),\n+\t\t\t() -> highAvailabilityServices);\n+\n+\t\tminiCluster.start();\n+\t}\n+\n+\tprivate static Configuration createConfiguration() throws IOException {\n+\t\tfinal Configuration configuration = new Configuration();\n+\t\tfinal String checkPointDir = Path.fromLocalFile(TMP_FOLDER.newFolder()).toUri().toString();\n+\t\tconfiguration.set(CheckpointingOptions.CHECKPOINTS_DIRECTORY, checkPointDir);\n+\t\treturn configuration;\n+\t}\n+\n+\t@Before\n+\tpublic void setup() {\n+\t\tcheckpointStore = new RecoverableCompletedCheckpointStore();\n+\t}\n+\n+\t@AfterClass\n+\tpublic static void shutdownMiniCluster() throws Exception {\n+\t\tif (miniCluster != null) {\n+\t\t\tminiCluster.close();\n+\t\t}\n+\t\tif (highAvailabilityServices != null) {\n+\t\t\thighAvailabilityServices.closeAndCleanupAllData();\n+\t\t\thighAvailabilityServices = null;\n+\t\t}\n+\t}\n \n+\t// ------------------------------------------------------------------------\n+\t//  test cases\n \t// ------------------------------------------------------------------------\n \n \t/**\n \t * This test runs a job reading bounded input with a stream record format (text lines).\n \t */\n \t@Test\n \tpublic void testBoundedTextFileSource() throws Exception {\n+\t\ttestBoundedTextFileSource(FailoverType.NONE);\n+\t}\n+\n+\t/**\n+\t * This test runs a job reading bounded input with a stream record format (text lines)\n+\t * and restarts TaskManager.\n+\t */\n+\t@Test\n+\tpublic void testBoundedTextFileSourceWithTaskManagerFailover() throws Exception {\n+\t\ttestBoundedTextFileSource(FailoverType.TM);\n+\t}\n+\n+\t/**\n+\t * This test runs a job reading bounded input with a stream record format (text lines)\n+\t * and triggers JobManager failover.\n+\t */\n+\t@Test\n+\tpublic void testBoundedTextFileSourceWithJobManagerFailover() throws Exception {\n+\t\ttestBoundedTextFileSource(FailoverType.JM);\n+\t}\n+\n+\tprivate void testBoundedTextFileSource(FailoverType failoverType) throws Exception {\n \t\tfinal File testDir = TMP_FOLDER.newFolder();\n \n \t\t// our main test data\n@@ -84,18 +171,32 @@ public void testBoundedTextFileSource() throws Exception {\n \t\twriteHiddenJunkFiles(testDir);\n \n \t\tfinal FileSource<String> source = FileSource\n-\t\t\t\t.forRecordStreamFormat(new TextLineFormat(), Path.fromLocalFile(testDir))\n-\t\t\t\t.build();\n+\t\t\t.forRecordStreamFormat(new TextLineFormat(), Path.fromLocalFile(testDir))\n+\t\t\t.build();\n \n-\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tfinal StreamExecutionEnvironment env = new TestStreamEnvironment(miniCluster, PARALLELISM);\n \t\tenv.setParallelism(PARALLELISM);\n \n \t\tfinal DataStream<String> stream = env.fromSource(\n-\t\t\t\tsource,\n-\t\t\t\tWatermarkStrategy.noWatermarks(),\n-\t\t\t\t\"file-source\");\n+\t\t\tsource,\n+\t\t\tWatermarkStrategy.noWatermarks(),\n+\t\t\t\"file-source\");\n+\n+\t\tfinal DataStream<String> streamFailingInTheMiddleOfReading =\n+\t\t\tRecordCounterToFail.wrapWithFailureAfter(stream, LINES.length / 2);\n \n-\t\tfinal List<String> result = DataStreamUtils.collectBoundedStream(stream, \"Bounded TextFiles Test\");\n+\t\tfinal ClientAndIterator<String> client = DataStreamUtils.collectWithClient(\n+\t\t\tstreamFailingInTheMiddleOfReading,\n+\t\t\t\"Bounded TextFiles Test\");\n+\t\tfinal JobID jobId = client.client.getJobID();\n+\n+\t\tRecordCounterToFail.waitToFail();\n+\t\ttriggerFailover(failoverType, jobId, RecordCounterToFail::continueProcessing);\n+\n+\t\tfinal List<String> result = new ArrayList<>();\n+\t\twhile (client.iterator.hasNext()) {\n+\t\t\tresult.add(client.iterator.next());\n+\t\t}\n \n \t\tverifyResult(result);\n \t}\n@@ -106,23 +207,47 @@ public void testBoundedTextFileSource() throws Exception {\n \t */\n \t@Test\n \tpublic void testContinuousTextFileSource() throws Exception {\n+\t\ttestContinuousTextFileSource(FailoverType.NONE);\n+\t}\n+\n+\t/**\n+\t * This test runs a job reading continuous input (files appearing over time)\n+\t * with a stream record format (text lines) and restarts TaskManager.\n+\t */\n+\t@Test\n+\tpublic void testContinuousTextFileSourceWithTaskManagerFailover() throws Exception {\n+\t\ttestContinuousTextFileSource(FailoverType.TM);\n+\t}\n+\n+\t/**\n+\t * This test runs a job reading continuous input (files appearing over time)\n+\t * with a stream record format (text lines) and triggers JobManager failover.\n+\t */\n+\t@Test\n+\tpublic void testContinuousTextFileSourceWithJobManagerFailover() throws Exception {\n+\t\ttestContinuousTextFileSource(FailoverType.JM);\n+\t}\n+\n+\tprivate void testContinuousTextFileSource(FailoverType type) throws Exception {\n \t\tfinal File testDir = TMP_FOLDER.newFolder();\n \n \t\tfinal FileSource<String> source = FileSource\n-\t\t\t\t.forRecordStreamFormat(new TextLineFormat(), Path.fromLocalFile(testDir))\n-\t\t\t\t.monitorContinuously(Duration.ofMillis(5))\n-\t\t\t\t.build();\n+\t\t\t.forRecordStreamFormat(new TextLineFormat(), Path.fromLocalFile(testDir))\n+\t\t\t.monitorContinuously(Duration.ofMillis(5))\n+\t\t\t.build();\n \n-\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+\t\tfinal StreamExecutionEnvironment env = new TestStreamEnvironment(miniCluster, PARALLELISM);\n \t\tenv.setParallelism(PARALLELISM);\n+\t\tenv.enableCheckpointing(10L);\n \n \t\tfinal DataStream<String> stream = env.fromSource(\n-\t\t\t\tsource,\n-\t\t\t\tWatermarkStrategy.noWatermarks(),\n-\t\t\t\t\"file-source\");\n+\t\t\tsource,\n+\t\t\tWatermarkStrategy.noWatermarks(),\n+\t\t\t\"file-source\");\n \n \t\tfinal ClientAndIterator<String> client =\n \t\t\t\tDataStreamUtils.collectWithClient(stream, \"Continuous TextFiles Monitoring Test\");\n+\t\tfinal JobID jobId = client.client.getJobID();\n \n \t\t// write one file, execute, and wait for its result\n \t\t// that way we know that the application was running and the source has\n@@ -138,9 +263,15 @@ public void testContinuousTextFileSource() throws Exception {\n \t\tfor (int i = 1; i < LINES_PER_FILE.length; i++) {\n \t\t\tThread.sleep(10);\n \t\t\twriteFile(testDir, i);\n+\t\t\tfinal boolean failAfterHalfOfInput = i == LINES_PER_FILE.length / 2;\n+\t\t\tif (failAfterHalfOfInput) {\n+\t\t\t\ttriggerFailover(type, jobId, () -> {});\n+\t\t\t}\n \t\t}\n \n-\t\tfinal List<String> result2 = DataStreamUtils.collectRecordsFromUnboundedStream(client, numLinesAfter);\n+\t\tfinal List<String> result2 = DataStreamUtils.collectRecordsFromUnboundedStream(\n+\t\t\tclient,\n+\t\t\tnumLinesAfter);\n \n \t\t// shut down the job, now that we have all the results we expected.\n \t\tclient.client.cancel().get();\n@@ -149,6 +280,45 @@ public void testContinuousTextFileSource() throws Exception {\n \t\tverifyResult(result1);\n \t}\n \n+\t// ------------------------------------------------------------------------\n+\t//  test utilities\n+\t// ------------------------------------------------------------------------\n+\n+\tprivate enum FailoverType {\n+\t\tNONE,\n+\t\tTM,\n+\t\tJM\n+\t}\n+\n+\tprivate static void triggerFailover(\n+\t\t\tFailoverType type,\n+\t\t\tJobID jobId,\n+\t\t\tRunnable afterFailAction) throws Exception {\n+\t\tswitch (type) {\n+\t\t\tcase NONE:\n+\t\t\t\tafterFailAction.run();\n+\t\t\t\tbreak;\n+\t\t\tcase TM:\n+\t\t\t\trestartTaskManager(afterFailAction);\n+\t\t\t\tbreak;\n+\t\t\tcase JM:\n+\t\t\t\ttriggerJobManagerFailover(jobId, afterFailAction);\n+\t\t\t\tbreak;\n+\t\t}\n+\t}\n+\n+\tprivate static void triggerJobManagerFailover(JobID jobId, Runnable afterFailAction) throws Exception {\n+\t\thighAvailabilityServices.revokeJobMasterLeadership(jobId).get();\n+\t\tafterFailAction.run();\n+\t\thighAvailabilityServices.grantJobMasterLeadership(jobId).get();\n+\t}\n+\n+\tprivate static void restartTaskManager(Runnable afterFailAction) throws Exception {\n+\t\tminiCluster.terminateTaskExecutor(0).get();\n+\t\tafterFailAction.run();\n+\t\tminiCluster.startTaskExecutor();\n+\t}\n+\n \t// ------------------------------------------------------------------------\n \t//  verification\n \t// ------------------------------------------------------------------------\n@@ -308,4 +478,87 @@ private static void writeFileAtomically(\n \n \t\tassertTrue(stagingFile.renameTo(file));\n \t}\n+\n+\t// ------------------------------------------------------------------------\n+\t//  mini cluster failover utilities\n+\t// ------------------------------------------------------------------------\n+\n+\tprivate static class HaServices extends TestingEmbeddedHaServices {\n+\t\tprivate final Supplier<CompletedCheckpointStore> completedCheckpointStoreFactory;\n+\t\tprivate final CheckpointIDCounter checkpointIDCounter;\n+\n+\t\tprivate HaServices(\n+\t\t\t\tExecutor executor,\n+\t\t\t\tSupplier<CompletedCheckpointStore> completedCheckpointStoreFactory,\n+\t\t\t\tCheckpointIDCounter checkpointIDCounter) {\n+\t\t\tsuper(executor);\n+\t\t\tthis.completedCheckpointStoreFactory = completedCheckpointStoreFactory;\n+\t\t\tthis.checkpointIDCounter = checkpointIDCounter;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CheckpointRecoveryFactory getCheckpointRecoveryFactory() {\n+\t\t\treturn new CheckpointRecoveryFactoryWithSettableStore(\n+\t\t\t\tcompletedCheckpointStoreFactory,\n+\t\t\t\tcheckpointIDCounter);\n+\t\t}\n+\t}\n+\n+\tprivate static class CheckpointRecoveryFactoryWithSettableStore implements CheckpointRecoveryFactory {\n+\t\tprivate final Supplier<CompletedCheckpointStore> completedCheckpointStoreFactory;\n+\t\tprivate final CheckpointIDCounter checkpointIDCounter;\n+\n+\t\tprivate CheckpointRecoveryFactoryWithSettableStore(\n+\t\t\t\tSupplier<CompletedCheckpointStore> completedCheckpointStoreFactory,\n+\t\t\t\tCheckpointIDCounter checkpointIDCounter) {\n+\t\t\tthis.completedCheckpointStoreFactory = completedCheckpointStoreFactory;\n+\t\t\tthis.checkpointIDCounter = checkpointIDCounter;\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CompletedCheckpointStore createCheckpointStore(\n+\t\t\t\tJobID jobId,\n+\t\t\t\tint maxNumberOfCheckpointsToRetain,\n+\t\t\t\tClassLoader userClassLoader) {\n+\t\t\treturn completedCheckpointStoreFactory.get();\n+\t\t}\n+\n+\t\t@Override\n+\t\tpublic CheckpointIDCounter createCheckpointIDCounter(JobID jobId) {\n+\t\t\treturn checkpointIDCounter;\n+\t\t}\n+\t}\n+\n+\tprivate static class RecordCounterToFail {\n+\n+\t\tprivate static AtomicInteger records;\n+\t\tprivate static CompletableFuture<Void> fail;\n+\t\tprivate static CompletableFuture<Void> continueProcessing;\n+\n+\t\tprivate static <T> DataStream<T> wrapWithFailureAfter(\n+\t\t\t\tDataStream<T> stream,\n+\t\t\t\tint failAfter) {\n+\n+\t\t\trecords = new AtomicInteger();\n+\t\t\tfail = new CompletableFuture<>();\n+\t\t\tcontinueProcessing = new CompletableFuture<>();\n+\t\t\treturn stream.map(record -> {\n+\t\t\t\tfinal boolean halfOfInputIsRead = records.incrementAndGet() > failAfter;\n+\t\t\t\tfinal boolean notFailedYet = !fail.isDone();\n+\t\t\t\tif (notFailedYet && halfOfInputIsRead) {\n+\t\t\t\t\tfail.complete(null);\n+\t\t\t\t\tcontinueProcessing.get();\n+\t\t\t\t}\n+\t\t\t\treturn record;\n+\t\t\t});\n+\t\t}\n+\n+\t\tprivate static void waitToFail() throws ExecutionException, InterruptedException {\n+\t\t\tfail.get();\n+\t\t}\n+\n+\t\tprivate static void continueProcessing() {\n+\t\t\tcontinueProcessing.complete(null);\n+\t\t}\n+\t}\n }",
      "parent_sha": "aa9a9a80c7cf4140f1c5ab9fcfea3722f7748445"
    }
  },
  {
    "oid": "90075efccb4200b57d774322e9dd0acd9edfdba5",
    "message": "[hotfix][checkpoint/test] Don't swallow the original exception when cleanup fails as well.",
    "date": "2020-11-06T07:03:57Z",
    "url": "https://github.com/apache/flink/commit/90075efccb4200b57d774322e9dd0acd9edfdba5",
    "details": {
      "sha": "2dbe5d79850c9db3fc31ce2e250ce1bf57fba2c0",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/channel/SequentialChannelStateReaderImplTest.java",
      "status": "modified",
      "additions": 22,
      "deletions": 23,
      "changes": 45,
      "blob_url": "https://github.com/apache/flink/blob/90075efccb4200b57d774322e9dd0acd9edfdba5/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2Fchannel%2FSequentialChannelStateReaderImplTest.java",
      "raw_url": "https://github.com/apache/flink/raw/90075efccb4200b57d774322e9dd0acd9edfdba5/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2Fchannel%2FSequentialChannelStateReaderImplTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2Fchannel%2FSequentialChannelStateReaderImplTest.java?ref=90075efccb4200b57d774322e9dd0acd9edfdba5",
      "patch": "@@ -42,6 +42,8 @@\n import org.apache.flink.runtime.state.memory.ByteStreamStateHandle;\n import org.apache.flink.util.function.ThrowingConsumer;\n \n+import org.apache.flink.shaded.guava18.com.google.common.io.Closer;\n+\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n@@ -162,30 +164,27 @@ private void withInputGates(ThrowingConsumer<InputGate[], Exception> action) thr\n \t\tSingleInputGate[] gates = new SingleInputGate[parLevel];\n \t\tfinal int segmentsToAllocate = parLevel + parLevel * parLevel * buffersPerChannel;\n \t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(segmentsToAllocate, bufferSize);\n-\t\ttry {\n-\t\t\tfor (int i = 0; i < parLevel; i++) {\n-\t\t\t\tgates[i] = new SingleInputGateBuilder()\n-\t\t\t\t\t.setNumberOfChannels(parLevel)\n-\t\t\t\t\t.setSingleInputGateIndex(i)\n-\t\t\t\t\t.setBufferPoolFactory(networkBufferPool.createBufferPool(1, buffersPerChannel))\n-\t\t\t\t\t.setSegmentProvider(networkBufferPool)\n-\t\t\t\t\t.setChannelFactory((builder, gate) -> builder\n-\t\t\t\t\t.setNetworkBuffersPerChannel(buffersPerChannel)\n-\t\t\t\t\t.buildRemoteRecoveredChannel(gate))\n-\t\t\t\t\t.build();\n-\t\t\t\tgates[i].setup();\n-\t\t\t}\n-\t\t\taction.accept(gates);\n-\t\t} finally {\n-\t\t\tfor (InputGate inputGate: gates) {\n-\t\t\t\tinputGate.close();\n-\t\t\t}\n-\t\t\ttry {\n-\t\t\t\tassertEquals(segmentsToAllocate, networkBufferPool.getNumberOfAvailableMemorySegments());\n-\t\t\t} finally {\n-\t\t\t\tnetworkBufferPool.destroyAllBufferPools();\n-\t\t\t\tnetworkBufferPool.destroy();\n+\t\ttry (Closer poolCloser = Closer.create()) {\n+\t\t\tpoolCloser.register(networkBufferPool::destroy);\n+\t\t\tpoolCloser.register(networkBufferPool::destroyAllBufferPools);\n+\n+\t\t\ttry (Closer gateCloser = Closer.create()) {\n+\t\t\t\tfor (int i = 0; i < parLevel; i++) {\n+\t\t\t\t\tgates[i] = new SingleInputGateBuilder()\n+\t\t\t\t\t\t.setNumberOfChannels(parLevel)\n+\t\t\t\t\t\t.setSingleInputGateIndex(i)\n+\t\t\t\t\t\t.setBufferPoolFactory(networkBufferPool.createBufferPool(1, buffersPerChannel))\n+\t\t\t\t\t\t.setSegmentProvider(networkBufferPool)\n+\t\t\t\t\t\t.setChannelFactory((builder, gate) -> builder\n+\t\t\t\t\t\t\t.setNetworkBuffersPerChannel(buffersPerChannel)\n+\t\t\t\t\t\t\t.buildRemoteRecoveredChannel(gate))\n+\t\t\t\t\t\t.build();\n+\t\t\t\t\tgates[i].setup();\n+\t\t\t\t\tgateCloser.register(gates[i]::close);\n+\t\t\t\t}\n+\t\t\t\taction.accept(gates);\n \t\t\t}\n+\t\t\tassertEquals(segmentsToAllocate, networkBufferPool.getNumberOfAvailableMemorySegments());\n \t\t}\n \t}\n ",
      "parent_sha": "8abb2599fb878b1b72c0a9d52b8cf956d9c5256d"
    }
  },
  {
    "oid": "9d2ae5572897f3e2d9089414261a250cfc2a2ab8",
    "message": "[FLINK-28902][tests] rename FileSystemJobResultStoreTestInternal to FileSystemJobResultStoreFileOperationsTest",
    "date": "2022-09-16T09:48:38Z",
    "url": "https://github.com/apache/flink/commit/9d2ae5572897f3e2d9089414261a250cfc2a2ab8",
    "details": {
      "sha": "cf7c052d02b52058ed0fadbaad5176e310ab27fc",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/highavailability/FileSystemJobResultStoreFileOperationsTest.java",
      "status": "renamed",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/flink/blob/9d2ae5572897f3e2d9089414261a250cfc2a2ab8/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fhighavailability%2FFileSystemJobResultStoreFileOperationsTest.java",
      "raw_url": "https://github.com/apache/flink/raw/9d2ae5572897f3e2d9089414261a250cfc2a2ab8/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fhighavailability%2FFileSystemJobResultStoreFileOperationsTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fhighavailability%2FFileSystemJobResultStoreFileOperationsTest.java?ref=9d2ae5572897f3e2d9089414261a250cfc2a2ab8",
      "patch": "@@ -44,7 +44,7 @@\n \n /** Tests for the internal {@link FileSystemJobResultStore} mechanisms. */\n @ExtendWith(TestLoggerExtension.class)\n-public class FileSystemJobResultStoreTestInternal {\n+public class FileSystemJobResultStoreFileOperationsTest {\n \n     private static final ObjectMapper MAPPER = JacksonMapperFactory.createObjectMapper();\n \n@@ -66,7 +66,7 @@ public void testValidEntryPathCreation() {\n         final Path entryParent =\n                 fileSystemJobResultStore.constructEntryPath(\"random-name\").getParent();\n         assertThat(entryParent)\n-                .extracting(FileSystemJobResultStoreTestInternal::stripSucceedingSlash)\n+                .extracting(FileSystemJobResultStoreFileOperationsTest::stripSucceedingSlash)\n                 .isEqualTo(stripSucceedingSlash(basePath));\n     }\n ",
      "previous_filename": "flink-runtime/src/test/java/org/apache/flink/runtime/highavailability/FileSystemJobResultStoreTestInternal.java",
      "parent_sha": "d268ea105cc65407c7c3e4004d620f698e7729a2"
    }
  },
  {
    "oid": "dff76e185ed39cd1123ae094ac73b1530d56ed52",
    "message": "Changed default testing log verbosity for TestBase.",
    "date": "2013-11-08T11:21:34Z",
    "url": "https://github.com/apache/flink/commit/dff76e185ed39cd1123ae094ac73b1530d56ed52",
    "details": {
      "sha": "41b68ff185e66d8a503a8aa6e985eb0d8e6d5ece",
      "filename": "pact/pact-tests/src/test/java/eu/stratosphere/pact/test/util/TestBase.java",
      "status": "modified",
      "additions": 3,
      "deletions": 2,
      "changes": 5,
      "blob_url": "https://github.com/apache/flink/blob/dff76e185ed39cd1123ae094ac73b1530d56ed52/pact%2Fpact-tests%2Fsrc%2Ftest%2Fjava%2Feu%2Fstratosphere%2Fpact%2Ftest%2Futil%2FTestBase.java",
      "raw_url": "https://github.com/apache/flink/raw/dff76e185ed39cd1123ae094ac73b1530d56ed52/pact%2Fpact-tests%2Fsrc%2Ftest%2Fjava%2Feu%2Fstratosphere%2Fpact%2Ftest%2Futil%2FTestBase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/pact%2Fpact-tests%2Fsrc%2Ftest%2Fjava%2Feu%2Fstratosphere%2Fpact%2Ftest%2Futil%2FTestBase.java?ref=dff76e185ed39cd1123ae094ac73b1530d56ed52",
      "patch": "@@ -66,13 +66,14 @@ public abstract class TestBase {\n \tpublic TestBase(Configuration config, String clusterConfig) {\n \t\tthis.clusterConfig = clusterConfig;\n \t\tthis.config = config;\n+\t\t\n+\t\tLogUtils.initializeDefaultConsoleLogger(Level.WARN);\n+\t\t\n \t\tverifyJvmOptions();\n \t}\n \n \tpublic TestBase(Configuration testConfig) {\n \t\tthis(testConfig, Constants.DEFAULT_TEST_CONFIG);\n-\t\t\n-\t\tLogUtils.initializeDefaultConsoleLogger(Level.WARN);\n \t}\n \n \tprivate void verifyJvmOptions() {",
      "parent_sha": "710b12ab97ddf8f8f996959afedb1663da3c1252"
    }
  },
  {
    "oid": "fe2b2d63857816f3d3cb1a2749bc0bef33b68f96",
    "message": "[hotfix] Ignore empty yarn DYNAMIC_PROPERTIES when creating configuration",
    "date": "2019-11-21T12:56:37Z",
    "url": "https://github.com/apache/flink/commit/fe2b2d63857816f3d3cb1a2749bc0bef33b68f96",
    "details": {
      "sha": "362deda06d7ef9e4ce359caa6ebae113c0ba851b",
      "filename": "flink-yarn/src/main/java/org/apache/flink/yarn/cli/FlinkYarnSessionCli.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/flink/blob/fe2b2d63857816f3d3cb1a2749bc0bef33b68f96/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2Fcli%2FFlinkYarnSessionCli.java",
      "raw_url": "https://github.com/apache/flink/raw/fe2b2d63857816f3d3cb1a2749bc0bef33b68f96/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2Fcli%2FFlinkYarnSessionCli.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2Fcli%2FFlinkYarnSessionCli.java?ref=fe2b2d63857816f3d3cb1a2749bc0bef33b68f96",
      "patch": "@@ -416,7 +416,9 @@ private void applyDescriptorOptionToConfig(final CommandLine commandLine, final\n \t\t}\n \n \t\tfinal String dynamicPropertiesEncoded = encodeDynamicProperties(commandLine);\n-\t\tconfiguration.setString(YarnConfigOptionsInternal.DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n+\t\tif (dynamicPropertiesEncoded != null && !dynamicPropertiesEncoded.isEmpty()) {\n+\t\t\tconfiguration.setString(YarnConfigOptionsInternal.DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n+\t\t}\n \n \t\tfinal boolean detached = commandLine.hasOption(YARN_DETACHED_OPTION.getOpt()) || commandLine.hasOption(DETACHED_OPTION.getOpt());\n \t\tconfiguration.setBoolean(DeploymentOptions.ATTACHED, !detached);",
      "parent_sha": "3aed29a8c92a577e5e2361fae5bc02002add2e67"
    }
  },
  {
    "oid": "d1725a9a2d2d4c2780312ebe82f9d3a26f018cf8",
    "message": "[hotfix][statebackend] Removed use of rawtype access for generic collection",
    "date": "2018-04-19T09:12:22Z",
    "url": "https://github.com/apache/flink/commit/d1725a9a2d2d4c2780312ebe82f9d3a26f018cf8",
    "details": {
      "sha": "ad40c7055b9ffeb470b7aab3e827d315f1668127",
      "filename": "flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java",
      "status": "modified",
      "additions": 2,
      "deletions": 3,
      "changes": 5,
      "blob_url": "https://github.com/apache/flink/blob/d1725a9a2d2d4c2780312ebe82f9d3a26f018cf8/flink-state-backends%2Fflink-statebackend-rocksdb%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcontrib%2Fstreaming%2Fstate%2FRocksDBKeyedStateBackend.java",
      "raw_url": "https://github.com/apache/flink/raw/d1725a9a2d2d4c2780312ebe82f9d3a26f018cf8/flink-state-backends%2Fflink-statebackend-rocksdb%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcontrib%2Fstreaming%2Fstate%2FRocksDBKeyedStateBackend.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-state-backends%2Fflink-statebackend-rocksdb%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcontrib%2Fstreaming%2Fstate%2FRocksDBKeyedStateBackend.java?ref=d1725a9a2d2d4c2780312ebe82f9d3a26f018cf8",
      "patch": "@@ -1195,10 +1195,9 @@ protected <N, S> ColumnFamilyHandle getColumnFamily(\n \t\t\tthrow new IOException(\"Error creating ColumnFamilyHandle.\", e);\n \t\t}\n \n-\t\tTuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<N, S>> tuple =\n+\t\tTuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>> tuple =\n \t\t\tnew Tuple2<>(columnFamily, newMetaInfo);\n-\t\tMap rawAccess = kvStateInformation;\n-\t\trawAccess.put(descriptor.getName(), tuple);\n+\t\tkvStateInformation.put(descriptor.getName(), tuple);\n \t\treturn columnFamily;\n \t}\n ",
      "parent_sha": "1bfe87f12274a96e517f6229ddbc01622cfb50a0"
    }
  },
  {
    "oid": "5c17f4a38e730094801fe38ec376e9090e989ea1",
    "message": "[hotfix][network] Remove redundant word from comment",
    "date": "2021-05-04T11:23:04Z",
    "url": "https://github.com/apache/flink/commit/5c17f4a38e730094801fe38ec376e9090e989ea1",
    "details": {
      "sha": "86d05e45e5118e468a0349adb5ee47a213995e0d",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/CreditBasedSequenceNumberingViewReader.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/5c17f4a38e730094801fe38ec376e9090e989ea1/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fio%2Fnetwork%2Fnetty%2FCreditBasedSequenceNumberingViewReader.java",
      "raw_url": "https://github.com/apache/flink/raw/5c17f4a38e730094801fe38ec376e9090e989ea1/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fio%2Fnetwork%2Fnetty%2FCreditBasedSequenceNumberingViewReader.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fio%2Fnetwork%2Fnetty%2FCreditBasedSequenceNumberingViewReader.java?ref=5c17f4a38e730094801fe38ec376e9090e989ea1",
      "patch": "@@ -80,7 +80,7 @@ public void requestSubpartitionView(\n \n         synchronized (requestLock) {\n             if (subpartitionView == null) {\n-                // This this call can trigger a notification we have to\n+                // This call can trigger a notification we have to\n                 // schedule a separate task at the event loop that will\n                 // start consuming this. Otherwise the reference to the\n                 // view cannot be available in getNextBuffer().",
      "parent_sha": "368b2643eb073b69251745b8de4676b9fbc46321"
    }
  },
  {
    "oid": "13150a4ba26127b9ee2035fd3509b57bc3f7aa61",
    "message": "[FLINK-4631] Prevent NPE in TwoInputStreamTask\n\nCheck that the input processor has been created before cleaning it up.",
    "date": "2016-12-07T16:24:37Z",
    "url": "https://github.com/apache/flink/commit/13150a4ba26127b9ee2035fd3509b57bc3f7aa61",
    "details": {
      "sha": "233e9f10db0c809213cafdedac435b7c84af65ef",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/flink/blob/13150a4ba26127b9ee2035fd3509b57bc3f7aa61/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FTwoInputStreamTask.java",
      "raw_url": "https://github.com/apache/flink/raw/13150a4ba26127b9ee2035fd3509b57bc3f7aa61/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FTwoInputStreamTask.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FTwoInputStreamTask.java?ref=13150a4ba26127b9ee2035fd3509b57bc3f7aa61",
      "patch": "@@ -91,7 +91,9 @@ protected void run() throws Exception {\n \n \t@Override\n \tprotected void cleanup() throws Exception {\n-\t\tinputProcessor.cleanup();\n+\t\tif (inputProcessor != null) {\n+\t\t\tinputProcessor.cleanup();\n+\t\t}\n \t}\n \n \t@Override",
      "parent_sha": "4410c04a68c7b247bb3d7113e5f40f2a9c2165af"
    }
  },
  {
    "oid": "dfbf32ba66730388110316fe10e48edf28c83214",
    "message": "[hotfix][tests] Deduplicate code in ListCheckpointedTest",
    "date": "2018-08-17T07:33:03Z",
    "url": "https://github.com/apache/flink/commit/dfbf32ba66730388110316fe10e48edf28c83214",
    "details": {
      "sha": "d6d75910fbac915a39bc41a9a772973b759f5193",
      "filename": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/checkpoint/ListCheckpointedTest.java",
      "status": "modified",
      "additions": 7,
      "deletions": 16,
      "changes": 23,
      "blob_url": "https://github.com/apache/flink/blob/dfbf32ba66730388110316fe10e48edf28c83214/flink-streaming-java%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fcheckpoint%2FListCheckpointedTest.java",
      "raw_url": "https://github.com/apache/flink/raw/dfbf32ba66730388110316fe10e48edf28c83214/flink-streaming-java%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fcheckpoint%2FListCheckpointedTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fcheckpoint%2FListCheckpointedTest.java?ref=dfbf32ba66730388110316fe10e48edf28c83214",
      "patch": "@@ -37,31 +37,22 @@ public class ListCheckpointedTest {\n \n \t@Test\n \tpublic void testUDFReturningNull() throws Exception {\n-\t\tTestUserFunction userFunction = new TestUserFunction(null);\n-\t\tAbstractStreamOperatorTestHarness<Integer> testHarness =\n-\t\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamMap<>(userFunction), 1, 1, 0);\n-\t\ttestHarness.open();\n-\t\tOperatorSubtaskState snapshot = testHarness.snapshot(0L, 0L);\n-\t\ttestHarness.initializeState(snapshot);\n-\t\tAssert.assertTrue(userFunction.isRestored());\n+\t\ttestUDF(new TestUserFunction(null));\n \t}\n \n \t@Test\n \tpublic void testUDFReturningEmpty() throws Exception {\n-\t\tTestUserFunction userFunction = new TestUserFunction(Collections.<Integer>emptyList());\n-\t\tAbstractStreamOperatorTestHarness<Integer> testHarness =\n-\t\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamMap<>(userFunction), 1, 1, 0);\n-\t\ttestHarness.open();\n-\t\tOperatorSubtaskState snapshot = testHarness.snapshot(0L, 0L);\n-\t\ttestHarness.initializeState(snapshot);\n-\t\tAssert.assertTrue(userFunction.isRestored());\n+\t\ttestUDF(new TestUserFunction(Collections.<Integer>emptyList()));\n \t}\n \n \t@Test\n \tpublic void testUDFReturningData() throws Exception {\n-\t\tTestUserFunction userFunction = new TestUserFunction(Arrays.asList(1, 2, 3));\n+\t\ttestUDF(new TestUserFunction(Arrays.asList(1, 2, 3)));\n+\t}\n+\n+\tprivate static void testUDF(TestUserFunction userFunction) throws Exception {\n \t\tAbstractStreamOperatorTestHarness<Integer> testHarness =\n-\t\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamMap<>(userFunction), 1, 1, 0);\n+\t\t\tnew AbstractStreamOperatorTestHarness<>(new StreamMap<>(userFunction), 1, 1, 0);\n \t\ttestHarness.open();\n \t\tOperatorSubtaskState snapshot = testHarness.snapshot(0L, 0L);\n \t\ttestHarness.initializeState(snapshot);",
      "parent_sha": "8d88ff514ca892b1782a1ac724bb097c25395fb7"
    }
  },
  {
    "oid": "ef839ff65687e33b2e0e8914ca745ff97b07bf8e",
    "message": "[FLINK-21186][network] Wrap IOException in UncheckedIOException in RecordWriterOutput",
    "date": "2021-12-31T11:50:29Z",
    "url": "https://github.com/apache/flink/commit/ef839ff65687e33b2e0e8914ca745ff97b07bf8e",
    "details": {
      "sha": "8a63074815f3f61ca9b9053d8881145e70855104",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/RecordWriterOutput.java",
      "status": "modified",
      "additions": 9,
      "deletions": 8,
      "changes": 17,
      "blob_url": "https://github.com/apache/flink/blob/ef839ff65687e33b2e0e8914ca745ff97b07bf8e/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Fio%2FRecordWriterOutput.java",
      "raw_url": "https://github.com/apache/flink/raw/ef839ff65687e33b2e0e8914ca745ff97b07bf8e/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Fio%2FRecordWriterOutput.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Fio%2FRecordWriterOutput.java?ref=ef839ff65687e33b2e0e8914ca745ff97b07bf8e",
      "patch": "@@ -36,6 +36,7 @@\n import org.apache.flink.util.OutputTag;\n \n import java.io.IOException;\n+import java.io.UncheckedIOException;\n \n import static org.apache.flink.util.Preconditions.checkNotNull;\n \n@@ -101,8 +102,8 @@ private <X> void pushToRecordWriter(StreamRecord<X> record) {\n \n         try {\n             recordWriter.emit(serializationDelegate);\n-        } catch (Exception e) {\n-            throw new RuntimeException(e.getMessage(), e);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e.getMessage(), e);\n         }\n     }\n \n@@ -117,8 +118,8 @@ public void emitWatermark(Watermark mark) {\n \n         try {\n             recordWriter.broadcastEmit(serializationDelegate);\n-        } catch (Exception e) {\n-            throw new RuntimeException(e.getMessage(), e);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e.getMessage(), e);\n         }\n     }\n \n@@ -129,8 +130,8 @@ public void emitWatermarkStatus(WatermarkStatus watermarkStatus) {\n             serializationDelegate.setInstance(watermarkStatus);\n             try {\n                 recordWriter.broadcastEmit(serializationDelegate);\n-            } catch (Exception e) {\n-                throw new RuntimeException(e.getMessage(), e);\n+            } catch (IOException e) {\n+                throw new UncheckedIOException(e.getMessage(), e);\n             }\n         }\n     }\n@@ -141,8 +142,8 @@ public void emitLatencyMarker(LatencyMarker latencyMarker) {\n \n         try {\n             recordWriter.randomEmit(serializationDelegate);\n-        } catch (Exception e) {\n-            throw new RuntimeException(e.getMessage(), e);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e.getMessage(), e);\n         }\n     }\n ",
      "parent_sha": "7ccd525a15c8f9bc1edba3e53b815acf6dc90fb2"
    }
  },
  {
    "oid": "9e165a86119520814bb8117fed1b46c987610fc7",
    "message": "[hotfix] wrap root exception when instantiating catalog functions\n\nthis closes #10412.",
    "date": "2019-12-04T18:39:40Z",
    "url": "https://github.com/apache/flink/commit/9e165a86119520814bb8117fed1b46c987610fc7",
    "details": {
      "sha": "8eacd56527941b8b81f85bcd969de596feb39e6b",
      "filename": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/functions/FunctionDefinitionUtil.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/flink/blob/9e165a86119520814bb8117fed1b46c987610fc7/flink-table%2Fflink-table-api-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Ffunctions%2FFunctionDefinitionUtil.java",
      "raw_url": "https://github.com/apache/flink/raw/9e165a86119520814bb8117fed1b46c987610fc7/flink-table%2Fflink-table-api-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Ffunctions%2FFunctionDefinitionUtil.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table%2Fflink-table-api-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Ffunctions%2FFunctionDefinitionUtil.java?ref=9e165a86119520814bb8117fed1b46c987610fc7",
      "patch": "@@ -32,8 +32,7 @@ public static FunctionDefinition createFunctionDefinition(String name, CatalogFu\n \t\t\tfunc = Thread.currentThread().getContextClassLoader().loadClass(catalogFunction.getClassName()).newInstance();\n \t\t} catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n \t\t\tthrow new IllegalStateException(\n-\t\t\t\tString.format(\"Failed instantiating '%s'\", catalogFunction.getClassName())\n-\t\t\t);\n+\t\t\t\tString.format(\"Failed instantiating '%s'\", catalogFunction.getClassName()), e);\n \t\t}\n \n \t\tUserDefinedFunction udf = (UserDefinedFunction) func;",
      "parent_sha": "2cad7434973941ea738cd8ecaa4e90661810a833"
    }
  },
  {
    "oid": "20041cafbfe500ed386e11da5d09f116e7a45b81",
    "message": "[FLINK-13662][kinesis] Relax timing requirements\n\nThis closes #10551.",
    "date": "2019-12-24T06:48:30Z",
    "url": "https://github.com/apache/flink/commit/20041cafbfe500ed386e11da5d09f116e7a45b81",
    "details": {
      "sha": "137d220b68e863243aacf83db32ca760fcff50ff",
      "filename": "flink-connectors/flink-connector-kinesis/src/test/java/org/apache/flink/streaming/connectors/kinesis/FlinkKinesisProducerTest.java",
      "status": "modified",
      "additions": 13,
      "deletions": 5,
      "changes": 18,
      "blob_url": "https://github.com/apache/flink/blob/20041cafbfe500ed386e11da5d09f116e7a45b81/flink-connectors%2Fflink-connector-kinesis%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Fkinesis%2FFlinkKinesisProducerTest.java",
      "raw_url": "https://github.com/apache/flink/raw/20041cafbfe500ed386e11da5d09f116e7a45b81/flink-connectors%2Fflink-connector-kinesis%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Fkinesis%2FFlinkKinesisProducerTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors%2Fflink-connector-kinesis%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Fkinesis%2FFlinkKinesisProducerTest.java?ref=20041cafbfe500ed386e11da5d09f116e7a45b81",
      "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.flink.api.common.serialization.SerializationSchema;\n import org.apache.flink.api.common.serialization.SimpleStringSchema;\n+import org.apache.flink.api.common.time.Deadline;\n import org.apache.flink.core.testutils.CheckedThread;\n import org.apache.flink.core.testutils.MultiShotLatch;\n import org.apache.flink.runtime.state.FunctionSnapshotContext;\n@@ -43,6 +44,7 @@\n import org.mockito.stubbing.Answer;\n \n import java.nio.ByteBuffer;\n+import java.time.Duration;\n import java.util.LinkedList;\n import java.util.List;\n \n@@ -276,6 +278,8 @@ public void go() throws Exception {\n \t */\n \t@Test(timeout = 10000)\n \tpublic void testBackpressure() throws Throwable {\n+\t\tfinal Deadline deadline = Deadline.fromNow(Duration.ofSeconds(10));\n+\n \t\tfinal DummyFlinkKinesisProducer<String> producer = new DummyFlinkKinesisProducer<>(new SimpleStringSchema());\n \t\tproducer.setQueueLimit(1);\n \n@@ -294,7 +298,7 @@ public void go() throws Exception {\n \t\t\t}\n \t\t};\n \t\tmsg1.start();\n-\t\tmsg1.trySync(100);\n+\t\tmsg1.trySync(deadline.timeLeftIfAny().toMillis());\n \t\tassertFalse(\"Flush triggered before reaching queue limit\", msg1.isAlive());\n \n \t\t// consume msg-1 so that queue is empty again\n@@ -307,7 +311,7 @@ public void go() throws Exception {\n \t\t\t}\n \t\t};\n \t\tmsg2.start();\n-\t\tmsg2.trySync(100);\n+\t\tmsg2.trySync(deadline.timeLeftIfAny().toMillis());\n \t\tassertFalse(\"Flush triggered before reaching queue limit\", msg2.isAlive());\n \n \t\tCheckedThread moreElementsThread = new CheckedThread() {\n@@ -321,19 +325,23 @@ public void go() throws Exception {\n \t\t};\n \t\tmoreElementsThread.start();\n \n-\t\tmoreElementsThread.trySync(100);\n \t\tassertTrue(\"Producer should still block, but doesn't\", moreElementsThread.isAlive());\n \n \t\t// consume msg-2 from the queue, leaving msg-3 in the queue and msg-4 blocked\n+\t\twhile (producer.getPendingRecordFutures().size() < 2) {\n+\t\t\tThread.sleep(50);\n+\t\t}\n \t\tproducer.getPendingRecordFutures().get(1).set(result);\n \n-\t\tmoreElementsThread.trySync(100);\n \t\tassertTrue(\"Producer should still block, but doesn't\", moreElementsThread.isAlive());\n \n \t\t// consume msg-3, blocked msg-4 can be inserted into the queue and block is released\n+\t\twhile (producer.getPendingRecordFutures().size() < 3) {\n+\t\t\tThread.sleep(50);\n+\t\t}\n \t\tproducer.getPendingRecordFutures().get(2).set(result);\n \n-\t\tmoreElementsThread.trySync(100);\n+\t\tmoreElementsThread.trySync(deadline.timeLeftIfAny().toMillis());\n \n \t\tassertFalse(\"Prodcuer still blocks although the queue is flushed\", moreElementsThread.isAlive());\n ",
      "parent_sha": "a62641a0918aaedbac6312293cf8826e4d11f300"
    }
  },
  {
    "oid": "7555feaebfbac6ee8d985c8ae100aeb700594960",
    "message": "[refactor] Move exception handling into the legacy source thread.",
    "date": "2021-08-10T06:34:49Z",
    "url": "https://github.com/apache/flink/commit/7555feaebfbac6ee8d985c8ae100aeb700594960",
    "details": {
      "sha": "4568e42cfa6e7d56578108fdaf10af4cab7b808e",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/SourceStreamTask.java",
      "status": "modified",
      "additions": 12,
      "deletions": 8,
      "changes": 20,
      "blob_url": "https://github.com/apache/flink/blob/7555feaebfbac6ee8d985c8ae100aeb700594960/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FSourceStreamTask.java",
      "raw_url": "https://github.com/apache/flink/raw/7555feaebfbac6ee8d985c8ae100aeb700594960/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FSourceStreamTask.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FSourceStreamTask.java?ref=7555feaebfbac6ee8d985c8ae100aeb700594960",
      "patch": "@@ -166,14 +166,7 @@ protected void processInput(MailboxDefaultAction.Controller controller) throws E\n                 .getCompletionFuture()\n                 .whenComplete(\n                         (Void ignore, Throwable sourceThreadThrowable) -> {\n-                            if (isCanceled()\n-                                    && ExceptionUtils.findThrowable(\n-                                                    sourceThreadThrowable,\n-                                                    InterruptedException.class)\n-                                            .isPresent()) {\n-                                mailboxProcessor.reportThrowable(\n-                                        new CancelTaskException(sourceThreadThrowable));\n-                            } else if (!wasStoppedExternally && sourceThreadThrowable != null) {\n+                            if (sourceThreadThrowable != null) {\n                                 mailboxProcessor.reportThrowable(sourceThreadThrowable);\n                             } else {\n                                 mailboxProcessor.suspend();\n@@ -299,6 +292,17 @@ public void run() {\n             } catch (Throwable t) {\n                 // Note, t can be also an InterruptedException\n                 completionFuture.completeExceptionally(t);\n+                if (isCanceled()\n+                        && ExceptionUtils.findThrowable(t, InterruptedException.class)\n+                                .isPresent()) {\n+                    completionFuture.completeExceptionally(new CancelTaskException(t));\n+                } else if (wasStoppedExternally) {\n+                    // swallow all exceptions if the source was stopped externally\n+                    completionFuture.complete(null);\n+                } else {\n+                    completionFuture.completeExceptionally(t);\n+                }\n+            }\n             }\n         }\n ",
      "parent_sha": "64570e4c56955713ca599fd1d7ae7be891a314c6"
    }
  },
  {
    "oid": "e65b12e2b53cd5f37880fb757069ac9d651f21b4",
    "message": "[hotfix][tests] Adds proper multi-threading to the test case\n\nThe old implementation of the test didn't run in a multi-threaded context. That meant that there was no real race condition that needed to be handled (because the same thread tried to access DefaultLeaderElectionService#lock).\n\nThe logic can be implemented by validating both, leaderElectionDriver and leadershipOperationExecutor.\n\nSigned-off-by: Matthias Pohl <matthias.pohl@aiven.io>",
    "date": "2023-07-09T16:10:52Z",
    "url": "https://github.com/apache/flink/commit/e65b12e2b53cd5f37880fb757069ac9d651f21b4",
    "details": {
      "sha": "01b405323cd4759495816037a5a7d42566d660ae",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionServiceTest.java",
      "status": "modified",
      "additions": 26,
      "deletions": 2,
      "changes": 28,
      "blob_url": "https://github.com/apache/flink/blob/e65b12e2b53cd5f37880fb757069ac9d651f21b4/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fleaderelection%2FDefaultLeaderElectionServiceTest.java",
      "raw_url": "https://github.com/apache/flink/raw/e65b12e2b53cd5f37880fb757069ac9d651f21b4/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fleaderelection%2FDefaultLeaderElectionServiceTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fleaderelection%2FDefaultLeaderElectionServiceTest.java?ref=e65b12e2b53cd5f37880fb757069ac9d651f21b4",
      "patch": "@@ -311,11 +311,33 @@ void testMultipleDriverCreations() throws Exception {\n     @Test\n     void testGrantCallWhileInstantiatingDriver() throws Exception {\n         final UUID expectedLeaderSessionID = UUID.randomUUID();\n+        final TestingLeaderElectionDriver.Builder driverBuilder =\n+                TestingLeaderElectionDriver.newNoOpBuilder();\n         try (final DefaultLeaderElectionService testInstance =\n                 new DefaultLeaderElectionService(\n                         listener -> {\n-                            listener.onGrantLeadership(expectedLeaderSessionID);\n-                            return TestingLeaderElectionDriver.newNoOpBuilder().build(listener);\n+                            final OneShotLatch waitForGrantTriggerLatch = new OneShotLatch();\n+                            // calling the grant from a separate thread will let the thread\n+                            // end in the service's lock until the registration of the\n+                            // contender is done\n+                            CompletableFuture.runAsync(\n+                                    () -> {\n+                                        waitForGrantTriggerLatch.trigger();\n+                                        listener.onGrantLeadership(expectedLeaderSessionID);\n+                                    });\n+\n+                            waitForGrantTriggerLatch.await();\n+\n+                            // we can't ensure easily that the future finally called the\n+                            // method that triggers the grant event: Waiting for the method\n+                            // to return would lead to a deadlock because the grant\n+                            // processing is going to wait for the\n+                            // DefaultLeaderElectionService#lock before it can submit the\n+                            // event processing. Adding a short sleep here is a nasty\n+                            // workaround to simulate the race condition.\n+                            Thread.sleep(100);\n+\n+                            return driverBuilder.build(listener);\n                         },\n                         fatalErrorHandlerExtension.getTestingFatalErrorHandler(),\n                         Executors.newDirectExecutorService())) {\n@@ -326,6 +348,8 @@ void testGrantCallWhileInstantiatingDriver() throws Exception {\n                     new TestingContender(\"unused-address\", leaderElection);\n             testingContender.startLeaderElection();\n \n+            testingContender.waitForLeader();\n+\n             assertThat(testingContender.getLeaderSessionID()).isEqualTo(expectedLeaderSessionID);\n \n             leaderElection.close();",
      "parent_sha": "d2e6b94b374a40551a5cb1a81af2683f4a9ce86d"
    }
  },
  {
    "oid": "f5476f294d280eea73b977e21862fb1c7ad0a2f1",
    "message": "[FLINK2392] Increase memory for TaskManager",
    "date": "2015-09-21T11:47:22Z",
    "url": "https://github.com/apache/flink/commit/f5476f294d280eea73b977e21862fb1c7ad0a2f1",
    "details": {
      "sha": "aac9fe1f27015e8fa9de3e0195aebd85d22dfc68",
      "filename": "flink-yarn-tests/src/main/java/org/apache/flink/yarn/YARNSessionFIFOITCase.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/flink/blob/f5476f294d280eea73b977e21862fb1c7ad0a2f1/flink-yarn-tests%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FYARNSessionFIFOITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/f5476f294d280eea73b977e21862fb1c7ad0a2f1/flink-yarn-tests%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FYARNSessionFIFOITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn-tests%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FYARNSessionFIFOITCase.java?ref=f5476f294d280eea73b977e21862fb1c7ad0a2f1",
      "patch": "@@ -614,7 +614,7 @@ public void testJavaAPI() {\n \t\tAssert.assertNotNull(\"unable to get yarn client\", flinkYarnClient);\n \t\tflinkYarnClient.setTaskManagerCount(1);\n \t\tflinkYarnClient.setJobManagerMemory(768);\n-\t\tflinkYarnClient.setTaskManagerMemory(768);\n+\t\tflinkYarnClient.setTaskManagerMemory(1024);\n \t\tflinkYarnClient.setLocalJarPath(new Path(flinkUberjar.getAbsolutePath()));\n \t\tString confDirPath = System.getenv(\"FLINK_CONF_DIR\");\n \t\tflinkYarnClient.setConfigurationDirectory(confDirPath);\n@@ -637,7 +637,6 @@ public void testJavaAPI() {\n \t\t\t\tThread.sleep(1000);\n \t\t\t} catch (InterruptedException e) {\n \t\t\t\tLOG.warn(\"Interrupted\", e);\n-\t\t\t\tThread.interrupted();\n \t\t\t}\n \t\t\tFlinkYarnClusterStatus status = yarnCluster.getClusterStatus();\n \t\t\tif(status != null && status.equals(expectedStatus)) {",
      "parent_sha": "1ff8d04c061670e3f226220f0fc08a6c6d4a1ceb"
    }
  },
  {
    "oid": "9bda229dd362091c9889c026fb77928451faa7c7",
    "message": "[hotfix][table api] Fix logger arguments in CatalogManager\n\nThis closes #9401",
    "date": "2019-08-26T08:23:26Z",
    "url": "https://github.com/apache/flink/commit/9bda229dd362091c9889c026fb77928451faa7c7",
    "details": {
      "sha": "564770978f69bd6b6839758265bff05ae0b7c107",
      "filename": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/flink/blob/9bda229dd362091c9889c026fb77928451faa7c7/flink-table%2Fflink-table-api-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fcatalog%2FCatalogManager.java",
      "raw_url": "https://github.com/apache/flink/raw/9bda229dd362091c9889c026fb77928451faa7c7/flink-table%2Fflink-table-api-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fcatalog%2FCatalogManager.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table%2Fflink-table-api-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fcatalog%2FCatalogManager.java?ref=9bda229dd362091c9889c026fb77928451faa7c7",
      "patch": "@@ -182,8 +182,8 @@ public void setCurrentDatabase(String databaseName) {\n \n \t\t\tLOG.info(\n \t\t\t\t\"Set the current default database as [{}] in the current default catalog [{}].\",\n-\t\t\t\tcurrentCatalogName,\n-\t\t\t\tcurrentDatabaseName);\n+\t\t\t\tcurrentDatabaseName,\n+\t\t\t\tcurrentCatalogName);\n \t\t}\n \t}\n ",
      "parent_sha": "2c6441bbbe67f063b8d5c202b56e78083cb40eee"
    }
  },
  {
    "oid": "10d52f268db3eda7ee1511ea30afb9a982644148",
    "message": "[hotfix] Fix javadoc link in ClusterClient#triggerSavepoint\n\nThis closes #5592.",
    "date": "2018-03-05T16:54:28Z",
    "url": "https://github.com/apache/flink/commit/10d52f268db3eda7ee1511ea30afb9a982644148",
    "details": {
      "sha": "1a783fc22138492f91857d167c915cf858036f7e",
      "filename": "flink-clients/src/main/java/org/apache/flink/client/program/ClusterClient.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/10d52f268db3eda7ee1511ea30afb9a982644148/flink-clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fclient%2Fprogram%2FClusterClient.java",
      "raw_url": "https://github.com/apache/flink/raw/10d52f268db3eda7ee1511ea30afb9a982644148/flink-clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fclient%2Fprogram%2FClusterClient.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fclient%2Fprogram%2FClusterClient.java?ref=10d52f268db3eda7ee1511ea30afb9a982644148",
      "patch": "@@ -692,7 +692,7 @@ public void stop(final JobID jobId) throws Exception {\n \n \t/**\n \t * Triggers a savepoint for the job identified by the job id. The savepoint will be written to the given savepoint\n-\t * directory, or {@link org.apache.flink.configuration.CoreOptions#SAVEPOINT_DIRECTORY} if it is null.\n+\t * directory, or {@link org.apache.flink.configuration.CheckpointingOptions#SAVEPOINT_DIRECTORY} if it is null.\n \t *\n \t * @param jobId job id\n \t * @param savepointDirectory directory the savepoint should be written to",
      "parent_sha": "c1b805050946f851123bca0c2b02fc82d025da96"
    }
  },
  {
    "oid": "26eb8bb29989b9e65dbd0afe9c107c285b98cf87",
    "message": "[FLINK-9240][tests] Harden WebFrontendITCase#testStopYarn()\n\nThis closes #5886.",
    "date": "2018-04-24T11:24:18Z",
    "url": "https://github.com/apache/flink/commit/26eb8bb29989b9e65dbd0afe9c107c285b98cf87",
    "details": {
      "sha": "994966e8aa4113ea9f11d8ff411bacfec51b954f",
      "filename": "flink-runtime-web/src/test/java/org/apache/flink/runtime/webmonitor/WebFrontendITCase.java",
      "status": "modified",
      "additions": 19,
      "deletions": 15,
      "changes": 34,
      "blob_url": "https://github.com/apache/flink/blob/26eb8bb29989b9e65dbd0afe9c107c285b98cf87/flink-runtime-web%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fwebmonitor%2FWebFrontendITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/26eb8bb29989b9e65dbd0afe9c107c285b98cf87/flink-runtime-web%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fwebmonitor%2FWebFrontendITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime-web%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fwebmonitor%2FWebFrontendITCase.java?ref=26eb8bb29989b9e65dbd0afe9c107c285b98cf87",
      "patch": "@@ -317,6 +317,8 @@ public void testStop() throws Exception {\n \t\t\t\t\"\\\"execution-config\\\":{\\\"execution-mode\\\":\\\"PIPELINED\\\",\\\"restart-strategy\\\":\\\"default\\\",\" +\n \t\t\t\t\"\\\"job-parallelism\\\":-1,\\\"object-reuse-mode\\\":false,\\\"user-config\\\":{}}}\", response.getContent());\n \t\t}\n+\n+\t\tBlockingInvokable.reset();\n \t}\n \n \t@Test\n@@ -347,25 +349,27 @@ public void testStopYarn() throws Exception {\n \t\tfinal FiniteDuration testTimeout = new FiniteDuration(2, TimeUnit.MINUTES);\n \t\tfinal Deadline deadline = testTimeout.fromNow();\n \n-\t\twhile (!getRunningJobs(CLUSTER.getClusterClient()).isEmpty()) {\n-\t\t\ttry (HttpTestClient client = new HttpTestClient(\"localhost\", CLUSTER.getWebUIPort())) {\n-\t\t\t\t// Request the file from the web server\n-\t\t\t\tclient.sendGetRequest(\"/jobs/\" + jid + \"/yarn-stop\", deadline.timeLeft());\n-\n-\t\t\t\tHttpTestClient.SimpleHttpResponse response = client\n-\t\t\t\t\t.getNextResponse(deadline.timeLeft());\n-\n-\t\t\t\tif (Objects.equals(MiniClusterResource.NEW_CODEBASE, System.getProperty(MiniClusterResource.CODEBASE_KEY))) {\n-\t\t\t\t\tassertEquals(HttpResponseStatus.ACCEPTED, response.getStatus());\n-\t\t\t\t} else {\n-\t\t\t\t\tassertEquals(HttpResponseStatus.OK, response.getStatus());\n-\t\t\t\t}\n-\t\t\t\tassertEquals(\"application/json; charset=UTF-8\", response.getType());\n-\t\t\t\tassertEquals(\"{}\", response.getContent());\n+\t\ttry (HttpTestClient client = new HttpTestClient(\"localhost\", CLUSTER.getWebUIPort())) {\n+\t\t\t// Request the file from the web server\n+\t\t\tclient.sendGetRequest(\"/jobs/\" + jid + \"/yarn-stop\", deadline.timeLeft());\n+\n+\t\t\tHttpTestClient.SimpleHttpResponse response = client\n+\t\t\t\t.getNextResponse(deadline.timeLeft());\n+\n+\t\t\tif (Objects.equals(MiniClusterResource.NEW_CODEBASE, System.getProperty(MiniClusterResource.CODEBASE_KEY))) {\n+\t\t\t\tassertEquals(HttpResponseStatus.ACCEPTED, response.getStatus());\n+\t\t\t} else {\n+\t\t\t\tassertEquals(HttpResponseStatus.OK, response.getStatus());\n \t\t\t}\n+\t\t\tassertEquals(\"application/json; charset=UTF-8\", response.getType());\n+\t\t\tassertEquals(\"{}\", response.getContent());\n+\t\t}\n \n+\t\t// wait for cancellation to finish\n+\t\twhile (!getRunningJobs(CLUSTER.getClusterClient()).isEmpty()) {\n \t\t\tThread.sleep(20);\n \t\t}\n+\n \t\tBlockingInvokable.reset();\n \t}\n ",
      "parent_sha": "0321eb35edc7571a71ace768fe38f61f0a909890"
    }
  },
  {
    "oid": "bc16d526b36a37915d01005f849e7c7faf805ee1",
    "message": "[FLINK-18075][kafka] Call open method of SerializationSchema in Kafka producer\n\nThe open method of SerializationSchema was not called in the universal\r\nKafka producer.\r\n\r\nThis closes #12450",
    "date": "2020-06-03T13:31:20Z",
    "url": "https://github.com/apache/flink/commit/bc16d526b36a37915d01005f849e7c7faf805ee1",
    "details": {
      "sha": "25b359cba9702179a33f785f9d692ba9bc1fd1f5",
      "filename": "flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer.java",
      "status": "modified",
      "additions": 10,
      "deletions": 1,
      "changes": 11,
      "blob_url": "https://github.com/apache/flink/blob/bc16d526b36a37915d01005f849e7c7faf805ee1/flink-connectors%2Fflink-connector-kafka%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Fkafka%2FFlinkKafkaProducer.java",
      "raw_url": "https://github.com/apache/flink/raw/bc16d526b36a37915d01005f849e7c7faf805ee1/flink-connectors%2Fflink-connector-kafka%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Fkafka%2FFlinkKafkaProducer.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors%2Fflink-connector-kafka%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Fkafka%2FFlinkKafkaProducer.java?ref=bc16d526b36a37915d01005f849e7c7faf805ee1",
      "patch": "@@ -759,12 +759,21 @@ public void onCompletion(RecordMetadata metadata, Exception exception) {\n \t\t}\n \n \t\tif (kafkaSchema != null) {\n-\t\t\tkafkaSchema.open(() -> getRuntimeContext().getMetricGroup().addGroup(\"user\"));\n+\t\t\tkafkaSchema.open(createSerializationInitContext());\n+\t\t}\n+\n+\t\tif (keyedSchema != null && keyedSchema instanceof KeyedSerializationSchemaWrapper) {\n+\t\t\t((KeyedSerializationSchemaWrapper<IN>) keyedSchema).getSerializationSchema()\n+\t\t\t\t.open(createSerializationInitContext());\n \t\t}\n \n \t\tsuper.open(configuration);\n \t}\n \n+\tprivate SerializationSchema.InitializationContext createSerializationInitContext() {\n+\t\treturn () -> getRuntimeContext().getMetricGroup().addGroup(\"user\");\n+\t}\n+\n \t@Override\n \tpublic void invoke(FlinkKafkaProducer.KafkaTransactionState transaction, IN next, Context context) throws FlinkKafkaException {\n \t\tcheckErroneous();",
      "parent_sha": "a3250c6a82a71f7bd9617cbf085bb40393576292"
    }
  },
  {
    "oid": "64c6bb9076ed34acc0fcc81ad57c9f3999cfdf2b",
    "message": "[FLINK-22471][connector-elasticsearch] Do not repeat default value",
    "date": "2021-04-28T08:58:48Z",
    "url": "https://github.com/apache/flink/commit/64c6bb9076ed34acc0fcc81ad57c9f3999cfdf2b",
    "details": {
      "sha": "a595055aa2fe648cae87bf81af6f1d4c378cc8d7",
      "filename": "flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/ElasticsearchOptions.java",
      "status": "modified",
      "additions": 2,
      "deletions": 3,
      "changes": 5,
      "blob_url": "https://github.com/apache/flink/blob/64c6bb9076ed34acc0fcc81ad57c9f3999cfdf2b/flink-connectors%2Fflink-connector-elasticsearch-base%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Felasticsearch%2Ftable%2FElasticsearchOptions.java",
      "raw_url": "https://github.com/apache/flink/raw/64c6bb9076ed34acc0fcc81ad57c9f3999cfdf2b/flink-connectors%2Fflink-connector-elasticsearch-base%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Felasticsearch%2Ftable%2FElasticsearchOptions.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors%2Fflink-connector-elasticsearch-base%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Felasticsearch%2Ftable%2FElasticsearchOptions.java?ref=64c6bb9076ed34acc0fcc81ad57c9f3999cfdf2b",
      "patch": "@@ -143,9 +143,8 @@ public enum BackOffType {\n                     .stringType()\n                     .defaultValue(\"json\")\n                     .withDescription(\n-                            \"Elasticsearch connector requires to specify a format.\\n\"\n-                                    + \"The format must produce a valid json document. \\n\"\n-                                    + \"By default uses built-in 'json' format. Please refer to Table Formats section for more details.\");\n+                            \"The format must produce a valid JSON document. \"\n+                                    + \"Please refer to the documentation on formats for more details.\");\n \n     private ElasticsearchOptions() {}\n }",
      "parent_sha": "ff6ef3cef83a0665071fb6f9e69271f9b1f58967"
    }
  },
  {
    "oid": "b89736b1dd6f350d16529def539f1a9ebac909f1",
    "message": "[FLINK-22037][benchmark] Remove the redundant blocking queue from DeployingTasksBenchmarkBase",
    "date": "2021-03-31T01:57:33Z",
    "url": "https://github.com/apache/flink/commit/b89736b1dd6f350d16529def539f1a9ebac909f1",
    "details": {
      "sha": "4498255a3db11ede520c843cfa541b0af603d561",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/benchmark/deploying/DeployingTasksBenchmarkBase.java",
      "status": "modified",
      "additions": 1,
      "deletions": 13,
      "changes": 14,
      "blob_url": "https://github.com/apache/flink/blob/b89736b1dd6f350d16529def539f1a9ebac909f1/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fscheduler%2Fbenchmark%2Fdeploying%2FDeployingTasksBenchmarkBase.java",
      "raw_url": "https://github.com/apache/flink/raw/b89736b1dd6f350d16529def539f1a9ebac909f1/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fscheduler%2Fbenchmark%2Fdeploying%2FDeployingTasksBenchmarkBase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fscheduler%2Fbenchmark%2Fdeploying%2FDeployingTasksBenchmarkBase.java?ref=b89736b1dd6f350d16529def539f1a9ebac909f1",
      "patch": "@@ -18,20 +18,16 @@\n \n package org.apache.flink.runtime.scheduler.benchmark.deploying;\n \n-import org.apache.flink.runtime.deployment.TaskDeploymentDescriptor;\n import org.apache.flink.runtime.executiongraph.Execution;\n import org.apache.flink.runtime.executiongraph.ExecutionGraph;\n import org.apache.flink.runtime.executiongraph.ExecutionJobVertex;\n import org.apache.flink.runtime.executiongraph.ExecutionVertex;\n-import org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway;\n import org.apache.flink.runtime.jobgraph.JobVertex;\n import org.apache.flink.runtime.jobmaster.LogicalSlot;\n import org.apache.flink.runtime.jobmaster.TestingLogicalSlotBuilder;\n import org.apache.flink.runtime.scheduler.benchmark.JobConfiguration;\n \n import java.util.List;\n-import java.util.concurrent.ArrayBlockingQueue;\n-import java.util.concurrent.BlockingQueue;\n \n import static org.apache.flink.runtime.scheduler.benchmark.SchedulerBenchmarkUtils.createAndInitExecutionGraph;\n import static org.apache.flink.runtime.scheduler.benchmark.SchedulerBenchmarkUtils.createDefaultJobVertices;\n@@ -41,22 +37,14 @@ public class DeployingTasksBenchmarkBase {\n \n     List<JobVertex> jobVertices;\n     ExecutionGraph executionGraph;\n-    BlockingQueue<TaskDeploymentDescriptor> taskDeploymentDescriptors;\n \n     public void createAndSetupExecutionGraph(JobConfiguration jobConfiguration) throws Exception {\n \n         jobVertices = createDefaultJobVertices(jobConfiguration);\n \n         executionGraph = createAndInitExecutionGraph(jobVertices, jobConfiguration);\n \n-        taskDeploymentDescriptors = new ArrayBlockingQueue<>(jobConfiguration.getParallelism() * 2);\n-\n-        final SimpleAckingTaskManagerGateway taskManagerGateway =\n-                new SimpleAckingTaskManagerGateway();\n-        taskManagerGateway.setSubmitConsumer(taskDeploymentDescriptors::offer);\n-\n-        final TestingLogicalSlotBuilder slotBuilder =\n-                new TestingLogicalSlotBuilder().setTaskManagerGateway(taskManagerGateway);\n+        final TestingLogicalSlotBuilder slotBuilder = new TestingLogicalSlotBuilder();\n \n         for (ExecutionJobVertex ejv : executionGraph.getVerticesTopologically()) {\n             for (ExecutionVertex ev : ejv.getTaskVertices()) {",
      "parent_sha": "570cf5bb35af79664f363fe7c9c340c57927f044"
    }
  },
  {
    "oid": "03828308e75a19a777b7c999bd5bc9b09388daa2",
    "message": "[FLINK-9552][iterations] fix not syncing on checkpoint lock before emitting records\n\nWe need to make sure that concurrent access to the RecordWriter is protected by\na lock. It seems that everything but the StreamIterationHead was synchronizing\non the checkpoint lock and hence we sync here as well.",
    "date": "2018-12-10T15:02:23Z",
    "url": "https://github.com/apache/flink/commit/03828308e75a19a777b7c999bd5bc9b09388daa2",
    "details": {
      "sha": "ecef7f0f20719f95e04fd51170a120a0c9a7eada",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/StreamIterationHead.java",
      "status": "modified",
      "additions": 8,
      "deletions": 4,
      "changes": 12,
      "blob_url": "https://github.com/apache/flink/blob/03828308e75a19a777b7c999bd5bc9b09388daa2/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FStreamIterationHead.java",
      "raw_url": "https://github.com/apache/flink/raw/03828308e75a19a777b7c999bd5bc9b09388daa2/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FStreamIterationHead.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FStreamIterationHead.java?ref=03828308e75a19a777b7c999bd5bc9b09388daa2",
      "patch": "@@ -76,8 +76,10 @@ protected void run() throws Exception {\n \n \t\t\t// If timestamps are enabled we make sure to remove cyclic watermark dependencies\n \t\t\tif (isSerializingTimestamps()) {\n-\t\t\t\tfor (RecordWriterOutput<OUT> output : outputs) {\n-\t\t\t\t\toutput.emitWatermark(new Watermark(Long.MAX_VALUE));\n+\t\t\t\tsynchronized (getCheckpointLock()) {\n+\t\t\t\t\tfor (RecordWriterOutput<OUT> output : outputs) {\n+\t\t\t\t\t\toutput.emitWatermark(new Watermark(Long.MAX_VALUE));\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n \n@@ -87,8 +89,10 @@ protected void run() throws Exception {\n \t\t\t\t\tdataChannel.take();\n \n \t\t\t\tif (nextRecord != null) {\n-\t\t\t\t\tfor (RecordWriterOutput<OUT> output : outputs) {\n-\t\t\t\t\t\toutput.collect(nextRecord);\n+\t\t\t\t\tsynchronized (getCheckpointLock()) {\n+\t\t\t\t\t\tfor (RecordWriterOutput<OUT> output : outputs) {\n+\t\t\t\t\t\t\toutput.collect(nextRecord);\n+\t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t\telse {",
      "parent_sha": "9bcad1184547ef408aba253bbc044acc440c0435"
    }
  },
  {
    "oid": "8a8060eaa3a12703b594d4f4c581fa14bd846b19",
    "message": "[FLINK-893] Inconsistent Iteration Step Function gives nonesense error message\n\nI adjusted the error message.\n\nAuthor: Markus Holzemer <markus.holzemer@gmx.de>\n\nCloses #50 from markus-h/iteration_translation_exception and squashes the following commits:\n\n34a6f87 [Markus Holzemer] adjusted error message when encountering a translation failure for iterations",
    "date": "2014-07-03T10:34:52Z",
    "url": "https://github.com/apache/flink/commit/8a8060eaa3a12703b594d4f4c581fa14bd846b19",
    "details": {
      "sha": "be3fdf70214c865dd40ce27c01e41ebf28622f52",
      "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/IterativeDataSet.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/8a8060eaa3a12703b594d4f4c581fa14bd846b19/stratosphere-java%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fapi%2Fjava%2FIterativeDataSet.java",
      "raw_url": "https://github.com/apache/flink/raw/8a8060eaa3a12703b594d4f4c581fa14bd846b19/stratosphere-java%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fapi%2Fjava%2FIterativeDataSet.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fapi%2Fjava%2FIterativeDataSet.java?ref=8a8060eaa3a12703b594d4f4c581fa14bd846b19",
      "patch": "@@ -137,6 +137,6 @@ public AggregatorRegistry getAggregators() {\n \t@Override\n \tprotected eu.stratosphere.api.common.operators.SingleInputOperator<T, T, ?> translateToDataFlow(Operator<T> input) {\n \t\t// All the translation magic happens when the iteration end is encountered.\n-\t\tthrow new UnsupportedOperationException(\"This should never happen.\");\n+\t\tthrow new RuntimeException(\"Error while creating the data flow plan for an iteration: The iteration end was not specified correctly.\");\n \t}\n }",
      "parent_sha": "3c8bdee86132e48f99825706b97e24b4e119dc1e"
    }
  },
  {
    "oid": "fb28673b247d941dbd1088a3de0158fe6b0f6d78",
    "message": "[hotfix] Use properly shaded guava version",
    "date": "2022-02-07T13:40:53Z",
    "url": "https://github.com/apache/flink/commit/fb28673b247d941dbd1088a3de0158fe6b0f6d78",
    "details": {
      "sha": "cc0a62cd05046d1cbfa6f01e4f6511c8450cda99",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/leaderelection/ZooKeeperMultipleComponentLeaderElectionDriverTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/fb28673b247d941dbd1088a3de0158fe6b0f6d78/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fleaderelection%2FZooKeeperMultipleComponentLeaderElectionDriverTest.java",
      "raw_url": "https://github.com/apache/flink/raw/fb28673b247d941dbd1088a3de0158fe6b0f6d78/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fleaderelection%2FZooKeeperMultipleComponentLeaderElectionDriverTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fleaderelection%2FZooKeeperMultipleComponentLeaderElectionDriverTest.java?ref=fb28673b247d941dbd1088a3de0158fe6b0f6d78",
      "patch": "@@ -32,8 +32,8 @@\n import org.apache.flink.util.TestLoggerExtension;\n import org.apache.flink.util.function.RunnableWithException;\n \n-import org.apache.flink.shaded.curator5.com.google.common.collect.Iterables;\n import org.apache.flink.shaded.curator5.org.apache.curator.framework.CuratorFramework;\n+import org.apache.flink.shaded.guava30.com.google.common.collect.Iterables;\n \n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.extension.ExtendWith;",
      "parent_sha": "69d9fade5b0e3629c84548c06f0624e9e66fdefe"
    }
  },
  {
    "oid": "5dbb6dda8662a1b13314780c34463ba5c7f3c020",
    "message": "[FLINK-9666][refactor] Use short-circuit logic in boolean contexts\n\nThis closes #6230.",
    "date": "2018-07-11T10:05:09Z",
    "url": "https://github.com/apache/flink/commit/5dbb6dda8662a1b13314780c34463ba5c7f3c020",
    "details": {
      "sha": "b7259de196ff209b40529bcea48690a74d1dce49",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/5dbb6dda8662a1b13314780c34463ba5c7f3c020/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fenvironment%2FStreamExecutionEnvironment.java",
      "raw_url": "https://github.com/apache/flink/raw/5dbb6dda8662a1b13314780c34463ba5c7f3c020/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fenvironment%2FStreamExecutionEnvironment.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fenvironment%2FStreamExecutionEnvironment.java?ref=5dbb6dda8662a1b13314780c34463ba5c7f3c020",
      "patch": "@@ -1604,7 +1604,7 @@ public static StreamExecutionEnvironment getExecutionEnvironment() {\n \t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n \t\tif (env instanceof ContextEnvironment) {\n \t\t\treturn new StreamContextEnvironment((ContextEnvironment) env);\n-\t\t} else if (env instanceof OptimizerPlanEnvironment | env instanceof PreviewPlanEnvironment) {\n+\t\t} else if (env instanceof OptimizerPlanEnvironment || env instanceof PreviewPlanEnvironment) {\n \t\t\treturn new StreamPlanEnvironment(env);\n \t\t} else {\n \t\t\treturn createLocalEnvironment();",
      "parent_sha": "cad6e4d396e7b901b8c83257312860021f01c060"
    }
  },
  {
    "oid": "1848a886d8af1d4bf457d61031edb8fc8565efd0",
    "message": "[FLINK-23713][tests] Harden SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted\n\nThis commit makes the SavepointITCase.testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted no longer\nassert for an internal exception message that is an implementation detail of Flink. Instead the test now checks\nthat all tasks are running again after failing to write the savepoint.\n\nThis closes #16818.",
    "date": "2021-08-15T15:13:45Z",
    "url": "https://github.com/apache/flink/commit/1848a886d8af1d4bf457d61031edb8fc8565efd0",
    "details": {
      "sha": "76664725117cf5cb622aa8c4de11e08e88883e9b",
      "filename": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/SavepointITCase.java",
      "status": "modified",
      "additions": 14,
      "deletions": 14,
      "changes": 28,
      "blob_url": "https://github.com/apache/flink/blob/1848a886d8af1d4bf457d61031edb8fc8565efd0/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSavepointITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/1848a886d8af1d4bf457d61031edb8fc8565efd0/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSavepointITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSavepointITCase.java?ref=1848a886d8af1d4bf457d61031edb8fc8565efd0",
      "patch": "@@ -76,6 +76,7 @@\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.functions.source.ParallelSourceFunction;\n import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n import org.apache.flink.streaming.api.functions.source.SourceFunction;\n import org.apache.flink.streaming.api.graph.StreamGraph;\n@@ -230,6 +231,7 @@ public void testStopWithSavepointWithDrainCallsFinishBeforeSnapshotState() throw\n         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n         env.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n         env.addSource(new InfiniteTestSource())\n+                .setParallelism(1)\n                 .name(\"Infinite Source\")\n                 .addSink(new FinishingSink<>())\n                 // different parallelism to break chaining and add some concurrent tasks\n@@ -701,17 +703,20 @@ public void testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted() thr\n         Configuration configuration = new Configuration();\n         configuration.setString(\n                 HighAvailabilityOptions.HA_MODE, FailingSyncSavepointHAFactory.class.getName());\n+        final int parallelism = 2;\n         MiniClusterWithClientResource cluster =\n                 new MiniClusterWithClientResource(\n                         new MiniClusterResourceConfiguration.Builder()\n+                                .setNumberSlotsPerTaskManager(parallelism)\n                                 .setConfiguration(configuration)\n                                 .build());\n \n         StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n-        env.setParallelism(1);\n-        env.getConfig().setRestartStrategy(RestartStrategies.noRestart());\n+        env.setParallelism(parallelism);\n+        env.getConfig()\n+                .setRestartStrategy(RestartStrategies.fixedDelayRestart(Integer.MAX_VALUE, 0L));\n         env.addSource(new InfiniteTestSource())\n-                .name(\"Infinite Source\")\n+                .name(\"Infinite test source\")\n                 .addSink(new DiscardingSink<>());\n \n         final JobGraph jobGraph = env.getStreamGraph().getJobGraph();\n@@ -726,17 +731,12 @@ public void testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted() thr\n                 client.stopWithSavepoint(jobGraph.getJobID(), true, savepointDir.getAbsolutePath())\n                         .get();\n                 fail(\"The future should fail exceptionally.\");\n-            } catch (ExecutionException e) {\n-                assertThrowable(\n-                        e,\n-                        ex ->\n-                                ex.getMessage()\n-                                        .startsWith(\n-                                                \"org.apache.flink.util.FlinkException: Inconsistent execution state\"\n-                                                        + \" after stopping with savepoint. At least one execution\"\n-                                                        + \" is still in one of the following states: FAILED. \"\n-                                                        + \"A global fail-over is triggered to recover the job\"));\n+            } catch (ExecutionException ignored) {\n+                // expected\n             }\n+\n+            // make sure that we restart all tasks after the savepoint failure\n+            waitUntilAllTasksAreRunning(cluster.getRestAddres(), jobGraph.getJobID());\n         } finally {\n             cluster.after();\n         }\n@@ -1087,7 +1087,7 @@ private JobGraph createJobGraph(int parallelism, int numberOfRetries, long resta\n         return env.getStreamGraph().getJobGraph();\n     }\n \n-    private static class InfiniteTestSource implements SourceFunction<Integer> {\n+    private static class InfiniteTestSource implements ParallelSourceFunction<Integer> {\n \n         private static final long serialVersionUID = 1L;\n         private volatile boolean running = true;",
      "parent_sha": "f64261c91b195ecdcd99975b51de540db89a3f48"
    }
  },
  {
    "oid": "862e7f0e9b11c8c218b0fe35fcdb192ea205e2fa",
    "message": "[hotfix] [distributed runtime] Add overflow check for ZooKeeper checkpoint ID counter.",
    "date": "2016-08-08T17:27:10Z",
    "url": "https://github.com/apache/flink/commit/862e7f0e9b11c8c218b0fe35fcdb192ea205e2fa",
    "details": {
      "sha": "12839c1af5b88017228b8429994f33a46ad2b8dd",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/ZooKeeperCheckpointIDCounter.java",
      "status": "modified",
      "additions": 7,
      "deletions": 2,
      "changes": 9,
      "blob_url": "https://github.com/apache/flink/blob/862e7f0e9b11c8c218b0fe35fcdb192ea205e2fa/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2FZooKeeperCheckpointIDCounter.java",
      "raw_url": "https://github.com/apache/flink/raw/862e7f0e9b11c8c218b0fe35fcdb192ea205e2fa/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2FZooKeeperCheckpointIDCounter.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2FZooKeeperCheckpointIDCounter.java?ref=862e7f0e9b11c8c218b0fe35fcdb192ea205e2fa",
      "patch": "@@ -131,8 +131,13 @@ public long getAndIncrement() throws Exception {\n \t\t\t}\n \n \t\t\tVersionedValue<Integer> current = sharedCount.getVersionedValue();\n+\t\t\tint newCount = current.getValue() + 1;\n \n-\t\t\tInteger newCount = current.getValue() + 1;\n+\t\t\tif (newCount < 0) {\n+\t\t\t\t// overflow and wrap around\n+\t\t\t\tthrow new Exception(\"Checkpoint counter overflow. ZooKeeper checkpoint counter only supports \" +\n+\t\t\t\t\t\t\"checkpoints Ids up to \" + Integer.MAX_VALUE);\n+\t\t\t}\n \n \t\t\tif (sharedCount.trySetCount(current, newCount)) {\n \t\t\t\treturn current.getValue();\n@@ -161,7 +166,7 @@ public void setCount(long newId) throws Exception {\n \t * Connection state listener. In case of {@link ConnectionState#SUSPENDED} or {@link\n \t * ConnectionState#LOST} we are not guaranteed to read a current count from ZooKeeper.\n \t */\n-\tprivate class SharedCountConnectionStateListener implements ConnectionStateListener {\n+\tprivate static class SharedCountConnectionStateListener implements ConnectionStateListener {\n \n \t\tprivate volatile ConnectionState lastState;\n ",
      "parent_sha": "f1e9daece516db98aad15818e90e4e3bf78a6e13"
    }
  },
  {
    "oid": "aa86a86252881d5320e658b6f1315de7a62fac73",
    "message": "[FLINK-8703][tests] Port KafkaShortRetentionTestBase to MiniClusterResource\n\nThis closes #5666.",
    "date": "2018-03-14T19:46:23Z",
    "url": "https://github.com/apache/flink/commit/aa86a86252881d5320e658b6f1315de7a62fac73",
    "details": {
      "sha": "15d972f5570f2f16502aa0296f9cd5ee02555beb",
      "filename": "flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaShortRetentionTestBase.java",
      "status": "modified",
      "additions": 17,
      "deletions": 24,
      "changes": 41,
      "blob_url": "https://github.com/apache/flink/blob/aa86a86252881d5320e658b6f1315de7a62fac73/flink-connectors%2Fflink-connector-kafka-base%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Fkafka%2FKafkaShortRetentionTestBase.java",
      "raw_url": "https://github.com/apache/flink/raw/aa86a86252881d5320e658b6f1315de7a62fac73/flink-connectors%2Fflink-connector-kafka-base%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Fkafka%2FKafkaShortRetentionTestBase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors%2Fflink-connector-kafka-base%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Fkafka%2FKafkaShortRetentionTestBase.java?ref=aa86a86252881d5320e658b6f1315de7a62fac73",
      "patch": "@@ -24,15 +24,14 @@\n import org.apache.flink.configuration.ConfigConstants;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.configuration.TaskManagerOptions;\n-import org.apache.flink.runtime.minicluster.LocalFlinkMiniCluster;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n-import org.apache.flink.streaming.util.TestStreamEnvironment;\n import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema;\n import org.apache.flink.streaming.util.serialization.KeyedSerializationSchemaWrapper;\n+import org.apache.flink.test.util.MiniClusterResource;\n import org.apache.flink.util.InstantiationUtil;\n \n import org.junit.AfterClass;\n@@ -68,21 +67,32 @@ public class KafkaShortRetentionTestBase implements Serializable {\n \n \tprivate static KafkaTestEnvironment kafkaServer;\n \tprivate static Properties standardProps;\n-\tprivate static LocalFlinkMiniCluster flink;\n+\n+\t@ClassRule\n+\tpublic static MiniClusterResource flink = new MiniClusterResource(\n+\t\tnew MiniClusterResource.MiniClusterResourceConfiguration(\n+\t\t\tgetConfiguration(),\n+\t\t\tNUM_TMS,\n+\t\t\tTM_SLOTS));\n \n \t@ClassRule\n \tpublic static TemporaryFolder tempFolder = new TemporaryFolder();\n \n \tprotected static Properties secureProps = new Properties();\n \n+\tprivate static Configuration getConfiguration() {\n+\t\tConfiguration flinkConfig = new Configuration();\n+\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16L);\n+\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n+\t\treturn flinkConfig;\n+\t}\n+\n \t@BeforeClass\n-\tpublic static void prepare() throws IOException, ClassNotFoundException {\n+\tpublic static void prepare() throws ClassNotFoundException {\n \t\tLOG.info(\"-------------------------------------------------------------------------\");\n \t\tLOG.info(\"    Starting KafkaShortRetentionTestBase \");\n \t\tLOG.info(\"-------------------------------------------------------------------------\");\n \n-\t\tConfiguration flinkConfig = new Configuration();\n-\n \t\t// dynamically load the implementation for the test\n \t\tClass<?> clazz = Class.forName(\"org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl\");\n \t\tkafkaServer = (KafkaTestEnvironment) InstantiationUtil.instantiate(clazz);\n@@ -101,26 +111,10 @@ public static void prepare() throws IOException, ClassNotFoundException {\n \t\tkafkaServer.prepare(kafkaServer.createConfig().setKafkaServerProperties(specificProperties));\n \n \t\tstandardProps = kafkaServer.getStandardProperties();\n-\n-\t\t// start also a re-usable Flink mini cluster\n-\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, NUM_TMS);\n-\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, TM_SLOTS);\n-\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16L);\n-\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n-\n-\t\tflink = new LocalFlinkMiniCluster(flinkConfig, false);\n-\t\tflink.start();\n-\n-\t\tTestStreamEnvironment.setAsContext(flink, PARALLELISM);\n \t}\n \n \t@AfterClass\n \tpublic static void shutDownServices() throws Exception {\n-\t\tTestStreamEnvironment.unsetAsContext();\n-\n-\t\tif (flink != null) {\n-\t\t\tflink.stop();\n-\t\t}\n \t\tkafkaServer.shutdown();\n \n \t\tsecureProps.clear();\n@@ -238,8 +232,7 @@ public void runFailOnAutoOffsetResetNone() throws Exception {\n \n \t\tkafkaServer.createTestTopic(topic, parallelism, 1);\n \n-\t\tfinal StreamExecutionEnvironment env =\n-\t\t\t\tStreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flink.getLeaderRPCPort());\n+\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n \t\tenv.setParallelism(parallelism);\n \t\tenv.setRestartStrategy(RestartStrategies.noRestart()); // fail immediately\n \t\tenv.getConfig().disableSysoutLogging();",
      "parent_sha": "511f388d9d000a7ac84d45f41bffde514caa21b5"
    }
  },
  {
    "oid": "9ed3d04aa1fe638281dc3ca0983b4aa04f247eff",
    "message": "[FLINK-33520] Avoid using FLINK_HOME to find test scripts",
    "date": "2023-11-13T15:00:22Z",
    "url": "https://github.com/apache/flink/commit/9ed3d04aa1fe638281dc3ca0983b4aa04f247eff",
    "details": {
      "sha": "04a6eafdffa57cce82e55811bc9c2017bb4f1045",
      "filename": "flink-external-resources/flink-external-resource-gpu/src/test/java/org/apache/flink/externalresource/gpu/GPUDriverTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/flink/blob/9ed3d04aa1fe638281dc3ca0983b4aa04f247eff/flink-external-resources%2Fflink-external-resource-gpu%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fexternalresource%2Fgpu%2FGPUDriverTest.java",
      "raw_url": "https://github.com/apache/flink/raw/9ed3d04aa1fe638281dc3ca0983b4aa04f247eff/flink-external-resources%2Fflink-external-resource-gpu%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fexternalresource%2Fgpu%2FGPUDriverTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-external-resources%2Fflink-external-resource-gpu%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fexternalresource%2Fgpu%2FGPUDriverTest.java?ref=9ed3d04aa1fe638281dc3ca0983b4aa04f247eff",
      "patch": "@@ -29,6 +29,7 @@\n import java.io.FileNotFoundException;\n import java.nio.file.Files;\n import java.nio.file.Path;\n+import java.nio.file.Paths;\n import java.util.Set;\n import java.util.UUID;\n \n@@ -39,7 +40,7 @@\n class GPUDriverTest {\n \n     private static final String TESTING_DISCOVERY_SCRIPT_PATH =\n-            \"src/test/resources/testing-gpu-discovery.sh\";\n+            Paths.get(\"src/test/resources/testing-gpu-discovery.sh\").toAbsolutePath().toString();\n \n     @Test\n     void testGPUDriverWithTestScript() throws Exception {",
      "parent_sha": "9f04df35f51d63df8d98cd49d7c2bc5090e26004"
    }
  },
  {
    "oid": "a238be16c2f3a95f634feaec534e502e2e3cd1ca",
    "message": "[FLINK-23311][tests] nicer error messages in PojoSerializerTest via hamcrest",
    "date": "2021-07-16T21:00:11Z",
    "url": "https://github.com/apache/flink/commit/a238be16c2f3a95f634feaec534e502e2e3cd1ca",
    "details": {
      "sha": "312630e4b69959ed6d955ebbe8f9b807813f1680",
      "filename": "flink-core/src/test/java/org/apache/flink/api/java/typeutils/runtime/PojoSerializerTest.java",
      "status": "modified",
      "additions": 5,
      "deletions": 3,
      "changes": 8,
      "blob_url": "https://github.com/apache/flink/blob/a238be16c2f3a95f634feaec534e502e2e3cd1ca/flink-core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fjava%2Ftypeutils%2Fruntime%2FPojoSerializerTest.java",
      "raw_url": "https://github.com/apache/flink/raw/a238be16c2f3a95f634feaec534e502e2e3cd1ca/flink-core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fjava%2Ftypeutils%2Fruntime%2FPojoSerializerTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fjava%2Ftypeutils%2Fruntime%2FPojoSerializerTest.java?ref=a238be16c2f3a95f634feaec534e502e2e3cd1ca",
      "patch": "@@ -48,6 +48,8 @@\n import java.util.Objects;\n import java.util.Random;\n \n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.MatcherAssert.assertThat;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n \n@@ -417,7 +419,7 @@ public void testReconfigureDifferentSubclassRegistrationOrder() throws Exception\n         TypeSerializerSchemaCompatibility<TestUserClass> compatResult =\n                 pojoSerializerConfigSnapshot.resolveSchemaCompatibility(pojoSerializer);\n         assertTrue(compatResult.isCompatibleWithReconfiguredSerializer());\n-        assertTrue(compatResult.getReconfiguredSerializer() instanceof PojoSerializer);\n+        assertThat(compatResult.getReconfiguredSerializer(), instanceOf(PojoSerializer.class));\n \n         // reconfigure - check reconfiguration result and that registration ids remains the same\n         // assertEquals(ReconfigureResult.COMPATIBLE,\n@@ -487,7 +489,7 @@ public void testReconfigureRepopulateNonregisteredSubclassSerializerCache() thro\n         TypeSerializerSchemaCompatibility<TestUserClass> compatResult =\n                 pojoSerializerConfigSnapshot.resolveSchemaCompatibility(pojoSerializer);\n         assertTrue(compatResult.isCompatibleWithReconfiguredSerializer());\n-        assertTrue(compatResult.getReconfiguredSerializer() instanceof PojoSerializer);\n+        assertThat(compatResult.getReconfiguredSerializer(), instanceOf(PojoSerializer.class));\n \n         PojoSerializer<TestUserClass> reconfiguredPojoSerializer =\n                 (PojoSerializer<TestUserClass>) compatResult.getReconfiguredSerializer();\n@@ -571,7 +573,7 @@ public void testReconfigureWithPreviouslyNonregisteredSubclasses() throws Except\n         TypeSerializerSchemaCompatibility<TestUserClass> compatResult =\n                 pojoSerializerConfigSnapshot.resolveSchemaCompatibility(pojoSerializer);\n         assertTrue(compatResult.isCompatibleWithReconfiguredSerializer());\n-        assertTrue(compatResult.getReconfiguredSerializer() instanceof PojoSerializer);\n+        assertThat(compatResult.getReconfiguredSerializer(), instanceOf(PojoSerializer.class));\n \n         PojoSerializer<TestUserClass> reconfiguredPojoSerializer =\n                 (PojoSerializer<TestUserClass>) compatResult.getReconfiguredSerializer();",
      "parent_sha": "f8e11e1e57af2d638959eefe455751bb7354780a"
    }
  },
  {
    "oid": "0a9fb05d2b094632b5c2589fa8379d6d593f1cf6",
    "message": "[FLINK-22471][table-runtime-blink] Remove repetition of default values",
    "date": "2021-04-28T08:58:48Z",
    "url": "https://github.com/apache/flink/commit/0a9fb05d2b094632b5c2589fa8379d6d593f1cf6",
    "details": {
      "sha": "aeaac4dc9c84c67e22eb0dc42baca195094304d8",
      "filename": "flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemOptions.java",
      "status": "modified",
      "additions": 4,
      "deletions": 5,
      "changes": 9,
      "blob_url": "https://github.com/apache/flink/blob/0a9fb05d2b094632b5c2589fa8379d6d593f1cf6/flink-table%2Fflink-table-runtime-blink%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Ffilesystem%2FFileSystemOptions.java",
      "raw_url": "https://github.com/apache/flink/raw/0a9fb05d2b094632b5c2589fa8379d6d593f1cf6/flink-table%2Fflink-table-runtime-blink%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Ffilesystem%2FFileSystemOptions.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table%2Fflink-table-runtime-blink%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Ffilesystem%2FFileSystemOptions.java?ref=0a9fb05d2b094632b5c2589fa8379d6d593f1cf6",
      "patch": "@@ -38,22 +38,21 @@ public class FileSystemOptions {\n                     .defaultValue(\"__DEFAULT_PARTITION__\")\n                     .withDescription(\n                             \"The default partition name in case the dynamic partition\"\n-                                    + \" column value is null/empty string\");\n+                                    + \" column value is null/empty string.\");\n \n     public static final ConfigOption<MemorySize> SINK_ROLLING_POLICY_FILE_SIZE =\n             key(\"sink.rolling-policy.file-size\")\n                     .memoryType()\n                     .defaultValue(MemorySize.ofMebiBytes(128))\n-                    .withDescription(\n-                            \"The maximum part file size before rolling (by default 128MB).\");\n+                    .withDescription(\"The maximum part file size before rolling.\");\n \n     public static final ConfigOption<Duration> SINK_ROLLING_POLICY_ROLLOVER_INTERVAL =\n             key(\"sink.rolling-policy.rollover-interval\")\n                     .durationType()\n                     .defaultValue(Duration.ofMinutes(30))\n                     .withDescription(\n                             \"The maximum time duration a part file can stay open before rolling\"\n-                                    + \" (by default 30 min to avoid to many small files). The frequency at which\"\n+                                    + \" (by default long enough to avoid too many small files). The frequency at which\"\n                                     + \" this is checked is controlled by the 'sink.rolling-policy.check-interval' option.\");\n \n     public static final ConfigOption<Duration> SINK_ROLLING_POLICY_CHECK_INTERVAL =\n@@ -71,7 +70,7 @@ public class FileSystemOptions {\n                     .withDescription(\n                             \"The option to enable shuffle data by dynamic partition fields in sink\"\n                                     + \" phase, this can greatly reduce the number of file for filesystem sink but may\"\n-                                    + \" lead data skew, the default value is disabled.\");\n+                                    + \" lead data skew.\");\n \n     public static final ConfigOption<Boolean> STREAMING_SOURCE_ENABLE =\n             key(\"streaming-source.enable\")",
      "parent_sha": "e77dfdf111488992068443c597a98a94d9fd837d"
    }
  },
  {
    "oid": "e1f5eade68b54f4771fe7ec50c5812c567e97174",
    "message": "[FLINK-14434][coordination] Dispatcher#createJobManagerRunner returns on creation succeed\n\nThis closes #9940.",
    "date": "2019-10-21T21:19:58Z",
    "url": "https://github.com/apache/flink/commit/e1f5eade68b54f4771fe7ec50c5812c567e97174",
    "details": {
      "sha": "c38ef28e6f5e614aec03f89a4ba33b42904a172c",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java",
      "status": "modified",
      "additions": 2,
      "deletions": 3,
      "changes": 5,
      "blob_url": "https://github.com/apache/flink/blob/e1f5eade68b54f4771fe7ec50c5812c567e97174/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fdispatcher%2FDispatcher.java",
      "raw_url": "https://github.com/apache/flink/raw/e1f5eade68b54f4771fe7ec50c5812c567e97174/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fdispatcher%2FDispatcher.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fdispatcher%2FDispatcher.java?ref=e1f5eade68b54f4771fe7ec50c5812c567e97174",
      "patch": "@@ -362,6 +362,7 @@ private CompletableFuture<Void> runJob(JobGraph jobGraph) {\n \t\tjobManagerRunnerFutures.put(jobGraph.getJobID(), jobManagerRunnerFuture);\n \n \t\treturn jobManagerRunnerFuture\n+\t\t\t.thenApply(FunctionUtils.uncheckedFunction(this::startJobManagerRunner))\n \t\t\t.thenApply(FunctionUtils.nullFn())\n \t\t\t.whenCompleteAsync(\n \t\t\t\t(ignored, throwable) -> {\n@@ -375,7 +376,7 @@ private CompletableFuture<Void> runJob(JobGraph jobGraph) {\n \tprivate CompletableFuture<JobManagerRunner> createJobManagerRunner(JobGraph jobGraph) {\n \t\tfinal RpcService rpcService = getRpcService();\n \n-\t\tfinal CompletableFuture<JobManagerRunner> jobManagerRunnerFuture = CompletableFuture.supplyAsync(\n+\t\treturn CompletableFuture.supplyAsync(\n \t\t\tCheckedSupplier.unchecked(() ->\n \t\t\t\tjobManagerRunnerFactory.createJobManagerRunner(\n \t\t\t\t\tjobGraph,\n@@ -387,8 +388,6 @@ private CompletableFuture<JobManagerRunner> createJobManagerRunner(JobGraph jobG\n \t\t\t\t\tnew DefaultJobManagerJobMetricGroupFactory(jobManagerMetricGroup),\n \t\t\t\t\tfatalErrorHandler)),\n \t\t\trpcService.getExecutor());\n-\n-\t\treturn jobManagerRunnerFuture.thenApply(FunctionUtils.uncheckedFunction(this::startJobManagerRunner));\n \t}\n \n \tprivate JobManagerRunner startJobManagerRunner(JobManagerRunner jobManagerRunner) throws Exception {",
      "parent_sha": "e870235e6c8061ecb8a4364942396fd988e31cef"
    }
  },
  {
    "oid": "8430068db0d9fdf6e37cffd61143e709d5ecfff5",
    "message": "[FLINK-30613][serializer] Migrate BinaryRowDataSerializer to implement new method of resolving schema compatibility",
    "date": "2024-01-15T02:09:58Z",
    "url": "https://github.com/apache/flink/commit/8430068db0d9fdf6e37cffd61143e709d5ecfff5",
    "details": {
      "sha": "324482b8da5f14f82516f9059d0cc089d2490c5e",
      "filename": "flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/typeutils/BinaryRowDataSerializer.java",
      "status": "modified",
      "additions": 10,
      "deletions": 10,
      "changes": 20,
      "blob_url": "https://github.com/apache/flink/blob/8430068db0d9fdf6e37cffd61143e709d5ecfff5/flink-table%2Fflink-table-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fruntime%2Ftypeutils%2FBinaryRowDataSerializer.java",
      "raw_url": "https://github.com/apache/flink/raw/8430068db0d9fdf6e37cffd61143e709d5ecfff5/flink-table%2Fflink-table-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fruntime%2Ftypeutils%2FBinaryRowDataSerializer.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table%2Fflink-table-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fruntime%2Ftypeutils%2FBinaryRowDataSerializer.java?ref=8430068db0d9fdf6e37cffd61143e709d5ecfff5",
      "patch": "@@ -339,15 +339,15 @@ public static final class BinaryRowDataSerializerSnapshot\n             implements TypeSerializerSnapshot<BinaryRowData> {\n         private static final int CURRENT_VERSION = 3;\n \n-        private int previousNumFields;\n+        private int numFields;\n \n         @SuppressWarnings(\"unused\")\n         public BinaryRowDataSerializerSnapshot() {\n             // this constructor is used when restoring from a checkpoint/savepoint.\n         }\n \n         BinaryRowDataSerializerSnapshot(int numFields) {\n-            this.previousNumFields = numFields;\n+            this.numFields = numFields;\n         }\n \n         @Override\n@@ -357,30 +357,30 @@ public int getCurrentVersion() {\n \n         @Override\n         public void writeSnapshot(DataOutputView out) throws IOException {\n-            out.writeInt(previousNumFields);\n+            out.writeInt(numFields);\n         }\n \n         @Override\n         public void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader)\n                 throws IOException {\n-            this.previousNumFields = in.readInt();\n+            this.numFields = in.readInt();\n         }\n \n         @Override\n         public TypeSerializer<BinaryRowData> restoreSerializer() {\n-            return new BinaryRowDataSerializer(previousNumFields);\n+            return new BinaryRowDataSerializer(numFields);\n         }\n \n         @Override\n         public TypeSerializerSchemaCompatibility<BinaryRowData> resolveSchemaCompatibility(\n-                TypeSerializer<BinaryRowData> newSerializer) {\n-            if (!(newSerializer instanceof BinaryRowDataSerializer)) {\n+                TypeSerializerSnapshot<BinaryRowData> oldSerializerSnapshot) {\n+            if (!(oldSerializerSnapshot instanceof BinaryRowDataSerializerSnapshot)) {\n                 return TypeSerializerSchemaCompatibility.incompatible();\n             }\n \n-            BinaryRowDataSerializer newBinaryRowSerializer =\n-                    (BinaryRowDataSerializer) newSerializer;\n-            if (previousNumFields != newBinaryRowSerializer.numFields) {\n+            BinaryRowDataSerializerSnapshot oldBinaryRowSerializerSnapshot =\n+                    (BinaryRowDataSerializerSnapshot) oldSerializerSnapshot;\n+            if (numFields != oldBinaryRowSerializerSnapshot.numFields) {\n                 return TypeSerializerSchemaCompatibility.incompatible();\n             } else {\n                 return TypeSerializerSchemaCompatibility.compatibleAsIs();",
      "parent_sha": "aefd483ff7f0da1cac86616994e5169c0765cf2a"
    }
  },
  {
    "oid": "281b8e744dd2f8677d41d9e97c9b8ceeeddff450",
    "message": "[FLINK-26555][runtime] Adds closing and flushing to OutputStream",
    "date": "2022-03-10T08:35:00Z",
    "url": "https://github.com/apache/flink/commit/281b8e744dd2f8677d41d9e97c9b8ceeeddff450",
    "details": {
      "sha": "363f93eed4a020ec9a391e3cbadba4b7726f44ec",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/highavailability/FileSystemJobResultStore.java",
      "status": "modified",
      "additions": 4,
      "deletions": 2,
      "changes": 6,
      "blob_url": "https://github.com/apache/flink/blob/281b8e744dd2f8677d41d9e97c9b8ceeeddff450/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fhighavailability%2FFileSystemJobResultStore.java",
      "raw_url": "https://github.com/apache/flink/raw/281b8e744dd2f8677d41d9e97c9b8ceeeddff450/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fhighavailability%2FFileSystemJobResultStore.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fhighavailability%2FFileSystemJobResultStore.java?ref=281b8e744dd2f8677d41d9e97c9b8ceeeddff450",
      "patch": "@@ -131,8 +131,10 @@ private Path constructCleanPath(JobID jobId) {\n     @Override\n     public void createDirtyResultInternal(JobResultEntry jobResultEntry) throws IOException {\n         final Path path = constructDirtyPath(jobResultEntry.getJobId());\n-        OutputStream os = fileSystem.create(path, FileSystem.WriteMode.NO_OVERWRITE);\n-        mapper.writeValue(os, new JsonJobResultEntry(jobResultEntry));\n+        try (OutputStream os = fileSystem.create(path, FileSystem.WriteMode.NO_OVERWRITE)) {\n+            mapper.writeValue(os, new JsonJobResultEntry(jobResultEntry));\n+            os.flush();\n+        }\n     }\n \n     @Override",
      "parent_sha": "22b025aaf370d8eac47778b69ec1798c0d09f861"
    }
  },
  {
    "oid": "363ea248a756c10e03544dd4664b7bb2ddde531f",
    "message": "Implement toString of CsvOutputFormat",
    "date": "2014-06-05T12:17:06Z",
    "url": "https://github.com/apache/flink/commit/363ea248a756c10e03544dd4664b7bb2ddde531f",
    "details": {
      "sha": "73bc14f67120bd683cc34bb2c146597b6e681978",
      "filename": "stratosphere-java/src/main/java/eu/stratosphere/api/java/io/CsvOutputFormat.java",
      "status": "modified",
      "additions": 21,
      "deletions": 17,
      "changes": 38,
      "blob_url": "https://github.com/apache/flink/blob/363ea248a756c10e03544dd4664b7bb2ddde531f/stratosphere-java%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fapi%2Fjava%2Fio%2FCsvOutputFormat.java",
      "raw_url": "https://github.com/apache/flink/raw/363ea248a756c10e03544dd4664b7bb2ddde531f/stratosphere-java%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fapi%2Fjava%2Fio%2FCsvOutputFormat.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/stratosphere-java%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fapi%2Fjava%2Fio%2FCsvOutputFormat.java?ref=363ea248a756c10e03544dd4664b7bb2ddde531f",
      "patch": "@@ -28,7 +28,6 @@\n import eu.stratosphere.types.TypeInformation;\n import eu.stratosphere.core.fs.Path;\n import eu.stratosphere.types.StringValue;\n-\n /**\n  * This is an OutputFormat to serialize {@link eu.stratosphere.api.java.tuple.Tuple}s to text. The output is\n  * structured by record delimiters and field delimiters as common in CSV files.\n@@ -40,11 +39,11 @@ public class CsvOutputFormat<T extends Tuple> extends FileOutputFormat<T> implem\n \n \t@SuppressWarnings(\"unused\")\n \tprivate static final Log LOG = LogFactory.getLog(CsvOutputFormat.class);\n-\t\n+\n \t// --------------------------------------------------------------------------------------------\n-\t\n+\n \tpublic static final String DEFAULT_LINE_DELIMITER = CsvInputFormat.DEFAULT_LINE_DELIMITER;\n-\t\n+\n \tpublic static final String DEFAULT_FIELD_DELIMITER = String.valueOf(CsvInputFormat.DEFAULT_FIELD_DELIMITER);\n \n \t// --------------------------------------------------------------------------------------------\n@@ -68,7 +67,7 @@ public class CsvOutputFormat<T extends Tuple> extends FileOutputFormat<T> implem\n \t/**\n \t * Creates an instance of CsvOutputFormat. Lines are separated by the newline character '\\n',\n \t * fields are separated by ','.\n-\t * \n+\t *\n \t * @param outputPath The path where the CSV file is written.\n \t */\n \tpublic CsvOutputFormat(Path outputPath) {\n@@ -78,7 +77,7 @@ public CsvOutputFormat(Path outputPath) {\n \t/**\n \t * Creates an instance of CsvOutputFormat. Lines are separated by the newline character '\\n',\n \t * fields by the given field delimiter.\n-\t * \n+\t *\n \t * @param outputPath The path where the CSV file is written.\n \t * @param fieldDelimiter\n \t *            The delimiter that is used to separate fields in a tuple.\n@@ -89,7 +88,7 @@ public CsvOutputFormat(Path outputPath, String fieldDelimiter) {\n \n \t/**\n \t * Creates an instance of CsvOutputFormat.\n-\t * \n+\t *\n \t * @param outputPath The path where the CSV file is written.\n \t * @param recordDelimiter\n \t *            The delimiter that is used to separate the tuples.\n@@ -101,6 +100,7 @@ public CsvOutputFormat(Path outputPath, String recordDelimiter, String fieldDeli\n \t\tif (recordDelimiter == null) {\n \t\t\tthrow new IllegalArgumentException(\"RecordDelmiter shall not be null.\");\n \t\t}\n+\n \t\tif (fieldDelimiter == null) {\n \t\t\tthrow new IllegalArgumentException(\"FieldDelimiter shall not be null.\");\n \t\t}\n@@ -109,13 +109,13 @@ public CsvOutputFormat(Path outputPath, String recordDelimiter, String fieldDeli\n \t\tthis.recordDelimiter = recordDelimiter;\n \t\tthis.allowNullValues = false;\n \t}\n-\t\n+\n \t/**\n \t * Configures the format to either allow null values (writing an empty field),\n \t * or to throw an exception when encountering a null field.\n \t * <p>\n \t * by default, null values are allowed.\n-\t * \n+\t *\n \t * @param allowNulls Flag to indicate whether the output format should accept null values.\n \t */\n \tpublic void setAllowNullValues(boolean allowNulls) {\n@@ -125,7 +125,7 @@ public void setAllowNullValues(boolean allowNulls) {\n \t/**\n \t * Sets the charset with which the CSV strings are written to the file.\n \t * If not specified, the output format uses the systems default character encoding.\n-\t * \n+\t *\n \t * @param charsetName The name of charset to use for encoding the output.\n \t */\n \tpublic void setCharsetName(String charsetName) {\n@@ -138,13 +138,13 @@ public void setCharsetName(String charsetName) {\n \t * all subclasses of the latter.\n \t * <p>\n \t * By default, strings are not quoted.\n-\t * \n+\t *\n \t * @param quoteStrings Flag indicating whether string fields should be quoted.\n \t */\n \tpublic void setQuoteStrings(boolean quoteStrings) {\n \t\tthis.quoteStrings = quoteStrings;\n \t}\n-\t\n+\n \t// --------------------------------------------------------------------------------------------\n \n \t@Override\n@@ -198,18 +198,22 @@ public void writeRecord(T element) throws IOException {\n \t\t// add the record delimiter\n \t\tthis.wrt.write(this.recordDelimiter);\n \t}\n-\t\n+\n \t// --------------------------------------------------------------------------------------------\n-\t\n-\t/**\n-\t * \n+\t@Override\n+\tpublic String toString() {\n+\t\treturn \"CsvOutputFormat (path: \" + this.getOutputFilePath() + \", delimiter: \" + this.fieldDelimiter + \")\";\n+\t}\n+\n+    /**\n+\t *\n \t * The purpose of this method is solely to check whether the data type to be processed\n \t * is in fact a tuple type.\n \t */\n \t@Override\n \tpublic void setInputType(TypeInformation<?> type) {\n \t\tif (!type.isTupleType()) {\n-\t\t\tthrow new InvalidProgramException(\"The \" + CsvOutputFormat.class.getSimpleName() + \n+\t\t\tthrow new InvalidProgramException(\"The \" + CsvOutputFormat.class.getSimpleName() +\n \t\t\t\t\" can only be used to write tuple data sets.\");\n \t\t}\n \t}",
      "parent_sha": "1ce11d3a38f0e310bb8287a51208ac2e42b13d63"
    }
  },
  {
    "oid": "cc443e12f1c704434731dd7ad0aae3a8d094cbf2",
    "message": "[hotfix] [javadocs] remove inexistent memory unit reference from MemorySize.java\n\nThere is no \"q\" unit on `MemoryUnit` enum",
    "date": "2020-09-14T11:29:09Z",
    "url": "https://github.com/apache/flink/commit/cc443e12f1c704434731dd7ad0aae3a8d094cbf2",
    "details": {
      "sha": "b119a80ae0f63e4f5ab08108870fd4b7b7173ad0",
      "filename": "flink-core/src/main/java/org/apache/flink/configuration/MemorySize.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/cc443e12f1c704434731dd7ad0aae3a8d094cbf2/flink-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fconfiguration%2FMemorySize.java",
      "raw_url": "https://github.com/apache/flink/raw/cc443e12f1c704434731dd7ad0aae3a8d094cbf2/flink-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fconfiguration%2FMemorySize.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fconfiguration%2FMemorySize.java?ref=cc443e12f1c704434731dd7ad0aae3a8d094cbf2",
      "patch": "@@ -349,7 +349,7 @@ private static boolean matchesAny(String str, MemoryUnit unit) {\n \t * <p>To make larger values more compact, the common size suffixes are supported:\n \t *\n \t * <ul>\n-\t *     <li>q or 1b or 1bytes (bytes)\n+\t *     <li>1b or 1bytes (bytes)\n \t *     <li>1k or 1kb or 1kibibytes (interpreted as kibibytes = 1024 bytes)\n \t *     <li>1m or 1mb or 1mebibytes (interpreted as mebibytes = 1024 kibibytes)\n \t *     <li>1g or 1gb or 1gibibytes (interpreted as gibibytes = 1024 mebibytes)",
      "parent_sha": "7d41f1b728ba12c8bc4a2886d6d172a5e80b0e08"
    }
  },
  {
    "oid": "c7f9129cf03d4e6e9089ba518a74efaf6bcaac68",
    "message": "Fixed ctor of ArrayNode thus tracing of Reduce and CoGroups",
    "date": "2012-10-12T15:21:21Z",
    "url": "https://github.com/apache/flink/commit/c7f9129cf03d4e6e9089ba518a74efaf6bcaac68",
    "details": {
      "sha": "47f2dd78b3c85434c783835d31229bea00059ac7",
      "filename": "sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/type/ArrayNode.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/flink/blob/c7f9129cf03d4e6e9089ba518a74efaf6bcaac68/sopremo%2Fsopremo-common%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fsopremo%2Ftype%2FArrayNode.java",
      "raw_url": "https://github.com/apache/flink/raw/c7f9129cf03d4e6e9089ba518a74efaf6bcaac68/sopremo%2Fsopremo-common%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fsopremo%2Ftype%2FArrayNode.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/sopremo%2Fsopremo-common%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fsopremo%2Ftype%2FArrayNode.java?ref=c7f9129cf03d4e6e9089ba518a74efaf6bcaac68",
      "patch": "@@ -49,12 +49,12 @@ public ArrayNode(final IJsonNode... nodes) {\n \t}\n \n \t/**\n-\t * Initializes an ArrayNode which cointains all {@link IJsonNode}s from the given Collection in proper sequence.\n+\t * Initializes an ArrayNode which cointains all {@link IJsonNode}s from the given Iterable in proper sequence.\n \t * \n \t * @param nodes\n \t *        a Collection of nodes that should be added to this ArrayNode\n \t */\n-\tpublic ArrayNode(final Collection<? extends IJsonNode> nodes) {\n+\tpublic ArrayNode(final Iterable<? extends IJsonNode> nodes) {\n \t\tthis();\n \t\tfor (final IJsonNode node : nodes)\n \t\t\tthis.children.add(node);\n@@ -170,7 +170,7 @@ public IJsonNode remove(final int index) {\n \tpublic void clear() {\n \t\tthis.children.clear();\n \t}\n-\t\n+\n \t@Override\n \tpublic int hashCode() {\n \t\tfinal int prime = 31;",
      "parent_sha": "050930350278b27ab84cb6a79ed78f27adaf7b0d"
    }
  },
  {
    "oid": "e10af1f7b164605dea7695e4418b5e61e4bd92df",
    "message": "[FLINK-23097][tests] Harden test_queryable_state_restart_tm.sh\n\nThis commit hardens the test_queryable_state_restart_tm.sh e2e test by only writing the\ncount value of state entries on notifyCheckpointComplete. Before we have written out uncommitted\ninformation out that led to test failures.\n\nThis closes #16910.",
    "date": "2021-08-23T08:54:31Z",
    "url": "https://github.com/apache/flink/commit/e10af1f7b164605dea7695e4418b5e61e4bd92df",
    "details": {
      "sha": "28207c900ac70b4bbf34a6d085fde53d03dfec9e",
      "filename": "flink-end-to-end-tests/flink-queryable-state-test/src/main/java/org/apache/flink/streaming/tests/queryablestate/QsStateProducer.java",
      "status": "modified",
      "additions": 8,
      "deletions": 4,
      "changes": 12,
      "blob_url": "https://github.com/apache/flink/blob/e10af1f7b164605dea7695e4418b5e61e4bd92df/flink-end-to-end-tests%2Fflink-queryable-state-test%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Ftests%2Fqueryablestate%2FQsStateProducer.java",
      "raw_url": "https://github.com/apache/flink/raw/e10af1f7b164605dea7695e4418b5e61e4bd92df/flink-end-to-end-tests%2Fflink-queryable-state-test%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Ftests%2Fqueryablestate%2FQsStateProducer.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-end-to-end-tests%2Fflink-queryable-state-test%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Ftests%2Fqueryablestate%2FQsStateProducer.java?ref=e10af1f7b164605dea7695e4418b5e61e4bd92df",
      "patch": "@@ -18,6 +18,7 @@\n package org.apache.flink.streaming.tests.queryablestate;\n \n import org.apache.flink.api.common.functions.RichFlatMapFunction;\n+import org.apache.flink.api.common.state.CheckpointListener;\n import org.apache.flink.api.common.state.MapState;\n import org.apache.flink.api.common.state.MapStateDescriptor;\n import org.apache.flink.api.common.typeinfo.TypeHint;\n@@ -138,7 +139,7 @@ public void cancel() {\n     }\n \n     private static class TestFlatMap extends RichFlatMapFunction<Email, Object>\n-            implements CheckpointedFunction {\n+            implements CheckpointedFunction, CheckpointListener {\n \n         private static final long serialVersionUID = 7821128115999005941L;\n \n@@ -165,11 +166,14 @@ public void flatMap(Email value, Collector<Object> out) throws Exception {\n         }\n \n         @Override\n-        public void snapshotState(FunctionSnapshotContext context) {\n-            System.out.println(\"Count on snapshot: \" + count); // we look for it in the test\n-        }\n+        public void snapshotState(FunctionSnapshotContext context) {}\n \n         @Override\n         public void initializeState(FunctionInitializationContext context) {}\n+\n+        @Override\n+        public void notifyCheckpointComplete(long checkpointId) throws Exception {\n+            System.out.println(\"Count on snapshot: \" + count); // we look for it in the test\n+        }\n     }\n }",
      "parent_sha": "c0a41fc5361bc0219f23d646328abb8777f319bb"
    }
  },
  {
    "oid": "5e73062cc980e172a84821085a75d520f4fb2a65",
    "message": "[FLINK-9443][streaming] Remove unused parameter in StreamGraphHasherV2#generateNodeLocalHash",
    "date": "2018-10-16T19:57:57Z",
    "url": "https://github.com/apache/flink/commit/5e73062cc980e172a84821085a75d520f4fb2a65",
    "details": {
      "sha": "d7c51bcbdeb165cb69139ff05d7dbac39d3a1f1f",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphHasherV2.java",
      "status": "modified",
      "additions": 6,
      "deletions": 8,
      "changes": 14,
      "blob_url": "https://github.com/apache/flink/blob/5e73062cc980e172a84821085a75d520f4fb2a65/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fgraph%2FStreamGraphHasherV2.java",
      "raw_url": "https://github.com/apache/flink/raw/5e73062cc980e172a84821085a75d520f4fb2a65/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fgraph%2FStreamGraphHasherV2.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fgraph%2FStreamGraphHasherV2.java?ref=5e73062cc980e172a84821085a75d520f4fb2a65",
      "patch": "@@ -67,7 +67,7 @@ public class StreamGraphHasherV2 implements StreamGraphHasher {\n \t *\n \t * <p>The generated hash is deterministic with respect to:\n \t * <ul>\n-\t *   <li>node-local properties (like parallelism, UDF, node ID),\n+\t *   <li>node-local properties (node ID),\n \t *   <li>chained output nodes, and\n \t *   <li>input nodes hashes\n \t * </ul>\n@@ -216,16 +216,15 @@ private byte[] generateDeterministicHash(\n \t\t// hashes as the ID. We cannot use the node's ID, because it is\n \t\t// assigned from a static counter. This will result in two identical\n \t\t// programs having different hashes.\n-\t\tgenerateNodeLocalHash(node, hasher, hashes.size());\n+\t\tgenerateNodeLocalHash(hasher, hashes.size());\n \n \t\t// Include chained nodes to hash\n \t\tfor (StreamEdge outEdge : node.getOutEdges()) {\n \t\t\tif (isChainable(outEdge, isChainingEnabled)) {\n-\t\t\t\tStreamNode chainedNode = outEdge.getTargetVertex();\n \n \t\t\t\t// Use the hash size again, because the nodes are chained to\n \t\t\t\t// this node. This does not add a hash for the chained nodes.\n-\t\t\t\tgenerateNodeLocalHash(chainedNode, hasher, hashes.size());\n+\t\t\t\tgenerateNodeLocalHash(hasher, hashes.size());\n \t\t\t}\n \t\t}\n \n@@ -265,15 +264,14 @@ private byte[] generateDeterministicHash(\n \t}\n \n \t/**\n-\t * Applies the {@link Hasher} to the {@link StreamNode} (only node local\n-\t * attributes are taken into account). The hasher encapsulates the current\n-\t * state of the hash.\n+\t * Applies the {@link Hasher} to the {@link StreamNode} . The hasher encapsulates\n+\t * the current state of the hash.\n \t *\n \t * <p>The specified ID is local to this node. We cannot use the\n \t * {@link StreamNode#id}, because it is incremented in a static counter.\n \t * Therefore, the IDs for identical jobs will otherwise be different.\n \t */\n-\tprivate void generateNodeLocalHash(StreamNode node, Hasher hasher, int id) {\n+\tprivate void generateNodeLocalHash(Hasher hasher, int id) {\n \t\t// This resolves conflicts for otherwise identical source nodes. BUT\n \t\t// the generated hash codes depend on the ordering of the nodes in the\n \t\t// stream graph.",
      "parent_sha": "2e3e820571145463ad01a86cd68b0ce844dcd3a2"
    }
  },
  {
    "oid": "a68dd419718b4304343c2b27dab94394c88c67b5",
    "message": " [FLINK-32831][table-planner] Add check to ensure all tests in RuntimeFilterITcase have applied runtime filter.\n\nThis closes #23216",
    "date": "2023-08-28T01:41:48Z",
    "url": "https://github.com/apache/flink/commit/a68dd419718b4304343c2b27dab94394c88c67b5",
    "details": {
      "sha": "df8da0aaf9846581ba3bc6dc8238877ccd30417b",
      "filename": "flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/runtime/batch/sql/RuntimeFilterITCase.java",
      "status": "modified",
      "additions": 16,
      "deletions": 3,
      "changes": 19,
      "blob_url": "https://github.com/apache/flink/blob/a68dd419718b4304343c2b27dab94394c88c67b5/flink-table%2Fflink-table-planner%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fplanner%2Fruntime%2Fbatch%2Fsql%2FRuntimeFilterITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/a68dd419718b4304343c2b27dab94394c88c67b5/flink-table%2Fflink-table-planner%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fplanner%2Fruntime%2Fbatch%2Fsql%2FRuntimeFilterITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table%2Fflink-table-planner%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fplanner%2Fruntime%2Fbatch%2Fsql%2FRuntimeFilterITCase.java?ref=a68dd419718b4304343c2b27dab94394c88c67b5",
      "patch": "@@ -41,6 +41,8 @@\n import java.util.Arrays;\n import java.util.stream.Stream;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n /** IT case for runtime filter. */\n class RuntimeFilterITCase extends BatchTestBase {\n \n@@ -121,8 +123,11 @@ void testSimpleRuntimeFilter(BatchShuffleMode shuffleMode, boolean ofcg) {\n         tEnv.getConfig()\n                 .set(ExecutionConfigOptions.TABLE_EXEC_OPERATOR_FUSION_CODEGEN_ENABLED, ofcg);\n         configBatchShuffleMode(tEnv.getConfig(), shuffleMode);\n+        String sql = \"select * from fact, dim where x = a and z >= 3\";\n+        // Check runtime filter is working\n+        assertThat(tEnv().explainSql(sql)).contains(\"RuntimeFilter\");\n         checkResult(\n-                \"select * from fact, dim where x = a and z = 3\",\n+                sql,\n                 JavaScalaConversionUtil.toScala(\n                         Arrays.asList(\n                                 Row.of(3, 4, 3, \"Hallo Welt wie gehts?\", 2, 3, 4, 3),\n@@ -148,8 +153,12 @@ void testRuntimeFilterWithBuildSidePushDown(BatchShuffleMode shuffleMode, boolea\n         tEnv.getConfig()\n                 .set(OptimizerConfigOptions.TABLE_OPTIMIZER_AGG_PHASE_STRATEGY, \"ONE_PHASE\");\n \n+        String sql =\n+                \"select * from fact join (select x, sum(z) from dim where z = 2 group by x) dimSide on x = a\";\n+        // Check runtime filter is working\n+        assertThat(tEnv().explainSql(sql)).contains(\"RuntimeFilter\");\n         checkResult(\n-                \"select * from fact join (select x, sum(z) from dim where z = 2 group by x) dimSide on x = a\",\n+                sql,\n                 JavaScalaConversionUtil.toScala(\n                         Arrays.asList(\n                                 Row.of(2, 2, 1, \"Hallo Welt\", 2, 2, 2),\n@@ -198,8 +207,12 @@ void testRuntimeFilterWithProbeSidePushDown(BatchShuffleMode shuffleMode, boolea\n                         FlinkRuntimeFilterProgramTest.SUITABLE_FACT_ROW_COUNT, 1, 1, 1),\n                 false);\n \n+        String sql =\n+                \"select * from fact, fact2, dim where fact.a = fact2.a and fact.a = dim.x and z = 2\";\n+        // Check runtime filter is working\n+        assertThat(tEnv().explainSql(sql)).contains(\"RuntimeFilter\");\n         checkResult(\n-                \"select * from fact, fact2, dim where fact.a = fact2.a and fact.a = dim.x and z = 2\",\n+                sql,\n                 JavaScalaConversionUtil.toScala(\n                         Arrays.asList(\n                                 Row.of(2, 2, 1, \"Hallo Welt\", 2, 2, 2, 2, 2, 2),",
      "parent_sha": "07888af59bbc2d24afa049e8a6aedcd9eb822986"
    }
  },
  {
    "oid": "918e445f5f1e76507002a0281f29a416428b8b31",
    "message": "[hotfix][tests] Fix compile error",
    "date": "2022-04-08T09:02:28Z",
    "url": "https://github.com/apache/flink/commit/918e445f5f1e76507002a0281f29a416428b8b31",
    "details": {
      "sha": "ccc0555ec444c33b814251eae4ed12470f9d4b08",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinatorTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 3,
      "changes": 5,
      "blob_url": "https://github.com/apache/flink/blob/918e445f5f1e76507002a0281f29a416428b8b31/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2FCheckpointCoordinatorTest.java",
      "raw_url": "https://github.com/apache/flink/raw/918e445f5f1e76507002a0281f29a416428b8b31/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2FCheckpointCoordinatorTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fcheckpoint%2FCheckpointCoordinatorTest.java?ref=918e445f5f1e76507002a0281f29a416428b8b31",
      "patch": "@@ -3635,7 +3635,7 @@ public void testTimeoutWhileCheckpointOperatorCoordinatorNotFinishing() throws E\n         ExecutionGraph graph =\n                 new CheckpointCoordinatorTestingUtils.CheckpointExecutionGraphBuilder()\n                         .addJobVertex(jobVertexID)\n-                        .build();\n+                        .build(EXECUTOR_RESOURCE.getExecutor());\n \n         CheckpointCoordinatorTestingUtils.MockOperatorCoordinatorCheckpointContext context =\n                 new CheckpointCoordinatorTestingUtils\n@@ -3650,14 +3650,13 @@ public void testTimeoutWhileCheckpointOperatorCoordinatorNotFinishing() throws E\n         ScheduledExecutorService executorService = Executors.newSingleThreadScheduledExecutor();\n         CheckpointCoordinator checkpointCoordinator =\n                 new CheckpointCoordinatorBuilder()\n-                        .setExecutionGraph(graph)\n                         .setCheckpointCoordinatorConfiguration(\n                                 CheckpointCoordinatorConfiguration.builder()\n                                         .setCheckpointTimeout(10)\n                                         .build())\n                         .setTimer(manuallyTriggeredScheduledExecutor)\n                         .setCoordinatorsToCheckpoint(Collections.singleton(context))\n-                        .build();\n+                        .build(graph);\n         try {\n             checkpointCoordinator.triggerCheckpoint(false);\n             manuallyTriggeredScheduledExecutor.triggerAll();",
      "parent_sha": "f1867d316860033af5e83ab13145ed775edb12b0"
    }
  },
  {
    "oid": "78292c0393a0690564d802557257e1938b81e449",
    "message": "Fixed problem in MockEnvironment",
    "date": "2012-06-12T16:37:53Z",
    "url": "https://github.com/apache/flink/commit/78292c0393a0690564d802557257e1938b81e449",
    "details": {
      "sha": "e73b848532fd95e251d15ab7afa68ce0cb52c272",
      "filename": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/test/util/MockEnvironment.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/flink/blob/78292c0393a0690564d802557257e1938b81e449/pact%2Fpact-runtime%2Fsrc%2Ftest%2Fjava%2Feu%2Fstratosphere%2Fpact%2Fruntime%2Ftest%2Futil%2FMockEnvironment.java",
      "raw_url": "https://github.com/apache/flink/raw/78292c0393a0690564d802557257e1938b81e449/pact%2Fpact-runtime%2Fsrc%2Ftest%2Fjava%2Feu%2Fstratosphere%2Fpact%2Fruntime%2Ftest%2Futil%2FMockEnvironment.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/pact%2Fpact-runtime%2Fsrc%2Ftest%2Fjava%2Feu%2Fstratosphere%2Fpact%2Fruntime%2Ftest%2Futil%2FMockEnvironment.java?ref=78292c0393a0690564d802557257e1938b81e449",
      "patch": "@@ -274,7 +274,7 @@ public int getNumberOfInputGates() {\n \t@Override\n \tpublic void registerOutputGate(final OutputGate<? extends Record> outputGate) {\n \n-\t\tthrow new IllegalStateException(\"registerOutputGate called on MockEnvironment\");\n+\t\t// Nothing to do here\n \t}\n \n \t/**\n@@ -283,7 +283,7 @@ public void registerOutputGate(final OutputGate<? extends Record> outputGate) {\n \t@Override\n \tpublic void registerInputGate(final InputGate<? extends Record> inputGate) {\n \n-\t\tthrow new IllegalStateException(\"registerInputGate called on MockEnvironment\");\n+\t\t// Nothing to do here\n \t}\n \n \t/**",
      "parent_sha": "d738c4b6e0267f4f2bcd721f041e56e5ee488744"
    }
  },
  {
    "oid": "5776709885d7eaa52df5fc08bbed65da6aab6001",
    "message": "[hotfix][table] Removed unnecessary casts in ApiExpressionUtils",
    "date": "2019-03-20T16:29:46Z",
    "url": "https://github.com/apache/flink/commit/5776709885d7eaa52df5fc08bbed65da6aab6001",
    "details": {
      "sha": "3cdc011e95f671a2b2ef14122463bc28d1f81972",
      "filename": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/expressions/ApiExpressionUtils.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/apache/flink/blob/5776709885d7eaa52df5fc08bbed65da6aab6001/flink-table%2Fflink-table-api-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fexpressions%2FApiExpressionUtils.java",
      "raw_url": "https://github.com/apache/flink/raw/5776709885d7eaa52df5fc08bbed65da6aab6001/flink-table%2Fflink-table-api-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fexpressions%2FApiExpressionUtils.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table%2Fflink-table-api-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fexpressions%2FApiExpressionUtils.java?ref=5776709885d7eaa52df5fc08bbed65da6aab6001",
      "patch": "@@ -105,10 +105,10 @@ public static Expression toMonthInterval(Expression e, int multiplier) {\n \n \tpublic static Expression toMilliInterval(Expression e, long multiplier) {\n \t\tfinal Optional<Expression> intInterval = extractValue(e, BasicTypeInfo.INT_TYPE_INFO)\n-\t\t\t.map((v) -> (Expression) valueLiteral(v * multiplier, TimeIntervalTypeInfo.INTERVAL_MILLIS));\n+\t\t\t.map((v) -> valueLiteral(v * multiplier, TimeIntervalTypeInfo.INTERVAL_MILLIS));\n \n \t\tfinal Optional<Expression> longInterval = extractValue(e, BasicTypeInfo.LONG_TYPE_INFO)\n-\t\t\t.map((v) -> (Expression) valueLiteral(v * multiplier, TimeIntervalTypeInfo.INTERVAL_MILLIS));\n+\t\t\t.map((v) -> valueLiteral(v * multiplier, TimeIntervalTypeInfo.INTERVAL_MILLIS));\n \n \t\tif (intInterval.isPresent()) {\n \t\t\treturn intInterval.get();\n@@ -128,10 +128,10 @@ public static Expression toMilliInterval(Expression e, long multiplier) {\n \n \tpublic static Expression toRowInterval(Expression e) {\n \t\tfinal Optional<Expression> intInterval = extractValue(e, BasicTypeInfo.INT_TYPE_INFO)\n-\t\t\t.map((v) -> (Expression) valueLiteral((long) v, RowIntervalTypeInfo.INTERVAL_ROWS));\n+\t\t\t.map((v) -> valueLiteral((long) v, RowIntervalTypeInfo.INTERVAL_ROWS));\n \n \t\tfinal Optional<Expression> longInterval = extractValue(e, BasicTypeInfo.LONG_TYPE_INFO)\n-\t\t\t.map((v) -> (Expression) valueLiteral(v, RowIntervalTypeInfo.INTERVAL_ROWS));\n+\t\t\t.map((v) -> valueLiteral(v, RowIntervalTypeInfo.INTERVAL_ROWS));\n \n \t\tif (intInterval.isPresent()) {\n \t\t\treturn intInterval.get();",
      "parent_sha": "81afd216ac414511ffe29db36ed5e052f9f943de"
    }
  },
  {
    "oid": "d31c4505cb86c11dd0aa14f834cb65fe07640246",
    "message": "[FLINK-35801] Fix SnapshotFileMergingCompatibilityITCase, wait for file deletion before quit (#25066)",
    "date": "2024-07-11T09:35:33Z",
    "url": "https://github.com/apache/flink/commit/d31c4505cb86c11dd0aa14f834cb65fe07640246",
    "details": {
      "sha": "c2f0f4f6196bb1ed3cede36e9155667b362f254c",
      "filename": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/SnapshotFileMergingCompatibilityITCase.java",
      "status": "modified",
      "additions": 70,
      "deletions": 39,
      "changes": 109,
      "blob_url": "https://github.com/apache/flink/blob/d31c4505cb86c11dd0aa14f834cb65fe07640246/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSnapshotFileMergingCompatibilityITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/d31c4505cb86c11dd0aa14f834cb65fe07640246/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSnapshotFileMergingCompatibilityITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSnapshotFileMergingCompatibilityITCase.java?ref=d31c4505cb86c11dd0aa14f834cb65fe07640246",
      "patch": "@@ -32,6 +32,7 @@\n import org.apache.flink.runtime.testutils.MiniClusterResourceConfiguration;\n import org.apache.flink.test.util.MiniClusterWithClientResource;\n import org.apache.flink.test.util.TestUtils;\n+import org.apache.flink.util.TernaryBoolean;\n import org.apache.flink.util.TestLogger;\n \n import org.junit.jupiter.api.io.TempDir;\n@@ -56,6 +57,8 @@\n  */\n public class SnapshotFileMergingCompatibilityITCase extends TestLogger {\n \n+    private static final long DELETE_TIMEOUT_MILLS = 60000;\n+\n     public static Collection<Object[]> parameters() {\n         return Arrays.asList(\n                 new Object[][] {\n@@ -150,6 +153,11 @@ private void testSwitchingFileMerging(\n                             true);\n             assertThat(secondCheckpoint).isNotNull();\n             verifyStateHandleType(secondCheckpoint, secondFileMergingSwitch);\n+            verifyCheckpointExistOrWaitDeleted(\n+                    firstCheckpoint,\n+                    determineFileExist(\n+                            restoreMode, firstFileMergingSwitch, secondFileMergingSwitch),\n+                    firstFileMergingSwitch);\n         } finally {\n             secondCluster.after();\n         }\n@@ -177,6 +185,11 @@ private void testSwitchingFileMerging(\n                             true);\n             assertThat(thirdCheckpoint).isNotNull();\n             verifyStateHandleType(thirdCheckpoint, secondFileMergingSwitch);\n+            verifyCheckpointExistOrWaitDeleted(\n+                    secondCheckpoint,\n+                    determineFileExist(\n+                            restoreMode, secondFileMergingSwitch, secondFileMergingSwitch),\n+                    secondFileMergingSwitch);\n         } finally {\n             thirdCluster.after();\n         }\n@@ -204,18 +217,16 @@ private void testSwitchingFileMerging(\n                             consecutiveCheckpoint,\n                             false);\n             assertThat(fourthCheckpoint).isNotNull();\n+            verifyCheckpointExistOrWaitDeleted(\n+                    thirdCheckpoint,\n+                    determineFileExist(\n+                            restoreMode, secondFileMergingSwitch, secondFileMergingSwitch),\n+                    secondFileMergingSwitch);\n+            verifyCheckpointExistOrWaitDeleted(\n+                    fourthCheckpoint, TernaryBoolean.FALSE, secondFileMergingSwitch);\n         } finally {\n             fourthCluster.after();\n         }\n-\n-        waitUntilNoJobThreads();\n-        verifyCheckpointExist(\n-                firstCheckpoint, restoreMode != RestoreMode.CLAIM, firstFileMergingSwitch);\n-        verifyCheckpointExist(\n-                secondCheckpoint, restoreMode != RestoreMode.CLAIM, secondFileMergingSwitch);\n-        verifyCheckpointExist(\n-                thirdCheckpoint, restoreMode != RestoreMode.CLAIM, secondFileMergingSwitch);\n-        verifyCheckpointExist(fourthCheckpoint, false, secondFileMergingSwitch);\n     }\n \n     private void verifyStateHandleType(String checkpointPath, boolean fileMergingEnabled)\n@@ -249,48 +260,68 @@ private void verifyStateHandleType(String checkpointPath, boolean fileMergingEna\n         assertThat(hasKeyedState).isTrue();\n     }\n \n-    private static void waitUntilNoJobThreads() throws InterruptedException {\n-        SecurityManager securityManager = System.getSecurityManager();\n-        ThreadGroup group =\n-                (securityManager != null)\n-                        ? securityManager.getThreadGroup()\n-                        : Thread.currentThread().getThreadGroup();\n-\n-        boolean jobThreads = true;\n-        while (jobThreads) {\n-            jobThreads = false;\n-            Thread[] activeThreads = new Thread[group.activeCount() * 2];\n-            group.enumerate(activeThreads);\n-            for (Thread thread : activeThreads) {\n-                if (thread != null\n-                        && thread != Thread.currentThread()\n-                        && thread.getName().contains(\"jobmanager\")) {\n-                    jobThreads = true;\n-                    Thread.sleep(500);\n-                    break;\n-                }\n+    private static TernaryBoolean determineFileExist(\n+            RestoreMode mode, boolean lastFileMergingEnabled, boolean thisFileMergingEnabled) {\n+        if (mode == RestoreMode.CLAIM) {\n+            if (lastFileMergingEnabled || thisFileMergingEnabled) {\n+                // file merging will not reference files from previous jobs.\n+                return TernaryBoolean.FALSE;\n+            } else {\n+                return TernaryBoolean.UNDEFINED;\n             }\n+        } else {\n+            return TernaryBoolean.TRUE;\n         }\n     }\n \n-    private void verifyCheckpointExist(\n-            String checkpointPath, boolean exist, boolean fileMergingEnabled) throws IOException {\n+    private static void verifyCheckpointExistOrWaitDeleted(\n+            String checkpointPath, TernaryBoolean exist, boolean fileMergingEnabled)\n+            throws Exception {\n         org.apache.flink.core.fs.Path checkpointDir =\n                 new org.apache.flink.core.fs.Path(checkpointPath);\n         FileSystem fs = checkpointDir.getFileSystem();\n-        assertThat(fs.exists(checkpointDir)).isEqualTo(exist);\n         org.apache.flink.core.fs.Path baseDir = checkpointDir.getParent();\n-        assertThat(fs.exists(baseDir)).isTrue();\n         org.apache.flink.core.fs.Path sharedFile =\n                 new org.apache.flink.core.fs.Path(baseDir, CHECKPOINT_SHARED_STATE_DIR);\n-        assertThat(fs.exists(sharedFile)).isTrue();\n-        assertThat(fs.listStatus(sharedFile) != null && fs.listStatus(sharedFile).length > 0)\n-                .isEqualTo(exist);\n         org.apache.flink.core.fs.Path taskOwnedFile =\n                 new org.apache.flink.core.fs.Path(baseDir, CHECKPOINT_TASK_OWNED_STATE_DIR);\n+        assertThat(fs.exists(baseDir)).isTrue();\n+        assertThat(fs.exists(sharedFile)).isTrue();\n         assertThat(fs.exists(taskOwnedFile)).isTrue();\n-        // Since there is no exclusive state, we should consider fileMergingEnabled.\n-        assertThat(fs.exists(taskOwnedFile) && fs.listStatus(taskOwnedFile).length > 0)\n-                .isEqualTo(exist && fileMergingEnabled);\n+        if (exist.equals(TernaryBoolean.TRUE)) {\n+            // should exist, just check\n+            assertThat(fs.exists(checkpointDir)).isTrue();\n+            assertThat(fs.listStatus(sharedFile) != null && fs.listStatus(sharedFile).length > 0)\n+                    .isTrue();\n+            // Since there is no exclusive state, we should consider fileMergingEnabled.\n+            assertThat(\n+                            fs.listStatus(taskOwnedFile) != null\n+                                    && fs.listStatus(taskOwnedFile).length > 0)\n+                    .isEqualTo(fileMergingEnabled);\n+        } else if (exist.equals(TernaryBoolean.FALSE)) {\n+            // should be cleaned, since the job io threads may work slow, we wait.\n+            long waited = 0L;\n+            boolean fileExist = true;\n+            while (fileExist) {\n+                try {\n+                    fileExist =\n+                            (fs.exists(checkpointDir)\n+                                    || (fs.listStatus(sharedFile) != null\n+                                            && fs.listStatus(sharedFile).length > 0)\n+                                    || (fs.listStatus(taskOwnedFile) != null\n+                                            && fs.listStatus(taskOwnedFile).length > 0));\n+                } catch (IOException e) {\n+                    // Sometimes it may happen that the files are being deleted while we list them,\n+                    // thus an IOException is raised.\n+                }\n+                if (fileExist) {\n+                    // We wait\n+                    Thread.sleep(500L);\n+                    waited += 500L;\n+                    // Or timeout\n+                    assertThat(waited).isLessThan(DELETE_TIMEOUT_MILLS);\n+                }\n+            }\n+        }\n     }\n }",
      "parent_sha": "4041b2458a9eaef265c4b91e201e0c691056da51"
    }
  },
  {
    "oid": "f16e37d08a192094bfbea6acc33cdfea13586e3a",
    "message": "Changed command line syntax",
    "date": "2012-10-12T15:21:14Z",
    "url": "https://github.com/apache/flink/commit/f16e37d08a192094bfbea6acc33cdfea13586e3a",
    "details": {
      "sha": "734a163d1203b3e95872769e41c7e2af5ba033d7",
      "filename": "meteor/meteor-client/src/main/java/eu/stratosphere/meteor/client/CLClient.java",
      "status": "modified",
      "additions": 42,
      "deletions": 35,
      "changes": 77,
      "blob_url": "https://github.com/apache/flink/blob/f16e37d08a192094bfbea6acc33cdfea13586e3a/meteor%2Fmeteor-client%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fmeteor%2Fclient%2FCLClient.java",
      "raw_url": "https://github.com/apache/flink/raw/f16e37d08a192094bfbea6acc33cdfea13586e3a/meteor%2Fmeteor-client%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fmeteor%2Fclient%2FCLClient.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/meteor%2Fmeteor-client%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fmeteor%2Fclient%2FCLClient.java?ref=f16e37d08a192094bfbea6acc33cdfea13586e3a",
      "patch": "@@ -18,6 +18,7 @@\n import java.io.FileInputStream;\n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.util.List;\n \n import org.apache.commons.cli.CommandLine;\n import org.apache.commons.cli.CommandLineParser;\n@@ -53,9 +54,6 @@ public CLClient() {\n \n \t@SuppressWarnings(\"static-access\")\n \tprivate void initOptions() {\n-\t\tthis.options.addOption(OptionBuilder.isRequired().\n-\t\t\twithArgName(\"file\").hasArg(true).\n-\t\t\twithDescription(\"Executes the given script\").create(\"f\"));\n \t\tthis.options.addOption(OptionBuilder.\n \t\t\twithArgName(\"config\").hasArg(true).\n \t\t\twithDescription(\"Uses the given configuration\").withLongOpt(\"configDir\").create());\n@@ -67,7 +65,7 @@ private void initOptions() {\n \t\t\twithDescription(\"Uses the specified port\").withLongOpt(\"port\").create());\n \t\tthis.options.addOption(OptionBuilder.\n \t\t\twithArgName(\"updateTime\").hasArg(true).\n-\t\t\twithDescription(\"Checks with the given update time for the current status\").withLongOpt(\"updateTime\").create());\n+\t\t\twithDescription(\"Checks with the given update time in ms for the current status\").withLongOpt(\"updateTime\").create());\n \t\tthis.options.addOption(OptionBuilder.\n \t\t\thasArg(false).\n \t\t\twithDescription(\"Waits until the script terminates on the server\").withLongOpt(\"wait\").create());\n@@ -79,37 +77,46 @@ public static void main(String[] args) {\n \n \tprivate void process(String[] args) {\n \t\tCommandLine cmd = this.parseOptions(args);\n-\t\tfinal SopremoPlan plan = this.parseScript(cmd);\n+\t\t@SuppressWarnings(\"unchecked\")\n+\t\tfinal List<String> scripts = cmd.getArgList();\n+\t\tif (scripts.size() == 0)\n+\t\t\tdealWithError(null, \"No scripts to execute\");\n+\t\t\n \t\tthis.configureClient(cmd);\n-\n-\t\tthis.sopremoClient.submit(plan, new StateListener() {\n-\t\t\t@Override\n-\t\t\tpublic void stateChanged(ExecutionState executionState, String detail) {\n-\t\t\t\tSystem.out.println();\n-\t\t\t\tswitch (executionState) {\n-\t\t\t\tcase ENQUEUED:\n-\t\t\t\t\tSystem.out.print(\"Submitted script\");\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase RUNNING:\n-\t\t\t\t\tSystem.out.print(\"Executing script\");\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase FINISHED:\n-\t\t\t\t\tSystem.out.print(detail);\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase ERROR:\t\t\t\n-\t\t\t\t\tSystem.out.print(detail);\n-\t\t\t\t\tbreak;\n+\t\tfor (final String script : scripts) {\n+\t\t\tfinal SopremoPlan plan = this.parseScript(script);\n+\n+\t\t\tthis.sopremoClient.submit(plan, new StateListener() {\n+\t\t\t\t@Override\n+\t\t\t\tpublic void stateChanged(ExecutionState executionState, String detail) {\n+\t\t\t\t\tSystem.out.println();\n+\t\t\t\t\tswitch (executionState) {\n+\t\t\t\t\tcase ENQUEUED:\n+\t\t\t\t\t\tSystem.out.print(\"Submitted script \" + script);\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase RUNNING:\n+\t\t\t\t\t\tSystem.out.print(\"Executing script \"  + script);\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase FINISHED:\n+\t\t\t\t\t\tSystem.out.print(detail);\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\tcase ERROR:\n+\t\t\t\t\t\tSystem.out.print(detail);\n+\t\t\t\t\t\tbreak;\n+\t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t}\n \n-\t\t\t/* (non-Javadoc)\n-\t\t\t * @see eu.stratosphere.sopremo.client.StateListener#stateNotChanged(eu.stratosphere.sopremo.execution.ExecutionResponse.ExecutionState, java.lang.String)\n-\t\t\t */\n-\t\t\t@Override\n-\t\t\tprotected void stateNotChanged(ExecutionState state, String detail) {\n-\t\t\t\tSystem.out.print(\".\");\n-\t\t\t}\n-\t\t}, cmd.hasOption(\"wait\"));\n+\t\t\t\t/*\n+\t\t\t\t * (non-Javadoc)\n+\t\t\t\t * @see eu.stratosphere.sopremo.client.StateListener#stateNotChanged(eu.stratosphere.sopremo.execution.\n+\t\t\t\t * ExecutionResponse.ExecutionState, java.lang.String)\n+\t\t\t\t */\n+\t\t\t\t@Override\n+\t\t\t\tprotected void stateNotChanged(ExecutionState state, String detail) {\n+\t\t\t\t\tSystem.out.print(\".\");\n+\t\t\t\t}\n+\t\t\t}, cmd.hasOption(\"wait\"));\n+\t\t}\n \n \t\tthis.sopremoClient.close();\n \t}\n@@ -151,8 +158,8 @@ protected void dealWithError(Exception e, final String message, Object... args)\n \t\tSystem.exit(1);\n \t}\n \n-\tprivate SopremoPlan parseScript(CommandLine cmd) {\n-\t\tFile file = new File(cmd.getOptionValue(\"f\"));\n+\tprivate SopremoPlan parseScript(String script) {\n+\t\tFile file = new File(script);\n \t\tif (!file.exists())\n \t\t\tthis.dealWithError(null, \"Given file %s not found\", file);\n \n@@ -171,7 +178,7 @@ protected CommandLine parseOptions(String[] args) {\n \t\t} catch (ParseException e) {\n \t\t\tSystem.err.println(\"Cannot process the given arguments: \" + e);\n \t\t\tHelpFormatter formatter = new HelpFormatter();\n-\t\t\tformatter.printHelp(\"meteor-client.sh\", this.options);\n+\t\t\tformatter.printHelp(\"meteor-client.sh <scripts>\", this.options);\n \t\t\tSystem.exit(1);\n \t\t\treturn null;\n \t\t}",
      "parent_sha": "1d94736e40f006ce3e92e8bacdcea2ff780f4b4c"
    }
  },
  {
    "oid": "a27d73a4c2c52ad120c88fc7947d743c7d8abd6e",
    "message": "[FLINK-14683] Fix RemoteStreamEnvironment's constructor",
    "date": "2019-12-19T18:37:09Z",
    "url": "https://github.com/apache/flink/commit/a27d73a4c2c52ad120c88fc7947d743c7d8abd6e",
    "details": {
      "sha": "1a5829b107d79ed9200213bf3985a3737ee3a692",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/RemoteStreamEnvironment.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/a27d73a4c2c52ad120c88fc7947d743c7d8abd6e/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fenvironment%2FRemoteStreamEnvironment.java",
      "raw_url": "https://github.com/apache/flink/raw/a27d73a4c2c52ad120c88fc7947d743c7d8abd6e/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fenvironment%2FRemoteStreamEnvironment.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fenvironment%2FRemoteStreamEnvironment.java?ref=a27d73a4c2c52ad120c88fc7947d743c7d8abd6e",
      "patch": "@@ -111,7 +111,7 @@ public RemoteStreamEnvironment(String host, int port, Configuration clientConfig\n \t *            The protocol must be supported by the {@link java.net.URLClassLoader}.\n \t */\n \tpublic RemoteStreamEnvironment(String host, int port, Configuration clientConfiguration, String[] jarFiles, URL[] globalClasspaths) {\n-\t\tthis(host, port, clientConfiguration, jarFiles, null, null);\n+\t\tthis(host, port, clientConfiguration, jarFiles, globalClasspaths, null);\n \t}\n \n \t/**",
      "parent_sha": "e72a5a7ea41b307a0b6393d1988a5ebf90720e23"
    }
  },
  {
    "oid": "ae0b496c4d0f55a7752a0900bf32d4f1215302c2",
    "message": "[FLINK-9045][REST][addendum] Batch createLocalEnvironmentWithWebUI starts on 8081",
    "date": "2018-04-18T21:02:35Z",
    "url": "https://github.com/apache/flink/commit/ae0b496c4d0f55a7752a0900bf32d4f1215302c2",
    "details": {
      "sha": "f209b45429143507603d37c1b4784bac464cf4d5",
      "filename": "flink-clients/src/main/java/org/apache/flink/client/LocalExecutor.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/flink/blob/ae0b496c4d0f55a7752a0900bf32d4f1215302c2/flink-clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fclient%2FLocalExecutor.java",
      "raw_url": "https://github.com/apache/flink/raw/ae0b496c4d0f55a7752a0900bf32d4f1215302c2/flink-clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fclient%2FLocalExecutor.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fclient%2FLocalExecutor.java?ref=ae0b496c4d0f55a7752a0900bf32d4f1215302c2",
      "patch": "@@ -126,7 +126,9 @@ private JobExecutorService createJobExecutorService(Configuration configuration)\n \t\tfinal JobExecutorService newJobExecutorService;\n \t\tif (CoreOptions.NEW_MODE.equals(configuration.getString(CoreOptions.MODE))) {\n \n-\t\t\tconfiguration.setInteger(RestOptions.REST_PORT, 0);\n+\t\t\tif (!configuration.contains(RestOptions.REST_PORT)) {\n+\t\t\t\tconfiguration.setInteger(RestOptions.REST_PORT, 0);\n+\t\t\t}\n \n \t\t\tfinal MiniClusterConfiguration miniClusterConfiguration = new MiniClusterConfiguration.Builder()\n \t\t\t\t.setConfiguration(configuration)",
      "parent_sha": "11509898f365dac32c2130d3295a9be8958842af"
    }
  },
  {
    "oid": "c24c7ec3332d0eb6ebb24eb70c9aabd055cc129f",
    "message": "[FLINK-5929] [tests] Fix SavepointITCase instability\n\nWhen shutting down the testing cluster it can happen that checkpoint\nfiles lingered around (checkpoints independent of the savepoint).\n\nThis commit deactives checkpointing for the test and uses count down\nlatches to track progress, which also reduces the test time.\n\nThis closes #3427",
    "date": "2017-02-28T17:59:10Z",
    "url": "https://github.com/apache/flink/commit/c24c7ec3332d0eb6ebb24eb70c9aabd055cc129f",
    "details": {
      "sha": "ee371dda7fbb5070fcdc9c10f4b5b68a109a86a7",
      "filename": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/SavepointITCase.java",
      "status": "modified",
      "additions": 62,
      "deletions": 138,
      "changes": 200,
      "blob_url": "https://github.com/apache/flink/blob/c24c7ec3332d0eb6ebb24eb70c9aabd055cc129f/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSavepointITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/c24c7ec3332d0eb6ebb24eb70c9aabd055cc129f/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSavepointITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSavepointITCase.java?ref=c24c7ec3332d0eb6ebb24eb70c9aabd055cc129f",
      "patch": "@@ -24,7 +24,7 @@\n import com.google.common.collect.HashMultimap;\n import com.google.common.collect.Multimap;\n import java.io.FileNotFoundException;\n-import org.apache.commons.io.FileUtils;\n+import java.util.concurrent.CountDownLatch;\n import org.apache.flink.api.common.JobID;\n import org.apache.flink.api.common.functions.MapFunction;\n import org.apache.flink.api.common.functions.RichFlatMapFunction;\n@@ -54,7 +54,6 @@\n import org.apache.flink.runtime.messages.JobManagerMessages.TriggerSavepoint;\n import org.apache.flink.runtime.messages.JobManagerMessages.TriggerSavepointSuccess;\n import org.apache.flink.runtime.state.ChainedStateHandle;\n-import org.apache.flink.runtime.state.CheckpointListener;\n import org.apache.flink.runtime.state.StreamStateHandle;\n import org.apache.flink.runtime.state.filesystem.FileStateHandle;\n import org.apache.flink.runtime.state.filesystem.FsStateBackend;\n@@ -95,7 +94,6 @@\n import java.util.List;\n import java.util.Random;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.TimeoutException;\n \n import static org.apache.flink.runtime.messages.JobManagerMessages.getDisposeSavepointSuccess;\n import static org.junit.Assert.assertEquals;\n@@ -116,12 +114,11 @@ public class SavepointITCase extends TestLogger {\n \tpublic TemporaryFolder folder = new TemporaryFolder();\n \n \t/**\n-\t * Tests that it is possible to submit a job, trigger a savepoint, and\n-\t * later restart the job on a new cluster. The savepoint is written to\n-\t * a file.\n+\t * Triggers a savepoint for a job that uses the FsStateBackend. We expect\n+\t * that all checkpoint files are written to a new savepoint directory.\n \t *\n \t * <ol>\n-\t * <li>Submit job, wait for some checkpoints to complete</li>\n+\t * <li>Submit job, wait for some progress</li>\n \t * <li>Trigger savepoint and verify that savepoint has been created</li>\n \t * <li>Shut down the cluster, re-submit the job from the savepoint,\n \t * verify that the initial state has been reset, and\n@@ -131,23 +128,13 @@ public class SavepointITCase extends TestLogger {\n \t * </ol>\n \t */\n \t@Test\n-\tpublic void testTriggerSavepointAndResume() throws Exception {\n+\tpublic void testTriggerSavepointAndResumeWithFileBasedCheckpoints() throws Exception {\n \t\t// Config\n-\t\tint numTaskManagers = 2;\n-\t\tint numSlotsPerTaskManager = 2;\n-\t\tint parallelism = numTaskManagers * numSlotsPerTaskManager;\n-\n-\t\t// Test deadline\n+\t\tfinal int numTaskManagers = 2;\n+\t\tfinal int numSlotsPerTaskManager = 2;\n+\t\tfinal int parallelism = numTaskManagers * numSlotsPerTaskManager;\n \t\tfinal Deadline deadline = new FiniteDuration(5, TimeUnit.MINUTES).fromNow();\n-\n-\t\t// The number of checkpoints to complete before triggering the savepoint\n-\t\tfinal int numberOfCompletedCheckpoints = 2;\n-\t\tfinal int checkpointingInterval = 100;\n-\n-\t\t// Temporary directory for file state backend\n-\t\tfinal File tmpDir = folder.newFolder();\n-\n-\t\tLOG.info(\"Created temporary directory: \" + tmpDir + \".\");\n+\t\tfinal File testRoot = folder.newFolder();\n \n \t\tTestingCluster flink = null;\n \n@@ -160,70 +147,51 @@ public void testTriggerSavepointAndResume() throws Exception {\n \t\t\tconfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, numTaskManagers);\n \t\t\tconfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, numSlotsPerTaskManager);\n \n-\t\t\tfinal File checkpointDir = new File(tmpDir, \"checkpoints\");\n-\t\t\tfinal File savepointRootDir = new File(tmpDir, \"savepoints\");\n+\t\t\tfinal File checkpointDir = new File(testRoot, \"checkpoints\");\n+\t\t\tfinal File savepointRootDir = new File(testRoot, \"savepoints\");\n \n \t\t\tif (!checkpointDir.mkdir() || !savepointRootDir.mkdirs()) {\n \t\t\t\tfail(\"Test setup failed: failed to create temporary directories.\");\n \t\t\t}\n \n-\t\t\tLOG.info(\"Created temporary checkpoint directory: \" + checkpointDir + \".\");\n-\t\t\tLOG.info(\"Created temporary savepoint directory: \" + savepointRootDir + \".\");\n-\n+\t\t\t// Use file based checkpoints\n \t\t\tconfig.setString(CoreOptions.STATE_BACKEND, \"filesystem\");\n-\t\t\tconfig.setString(FsStateBackendFactory.CHECKPOINT_DIRECTORY_URI_CONF_KEY,\n-\t\t\t\tcheckpointDir.toURI().toString());\n+\t\t\tconfig.setString(FsStateBackendFactory.CHECKPOINT_DIRECTORY_URI_CONF_KEY, checkpointDir.toURI().toString());\n \t\t\tconfig.setString(FsStateBackendFactory.MEMORY_THRESHOLD_CONF_KEY, \"0\");\n-\t\t\tconfig.setString(ConfigConstants.SAVEPOINT_DIRECTORY_KEY,\n-\t\t\t\tsavepointRootDir.toURI().toString());\n-\n-\t\t\tLOG.info(\"Flink configuration: \" + config + \".\");\n+\t\t\tconfig.setString(ConfigConstants.SAVEPOINT_DIRECTORY_KEY, savepointRootDir.toURI().toString());\n \n \t\t\t// Start Flink\n \t\t\tflink = new TestingCluster(config);\n-\t\t\tflink.start();\n-\n-\t\t\t// Retrieve the job manager\n-\t\t\tActorGateway jobManager = Await.result(\n-\t\t\t\tflink.leaderGateway().future(),\n-\t\t\t\tdeadline.timeLeft());\n+\t\t\tflink.start(true);\n \n \t\t\t// Submit the job\n-\t\t\tfinal JobGraph jobGraph = createJobGraph(parallelism, 0, 1000, checkpointingInterval);\n+\t\t\tfinal JobGraph jobGraph = createJobGraph(parallelism, 0, 1000);\n \t\t\tfinal JobID jobId = jobGraph.getJobID();\n \n-\t\t\t// Wait for the source to be notified about the expected number\n-\t\t\t// of completed checkpoints\n-\t\t\tStatefulCounter.resetForTest();\n+\t\t\t// Reset the static test job helpers\n+\t\t\tStatefulCounter.resetForTest(parallelism);\n+\n+\t\t\t// Retrieve the job manager\n+\t\t\tActorGateway jobManager = Await.result(flink.leaderGateway().future(), deadline.timeLeft());\n \n \t\t\tLOG.info(\"Submitting job \" + jobGraph.getJobID() + \" in detached mode.\");\n \n \t\t\tflink.submitJobDetached(jobGraph);\n \n-\t\t\tLOG.info(\"Waiting for \" + numberOfCompletedCheckpoints + \" checkpoint complete notifications.\");\n+\t\t\tLOG.info(\"Waiting for some progress.\");\n \n-\t\t\t// Wait...\n-\t\t\tStatefulCounter.awaitCompletedCheckpoints(parallelism, numberOfCompletedCheckpoints, deadline.timeLeft().toMillis());\n+\t\t\tStatefulCounter.getProgressLatch().await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n \n-\t\t\tLOG.info(\"Received all \" + numberOfCompletedCheckpoints +\n-\t\t\t\t\" checkpoint complete notifications.\");\n-\n-\t\t\t// ...and then trigger the savepoint\n \t\t\tLOG.info(\"Triggering a savepoint.\");\n-\n-\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(\n-\t\t\t\tnew TriggerSavepoint(jobId, Option.<String>empty()), deadline.timeLeft());\n-\n-\t\t\tfinal String savepointPath = ((TriggerSavepointSuccess) Await\n-\t\t\t\t.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n+\t\t\tFuture<Object> savepointPathFuture = jobManager.ask(new TriggerSavepoint(jobId, Option.<String>empty()), deadline.timeLeft());\n+\t\t\tfinal String savepointPath = ((TriggerSavepointSuccess) Await.result(savepointPathFuture, deadline.timeLeft())).savepointPath();\n \t\t\tLOG.info(\"Retrieved savepoint path: \" + savepointPath + \".\");\n \n \t\t\t// Retrieve the savepoint from the testing job manager\n \t\t\tLOG.info(\"Requesting the savepoint.\");\n \t\t\tFuture<Object> savepointFuture = jobManager.ask(new RequestSavepoint(savepointPath), deadline.timeLeft());\n \n-\t\t\tSavepointV1 savepoint = (SavepointV1) ((ResponseSavepoint) Await.result(\n-\t\t\t\tsavepointFuture, deadline.timeLeft())).savepoint();\n+\t\t\tSavepointV1 savepoint = (SavepointV1) ((ResponseSavepoint) Await.result(savepointFuture, deadline.timeLeft())).savepoint();\n \t\t\tLOG.info(\"Retrieved savepoint: \" + savepointPath + \".\");\n \n \t\t\t// Shut down the Flink cluster (thereby canceling the job)\n@@ -243,26 +211,25 @@ public void testTriggerSavepointAndResume() throws Exception {\n \t\t\t\tFile savepointDir = files[0];\n \t\t\t\tFile[] savepointFiles = savepointDir.listFiles();\n \t\t\t\tassertNotNull(savepointFiles);\n-\t\t\t\tassertTrue(\"Did not write savepoint files to directory\",savepointFiles.length > 1);\n+\n+\t\t\t\t// Expect one metadata file and one checkpoint file per stateful\n+\t\t\t\t// parallel subtask\n+\t\t\t\tString errMsg = \"Did not write expected number of savepoint/checkpoint files to directory: \"\n+\t\t\t\t\t+ Arrays.toString(savepointFiles);\n+\t\t\t\tassertEquals(errMsg, 1 + parallelism, savepointFiles.length);\n \t\t\t} else {\n \t\t\t\tfail(\"Savepoint not created in expected directory\");\n \t\t\t}\n \n-\t\t\t// Only one checkpoint of the savepoint should exist\n \t\t\t// We currently have the following directory layout: checkpointDir/jobId/chk-ID\n \t\t\tFile jobCheckpoints = new File(checkpointDir, jobId.toString());\n \n \t\t\tif (jobCheckpoints.exists()) {\n \t\t\t\tfiles = jobCheckpoints.listFiles();\n \t\t\t\tassertNotNull(\"Checkpoint directory empty\", files);\n-\t\t\t\tassertEquals(\"Checkpoints directory not cleaned up: \" + Arrays.toString(files), 0, files.length);\n+\t\t\t\tassertEquals(\"Checkpoints directory not clean: \" + Arrays.toString(files), 0, files.length);\n \t\t\t}\n \n-\t\t\t// Only one savepoint should exist\n-\t\t\tfiles = savepointRootDir.listFiles();\n-\t\t\tassertNotNull(\"Savepoint directory empty\", files);\n-\t\t\tassertEquals(\"No savepoint found in savepoint directory\", 1, files.length);\n-\n \t\t\t// - Verification END ---------------------------------------------\n \n \t\t\t// Restart the cluster\n@@ -274,13 +241,14 @@ public void testTriggerSavepointAndResume() throws Exception {\n \t\t\tjobManager = Await.result(flink.leaderGateway().future(), deadline.timeLeft());\n \t\t\tLOG.info(\"JobManager: \" + jobManager + \".\");\n \n-\t\t\t// Reset for restore\n-\t\t\tStatefulCounter.resetForTest();\n+\t\t\t// Reset static test helpers\n+\t\t\tStatefulCounter.resetForTest(parallelism);\n \n \t\t\t// Gather all task deployment descriptors\n \t\t\tfinal Throwable[] error = new Throwable[1];\n \t\t\tfinal TestingCluster finalFlink = flink;\n \t\t\tfinal Multimap<JobVertexID, TaskDeploymentDescriptor> tdds = HashMultimap.create();\n+\n \t\t\tnew JavaTestKit(testActorSystem) {{\n \n \t\t\t\tnew Within(deadline.timeLeft()) {\n@@ -361,10 +329,10 @@ protected void run() {\n \t\t\t}\n \n \t\t\t// Await state is restored\n-\t\t\tStatefulCounter.awaitStateRestoredFromCheckpoint(deadline.timeLeft().toMillis());\n+\t\t\tStatefulCounter.getRestoreLatch().await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n \n \t\t\t// Await some progress after restore\n-\t\t\tStatefulCounter.awaitCompletedCheckpoints(parallelism, numberOfCompletedCheckpoints, deadline.timeLeft().toMillis());\n+\t\t\tStatefulCounter.getProgressLatch().await(deadline.timeLeft().toMillis(), TimeUnit.MILLISECONDS);\n \n \t\t\t// - Verification END ---------------------------------------------\n \n@@ -396,7 +364,7 @@ protected void run() {\n \t\t\t\t}\n \t\t\t}\n \n-\t\t\t// The checkpoint of the savepoint should have been discarded\n+\t\t\t// The checkpoint files of the savepoint should have been discarded\n \t\t\tfor (File f : checkpointFiles) {\n \t\t\t\terrMsg = \"Checkpoint file \" + f + \" not cleaned up properly.\";\n \t\t\t\tassertFalse(errMsg, f.exists());\n@@ -418,10 +386,6 @@ protected void run() {\n \t\t\tif (flink != null) {\n \t\t\t\tflink.shutdown();\n \t\t\t}\n-\n-\t\t\tif (tmpDir != null) {\n-\t\t\t\tFileUtils.deleteDirectory(tmpDir);\n-\t\t\t}\n \t\t}\n \t}\n \n@@ -467,7 +431,7 @@ public void testSubmitWithUnknownSavepointPath() throws Exception {\n \t\t\t// Submit the job\n \t\t\t// Long delay to ensure that the test times out if the job\n \t\t\t// manager tries to restart the job.\n-\t\t\tfinal JobGraph jobGraph = createJobGraph(parallelism, numberOfRetries, 3600000, 1000);\n+\t\t\tfinal JobGraph jobGraph = createJobGraph(parallelism, numberOfRetries, 3600000);\n \n \t\t\t// Set non-existing savepoint path\n \t\t\tjobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(\"unknown path\"));\n@@ -498,12 +462,10 @@ public void testSubmitWithUnknownSavepointPath() throws Exception {\n \tprivate JobGraph createJobGraph(\n \t\tint parallelism,\n \t\tint numberOfRetries,\n-\t\tlong restartDelay,\n-\t\tint checkpointingInterval) {\n+\t\tlong restartDelay) {\n \n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.enableCheckpointing(checkpointingInterval);\n \t\tenv.disableOperatorChaining();\n \t\tenv.getConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(numberOfRetries, restartDelay));\n \t\tenv.getConfig().disableSysoutLogging();\n@@ -526,7 +488,9 @@ private static class InfiniteTestSource implements SourceFunction<Integer> {\n \t\t@Override\n \t\tpublic void run(SourceContext<Integer> ctx) throws Exception {\n \t\t\twhile (running) {\n-\t\t\t\tctx.collect(1);\n+\t\t\t\tsynchronized (ctx.getCheckpointLock()) {\n+\t\t\t\t\tctx.collect(1);\n+\t\t\t\t}\n \t\t\t}\n \t\t}\n \n@@ -536,14 +500,12 @@ public void cancel() {\n \t\t}\n \t}\n \n-\tprivate static class StatefulCounter\n-\t\textends RichMapFunction<Integer, Integer>\n-\t\timplements ListCheckpointed<byte[]>, CheckpointListener {\n+\tprivate static class StatefulCounter extends RichMapFunction<Integer, Integer> implements ListCheckpointed<byte[]>{\n+\n+\t\tprivate static volatile CountDownLatch progressLatch = new CountDownLatch(0);\n+\t\tprivate static volatile CountDownLatch restoreLatch = new CountDownLatch(0);\n \n-\t\tprivate static final Object checkpointLock = new Object();\n-\t\tprivate static int numCompleteCalls;\n-\t\tprivate static int numRestoreCalls;\n-\t\tprivate static boolean restoredFromCheckpoint;\n+\t\tprivate int numCollectedElements = 0;\n \n \t\tprivate static final long serialVersionUID = 7317800376639115920L;\n \t\tprivate byte[] data;\n@@ -563,6 +525,11 @@ public Integer map(Integer value) throws Exception {\n \t\t\tfor (int i = 0; i < data.length; i++) {\n \t\t\t\tdata[i] += 1;\n \t\t\t}\n+\n+\t\t\tif (numCollectedElements++ > 10) {\n+\t\t\t\tprogressLatch.countDown();\n+\t\t\t}\n+\n \t\t\treturn value;\n \t\t}\n \n@@ -578,65 +545,22 @@ public void restoreState(List<byte[]> state) throws Exception {\n \t\t\t}\n \t\t\tthis.data = state.get(0);\n \n-\t\t\tsynchronized (checkpointLock) {\n-\t\t\t\tif (++numRestoreCalls == getRuntimeContext().getNumberOfParallelSubtasks()) {\n-\t\t\t\t\trestoredFromCheckpoint = true;\n-\t\t\t\t\tcheckpointLock.notifyAll();\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\t@Override\n-\t\tpublic void notifyCheckpointComplete(long checkpointId) throws Exception {\n-\t\t\tsynchronized (checkpointLock) {\n-\t\t\t\tnumCompleteCalls++;\n-\t\t\t\tcheckpointLock.notifyAll();\n-\t\t\t}\n+\t\t\trestoreLatch.countDown();\n \t\t}\n \n \t\t// --------------------------------------------------------------------\n \n-\t\tstatic void resetForTest() {\n-\t\t\tsynchronized (checkpointLock) {\n-\t\t\t\tnumCompleteCalls = 0;\n-\t\t\t\tnumRestoreCalls = 0;\n-\t\t\t\trestoredFromCheckpoint = false;\n-\t\t\t}\n+\t\tstatic CountDownLatch getProgressLatch() {\n+\t\t\treturn progressLatch;\n \t\t}\n \n-\t\tstatic void awaitCompletedCheckpoints(\n-\t\t\t\tint parallelism,\n-\t\t\t\tint expectedNumberOfCompletedCheckpoints,\n-\t\t\t\tlong timeoutMillis) throws InterruptedException, TimeoutException {\n-\n-\t\t\tlong deadline = System.nanoTime() + timeoutMillis * 1_000_000;\n-\n-\t\t\tsynchronized (checkpointLock) {\n-\t\t\t\t// One completion notification per parallel subtask\n-\t\t\t\tint expectedNumber = parallelism * expectedNumberOfCompletedCheckpoints;\n-\t\t\t\twhile (numCompleteCalls < expectedNumber && System.nanoTime() <= deadline) {\n-\t\t\t\t\tcheckpointLock.wait();\n-\t\t\t\t}\n-\n-\t\t\t\tif (numCompleteCalls < expectedNumber) {\n-\t\t\t\t\tthrow new TimeoutException(\"Did not complete \" + expectedNumberOfCompletedCheckpoints +\n-\t\t\t\t\t\t\" within timeout of \" + timeoutMillis + \" millis.\");\n-\t\t\t\t}\n-\t\t\t}\n+\t\tstatic CountDownLatch getRestoreLatch() {\n+\t\t\treturn restoreLatch;\n \t\t}\n \n-\t\tstatic void awaitStateRestoredFromCheckpoint(long timeoutMillis) throws InterruptedException, TimeoutException {\n-\t\t\tlong deadline = System.nanoTime() + timeoutMillis * 1_000_000;\n-\n-\t\t\tsynchronized (checkpointLock) {\n-\t\t\t\twhile (!restoredFromCheckpoint && System.currentTimeMillis() <= deadline) {\n-\t\t\t\t\tcheckpointLock.wait();\n-\t\t\t\t}\n-\n-\t\t\t\tif (!restoredFromCheckpoint) {\n-\t\t\t\t\tthrow new TimeoutException(\"Did not restore from checkpoint within timeout of \" + timeoutMillis + \" millis.\");\n-\t\t\t\t}\n-\t\t\t}\n+\t\tstatic void resetForTest(int parallelism) {\n+\t\t\tprogressLatch = new CountDownLatch(parallelism);\n+\t\t\trestoreLatch = new CountDownLatch(parallelism);\n \t\t}\n \t}\n ",
      "parent_sha": "7f244b8d4267b50c88aef69f6fd915595f23b368"
    }
  },
  {
    "oid": "31579621e16c6cdad3d965a91bb048282bf7942b",
    "message": "[FLINK-26977] Unify SavepointITCase#testStopWithSavepointFailingAfterSnapshotCreation for both schedulers\n\nThis closes #19439",
    "date": "2022-04-13T07:58:18Z",
    "url": "https://github.com/apache/flink/commit/31579621e16c6cdad3d965a91bb048282bf7942b",
    "details": {
      "sha": "fa0f155bba7cd6570d9f76e03dcfd80b02b6eba7",
      "filename": "flink-tests/src/test/java/org/apache/flink/test/checkpointing/SavepointITCase.java",
      "status": "modified",
      "additions": 6,
      "deletions": 40,
      "changes": 46,
      "blob_url": "https://github.com/apache/flink/blob/31579621e16c6cdad3d965a91bb048282bf7942b/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSavepointITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/31579621e16c6cdad3d965a91bb048282bf7942b/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSavepointITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Fcheckpointing%2FSavepointITCase.java?ref=31579621e16c6cdad3d965a91bb048282bf7942b",
      "patch": "@@ -1026,22 +1026,12 @@ public void testStopWithSavepointFailingAfterSnapshotCreation() throws Exception\n                 // 2. job failover triggered by SchedulerBase.stopWithSavepoint\n                 0,\n                 (jobId, actualException) -> {\n-                    if (ClusterOptions.isAdaptiveSchedulerEnabled(new Configuration())) {\n-                        return actualException\n-                                .getMessage()\n-                                .contains(\"Stop with savepoint operation could not be completed\");\n-                    } else {\n-                        Optional<StopWithSavepointStoppingException> actualFlinkException =\n-                                findThrowable(\n-                                        actualException, StopWithSavepointStoppingException.class);\n-                        return actualFlinkException\n-                                .map(\n-                                        e ->\n-                                                e.getMessage()\n-                                                        .startsWith(\n-                                                                \"A savepoint has been created at:\"))\n-                                .orElse(false);\n-                    }\n+                    Optional<StopWithSavepointStoppingException> actualFlinkException =\n+                            findThrowable(\n+                                    actualException, StopWithSavepointStoppingException.class);\n+                    return actualFlinkException\n+                            .map(e -> e.getMessage().startsWith(\"A savepoint has been created at:\"))\n+                            .orElse(false);\n                 },\n                 false);\n     }\n@@ -1096,30 +1086,6 @@ public void testStopWithSavepointWithDrainGlobalFailoverIfSavepointAborted() thr\n         }\n     }\n \n-    private static BiFunction<JobID, ExecutionException, Boolean>\n-            assertAfterSnapshotCreationFailure() {\n-        return (jobId, actualException) -> {\n-            if (ClusterOptions.isAdaptiveSchedulerEnabled(new Configuration())) {\n-                return actualException\n-                        .getMessage()\n-                        .contains(\"Stop with savepoint operation could not be completed\");\n-            } else {\n-                Optional<FlinkException> actualFlinkException =\n-                        findThrowable(actualException, FlinkException.class);\n-                if (!actualFlinkException.isPresent()) {\n-                    return false;\n-                }\n-                return actualFlinkException\n-                        .get()\n-                        .getMessage()\n-                        .contains(\n-                                String.format(\n-                                        \"A global fail-over is triggered to recover the job %s.\",\n-                                        jobId));\n-            }\n-        };\n-    }\n-\n     private static BiFunction<JobID, ExecutionException, Boolean>\n             assertInSnapshotCreationFailure() {\n         return (ignored, actualException) -> {",
      "parent_sha": "3b7a64e3cddcdbe7aa61b7bd8804ad5635ef96f7"
    }
  },
  {
    "oid": "412a8b447b0d9f8b3450fc0392a7cc084a6c22c7",
    "message": "convergence criterion finally working",
    "date": "2012-10-29T14:22:45Z",
    "url": "https://github.com/apache/flink/commit/412a8b447b0d9f8b3450fc0392a7cc084a6c22c7",
    "details": {
      "sha": "58595772e5226ad07d60361f11cb4275bd29df8d",
      "filename": "pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/iterative/driver/SortingTempDriver.java",
      "status": "modified",
      "additions": 13,
      "deletions": 44,
      "changes": 57,
      "blob_url": "https://github.com/apache/flink/blob/412a8b447b0d9f8b3450fc0392a7cc084a6c22c7/pact%2Fpact-runtime%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fpact%2Fruntime%2Fiterative%2Fdriver%2FSortingTempDriver.java",
      "raw_url": "https://github.com/apache/flink/raw/412a8b447b0d9f8b3450fc0392a7cc084a6c22c7/pact%2Fpact-runtime%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fpact%2Fruntime%2Fiterative%2Fdriver%2FSortingTempDriver.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/pact%2Fpact-runtime%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fpact%2Fruntime%2Fiterative%2Fdriver%2FSortingTempDriver.java?ref=412a8b447b0d9f8b3450fc0392a7cc084a6c22c7",
      "patch": "@@ -1,5 +1,3 @@\n-package eu.stratosphere.pact.runtime.iterative.driver;\n-\n /***********************************************************************************************************************\n  *\n  * Copyright (C) 2010 by the Stratosphere project (http://stratosphere.eu)\n@@ -15,6 +13,8 @@\n  *\n  **********************************************************************************************************************/\n \n+package eu.stratosphere.pact.runtime.iterative.driver;\n+\n import eu.stratosphere.pact.runtime.sort.Sorter;\n import eu.stratosphere.pact.runtime.sort.UnilateralSortMerger;\n import eu.stratosphere.pact.runtime.task.PactDriver;\n@@ -32,57 +32,39 @@ public class SortingTempDriver<T> implements PactDriver<Stub, T>\n {\n   private static final Log LOG = LogFactory.getLog(SortingTempDriver.class);\n \n-  private static final long MIN_REQUIRED_MEMORY = 512 * 1024;\t\t// minimal memory for the task to operate\n+  private static final long MIN_REQUIRED_MEMORY = 512 * 1024; // minimal memory for the task to operate\n \n   private PactTaskContext<Stub, T> taskContext;\n \n   private Sorter<T> sorter;\n \n   private volatile boolean running;\n \n-  // ------------------------------------------------------------------------\n-\n-\n-  /* (non-Javadoc)\n-    * @see eu.stratosphere.pact.runtime.task.PactDriver#setup(eu.stratosphere.pact.runtime.task.PactTaskContext)\n-    */\n   @Override\n   public void setup(PactTaskContext<Stub, T> context) {\n     this.taskContext = context;\n     this.running = true;\n   }\n \n-  /* (non-Javadoc)\n-    * @see eu.stratosphere.pact.runtime.task.AbstractPactTask#getNumberOfInputs()\n-    */\n   @Override\n   public int getNumberOfInputs() {\n     return 1;\n   }\n \n-  /* (non-Javadoc)\n-    * @see eu.stratosphere.pact.runtime.task.AbstractPactTask#getStubType()\n-    */\n   @Override\n   public Class<Stub> getStubType() {\n     return Stub.class;\n   }\n \n-  /* (non-Javadoc)\n-    * @see eu.stratosphere.pact.runtime.task.AbstractPactTask#requiresComparatorOnInput()\n-    */\n   @Override\n   public boolean requiresComparatorOnInput() {\n     return true;\n   }\n \n-  /* (non-Javadoc)\n-    * @see eu.stratosphere.pact.runtime.task.AbstractPactTask#prepare()\n-    */\n   @Override\n-  public void prepare() throws Exception\n-  {\n-    final TaskConfig config = this.taskContext.getTaskConfig();\n+  public void prepare() throws Exception {\n+\n+    final TaskConfig config = taskContext.getTaskConfig();\n \n     // set up memory and I/O parameters\n     final long availableMemory = config.getMemorySize();\n@@ -98,43 +80,30 @@ public void prepare() throws Exception\n         config.getSortSpillingTreshold());\n   }\n \n-\n-  /* (non-Javadoc)\n-    * @see eu.stratosphere.pact.runtime.task.AbstractPactTask#run()\n-    */\n   @Override\n-  public void run() throws Exception\n-  {\n+  public void run() throws Exception {\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(this.taskContext.formatLogString(\"Preprocessing done, iterator obtained.\"));\n     }\n \n     // cache references on the stack\n-    final TypeSerializer<T> serializer = this.taskContext.getInputSerializer(0);\n-    final Collector<T> output = this.taskContext.getOutputCollector();\n+    final TypeSerializer<T> serializer = taskContext.getInputSerializer(0);\n+    final Collector<T> output = taskContext.getOutputCollector();\n \n     final T record = serializer.createInstance();\n-    MutableObjectIterator<T> sorted = sorter.getIterator();\n+    final MutableObjectIterator<T> sorted = sorter.getIterator();\n \n-    while (this.running && sorted.next(record)) {\n+    while (running && sorted.next(record)) {\n       output.collect(record);\n     }\n-\n   }\n \n-  /* (non-Javadoc)\n-    * @see eu.stratosphere.pact.runtime.task.AbstractPactTask#cleanup()\n-    */\n   @Override\n-  public void cleanup() throws Exception {\n-  }\n+  public void cleanup() throws Exception {}\n \n-  /* (non-Javadoc)\n-    * @see eu.stratosphere.pact.runtime.task.PactDriver#cancel()\n-    */\n   @Override\n   public void cancel() {\n-    this.running = false;\n+    running = false;\n   }\n }\n ",
      "parent_sha": "c6a7da57e2f82ba8e6c4a796b80be3a10b81fa12"
    }
  },
  {
    "oid": "78c392abc7b4687d11a430ed5a9411d4413d5249",
    "message": "Switched to cached thread pool in task manager to speed up cancelling of tasks",
    "date": "2012-06-19T18:50:34Z",
    "url": "https://github.com/apache/flink/commit/78c392abc7b4687d11a430ed5a9411d4413d5249",
    "details": {
      "sha": "25a18fca882755304415f1420ed47d0da24e8b49",
      "filename": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/78c392abc7b4687d11a430ed5a9411d4413d5249/nephele%2Fnephele-server%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Ftaskmanager%2FTaskManager.java",
      "raw_url": "https://github.com/apache/flink/raw/78c392abc7b4687d11a430ed5a9411d4413d5249/nephele%2Fnephele-server%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Ftaskmanager%2FTaskManager.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele%2Fnephele-server%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Ftaskmanager%2FTaskManager.java?ref=78c392abc7b4687d11a430ed5a9411d4413d5249",
      "patch": "@@ -108,7 +108,7 @@ public class TaskManager implements TaskOperationProtocol, PluginCommunicationPr\n \n \tprivate final PluginCommunicationProtocol pluginCommunicationService;\n \n-\tprivate final ExecutorService executorService = Executors.newSingleThreadExecutor();\n+\tprivate final ExecutorService executorService = Executors.newCachedThreadPool();\n \n \tprivate static final int handlerCount = 1;\n ",
      "parent_sha": "4cc2f8278f87c579d376a4300542462c58b69cfe"
    }
  },
  {
    "oid": "2039eea8fe5f0f1415a391d7c276a691a3b5d481",
    "message": "[hotfix] Harden SlotManagerImplTest#testSlotReportWhileActiveSlotRequest\n\nThe SlotManagerImplTest#testSlotReportWhileActiveSlotRequest contained a race condition since\nwe were checking the internal state of the SlotManagerImpl's state. The test has been hardened\nby removing this check.",
    "date": "2019-08-29T15:55:39Z",
    "url": "https://github.com/apache/flink/commit/2039eea8fe5f0f1415a391d7c276a691a3b5d481",
    "details": {
      "sha": "4d427f004740a6c15e48f9e3d1cb0e1157ca19d8",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/slotmanager/SlotManagerImplTest.java",
      "status": "modified",
      "additions": 12,
      "deletions": 17,
      "changes": 29,
      "blob_url": "https://github.com/apache/flink/blob/2039eea8fe5f0f1415a391d7c276a691a3b5d481/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fresourcemanager%2Fslotmanager%2FSlotManagerImplTest.java",
      "raw_url": "https://github.com/apache/flink/raw/2039eea8fe5f0f1415a391d7c276a691a3b5d481/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fresourcemanager%2Fslotmanager%2FSlotManagerImplTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fresourcemanager%2Fslotmanager%2FSlotManagerImplTest.java?ref=2039eea8fe5f0f1415a391d7c276a691a3b5d481",
      "patch": "@@ -49,6 +49,7 @@\n import org.apache.flink.util.TestLogger;\n import org.apache.flink.util.function.FunctionUtils;\n import org.apache.flink.util.function.FunctionWithException;\n+import org.apache.flink.util.function.ThrowingRunnable;\n \n import org.junit.Test;\n \n@@ -806,9 +807,14 @@ public void testSlotReportWhileActiveSlotRequest() throws Exception {\n \t\tfinal SlotStatus slotStatus2 = new SlotStatus(slotId2, resourceProfile);\n \t\tfinal SlotReport slotReport = new SlotReport(Arrays.asList(slotStatus1, slotStatus2));\n \n-\t\tfinal Executor mainThreadExecutor = TestingUtils.defaultExecutor();\n+\t\tfinal ScheduledExecutor mainThreadExecutor = TestingUtils.defaultScheduledExecutor();\n \n-\t\ttry (final SlotManagerImpl slotManager = SlotManagerBuilder.newBuilder().build()) {\n+\t\tfinal SlotManagerImpl slotManager = SlotManagerBuilder\n+\t\t\t.newBuilder()\n+\t\t\t.setScheduledExecutor(mainThreadExecutor)\n+\t\t\t.build();\n+\n+\t\ttry {\n \n \t\t\tslotManager.start(resourceManagerId, mainThreadExecutor, resourceManagerActions);\n \n@@ -834,12 +840,6 @@ public void testSlotReportWhileActiveSlotRequest() throws Exception {\n \t\t\tfinal SlotID requestedSlotId = slotIds.take();\n \t\t\tfinal SlotID freeSlotId = requestedSlotId.equals(slotId1) ? slotId2 : slotId1;\n \n-\t\t\tCompletableFuture<Boolean> freeSlotFuture = CompletableFuture.supplyAsync(\n-\t\t\t\t() -> slotManager.getSlot(freeSlotId).getState() == TaskManagerSlot.State.FREE,\n-\t\t\t\tmainThreadExecutor);\n-\n-\t\t\tassertTrue(freeSlotFuture.get());\n-\n \t\t\tfinal SlotStatus newSlotStatus1 = new SlotStatus(requestedSlotId, resourceProfile, new JobID(), new AllocationID());\n \t\t\tfinal SlotStatus newSlotStatus2 = new SlotStatus(freeSlotId, resourceProfile);\n \t\t\tfinal SlotReport newSlotReport = new SlotReport(Arrays.asList(newSlotStatus1, newSlotStatus2));\n@@ -853,16 +853,11 @@ public void testSlotReportWhileActiveSlotRequest() throws Exception {\n \n \t\t\tfinal SlotID requestedSlotId2 = slotIds.take();\n \n-\t\t\tassertEquals(slotId2, requestedSlotId2);\n-\n-\t\t\tCompletableFuture<TaskManagerSlot> requestedSlotFuture = CompletableFuture.supplyAsync(\n-\t\t\t\t() -> slotManager.getSlot(requestedSlotId2),\n+\t\t\tassertEquals(freeSlotId, requestedSlotId2);\n+\t\t} finally {\n+\t\t\tCompletableFuture.runAsync(\n+\t\t\t\tThrowingRunnable.unchecked(slotManager::close),\n \t\t\t\tmainThreadExecutor);\n-\n-\t\t\tTaskManagerSlot slot = requestedSlotFuture.get();\n-\n-\t\t\tassertTrue(slot.getState() == TaskManagerSlot.State.ALLOCATED);\n-\t\t\tassertEquals(allocationId, slot.getAllocationId());\n \t\t}\n \t}\n ",
      "parent_sha": "17ed5de47754cd0da912b805c465356ec3c7a8fb"
    }
  },
  {
    "oid": "864ad6a752b033b20292ff7ef42ca54eea81e311",
    "message": "Added test case for PactNull datatype",
    "date": "2011-03-13T09:08:09Z",
    "url": "https://github.com/apache/flink/commit/864ad6a752b033b20292ff7ef42ca54eea81e311",
    "details": {
      "sha": "6e0a78e08be36a6b4510897de2c748257b90c58b",
      "filename": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/resettable/SpillingResettableIteratorTest.java",
      "status": "modified",
      "additions": 173,
      "deletions": 94,
      "changes": 267,
      "blob_url": "https://github.com/apache/flink/blob/864ad6a752b033b20292ff7ef42ca54eea81e311/pact%2Fpact-runtime%2Fsrc%2Ftest%2Fjava%2Feu%2Fstratosphere%2Fpact%2Fruntime%2Fresettable%2FSpillingResettableIteratorTest.java",
      "raw_url": "https://github.com/apache/flink/raw/864ad6a752b033b20292ff7ef42ca54eea81e311/pact%2Fpact-runtime%2Fsrc%2Ftest%2Fjava%2Feu%2Fstratosphere%2Fpact%2Fruntime%2Fresettable%2FSpillingResettableIteratorTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/pact%2Fpact-runtime%2Fsrc%2Ftest%2Fjava%2Feu%2Fstratosphere%2Fpact%2Fruntime%2Fresettable%2FSpillingResettableIteratorTest.java?ref=864ad6a752b033b20292ff7ef42ca54eea81e311",
      "patch": "@@ -21,6 +21,8 @@\n import java.util.List;\n import java.util.Vector;\n \n+import junit.framework.Assert;\n+\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -37,16 +39,15 @@\n import eu.stratosphere.nephele.template.AbstractInvokable;\n import eu.stratosphere.nephele.types.Record;\n import eu.stratosphere.pact.common.type.base.PactInteger;\n-import eu.stratosphere.pact.runtime.resettable.SpillingResettableIterator;\n+import eu.stratosphere.pact.common.type.base.PactNull;\n import eu.stratosphere.pact.runtime.test.util.DummyInvokable;\n-import junit.framework.Assert;\n \n public class SpillingResettableIteratorTest {\n \n \tprivate static final int NUMTESTRECORDS = 1000;\n \n \tprivate static final int memoryCapacity = 100000;\n-\t\n+\n \tprivate IOManager ioman;\n \n \tprivate MemoryManager memman;\n@@ -56,8 +57,6 @@ public class SpillingResettableIteratorTest {\n \tprivate List<PactInteger> objects;\n \n \tprivate RecordDeserializer<PactInteger> deserializer;\n-\t\n-\t\n \n \tprotected class CollectionReader<T extends Record> implements Reader<T> {\n \t\tprivate Vector<T> objects;\n@@ -96,48 +95,50 @@ public List<AbstractInputChannel<T>> getInputChannels() {\n \t@Before\n \tpublic void startup() {\n \t\t// set up IO and memory manager\n-\t\tthis.memman  = new DefaultMemoryManager(memoryCapacity);\n+\t\tthis.memman = new DefaultMemoryManager(memoryCapacity);\n \t\tthis.ioman = new IOManager();\n-\t\t\n+\n \t\t// create test objects\n \t\tthis.objects = new ArrayList<PactInteger>(NUMTESTRECORDS);\n-\t\t\n+\n \t\tfor (int i = 0; i < NUMTESTRECORDS; ++i) {\n \t\t\tPactInteger tmp = new PactInteger(i);\n \t\t\tthis.objects.add(tmp);\n \t\t}\n \t\t// create the deserializer\n-\t\tthis.deserializer = new DefaultRecordDeserializer<PactInteger>(PactInteger.class);\n+\t\tthis.deserializer = new DefaultRecordDeserializer<PactInteger>(\n+\t\t\t\tPactInteger.class);\n \t}\n-\t\n+\n \t@After\n \tpublic void shutdown() {\n \t\tthis.deserializer = null;\n \t\tthis.objects = null;\n-\t\t\n+\n \t\tthis.ioman.shutdown();\n \t\tthis.ioman = null;\n-\t\t\n+\n \t\tthis.memman.shutdown();\n \t\tthis.memman = null;\n \t}\n \n \t/**\n-\t * Tests the resettable iterator with too few memory, so that the data\n-\t * has to be written to disk.\n+\t * Tests the resettable iterator with too few memory, so that the data has\n+\t * to be written to disk.\n \t * \n \t * @throws ServiceException\n \t * @throws InterruptedException\n \t */\n \t@Test\n-\tpublic void testResettableIterator() throws ServiceException, InterruptedException {\n+\tpublic void testResettableIterator() throws ServiceException,\n+\t\t\tInterruptedException {\n \t\tfinal AbstractInvokable memOwner = new DummyInvokable();\n-\t\t\n+\n \t\t// create the reader\n \t\treader = new CollectionReader<PactInteger>(objects);\n \t\t// create the resettable Iterator\n-\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(memman, ioman,\n-\t\t\treader, 1000, deserializer, memOwner);\n+\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(\n+\t\t\t\tmemman, ioman, reader, 1000, deserializer, memOwner);\n \t\t// open the iterator\n \t\ttry {\n \t\t\titerator.open();\n@@ -147,48 +148,55 @@ public void testResettableIterator() throws ServiceException, InterruptedExcepti\n \t\t// now test walking through the iterator\n \t\tint count = 0;\n \t\twhile (iterator.hasNext())\n-\t\t\tAssert.assertEquals(\"In initial run, element \" + count + \" does not match expected value!\", count++,\n-\t\t\t\titerator.next().getValue());\n-\t\tAssert.assertEquals(\"Too few elements were deserialzied in initial run!\", NUMTESTRECORDS, count);\n+\t\t\tAssert.assertEquals(\"In initial run, element \" + count\n+\t\t\t\t\t+ \" does not match expected value!\", count++, iterator\n+\t\t\t\t\t.next().getValue());\n+\t\tAssert.assertEquals(\n+\t\t\t\t\"Too few elements were deserialzied in initial run!\",\n+\t\t\t\tNUMTESTRECORDS, count);\n \t\t// test resetting the iterator a few times\n \t\tfor (int j = 0; j < 10; ++j) {\n \t\t\tcount = 0;\n \t\t\titerator.reset();\n \t\t\t// now we should get the same results\n \t\t\twhile (iterator.hasNext())\n-\t\t\t\tAssert.assertEquals(\"After reset nr. \" + j + 1 + \" element \" + count\n-\t\t\t\t\t+ \" does not match expected value!\", count++, iterator.next().getValue());\n-\t\t\tAssert.assertEquals(\"Too few elements were deserialzied after reset nr. \" + j + 1 + \"!\", NUMTESTRECORDS, count);\n+\t\t\t\tAssert.assertEquals(\"After reset nr. \" + j + 1 + \" element \"\n+\t\t\t\t\t\t+ count + \" does not match expected value!\", count++,\n+\t\t\t\t\t\titerator.next().getValue());\n+\t\t\tAssert.assertEquals(\n+\t\t\t\t\t\"Too few elements were deserialzied after reset nr. \" + j\n+\t\t\t\t\t\t\t+ 1 + \"!\", NUMTESTRECORDS, count);\n \t\t}\n \t\t// close the iterator\n \t\titerator.close();\n-\t\t\n+\n \t\t// make sure there are no memory leaks\n \t\ttry {\n-\t\t\tMemorySegment test = memman.allocate(new DummyInvokable(), memoryCapacity);\n+\t\t\tMemorySegment test = memman.allocate(new DummyInvokable(),\n+\t\t\t\t\tmemoryCapacity);\n \t\t\tmemman.release(test);\n-\t\t}\n-\t\tcatch (Exception e) {\n+\t\t} catch (Exception e) {\n \t\t\tAssert.fail(\"Memory leak detected. SpillingResettableIterator does not release all memory.\");\n \t\t}\n \t}\n \n \t/**\n-\t * Tests the resettable iterator with enough memory so that all data\n-\t * is kept locally in a membuffer.\n+\t * Tests the resettable iterator with enough memory so that all data is kept\n+\t * locally in a membuffer.\n \t * \n \t * @throws ServiceException\n \t * @throws InterruptedException\n \t */\n \t@Test\n-\tpublic void testResettableIteratorInMemory() throws ServiceException, InterruptedException {\n+\tpublic void testResettableIteratorInMemory() throws ServiceException,\n+\t\t\tInterruptedException {\n \t\tfinal AbstractInvokable memOwner = new DummyInvokable();\n-\t\t\n+\n \t\t// create the reader\n \t\treader = new CollectionReader<PactInteger>(objects);\n \t\t// create the resettable iterator\n-\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(memman, ioman,\n-\t\t\treader, 10000, deserializer, memOwner);\n+\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(\n+\t\t\t\tmemman, ioman, reader, 10000, deserializer, memOwner);\n \t\t// open the iterator\n \t\ttry {\n \t\t\titerator.open();\n@@ -198,60 +206,112 @@ public void testResettableIteratorInMemory() throws ServiceException, Interrupte\n \t\t// now test walking through the iterator\n \t\tint count = 0;\n \t\twhile (iterator.hasNext())\n-\t\t\tAssert.assertEquals(\"In initial run, element \" + count + \" does not match expected value!\", count++,\n-\t\t\t\titerator.next().getValue());\n-\t\tAssert.assertEquals(\"Too few elements were deserialzied in initial run!\", NUMTESTRECORDS, count);\n+\t\t\tAssert.assertEquals(\"In initial run, element \" + count\n+\t\t\t\t\t+ \" does not match expected value!\", count++, iterator\n+\t\t\t\t\t.next().getValue());\n+\t\tAssert.assertEquals(\n+\t\t\t\t\"Too few elements were deserialzied in initial run!\",\n+\t\t\t\tNUMTESTRECORDS, count);\n \t\t// test resetting the iterator a few times\n \t\tfor (int j = 0; j < 10; ++j) {\n \t\t\tcount = 0;\n \t\t\titerator.reset();\n \t\t\t// now we should get the same results\n \t\t\twhile (iterator.hasNext())\n-\t\t\t\tAssert.assertEquals(\"After reset nr. \" + j + 1 + \" element \" + count\n-\t\t\t\t\t+ \" does not match expected value!\", count++, iterator.next().getValue());\n-\t\t\tAssert.assertEquals(\"Too few elements were deserialzied after reset nr. \" + j + 1 + \"!\", NUMTESTRECORDS, count);\n+\t\t\t\tAssert.assertEquals(\"After reset nr. \" + j + 1 + \" element \"\n+\t\t\t\t\t\t+ count + \" does not match expected value!\", count++,\n+\t\t\t\t\t\titerator.next().getValue());\n+\t\t\tAssert.assertEquals(\n+\t\t\t\t\t\"Too few elements were deserialzied after reset nr. \" + j\n+\t\t\t\t\t\t\t+ 1 + \"!\", NUMTESTRECORDS, count);\n \t\t}\n \t\t// close the iterator\n \t\titerator.close();\n \t\t// make sure there are no memory leaks\n \t\ttry {\n-\t\t\tMemorySegment test = memman.allocate(new DummyInvokable(), memoryCapacity);\n+\t\t\tMemorySegment test = memman.allocate(new DummyInvokable(),\n+\t\t\t\t\tmemoryCapacity);\n \t\t\tmemman.release(test);\n \t\t} catch (Exception e) {\n \t\t\tAssert.fail(\"Memory leak detected. SpillingResettableIterator does not release all memory.\");\n \t\t}\n \t}\n-\t\n+\n \t/**\n-\t * Tests whether multiple call of hasNext() changes the state of the iterator\n+\t * Tests whether multiple call of hasNext() changes the state of the\n+\t * iterator\n \t */\n \t@Test\n-\tpublic void testHasNext() throws ServiceException, InterruptedException  {\n+\tpublic void testHasNext() throws ServiceException, InterruptedException {\n \t\tfinal AbstractInvokable memOwner = new DummyInvokable();\n-\t\t\n+\n \t\t// create the reader\n \t\treader = new CollectionReader<PactInteger>(objects);\n \t\t// create the resettable Iterator\n-\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(memman, ioman,\n-\t\t\treader, 1000, deserializer, memOwner);\n+\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(\n+\t\t\t\tmemman, ioman, reader, 1000, deserializer, memOwner);\n \t\t// open the iterator\n \t\ttry {\n \t\t\titerator.open();\n \t\t} catch (IOException e) {\n \t\t\tAssert.fail(\"Could not open resettable iterator:\" + e.getMessage());\n \t\t}\n-\t\t\n+\n \t\tint cnt = 0;\n-\t\twhile(iterator.hasNext()) {\n+\t\twhile (iterator.hasNext()) {\n \t\t\titerator.hasNext();\n \t\t\titerator.next();\n \t\t\tcnt++;\n \t\t}\n-\t\t\n-\t\tAssert.assertTrue(cnt+\" elements read from iterator, but \"+NUMTESTRECORDS+\" expected\",cnt == NUMTESTRECORDS);\n-\t\t\n+\n+\t\tAssert.assertTrue(cnt + \" elements read from iterator, but \"\n+\t\t\t\t+ NUMTESTRECORDS + \" expected\", cnt == NUMTESTRECORDS);\n+\n \t}\n-\t\n+\n+\t/**\n+\t * Tests whether multiple call of hasNext() changes the state of the\n+\t * iterator\n+\t */\n+\t@Test\n+\tpublic void testHasNextPactNull() throws ServiceException,\n+\t\t\tInterruptedException {\n+\t\tfinal AbstractInvokable memOwner = new DummyInvokable();\n+\n+\t\tList<PactNull> objects = new ArrayList<PactNull>();\n+\t\tfinal int NUM_ELEMENTS = 12;\n+\t\tfor (int i = 0; i < NUM_ELEMENTS; i++) {\n+\t\t\tobjects.add(new PactNull());\n+\t\t}\n+\n+\t\tDefaultRecordDeserializer<PactNull> deserializer = new DefaultRecordDeserializer<PactNull>(\n+\t\t\t\tPactNull.class);\n+\n+\t\t// create the reader\n+\t\tCollectionReader<PactNull> reader = new CollectionReader<PactNull>(\n+\t\t\t\tobjects);\n+\t\t// create the resettable Iterator\n+\t\tSpillingResettableIterator<PactNull> iterator = new SpillingResettableIterator<PactNull>(\n+\t\t\t\tmemman, ioman, reader, 1000, deserializer, memOwner);\n+\t\t// open the iterator\n+\t\ttry {\n+\t\t\titerator.open();\n+\t\t} catch (IOException e) {\n+\t\t\tAssert.fail(\"Could not open resettable iterator:\" + e.getMessage());\n+\t\t}\n+\n+\t\tint cnt = 0;\n+\t\twhile (iterator.hasNext()) {\n+\t\t\titerator.hasNext();\n+\t\t\titerator.next();\n+\t\t\tcnt++;\n+\t\t}\n+\n+\t\tAssert.assertTrue(cnt + \" elements read from iterator, but \"\n+\t\t\t\t+ NUM_ELEMENTS + \" expected\", cnt == NUM_ELEMENTS);\n+\n+\t}\n+\n \t/**\n \t * Test whether next() depends on previous call of hasNext()\n \t * \n@@ -261,109 +321,128 @@ public void testHasNext() throws ServiceException, InterruptedException  {\n \t@Test\n \tpublic void testNext() throws ServiceException, InterruptedException {\n \t\tfinal AbstractInvokable memOwner = new DummyInvokable();\n-\t\t\n+\n \t\t// create the reader\n \t\treader = new CollectionReader<PactInteger>(objects);\n \t\t// create the resettable Iterator\n-\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(memman, ioman,\n-\t\t\treader, 1000, deserializer, memOwner);\n+\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(\n+\t\t\t\tmemman, ioman, reader, 1000, deserializer, memOwner);\n \t\t// open the iterator\n \t\ttry {\n \t\t\titerator.open();\n \t\t} catch (IOException e) {\n \t\t\tAssert.fail(\"Could not open resettable iterator:\" + e.getMessage());\n \t\t}\n-\t\t\n+\n \t\tPactInteger record;\n \t\tint cnt = 0;\n-\t\twhile(cnt < NUMTESTRECORDS) {\n+\t\twhile (cnt < NUMTESTRECORDS) {\n \t\t\trecord = iterator.next();\n-\t\t\tAssert.assertTrue(\"Record was not read from iterator\", record != null);\n+\t\t\tAssert.assertTrue(\"Record was not read from iterator\",\n+\t\t\t\t\trecord != null);\n \t\t\tcnt++;\n \t\t}\n-\t\t\n+\n \t\trecord = iterator.next();\n-\t\tAssert.assertTrue(\"Too many records were read from iterator\",record == null);\n+\t\tAssert.assertTrue(\"Too many records were read from iterator\",\n+\t\t\t\trecord == null);\n \t}\n-\t\n+\n \t/**\n-\t * Tests whether lastReturned() returns the latest returned element read from spilled file. \n+\t * Tests whether lastReturned() returns the latest returned element read\n+\t * from spilled file.\n \t */\n \t@Test\n \tpublic void testRepeatLast() throws ServiceException, InterruptedException {\n \t\tfinal AbstractInvokable memOwner = new DummyInvokable();\n-\t\t\n+\n \t\t// create the reader\n \t\treader = new CollectionReader<PactInteger>(objects);\n \t\t// create the resettable Iterator\n-\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(memman, ioman,\n-\t\t\treader, 1000, deserializer, memOwner);\n+\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(\n+\t\t\t\tmemman, ioman, reader, 1000, deserializer, memOwner);\n \t\t// open the iterator\n \t\ttry {\n \t\t\titerator.open();\n \t\t} catch (IOException e) {\n \t\t\tAssert.fail(\"Could not open resettable iterator:\" + e.getMessage());\n \t\t}\n-\t\t\n+\n \t\tPactInteger record1;\n \t\tPactInteger record2;\n \t\tPactInteger compare;\n \t\tint cnt = 0;\n-\t\twhile(iterator.hasNext()) {\n-\t\t\t\n+\t\twhile (iterator.hasNext()) {\n+\n \t\t\trecord1 = iterator.next();\n \t\t\trecord2 = iterator.repeatLast();\n \t\t\tcompare = objects.get(cnt);\n-\t\t\t\n-\t\t\tAssert.assertTrue(\"Record read with next() does not equal expected value\",record1.equals(compare));\n-\t\t\tAssert.assertTrue(\"Record read with next() does not equal record read with lastReturned()\",record1.equals(record2));\n-\t\t\tAssert.assertTrue(\"Records read with next() and lastReturned have same reference\",record1 != record2);\n-\t\t\t\n+\n+\t\t\tAssert.assertTrue(\n+\t\t\t\t\t\"Record read with next() does not equal expected value\",\n+\t\t\t\t\trecord1.equals(compare));\n+\t\t\tAssert.assertTrue(\n+\t\t\t\t\t\"Record read with next() does not equal record read with lastReturned()\",\n+\t\t\t\t\trecord1.equals(record2));\n+\t\t\tAssert.assertTrue(\n+\t\t\t\t\t\"Records read with next() and lastReturned have same reference\",\n+\t\t\t\t\trecord1 != record2);\n+\n \t\t\tcnt++;\n \t\t}\n-\t\t\n-\t\tAssert.assertTrue(cnt+\" elements read from iterator, but \"+NUMTESTRECORDS+\" expected\",cnt == NUMTESTRECORDS);\n-\t\t\n+\n+\t\tAssert.assertTrue(cnt + \" elements read from iterator, but \"\n+\t\t\t\t+ NUMTESTRECORDS + \" expected\", cnt == NUMTESTRECORDS);\n+\n \t}\n-\t\n+\n \t/**\n-\t * Tests whether lastReturned() returns the latest returned element read from memory. \n+\t * Tests whether lastReturned() returns the latest returned element read\n+\t * from memory.\n \t */\n \t@Test\n-\tpublic void testRepeatLastInMemory() throws ServiceException, InterruptedException {\n+\tpublic void testRepeatLastInMemory() throws ServiceException,\n+\t\t\tInterruptedException {\n \t\tfinal AbstractInvokable memOwner = new DummyInvokable();\n-\t\t\n+\n \t\t// create the reader\n \t\treader = new CollectionReader<PactInteger>(objects);\n \t\t// create the resettable Iterator\n-\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(memman, ioman,\n-\t\t\treader, 10000, deserializer, memOwner);\n+\t\tSpillingResettableIterator<PactInteger> iterator = new SpillingResettableIterator<PactInteger>(\n+\t\t\t\tmemman, ioman, reader, 10000, deserializer, memOwner);\n \t\t// open the iterator\n \t\ttry {\n \t\t\titerator.open();\n \t\t} catch (IOException e) {\n \t\t\tAssert.fail(\"Could not open resettable iterator:\" + e.getMessage());\n \t\t}\n-\t\t\n+\n \t\tPactInteger record1;\n \t\tPactInteger record2;\n \t\tPactInteger compare;\n \t\tint cnt = 0;\n-\t\twhile(iterator.hasNext()) {\n-\t\t\t\n+\t\twhile (iterator.hasNext()) {\n+\n \t\t\trecord1 = iterator.next();\n \t\t\trecord2 = iterator.repeatLast();\n \t\t\tcompare = objects.get(cnt);\n-\t\t\t\n-\t\t\tAssert.assertTrue(\"Record read with next() does not equal expected value\",record1.equals(compare));\n-\t\t\tAssert.assertTrue(\"Record read with next() does not equal record read with lastReturned()\",record1.equals(record2));\n-\t\t\tAssert.assertTrue(\"Records read with next() and lastReturned have same reference\",record1 != record2);\n-\t\t\t\n+\n+\t\t\tAssert.assertTrue(\n+\t\t\t\t\t\"Record read with next() does not equal expected value\",\n+\t\t\t\t\trecord1.equals(compare));\n+\t\t\tAssert.assertTrue(\n+\t\t\t\t\t\"Record read with next() does not equal record read with lastReturned()\",\n+\t\t\t\t\trecord1.equals(record2));\n+\t\t\tAssert.assertTrue(\n+\t\t\t\t\t\"Records read with next() and lastReturned have same reference\",\n+\t\t\t\t\trecord1 != record2);\n+\n \t\t\tcnt++;\n \t\t}\n-\t\t\n-\t\tAssert.assertTrue(cnt+\" elements read from iterator, but \"+NUMTESTRECORDS+\" expected\",cnt == NUMTESTRECORDS);\n-\t\t\n+\n+\t\tAssert.assertTrue(cnt + \" elements read from iterator, but \"\n+\t\t\t\t+ NUMTESTRECORDS + \" expected\", cnt == NUMTESTRECORDS);\n+\n \t}\n \n }",
      "parent_sha": "e7ac5f57e34e5e08d4264d80157e457817be1a39"
    }
  },
  {
    "oid": "ff89afde95cda9a6507668e57b1631e0e57bd135",
    "message": "[hotfix][runtime, tests] Fix typos in parameter names\n\nRename parameter of SimpleAckingTaskManagerGateway#setSubmitConsumer() from\npredicate to submitConsumer.\n\nRename parameter of SimpleAckingTaskManagerGateway#setCancelConsumer() from\npredicate to cancelConsumer.",
    "date": "2019-06-25T11:57:00Z",
    "url": "https://github.com/apache/flink/commit/ff89afde95cda9a6507668e57b1631e0e57bd135",
    "details": {
      "sha": "02e14864718887946cfaa17524b7f28d353d35c0",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/utils/SimpleAckingTaskManagerGateway.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/apache/flink/blob/ff89afde95cda9a6507668e57b1631e0e57bd135/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fexecutiongraph%2Futils%2FSimpleAckingTaskManagerGateway.java",
      "raw_url": "https://github.com/apache/flink/raw/ff89afde95cda9a6507668e57b1631e0e57bd135/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fexecutiongraph%2Futils%2FSimpleAckingTaskManagerGateway.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fexecutiongraph%2Futils%2FSimpleAckingTaskManagerGateway.java?ref=ff89afde95cda9a6507668e57b1631e0e57bd135",
      "patch": "@@ -54,12 +54,12 @@ public class SimpleAckingTaskManagerGateway implements TaskManagerGateway {\n \n \tprivate BiConsumer<JobID, Collection<ResultPartitionID>> releasePartitionsConsumer = (ignore1, ignore2) -> { };\n \n-\tpublic void setSubmitConsumer(Consumer<TaskDeploymentDescriptor> predicate) {\n-\t\tsubmitConsumer = predicate;\n+\tpublic void setSubmitConsumer(Consumer<TaskDeploymentDescriptor> submitConsumer) {\n+\t\tthis.submitConsumer = submitConsumer;\n \t}\n \n-\tpublic void setCancelConsumer(Consumer<ExecutionAttemptID> predicate) {\n-\t\tcancelConsumer = predicate;\n+\tpublic void setCancelConsumer(Consumer<ExecutionAttemptID> cancelConsumer) {\n+\t\tthis.cancelConsumer = cancelConsumer;\n \t}\n \n \tpublic void setFreeSlotFunction(BiFunction<AllocationID, Throwable, CompletableFuture<Acknowledge>> freeSlotFunction) {",
      "parent_sha": "5992662e6ed5f1c32015652e9bbad06d27bf4712"
    }
  },
  {
    "oid": "0f2711618f543248a5dcd6ba8e3c4dc2b55fa568",
    "message": "[FLINK-8699][checkpointing] Create deep copy of state meta data to avoid concurrency problem with checkpoints",
    "date": "2018-02-25T14:59:54Z",
    "url": "https://github.com/apache/flink/commit/0f2711618f543248a5dcd6ba8e3c4dc2b55fa568",
    "details": {
      "sha": "3accbe52daaaaba1d9a6fcf6a37e9325285dca91",
      "filename": "flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java",
      "status": "modified",
      "additions": 9,
      "deletions": 6,
      "changes": 15,
      "blob_url": "https://github.com/apache/flink/blob/0f2711618f543248a5dcd6ba8e3c4dc2b55fa568/flink-state-backends%2Fflink-statebackend-rocksdb%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcontrib%2Fstreaming%2Fstate%2FRocksDBKeyedStateBackend.java",
      "raw_url": "https://github.com/apache/flink/raw/0f2711618f543248a5dcd6ba8e3c4dc2b55fa568/flink-state-backends%2Fflink-statebackend-rocksdb%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcontrib%2Fstreaming%2Fstate%2FRocksDBKeyedStateBackend.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-state-backends%2Fflink-statebackend-rocksdb%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcontrib%2Fstreaming%2Fstate%2FRocksDBKeyedStateBackend.java?ref=0f2711618f543248a5dcd6ba8e3c4dc2b55fa568",
      "patch": "@@ -1818,6 +1818,7 @@ static class RocksDBFullSnapshotOperation<K>\n \n \t\tprivate Snapshot snapshot;\n \t\tprivate ReadOptions readOptions;\n+\t\tprivate List<Tuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>>> kvStateInformationCopy;\n \t\tprivate List<Tuple2<RocksIterator, Integer>> kvStateIterators;\n \n \t\tprivate CheckpointStreamWithResultProvider checkpointStreamWithResultProvider;\n@@ -1841,7 +1842,7 @@ static class RocksDBFullSnapshotOperation<K>\n \t\t */\n \t\tpublic void takeDBSnapShot() {\n \t\t\tPreconditions.checkArgument(snapshot == null, \"Only one ongoing snapshot allowed!\");\n-\t\t\tthis.kvStateIterators = new ArrayList<>(stateBackend.kvStateInformation.size());\n+\t\t\tthis.kvStateInformationCopy = new ArrayList<>(stateBackend.kvStateInformation.values());\n \t\t\tthis.snapshot = stateBackend.db.getSnapshot();\n \t\t}\n \n@@ -1928,20 +1929,22 @@ public void releaseSnapshotResources() {\n \t\tprivate void writeKVStateMetaData() throws IOException {\n \n \t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> metaInfoSnapshots =\n-\t\t\t\tnew ArrayList<>(stateBackend.kvStateInformation.size());\n+\t\t\t\tnew ArrayList<>(kvStateInformationCopy.size());\n+\n+\t\t\tthis.kvStateIterators = new ArrayList<>(kvStateInformationCopy.size());\n \n \t\t\tint kvStateId = 0;\n-\t\t\tfor (Map.Entry<String, Tuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>>> column :\n-\t\t\t\tstateBackend.kvStateInformation.entrySet()) {\n+\t\t\tfor (Tuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>> column :\n+\t\t\t\tkvStateInformationCopy) {\n \n-\t\t\t\tmetaInfoSnapshots.add(column.getValue().f1.snapshot());\n+\t\t\t\tmetaInfoSnapshots.add(column.f1.snapshot());\n \n \t\t\t\t//retrieve iterator for this k/v states\n \t\t\t\treadOptions = new ReadOptions();\n \t\t\t\treadOptions.setSnapshot(snapshot);\n \n \t\t\t\tkvStateIterators.add(\n-\t\t\t\t\tnew Tuple2<>(stateBackend.db.newIterator(column.getValue().f0, readOptions), kvStateId));\n+\t\t\t\t\tnew Tuple2<>(stateBackend.db.newIterator(column.f0, readOptions), kvStateId));\n \n \t\t\t\t++kvStateId;\n \t\t\t}",
      "parent_sha": "08d088103b1b4b2c0c772ec36a8b3e52b2588b5f"
    }
  },
  {
    "oid": "30a8d2c9423d0ed9da4b80d41889e64de7ac1576",
    "message": "[FLINK-30613][serializer] Migrate GenericTypeSerializerSnapshot to implement new method of resolving schema compatibility",
    "date": "2024-01-15T02:10:09Z",
    "url": "https://github.com/apache/flink/commit/30a8d2c9423d0ed9da4b80d41889e64de7ac1576",
    "details": {
      "sha": "9cc6cfe60e499070889f0f759b888cd1ba1fd7ae",
      "filename": "flink-core/src/main/java/org/apache/flink/api/common/typeutils/GenericTypeSerializerSnapshot.java",
      "status": "modified",
      "additions": 8,
      "deletions": 5,
      "changes": 13,
      "blob_url": "https://github.com/apache/flink/blob/30a8d2c9423d0ed9da4b80d41889e64de7ac1576/flink-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fcommon%2Ftypeutils%2FGenericTypeSerializerSnapshot.java",
      "raw_url": "https://github.com/apache/flink/raw/30a8d2c9423d0ed9da4b80d41889e64de7ac1576/flink-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fcommon%2Ftypeutils%2FGenericTypeSerializerSnapshot.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fcommon%2Ftypeutils%2FGenericTypeSerializerSnapshot.java?ref=30a8d2c9423d0ed9da4b80d41889e64de7ac1576",
      "patch": "@@ -81,13 +81,16 @@ public final TypeSerializer<T> restoreSerializer() {\n \n     @Override\n     public final TypeSerializerSchemaCompatibility<T> resolveSchemaCompatibility(\n-            TypeSerializer<T> newSerializer) {\n-        if (!serializerClass().isInstance(newSerializer)) {\n+            TypeSerializerSnapshot<T> oldSerializerSnapshot) {\n+        if (!(oldSerializerSnapshot instanceof GenericTypeSerializerSnapshot)) {\n             return TypeSerializerSchemaCompatibility.incompatible();\n         }\n-        @SuppressWarnings(\"unchecked\")\n-        S casted = (S) newSerializer;\n-        if (typeClass == getTypeClass(casted)) {\n+        GenericTypeSerializerSnapshot<T, S> previousGenericTypeSerializerSnapshot =\n+                (GenericTypeSerializerSnapshot<T, S>) oldSerializerSnapshot;\n+        if (serializerClass() != previousGenericTypeSerializerSnapshot.serializerClass()) {\n+            return TypeSerializerSchemaCompatibility.incompatible();\n+        }\n+        if (typeClass == previousGenericTypeSerializerSnapshot.typeClass) {\n             return TypeSerializerSchemaCompatibility.compatibleAsIs();\n         } else {\n             return TypeSerializerSchemaCompatibility.incompatible();",
      "parent_sha": "0d11103b7bc88c6eb5e4fa598008591d332d8d30"
    }
  },
  {
    "oid": "f152542468b37783932fc2c7725a3a5871b7a701",
    "message": "[FLINK-8814] [file system sinks] Control over the extension of part files created by BucketingSink.",
    "date": "2018-03-01T14:04:53Z",
    "url": "https://github.com/apache/flink/commit/f152542468b37783932fc2c7725a3a5871b7a701",
    "details": {
      "sha": "faf3c566803e8c386def4f704dd724db4e3fa20e",
      "filename": "flink-connectors/flink-connector-filesystem/src/main/java/org/apache/flink/streaming/connectors/fs/bucketing/BucketingSink.java",
      "status": "modified",
      "additions": 20,
      "deletions": 2,
      "changes": 22,
      "blob_url": "https://github.com/apache/flink/blob/f152542468b37783932fc2c7725a3a5871b7a701/flink-connectors%2Fflink-connector-filesystem%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Ffs%2Fbucketing%2FBucketingSink.java",
      "raw_url": "https://github.com/apache/flink/raw/f152542468b37783932fc2c7725a3a5871b7a701/flink-connectors%2Fflink-connector-filesystem%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Ffs%2Fbucketing%2FBucketingSink.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors%2Fflink-connector-filesystem%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fconnectors%2Ffs%2Fbucketing%2FBucketingSink.java?ref=f152542468b37783932fc2c7725a3a5871b7a701",
      "patch": "@@ -226,7 +226,12 @@ public class BucketingSink<T>\n \t/**\n \t * The default prefix for part files.\n \t */\n-\tprivate static final String DEFAULT_PART_REFIX = \"part\";\n+\tprivate static final String DEFAULT_PART_PREFIX = \"part\";\n+\n+\t/**\n+\t * The default suffix for part files.\n+\t */\n+\tprivate static final String DEFAULT_PART_SUFFIX = null;\n \n \t/**\n \t * The default timeout for asynchronous operations such as recoverLease and truncate (in {@code ms}).\n@@ -263,7 +268,8 @@ public class BucketingSink<T>\n \tprivate String validLengthSuffix = DEFAULT_VALID_SUFFIX;\n \tprivate String validLengthPrefix = DEFAULT_VALID_PREFIX;\n \n-\tprivate String partPrefix = DEFAULT_PART_REFIX;\n+\tprivate String partPrefix = DEFAULT_PART_PREFIX;\n+\tprivate String partSuffix = DEFAULT_PART_SUFFIX;\n \n \tprivate boolean useTruncate = true;\n \n@@ -530,6 +536,10 @@ private void openNewPartFile(Path bucketPath, BucketState<T> bucketState) throws\n \t\t\tpartPath = new Path(bucketPath, partPrefix + \"-\" + subtaskIndex + \"-\" + bucketState.partCounter);\n \t\t}\n \n+\t\tif (partSuffix != null) {\n+\t\t\tpartPath = partPath.suffix(partSuffix);\n+\t\t}\n+\n \t\t// increase, so we don't have to check for this name next time\n \t\tbucketState.partCounter++;\n \n@@ -986,6 +996,14 @@ public BucketingSink<T> setValidLengthPrefix(String validLengthPrefix) {\n \t\treturn this;\n \t}\n \n+\t/**\n+\t * Sets the prefix of part files.  The default is no suffix.\n+\t */\n+\tpublic BucketingSink<T> setPartSuffix(String partSuffix) {\n+\t\tthis.partSuffix = partSuffix;\n+\t\treturn this;\n+\t}\n+\n \t/**\n \t * Sets the prefix of part files.  The default is {@code \"part\"}.\n \t */",
      "parent_sha": "53345c841a9398f2e36a30b648301d535b0563cc"
    }
  },
  {
    "oid": "1e61e92f59a2bd713cb7813aad56282d2a826bcb",
    "message": "Corrected code style of OutgoingConnection.java",
    "date": "2011-04-05T19:08:40Z",
    "url": "https://github.com/apache/flink/commit/1e61e92f59a2bd713cb7813aad56282d2a826bcb",
    "details": {
      "sha": "a5e21801040ae30128d56438f41369f8595c51b5",
      "filename": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/OutgoingConnection.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/flink/blob/1e61e92f59a2bd713cb7813aad56282d2a826bcb/nephele%2Fnephele-server%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Ftaskmanager%2Fbytebuffered%2FOutgoingConnection.java",
      "raw_url": "https://github.com/apache/flink/raw/1e61e92f59a2bd713cb7813aad56282d2a826bcb/nephele%2Fnephele-server%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Ftaskmanager%2Fbytebuffered%2FOutgoingConnection.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele%2Fnephele-server%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Ftaskmanager%2Fbytebuffered%2FOutgoingConnection.java?ref=1e61e92f59a2bd713cb7813aad56282d2a826bcb",
      "patch": "@@ -493,10 +493,10 @@ public void dropAllQueuedEnvelopesForChannel(ChannelID channelID, boolean source\n \t\t\t\t}\n \t\t\t}\n \t\t}\n-\t\t\n+\n \t\t// Recycle buffer outside of queuedEnvelopes monitor, otherwise dead locks might occur\n \t\tfinal Iterator<Buffer> it = buffersToRecycle.iterator();\n-\t\twhile(it.hasNext()) {\n+\t\twhile (it.hasNext()) {\n \t\t\tit.next().recycleBuffer();\n \t\t}\n \t}",
      "parent_sha": "b7f2f83fc09c19f1805e8b31a3de6970b983d3e5"
    }
  },
  {
    "oid": "dbae7aefb2111a4805f9aecbbd3ba2422e353531",
    "message": "[hotfix][tests] Cleaned up JDK/assertj plugin warnings",
    "date": "2022-09-13T07:10:25Z",
    "url": "https://github.com/apache/flink/commit/dbae7aefb2111a4805f9aecbbd3ba2422e353531",
    "details": {
      "sha": "be48235d7e877fce7d828af4d9644efbc7db80a2",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/dispatcher/DefaultJobManagerRunnerRegistryTest.java",
      "status": "modified",
      "additions": 5,
      "deletions": 5,
      "changes": 10,
      "blob_url": "https://github.com/apache/flink/blob/dbae7aefb2111a4805f9aecbbd3ba2422e353531/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fdispatcher%2FDefaultJobManagerRunnerRegistryTest.java",
      "raw_url": "https://github.com/apache/flink/raw/dbae7aefb2111a4805f9aecbbd3ba2422e353531/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fdispatcher%2FDefaultJobManagerRunnerRegistryTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fdispatcher%2FDefaultJobManagerRunnerRegistryTest.java?ref=dbae7aefb2111a4805f9aecbbd3ba2422e353531",
      "patch": "@@ -101,9 +101,9 @@ public void testGetOnNonExistingJobManagerRunner() {\n \n     @Test\n     public void size() {\n-        assertThat(testInstance.size()).isEqualTo(0);\n+        assertThat(testInstance.size()).isZero();\n         testInstance.register(TestingJobManagerRunner.newBuilder().build());\n-        assertThat(testInstance.size()).isEqualTo(1);\n+        assertThat(testInstance.size()).isOne();\n         testInstance.register(TestingJobManagerRunner.newBuilder().build());\n         assertThat(testInstance.size()).isEqualTo(2);\n     }\n@@ -134,7 +134,7 @@ public void testGetJobManagerRunners() {\n     }\n \n     @Test\n-    public void testSuccessfulLocalCleanup() throws Throwable {\n+    public void testSuccessfulLocalCleanup() {\n         final TestingJobManagerRunner jobManagerRunner = registerTestingJobManagerRunner();\n \n         assertThat(\n@@ -168,7 +168,7 @@ public void testFailingLocalCleanup() {\n     }\n \n     @Test\n-    public void testSuccessfulLocalCleanupAsync() throws Exception {\n+    public void testSuccessfulLocalCleanupAsync() {\n         final TestingJobManagerRunner jobManagerRunner = registerTestingJobManagerRunner();\n \n         final CompletableFuture<Void> cleanupResult =\n@@ -179,7 +179,7 @@ public void testSuccessfulLocalCleanupAsync() throws Exception {\n     }\n \n     @Test\n-    public void testFailingLocalCleanupAsync() throws Exception {\n+    public void testFailingLocalCleanupAsync() {\n         final TestingJobManagerRunner jobManagerRunner = registerTestingJobManagerRunner();\n \n         assertThat(testInstance.isRegistered(jobManagerRunner.getJobID())).isTrue();",
      "parent_sha": "4e99d64b4914ca5fbb1fe338455a8f653c164172"
    }
  },
  {
    "oid": "7c0f4a3c7f1d14587f715c2d6e75efcfadb6f001",
    "message": "[FLINK-13934][rest] Use hamcrest for checking String contents",
    "date": "2019-09-06T14:19:55Z",
    "url": "https://github.com/apache/flink/commit/7c0f4a3c7f1d14587f715c2d6e75efcfadb6f001",
    "details": {
      "sha": "ef1ea1662b059bf0850804864517bbcf250ca045",
      "filename": "flink-runtime-web/src/test/java/org/apache/flink/runtime/webmonitor/history/HistoryServerStaticFileServerHandlerTest.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/flink/blob/7c0f4a3c7f1d14587f715c2d6e75efcfadb6f001/flink-runtime-web%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fwebmonitor%2Fhistory%2FHistoryServerStaticFileServerHandlerTest.java",
      "raw_url": "https://github.com/apache/flink/raw/7c0f4a3c7f1d14587f715c2d6e75efcfadb6f001/flink-runtime-web%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fwebmonitor%2Fhistory%2FHistoryServerStaticFileServerHandlerTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime-web%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fwebmonitor%2Fhistory%2FHistoryServerStaticFileServerHandlerTest.java?ref=7c0f4a3c7f1d14587f715c2d6e75efcfadb6f001",
      "patch": "@@ -63,7 +63,7 @@ public void testRespondWithFile() throws Exception {\n \t\t\t// verify that a) a file can be loaded using the ClassLoader and b) that the HistoryServer\n \t\t\t// index_hs.html is injected\n \t\t\tString index = HistoryServerTest.getFromHTTP(\"http://localhost:\" + port + \"/index.html\");\n-\t\t\tAssert.assertTrue(index.contains(\"Apache Flink Web Dashboard\"));\n+\t\t\tAssert.assertThat(index, containsString(\"Apache Flink Web Dashboard\"));\n \n \t\t\t// verify that index.html is appended if the request path ends on '/'\n \t\t\tString index2 = HistoryServerTest.getFromHTTP(\"http://localhost:\" + port + \"/\");\n@@ -73,12 +73,12 @@ public void testRespondWithFile() throws Exception {\n \t\t\tFile dir = new File(webDir, \"dir.json\");\n \t\t\tdir.mkdirs();\n \t\t\tString dirNotFound404 = HistoryServerTest.getFromHTTP(\"http://localhost:\" + port + \"/dir\");\n-\t\t\tAssert.assertTrue(dirNotFound404.contains(\"not found\"));\n+\t\t\tAssert.assertThat(dirNotFound404, containsString(\"not found\"));\n \n \t\t\t// verify that a 404 message is returned when requesting a file outside the webDir\n \t\t\ttmp.newFile(\"secret\");\n \t\t\tString x = HistoryServerTest.getFromHTTP(\"http://localhost:\" + port + \"/../secret\");\n-\t\t\tAssert.assertTrue(x.contains(\"not found\"));\n+\t\t\tAssert.assertThat(x, containsString(\"not found\"));\n \t\t} finally {\n \t\t\twebUI.shutdown();\n \t\t}",
      "parent_sha": "fe84fc5d06a76e3e32e394e7f8bcf87c47b87e5b"
    }
  },
  {
    "oid": "dfb0275d8249785da8e6c47c475dc0d00970e42c",
    "message": "[FLINK-21151] Pass whole resources instead of fields in RocksFullSnapshotStrategy\n\nWe also move the fillMetaData() method and hide it in the resources.",
    "date": "2021-01-29T14:53:50Z",
    "url": "https://github.com/apache/flink/commit/dfb0275d8249785da8e6c47c475dc0d00970e42c",
    "details": {
      "sha": "4372c354221319d5c96ba4122fd9c1906034280d",
      "filename": "flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/snapshot/RocksFullSnapshotStrategy.java",
      "status": "modified",
      "additions": 34,
      "deletions": 35,
      "changes": 69,
      "blob_url": "https://github.com/apache/flink/blob/dfb0275d8249785da8e6c47c475dc0d00970e42c/flink-state-backends%2Fflink-statebackend-rocksdb%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcontrib%2Fstreaming%2Fstate%2Fsnapshot%2FRocksFullSnapshotStrategy.java",
      "raw_url": "https://github.com/apache/flink/raw/dfb0275d8249785da8e6c47c475dc0d00970e42c/flink-state-backends%2Fflink-statebackend-rocksdb%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcontrib%2Fstreaming%2Fstate%2Fsnapshot%2FRocksFullSnapshotStrategy.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-state-backends%2Fflink-statebackend-rocksdb%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fcontrib%2Fstreaming%2Fstate%2Fsnapshot%2FRocksFullSnapshotStrategy.java?ref=dfb0275d8249785da8e6c47c475dc0d00970e42c",
      "patch": "@@ -153,10 +153,7 @@ public SnapshotResultSupplier<KeyedStateHandle> asyncSnapshot(\n                                 checkpointId, checkpointStreamFactory, checkpointOptions);\n \n         return new SnapshotAsynchronousPartCallable(\n-                checkpointStreamSupplier,\n-                fullRocksDBSnapshotResources.snapshot,\n-                fullRocksDBSnapshotResources.stateMetaInfoSnapshots,\n-                fullRocksDBSnapshotResources.metaDataCopy);\n+                checkpointStreamSupplier, fullRocksDBSnapshotResources);\n     }\n \n     @Override\n@@ -198,24 +195,16 @@ private class SnapshotAsynchronousPartCallable\n                 checkpointStreamSupplier;\n \n         /** RocksDB snapshot. */\n-        @Nonnull private final Snapshot snapshot;\n-\n-        @Nonnull private final List<StateMetaInfoSnapshot> stateMetaInfoSnapshots;\n-\n-        @Nonnull private final List<MetaData> metaData;\n+        @Nonnull private final FullRocksDBSnapshotResources snapshotResources;\n \n         SnapshotAsynchronousPartCallable(\n                 @Nonnull\n                         SupplierWithException<CheckpointStreamWithResultProvider, Exception>\n                                 checkpointStreamSupplier,\n-                @Nonnull Snapshot snapshot,\n-                @Nonnull List<StateMetaInfoSnapshot> stateMetaInfoSnapshots,\n-                @Nonnull List<RocksDbKvStateInfo> metaDataCopy) {\n+                @Nonnull FullRocksDBSnapshotResources snapshotResources) {\n \n             this.checkpointStreamSupplier = checkpointStreamSupplier;\n-            this.snapshot = snapshot;\n-            this.stateMetaInfoSnapshots = stateMetaInfoSnapshots;\n-            this.metaData = fillMetaData(metaDataCopy);\n+            this.snapshotResources = snapshotResources;\n         }\n \n         @Override\n@@ -253,7 +242,7 @@ private void writeSnapshotToOutputStream(\n             final ReadOptions readOptions = new ReadOptions();\n \n             try {\n-                readOptions.setSnapshot(snapshot);\n+                readOptions.setSnapshot(snapshotResources.snapshot);\n                 kvStateIterators = createKVStateIterators(readOptions);\n \n                 writeKVStateData(\n@@ -272,6 +261,8 @@ private void writeSnapshotToOutputStream(\n \n         private List<Tuple2<RocksIteratorWrapper, Integer>> createKVStateIterators(\n                 ReadOptions readOptions) {\n+\n+            List<MetaData> metaData = snapshotResources.getMetaData();\n             final List<Tuple2<RocksIteratorWrapper, Integer>> kvStateIterators =\n                     new ArrayList<>(metaData.size());\n \n@@ -300,7 +291,7 @@ private void writeKVStateMetaData(final DataOutputView outputView) throws IOExce\n                             // get a serialized form already at state registration time in the\n                             // future\n                             keySerializer,\n-                            stateMetaInfoSnapshots,\n+                            snapshotResources.stateMetaInfoSnapshots,\n                             !Objects.equals(\n                                     UncompressedStreamCompressionDecorator.INSTANCE,\n                                     keyGroupCompressionDecorator));\n@@ -429,22 +420,6 @@ private void checkInterrupted() throws InterruptedException {\n         }\n     }\n \n-    private static List<MetaData> fillMetaData(List<RocksDbKvStateInfo> metaDataCopy) {\n-        List<MetaData> metaData = new ArrayList<>(metaDataCopy.size());\n-        for (RocksDbKvStateInfo rocksDbKvStateInfo : metaDataCopy) {\n-            StateSnapshotTransformer<byte[]> stateSnapshotTransformer = null;\n-            if (rocksDbKvStateInfo.metaInfo instanceof RegisteredKeyValueStateBackendMetaInfo) {\n-                stateSnapshotTransformer =\n-                        ((RegisteredKeyValueStateBackendMetaInfo<?, ?>) rocksDbKvStateInfo.metaInfo)\n-                                .getStateSnapshotTransformFactory()\n-                                .createForSerializedState()\n-                                .orElse(null);\n-            }\n-            metaData.add(new MetaData(rocksDbKvStateInfo, stateSnapshotTransformer));\n-        }\n-        return metaData;\n-    }\n-\n     private static RocksIteratorWrapper getRocksIterator(\n             RocksDB db,\n             ColumnFamilyHandle columnFamilyHandle,\n@@ -471,10 +446,10 @@ private MetaData(\n \n     static class FullRocksDBSnapshotResources implements SnapshotResources {\n         private final List<StateMetaInfoSnapshot> stateMetaInfoSnapshots;\n-        private final List<RocksDbKvStateInfo> metaDataCopy;\n         private final ResourceGuard.Lease lease;\n         private final Snapshot snapshot;\n         private final RocksDB db;\n+        private final List<MetaData> metaData;\n \n         public FullRocksDBSnapshotResources(\n                 ResourceGuard.Lease lease,\n@@ -484,9 +459,33 @@ public FullRocksDBSnapshotResources(\n                 RocksDB db) {\n             this.lease = lease;\n             this.snapshot = snapshot;\n-            this.metaDataCopy = metaDataCopy;\n             this.stateMetaInfoSnapshots = stateMetaInfoSnapshots;\n             this.db = db;\n+\n+            // we need to to this in the constructor, i.e. in the synchronous part of the snapshot\n+            // TODO: better yet, we can do it outside the constructor\n+            this.metaData = fillMetaData(metaDataCopy);\n+        }\n+\n+        private List<MetaData> fillMetaData(List<RocksDbKvStateInfo> metaDataCopy) {\n+            List<MetaData> metaData = new ArrayList<>(metaDataCopy.size());\n+            for (RocksDbKvStateInfo rocksDbKvStateInfo : metaDataCopy) {\n+                StateSnapshotTransformer<byte[]> stateSnapshotTransformer = null;\n+                if (rocksDbKvStateInfo.metaInfo instanceof RegisteredKeyValueStateBackendMetaInfo) {\n+                    stateSnapshotTransformer =\n+                            ((RegisteredKeyValueStateBackendMetaInfo<?, ?>)\n+                                            rocksDbKvStateInfo.metaInfo)\n+                                    .getStateSnapshotTransformFactory()\n+                                    .createForSerializedState()\n+                                    .orElse(null);\n+                }\n+                metaData.add(new MetaData(rocksDbKvStateInfo, stateSnapshotTransformer));\n+            }\n+            return metaData;\n+        }\n+\n+        private List<MetaData> getMetaData() {\n+            return metaData;\n         }\n \n         @Override",
      "parent_sha": "033e3e36849078af2a3806c961d1260cc8c1b19c"
    }
  },
  {
    "oid": "b25dfaee80727d6662a5fd445fe51cc139a8b9eb",
    "message": "[FLINK-33968][runtime] Advance the calculation of num of subpartitions to the time of initializing execution job vertex\n\nThis closes #24019.",
    "date": "2024-01-05T02:37:01Z",
    "url": "https://github.com/apache/flink/commit/b25dfaee80727d6662a5fd445fe51cc139a8b9eb",
    "details": {
      "sha": "e132f9079f10fc0ea82c22b5039dbdfd73b56dc8",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/IntermediateResultPartition.java",
      "status": "modified",
      "additions": 24,
      "deletions": 20,
      "changes": 44,
      "blob_url": "https://github.com/apache/flink/blob/b25dfaee80727d6662a5fd445fe51cc139a8b9eb/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fexecutiongraph%2FIntermediateResultPartition.java",
      "raw_url": "https://github.com/apache/flink/raw/b25dfaee80727d6662a5fd445fe51cc139a8b9eb/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fexecutiongraph%2FIntermediateResultPartition.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fexecutiongraph%2FIntermediateResultPartition.java?ref=b25dfaee80727d6662a5fd445fe51cc139a8b9eb",
      "patch": "@@ -43,8 +43,8 @@ public class IntermediateResultPartition {\n \n     private final EdgeManager edgeManager;\n \n-    /** Number of subpartitions. Initialized lazily and will not change once set. */\n-    private int numberOfSubpartitions = NUM_SUBPARTITIONS_UNKNOWN;\n+    /** Number of subpartitions for dynamic graph. */\n+    private final int numberOfSubpartitionsForDynamicGraph;\n \n     /** Whether this partition has produced all data. */\n     private boolean dataAllProduced = false;\n@@ -64,6 +64,17 @@ public IntermediateResultPartition(\n         this.producer = producer;\n         this.partitionId = new IntermediateResultPartitionID(totalResult.getId(), partitionNumber);\n         this.edgeManager = edgeManager;\n+\n+        if (!producer.getExecutionGraphAccessor().isDynamic()) {\n+            this.numberOfSubpartitionsForDynamicGraph = NUM_SUBPARTITIONS_UNKNOWN;\n+        } else {\n+            this.numberOfSubpartitionsForDynamicGraph =\n+                    computeNumberOfSubpartitionsForDynamicGraph();\n+            checkState(\n+                    numberOfSubpartitionsForDynamicGraph > 0,\n+                    \"Number of subpartitions is an unexpected value: \"\n+                            + numberOfSubpartitionsForDynamicGraph);\n+        }\n     }\n \n     public void markPartitionGroupReleasable(ConsumedPartitionGroup partitionGroup) {\n@@ -114,17 +125,6 @@ public List<ConsumedPartitionGroup> getConsumedPartitionGroups() {\n     }\n \n     public int getNumberOfSubpartitions() {\n-        if (numberOfSubpartitions == NUM_SUBPARTITIONS_UNKNOWN) {\n-            numberOfSubpartitions = computeNumberOfSubpartitions();\n-            checkState(\n-                    numberOfSubpartitions > 0,\n-                    \"Number of subpartitions is an unexpected value: \" + numberOfSubpartitions);\n-        }\n-\n-        return numberOfSubpartitions;\n-    }\n-\n-    private int computeNumberOfSubpartitions() {\n         if (!getProducer().getExecutionGraphAccessor().isDynamic()) {\n             List<ConsumerVertexGroup> consumerVertexGroups = getConsumerVertexGroups();\n             checkState(!consumerVertexGroups.isEmpty());\n@@ -134,13 +134,17 @@ private int computeNumberOfSubpartitions() {\n             // for non-dynamic graph.\n             return consumerVertexGroups.get(0).size();\n         } else {\n-            if (totalResult.isBroadcast()) {\n-                // for dynamic graph and broadcast result, we only produced one subpartition,\n-                // and all the downstream vertices should consume this subpartition.\n-                return 1;\n-            } else {\n-                return computeNumberOfMaxPossiblePartitionConsumers();\n-            }\n+            return numberOfSubpartitionsForDynamicGraph;\n+        }\n+    }\n+\n+    private int computeNumberOfSubpartitionsForDynamicGraph() {\n+        if (totalResult.isBroadcast()) {\n+            // for dynamic graph and broadcast result, we only produced one subpartition,\n+            // and all the downstream vertices should consume this subpartition.\n+            return 1;\n+        } else {\n+            return computeNumberOfMaxPossiblePartitionConsumers();\n         }\n     }\n ",
      "parent_sha": "aba1ee85d6a3854fdb1f8a628fed0ad19460d086"
    }
  },
  {
    "oid": "e3cfa40bf67def24d4b528055bb479298241764d",
    "message": "[FLINK-22345][coordination] Catch pre-mature state restore for Operator Coordinators",
    "date": "2021-04-22T09:35:01Z",
    "url": "https://github.com/apache/flink/commit/e3cfa40bf67def24d4b528055bb479298241764d",
    "details": {
      "sha": "10696b8fa12e42f4e4c4d02747e17fddbb3757b5",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/operators/coordination/OperatorCoordinatorHolder.java",
      "status": "modified",
      "additions": 12,
      "deletions": 8,
      "changes": 20,
      "blob_url": "https://github.com/apache/flink/blob/e3cfa40bf67def24d4b528055bb479298241764d/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Foperators%2Fcoordination%2FOperatorCoordinatorHolder.java",
      "raw_url": "https://github.com/apache/flink/raw/e3cfa40bf67def24d4b528055bb479298241764d/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Foperators%2Fcoordination%2FOperatorCoordinatorHolder.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Foperators%2Fcoordination%2FOperatorCoordinatorHolder.java?ref=e3cfa40bf67def24d4b528055bb479298241764d",
      "patch": "@@ -390,20 +390,24 @@ private void setupSubtaskGateway(int subtask) {\n \n         // We need to do this synchronously here, otherwise we violate the contract that\n         // 'subtaskFailed()' will never overtake 'subtaskReady()'.\n-        // An alternative, if we ever figure out that this cannot work synchronously here,\n-        // is that we re-enqueue all actions (like 'subtaskFailed()' and 'subtaskRestored()')\n-        // back into the main thread executor, rather than directly calling the OperatorCoordinator\n+        // ---\n+        // It is also possible that by the time this method here is called, the task execution is in\n+        // a no-longer running state. That happens when the scheduler deals with overlapping global\n+        // failures and the restore method is in fact not yet restoring to the new execution\n+        // attempts, but still targeting the previous execution attempts (and is later subsumed\n+        // by another restore to the new execution attempt). This is tricky behavior that we need\n+        // to work around. So if the task is no longer running, we don't call the 'subtaskReady()'\n+        // method.\n         FutureUtils.assertNoException(\n                 sta.hasSwitchedToRunning()\n                         .thenAccept(\n                                 (ignored) -> {\n                                     mainThreadExecutor.assertRunningInMainThread();\n \n-                                    // this is a guard in case someone accidentally makes the\n-                                    // notification asynchronous\n-                                    assert sta.isStillRunning();\n-\n-                                    notifySubtaskReady(subtask, gateway);\n+                                    // see bigger comment above\n+                                    if (sta.isStillRunning()) {\n+                                        notifySubtaskReady(subtask, gateway);\n+                                    }\n                                 }));\n     }\n ",
      "parent_sha": "fa0be65cf81cf4cb53ddbfb8c5cedb271e264f19"
    }
  },
  {
    "oid": "2f1c4740cebcdd66aad119214501bb51a1031d3d",
    "message": "[hotfix] [streaming api] Non-merging triggers throw UnsupportedOperationException instead of RuntimeException",
    "date": "2017-01-22T20:22:21Z",
    "url": "https://github.com/apache/flink/commit/2f1c4740cebcdd66aad119214501bb51a1031d3d",
    "details": {
      "sha": "11a0d6dcdac0ad3af30edf58cbe6937d60016c96",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/windowing/triggers/Trigger.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/2f1c4740cebcdd66aad119214501bb51a1031d3d/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fwindowing%2Ftriggers%2FTrigger.java",
      "raw_url": "https://github.com/apache/flink/raw/2f1c4740cebcdd66aad119214501bb51a1031d3d/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fwindowing%2Ftriggers%2FTrigger.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fapi%2Fwindowing%2Ftriggers%2FTrigger.java?ref=2f1c4740cebcdd66aad119214501bb51a1031d3d",
      "patch": "@@ -105,7 +105,7 @@ public boolean canMerge() {\n \t * @param ctx A context object that can be used to register timer callbacks and access state.\n \t */\n \tpublic void onMerge(W window, OnMergeContext ctx) throws Exception {\n-\t\tthrow new RuntimeException(\"This trigger does not support merging.\");\n+\t\tthrow new UnsupportedOperationException(\"This trigger does not support merging.\");\n \t}\n \n \t/**",
      "parent_sha": "254a70072aa3bbd51277e99e0982c5e29908d684"
    }
  },
  {
    "oid": "77b327c44a91041e810863d5d8dc6b1aff978d5e",
    "message": "[hotfix] Add @Nullable annotation to JobGraph constructors",
    "date": "2021-03-05T18:37:53Z",
    "url": "https://github.com/apache/flink/commit/77b327c44a91041e810863d5d8dc6b1aff978d5e",
    "details": {
      "sha": "5ead550fbe3f48aa051fefd59fdf466d126069e4",
      "filename": "flink-runtime/src/main/java/org/apache/flink/runtime/jobgraph/JobGraph.java",
      "status": "modified",
      "additions": 4,
      "deletions": 2,
      "changes": 6,
      "blob_url": "https://github.com/apache/flink/blob/77b327c44a91041e810863d5d8dc6b1aff978d5e/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fjobgraph%2FJobGraph.java",
      "raw_url": "https://github.com/apache/flink/raw/77b327c44a91041e810863d5d8dc6b1aff978d5e/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fjobgraph%2FJobGraph.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fjobgraph%2FJobGraph.java?ref=77b327c44a91041e810863d5d8dc6b1aff978d5e",
      "patch": "@@ -32,6 +32,8 @@\n import org.apache.flink.util.IterableUtils;\n import org.apache.flink.util.SerializedValue;\n \n+import javax.annotation.Nullable;\n+\n import java.io.IOException;\n import java.io.Serializable;\n import java.net.URISyntaxException;\n@@ -134,7 +136,7 @@ public JobGraph(String jobName) {\n      * @param jobId The id of the job. A random ID is generated, if {@code null} is passed.\n      * @param jobName The name of the job.\n      */\n-    public JobGraph(JobID jobId, String jobName) {\n+    public JobGraph(@Nullable JobID jobId, String jobName) {\n         this.jobID = jobId == null ? new JobID() : jobId;\n         this.jobName = jobName == null ? \"(unnamed job)\" : jobName;\n \n@@ -178,7 +180,7 @@ public JobGraph(String jobName, JobVertex... vertices) {\n      * @param jobName The name of the job.\n      * @param vertices The vertices to add to the graph.\n      */\n-    public JobGraph(JobID jobId, String jobName, JobVertex... vertices) {\n+    public JobGraph(@Nullable JobID jobId, String jobName, JobVertex... vertices) {\n         this(jobId, jobName);\n \n         for (JobVertex vertex : vertices) {",
      "parent_sha": "88c2d1026a43965dff8fcb0d44230e76c5ebd0d7"
    }
  },
  {
    "oid": "006c4f07a60e1a4aa9caa3e9b1a0db77e695d954",
    "message": "[hotfix][tests] Cleanup StopWithSavepointTest",
    "date": "2022-06-22T06:55:13Z",
    "url": "https://github.com/apache/flink/commit/006c4f07a60e1a4aa9caa3e9b1a0db77e695d954",
    "details": {
      "sha": "4b2df1246701bd48a44046d13b0aec5cd386727a",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adaptive/StopWithSavepointTest.java",
      "status": "modified",
      "additions": 88,
      "deletions": 91,
      "changes": 179,
      "blob_url": "https://github.com/apache/flink/blob/006c4f07a60e1a4aa9caa3e9b1a0db77e695d954/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fscheduler%2Fadaptive%2FStopWithSavepointTest.java",
      "raw_url": "https://github.com/apache/flink/raw/006c4f07a60e1a4aa9caa3e9b1a0db77e695d954/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fscheduler%2Fadaptive%2FStopWithSavepointTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fscheduler%2Fadaptive%2FStopWithSavepointTest.java?ref=006c4f07a60e1a4aa9caa3e9b1a0db77e695d954",
      "patch": "@@ -19,6 +19,7 @@\n package org.apache.flink.runtime.scheduler.adaptive;\n \n import org.apache.flink.api.common.JobStatus;\n+import org.apache.flink.core.testutils.FlinkAssertions;\n import org.apache.flink.runtime.checkpoint.CheckpointScheduling;\n import org.apache.flink.runtime.execution.ExecutionState;\n import org.apache.flink.runtime.executiongraph.ErrorInfo;\n@@ -29,9 +30,12 @@\n import org.apache.flink.runtime.scheduler.exceptionhistory.ExceptionHistoryEntry;\n import org.apache.flink.runtime.scheduler.exceptionhistory.TestingAccessExecution;\n import org.apache.flink.util.FlinkException;\n-import org.apache.flink.util.TestLogger;\n+import org.apache.flink.util.TestLoggerExtension;\n \n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.time.Duration;\n import java.util.ArrayList;\n@@ -42,18 +46,19 @@\n import java.util.function.Consumer;\n import java.util.function.Function;\n \n-import static org.apache.flink.core.testutils.FlinkMatchers.containsCause;\n import static org.apache.flink.runtime.scheduler.adaptive.WaitingForResourcesTest.assertNonNull;\n import static org.apache.flink.util.Preconditions.checkNotNull;\n-import static org.hamcrest.CoreMatchers.is;\n-import static org.junit.Assert.assertThat;\n+import static org.assertj.core.api.AssertionsForClassTypes.assertThat;\n \n /** Tests for the {@link StopWithSavepoint} state. */\n-public class StopWithSavepointTest extends TestLogger {\n+@ExtendWith(TestLoggerExtension.class)\n+class StopWithSavepointTest {\n+    private static final Logger LOG = LoggerFactory.getLogger(StopWithSavepointTest.class);\n+\n     private static final String SAVEPOINT_PATH = \"test://savepoint/path\";\n \n     @Test\n-    public void testFinishedOnSuccessfulStopWithSavepoint() throws Exception {\n+    void testFinishedOnSuccessfulStopWithSavepoint() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             StateTrackingMockExecutionGraph mockExecutionGraph =\n                     new StateTrackingMockExecutionGraph();\n@@ -68,12 +73,12 @@ public void testFinishedOnSuccessfulStopWithSavepoint() throws Exception {\n             savepointFuture.complete(SAVEPOINT_PATH);\n             ctx.triggerExecutors();\n \n-            assertThat(sws.getOperationFuture().get(), is(SAVEPOINT_PATH));\n+            assertThat(sws.getOperationFuture().get()).isEqualTo(SAVEPOINT_PATH);\n         }\n     }\n \n     @Test\n-    public void testJobFailed() throws Exception {\n+    void testJobFailed() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             StateTrackingMockExecutionGraph mockExecutionGraph =\n                     new StateTrackingMockExecutionGraph();\n@@ -83,24 +88,22 @@ public void testJobFailed() throws Exception {\n \n             ctx.setExpectFailing(\n                     failingArguments -> {\n-                        assertThat(\n-                                failingArguments.getExecutionGraph().getState(),\n-                                is(JobStatus.FAILED));\n-                        assertThat(\n-                                failingArguments.getFailureCause(),\n-                                containsCause(FlinkException.class));\n+                        assertThat(failingArguments.getExecutionGraph().getState())\n+                                .isEqualTo(JobStatus.FAILED);\n+                        assertThat(failingArguments.getFailureCause())\n+                                .satisfies(FlinkAssertions.anyCauseMatches(FlinkException.class));\n                     });\n \n             // fail job:\n             mockExecutionGraph.completeTerminationFuture(JobStatus.FAILED);\n             ctx.triggerExecutors();\n \n-            assertThat(sws.getOperationFuture().isCompletedExceptionally(), is(true));\n+            assertThat(sws.getOperationFuture()).isCompletedExceptionally();\n         }\n     }\n \n     @Test\n-    public void testJobFailedAndSavepointOperationFails() throws Exception {\n+    void testJobFailedAndSavepointOperationFails() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             StateTrackingMockExecutionGraph mockExecutionGraph =\n                     new StateTrackingMockExecutionGraph();\n@@ -112,25 +115,23 @@ public void testJobFailedAndSavepointOperationFails() throws Exception {\n \n             ctx.setExpectFailing(\n                     failingArguments -> {\n-                        assertThat(\n-                                failingArguments.getExecutionGraph().getState(),\n-                                is(JobStatus.FAILED));\n-                        assertThat(\n-                                failingArguments.getFailureCause(),\n-                                containsCause(FlinkException.class));\n+                        assertThat(failingArguments.getExecutionGraph().getState())\n+                                .isEqualTo(JobStatus.FAILED);\n+                        assertThat(failingArguments.getFailureCause())\n+                                .satisfies(FlinkAssertions.anyCauseMatches(FlinkException.class));\n                     });\n \n             // fail job:\n             mockExecutionGraph.completeTerminationFuture(JobStatus.FAILED);\n             savepointFuture.completeExceptionally(new RuntimeException());\n             ctx.triggerExecutors();\n \n-            assertThat(sws.getOperationFuture().isCompletedExceptionally(), is(true));\n+            assertThat(sws.getOperationFuture()).isCompletedExceptionally();\n         }\n     }\n \n     @Test\n-    public void testJobFinishedBeforeSavepointFuture() throws Exception {\n+    void testJobFinishedBeforeSavepointFuture() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             StateTrackingMockExecutionGraph mockExecutionGraph =\n                     new StateTrackingMockExecutionGraph();\n@@ -146,12 +147,12 @@ public void testJobFinishedBeforeSavepointFuture() throws Exception {\n             savepointFuture.complete(SAVEPOINT_PATH);\n             ctx.triggerExecutors();\n \n-            assertThat(sws.getOperationFuture().get(), is(SAVEPOINT_PATH));\n+            assertThat(sws.getOperationFuture().get()).isEqualTo(SAVEPOINT_PATH);\n         }\n     }\n \n     @Test\n-    public void testTransitionToCancellingOnCancel() throws Exception {\n+    void testTransitionToCancellingOnCancel() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             StopWithSavepoint sws = createStopWithSavepoint(ctx);\n             ctx.setStopWithSavepoint(sws);\n@@ -162,20 +163,21 @@ public void testTransitionToCancellingOnCancel() throws Exception {\n     }\n \n     @Test\n-    public void testTransitionToFinishedOnSuspend() throws Exception {\n+    void testTransitionToFinishedOnSuspend() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             StopWithSavepoint sws = createStopWithSavepoint(ctx);\n             ctx.setExpectFinished(\n                     archivedExecutionGraph -> {\n-                        assertThat(archivedExecutionGraph.getState(), is(JobStatus.SUSPENDED));\n+                        assertThat(archivedExecutionGraph.getState())\n+                                .isEqualTo(JobStatus.SUSPENDED);\n                     });\n \n             sws.suspend(new RuntimeException());\n         }\n     }\n \n     @Test\n-    public void testRestartOnGlobalFailureIfRestartConfigured() throws Exception {\n+    void testRestartOnGlobalFailureIfRestartConfigured() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             StopWithSavepoint sws = createStopWithSavepoint(ctx);\n             ctx.setStopWithSavepoint(sws);\n@@ -188,7 +190,7 @@ public void testRestartOnGlobalFailureIfRestartConfigured() throws Exception {\n     }\n \n     @Test\n-    public void testFailingOnGlobalFailureIfNoRestartConfigured() throws Exception {\n+    void testFailingOnGlobalFailureIfNoRestartConfigured() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n \n             StopWithSavepoint sws = createStopWithSavepoint(ctx);\n@@ -197,17 +199,16 @@ public void testFailingOnGlobalFailureIfNoRestartConfigured() throws Exception {\n \n             ctx.setExpectFailing(\n                     failingArguments -> {\n-                        assertThat(\n-                                failingArguments.getFailureCause(),\n-                                containsCause(RuntimeException.class));\n+                        assertThat(failingArguments.getFailureCause())\n+                                .satisfies(FlinkAssertions.anyCauseMatches(RuntimeException.class));\n                     });\n \n             sws.handleGlobalFailure(new RuntimeException());\n         }\n     }\n \n     @Test\n-    public void testFailingOnUpdateTaskExecutionStateWithNoRestart() throws Exception {\n+    void testFailingOnUpdateTaskExecutionStateWithNoRestart() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             StateTrackingMockExecutionGraph executionGraph = new StateTrackingMockExecutionGraph();\n             StopWithSavepoint sws = createStopWithSavepoint(ctx, executionGraph);\n@@ -216,9 +217,8 @@ public void testFailingOnUpdateTaskExecutionStateWithNoRestart() throws Exceptio\n \n             ctx.setExpectFailing(\n                     failingArguments -> {\n-                        assertThat(\n-                                failingArguments.getFailureCause(),\n-                                containsCause(RuntimeException.class));\n+                        assertThat(failingArguments.getFailureCause())\n+                                .satisfies(FlinkAssertions.anyCauseMatches(RuntimeException.class));\n                     });\n \n             Exception exception = new RuntimeException();\n@@ -230,12 +230,12 @@ public void testFailingOnUpdateTaskExecutionStateWithNoRestart() throws Exceptio\n             executionGraph.registerExecution(execution);\n             TaskExecutionStateTransition taskExecutionStateTransition =\n                     ExecutingTest.createFailingStateTransition(execution.getAttemptId(), exception);\n-            assertThat(sws.updateTaskExecutionState(taskExecutionStateTransition), is(true));\n+            assertThat(sws.updateTaskExecutionState(taskExecutionStateTransition)).isTrue();\n         }\n     }\n \n     @Test\n-    public void testRestartingOnUpdateTaskExecutionStateWithRestart() throws Exception {\n+    void testRestartingOnUpdateTaskExecutionStateWithRestart() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             StateTrackingMockExecutionGraph executionGraph = new StateTrackingMockExecutionGraph();\n             StopWithSavepoint sws = createStopWithSavepoint(ctx, executionGraph);\n@@ -253,62 +253,59 @@ public void testRestartingOnUpdateTaskExecutionStateWithRestart() throws Excepti\n             executionGraph.registerExecution(execution);\n             TaskExecutionStateTransition taskExecutionStateTransition =\n                     ExecutingTest.createFailingStateTransition(execution.getAttemptId(), exception);\n-            assertThat(sws.updateTaskExecutionState(taskExecutionStateTransition), is(true));\n+            assertThat(sws.updateTaskExecutionState(taskExecutionStateTransition)).isTrue();\n         }\n     }\n \n     @Test\n-    public void testExceptionalOperationFutureCompletionOnLeaveWhileWaitingOnSavepointCompletion()\n+    void testExceptionalOperationFutureCompletionOnLeaveWhileWaitingOnSavepointCompletion()\n             throws Exception {\n-        MockStopWithSavepointContext ctx = new MockStopWithSavepointContext();\n-        StopWithSavepoint sws = createStopWithSavepoint(ctx);\n-        ctx.setStopWithSavepoint(sws);\n-\n-        sws.onLeave(Canceling.class);\n+        final StopWithSavepoint sws;\n+        try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n+            sws = createStopWithSavepoint(ctx);\n+            ctx.setStopWithSavepoint(sws);\n \n-        ctx.close();\n-        assertThat(sws.getOperationFuture().isCompletedExceptionally(), is(true));\n+            sws.onLeave(Canceling.class);\n+        }\n+        assertThat(sws.getOperationFuture()).isCompletedExceptionally();\n     }\n \n     @Test\n-    public void testExceptionalSavepointCompletionLeadsToExceptionalOperationFutureCompletion()\n+    void testExceptionalSavepointCompletionLeadsToExceptionalOperationFutureCompletion()\n             throws Exception {\n-        MockStopWithSavepointContext ctx = new MockStopWithSavepointContext();\n-        CheckpointScheduling mockStopWithSavepointOperations = new MockCheckpointScheduling();\n-        CompletableFuture<String> savepointFuture = new CompletableFuture<>();\n-        StopWithSavepoint sws =\n-                createStopWithSavepoint(ctx, mockStopWithSavepointOperations, savepointFuture);\n-        ctx.setStopWithSavepoint(sws);\n-        ctx.setExpectExecuting(assertNonNull());\n-\n-        savepointFuture.completeExceptionally(new RuntimeException(\"Test error\"));\n-\n-        ctx.close();\n-        assertThat(sws.getOperationFuture().isCompletedExceptionally(), is(true));\n+        final StopWithSavepoint sws;\n+        try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n+            CheckpointScheduling mockStopWithSavepointOperations = new MockCheckpointScheduling();\n+            CompletableFuture<String> savepointFuture = new CompletableFuture<>();\n+            sws = createStopWithSavepoint(ctx, mockStopWithSavepointOperations, savepointFuture);\n+            ctx.setStopWithSavepoint(sws);\n+            ctx.setExpectExecuting(assertNonNull());\n+\n+            savepointFuture.completeExceptionally(new RuntimeException(\"Test error\"));\n+        }\n+        assertThat(sws.getOperationFuture()).isCompletedExceptionally();\n     }\n \n     @Test\n-    public void testErrorCreatingSavepointLeadsToTransitionToExecutingState() throws Exception {\n-        MockStopWithSavepointContext ctx = new MockStopWithSavepointContext();\n-        CheckpointScheduling mockStopWithSavepointOperations = new MockCheckpointScheduling();\n-        CompletableFuture<String> savepointFuture = new CompletableFuture<>();\n-        StopWithSavepoint sws =\n-                createStopWithSavepoint(ctx, mockStopWithSavepointOperations, savepointFuture);\n-        ctx.setStopWithSavepoint(sws);\n-        ctx.setExpectExecuting(\n-                executingArguments ->\n-                        assertThat(\n-                                executingArguments.getExecutionGraph().getState(),\n-                                is(JobStatus.RUNNING)));\n-\n-        savepointFuture.completeExceptionally(new RuntimeException(\"Test error\"));\n-\n-        ctx.close();\n-        assertThat(sws.getOperationFuture().isCompletedExceptionally(), is(true));\n+    void testErrorCreatingSavepointLeadsToTransitionToExecutingState() throws Exception {\n+        final StopWithSavepoint sws;\n+        try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n+            CheckpointScheduling mockStopWithSavepointOperations = new MockCheckpointScheduling();\n+            CompletableFuture<String> savepointFuture = new CompletableFuture<>();\n+            sws = createStopWithSavepoint(ctx, mockStopWithSavepointOperations, savepointFuture);\n+            ctx.setStopWithSavepoint(sws);\n+            ctx.setExpectExecuting(\n+                    executingArguments ->\n+                            assertThat(executingArguments.getExecutionGraph().getState())\n+                                    .isEqualTo(JobStatus.RUNNING));\n+\n+            savepointFuture.completeExceptionally(new RuntimeException(\"Test error\"));\n+        }\n+        assertThat(sws.getOperationFuture()).isCompletedExceptionally();\n     }\n \n     @Test\n-    public void testRestartOnTaskFailureAfterSavepointCompletion() throws Exception {\n+    void testRestartOnTaskFailureAfterSavepointCompletion() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             CheckpointScheduling mockStopWithSavepointOperations = new MockCheckpointScheduling();\n             CompletableFuture<String> savepointFuture = new CompletableFuture<>();\n@@ -336,17 +333,17 @@ public void testRestartOnTaskFailureAfterSavepointCompletion() throws Exception\n             executionGraph.registerExecution(execution);\n             TaskExecutionStateTransition taskExecutionStateTransition =\n                     ExecutingTest.createFailingStateTransition(execution.getAttemptId(), exception);\n-            assertThat(sws.updateTaskExecutionState(taskExecutionStateTransition), is(true));\n+            assertThat(sws.updateTaskExecutionState(taskExecutionStateTransition)).isTrue();\n         }\n     }\n \n     @Test\n-    public void testEnsureCheckpointSchedulerIsStartedAgain() throws Exception {\n+    void testEnsureCheckpointSchedulerIsStartedAgain() throws Exception {\n         try (MockStopWithSavepointContext ctx = new MockStopWithSavepointContext()) {\n             MockCheckpointScheduling mockStopWithSavepointOperations =\n                     new MockCheckpointScheduling();\n \n-            assertThat(mockStopWithSavepointOperations.isCheckpointSchedulerStarted(), is(false));\n+            assertThat(mockStopWithSavepointOperations.isCheckpointSchedulerStarted()).isFalse();\n \n             CompletableFuture<String> savepointFuture = new CompletableFuture<>();\n             StopWithSavepoint sws =\n@@ -357,48 +354,48 @@ public void testEnsureCheckpointSchedulerIsStartedAgain() throws Exception {\n             // a failure should start the scheduler again\n             savepointFuture.completeExceptionally(new RuntimeException(\"Test error\"));\n             ctx.triggerExecutors();\n-            assertThat(mockStopWithSavepointOperations.isCheckpointSchedulerStarted(), is(true));\n+            assertThat(mockStopWithSavepointOperations.isCheckpointSchedulerStarted()).isTrue();\n         }\n     }\n \n-    private StopWithSavepoint createStopWithSavepoint(MockStopWithSavepointContext ctx) {\n+    private static StopWithSavepoint createStopWithSavepoint(MockStopWithSavepointContext ctx) {\n         return createStopWithSavepoint(\n                 ctx,\n                 new MockCheckpointScheduling(),\n                 new StateTrackingMockExecutionGraph(),\n                 new CompletableFuture<>());\n     }\n \n-    private StopWithSavepoint createStopWithSavepoint(\n+    private static StopWithSavepoint createStopWithSavepoint(\n             MockStopWithSavepointContext ctx,\n             ExecutionGraph executionGraph,\n             CompletableFuture<String> savepointFuture) {\n         return createStopWithSavepoint(\n                 ctx, new MockCheckpointScheduling(), executionGraph, savepointFuture);\n     }\n \n-    private StopWithSavepoint createStopWithSavepoint(\n+    private static StopWithSavepoint createStopWithSavepoint(\n             MockStopWithSavepointContext ctx, ExecutionGraph executionGraph) {\n         return createStopWithSavepoint(ctx, executionGraph, new CompletableFuture<>());\n     }\n \n-    private StopWithSavepoint createStopWithSavepoint(\n+    private static StopWithSavepoint createStopWithSavepoint(\n             MockStopWithSavepointContext ctx,\n             CheckpointScheduling checkpointScheduling,\n             CompletableFuture<String> savepointFuture) {\n         return createStopWithSavepoint(\n                 ctx, checkpointScheduling, new StateTrackingMockExecutionGraph(), savepointFuture);\n     }\n \n-    private StopWithSavepoint createStopWithSavepoint(\n+    private static StopWithSavepoint createStopWithSavepoint(\n             MockStopWithSavepointContext ctx,\n             CheckpointScheduling checkpointScheduling,\n             ExecutionGraph executionGraph,\n             CompletableFuture<String> savepointFuture) {\n         final ExecutionGraphHandler executionGraphHandler =\n                 new ExecutionGraphHandler(\n                         executionGraph,\n-                        log,\n+                        LOG,\n                         ctx.getMainThreadExecutor(),\n                         ctx.getMainThreadExecutor());\n         OperatorCoordinatorHandler operatorCoordinatorHandler =\n@@ -412,7 +409,7 @@ private StopWithSavepoint createStopWithSavepoint(\n                 executionGraphHandler,\n                 operatorCoordinatorHandler,\n                 checkpointScheduling,\n-                log,\n+                LOG,\n                 ClassLoader.getSystemClassLoader(),\n                 savepointFuture,\n                 new ArrayList<>());",
      "parent_sha": "f88d276bb4271cc5f1f52b6af68a5a7250f67c89"
    }
  },
  {
    "oid": "da226b3f0a4ea2d80cdecdaa082de181cbb9dc77",
    "message": "[hotfix] Skip Azure tests if ACCOUNT isn't configured",
    "date": "2022-10-27T13:04:21Z",
    "url": "https://github.com/apache/flink/commit/da226b3f0a4ea2d80cdecdaa082de181cbb9dc77",
    "details": {
      "sha": "6830e29a670d5dbf8486cfc37150e6e96d290163",
      "filename": "flink-filesystems/flink-azure-fs-hadoop/src/test/java/org/apache/flink/fs/azurefs/AzureFileSystemBehaviorITCase.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/flink/blob/da226b3f0a4ea2d80cdecdaa082de181cbb9dc77/flink-filesystems%2Fflink-azure-fs-hadoop%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ffs%2Fazurefs%2FAzureFileSystemBehaviorITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/da226b3f0a4ea2d80cdecdaa082de181cbb9dc77/flink-filesystems%2Fflink-azure-fs-hadoop%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ffs%2Fazurefs%2FAzureFileSystemBehaviorITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-filesystems%2Fflink-azure-fs-hadoop%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ffs%2Fazurefs%2FAzureFileSystemBehaviorITCase.java?ref=da226b3f0a4ea2d80cdecdaa082de181cbb9dc77",
      "patch": "@@ -76,9 +76,6 @@ static void onlyRunIfHttps() throws IOException {\n             assumeThat(TOKEN_CREDENTIALS_FILE)\n                     .describedAs(\"Azure token credentials not configured, skipping test...\")\n                     .isNotBlank();\n-            assumeThat(ACCOUNT)\n-                    .describedAs(\"Azure storage account not configured, skipping test...\")\n-                    .isNotBlank();\n             assumeThat(isHttpsTrafficOnly()).isFalse();\n         }\n \n@@ -113,6 +110,9 @@ private static boolean isHttpsTrafficOnly() throws IOException {\n     @BeforeAll\n     static void checkCredentialsAndSetup() {\n         // check whether credentials and container details exist\n+        assumeThat(ACCOUNT)\n+                .describedAs(\"Azure storage account not configured, skipping test...\")\n+                .isNotBlank();\n         assumeThat(CONTAINER)\n                 .describedAs(\"Azure container not configured, skipping test...\")\n                 .isNotBlank();",
      "parent_sha": "1008114b7959a09e73b5b793ffb770e1537414e9"
    }
  },
  {
    "oid": "63468c09938755445b166173a5abd7962d531cac",
    "message": "[hotfix][runtime] Refactor SimpleAckingTaskManagerGateway to not use optional",
    "date": "2019-02-21T11:51:30Z",
    "url": "https://github.com/apache/flink/commit/63468c09938755445b166173a5abd7962d531cac",
    "details": {
      "sha": "dbc49ad6b01f3e8842f54b6d4a8bf824c787127e",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/executiongraph/utils/SimpleAckingTaskManagerGateway.java",
      "status": "modified",
      "additions": 7,
      "deletions": 13,
      "changes": 20,
      "blob_url": "https://github.com/apache/flink/blob/63468c09938755445b166173a5abd7962d531cac/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fexecutiongraph%2Futils%2FSimpleAckingTaskManagerGateway.java",
      "raw_url": "https://github.com/apache/flink/raw/63468c09938755445b166173a5abd7962d531cac/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fexecutiongraph%2Futils%2FSimpleAckingTaskManagerGateway.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fexecutiongraph%2Futils%2FSimpleAckingTaskManagerGateway.java?ref=63468c09938755445b166173a5abd7962d531cac",
      "patch": "@@ -30,7 +30,6 @@\n import org.apache.flink.runtime.messages.Acknowledge;\n import org.apache.flink.runtime.messages.StackTraceSampleResponse;\n \n-import java.util.Optional;\n import java.util.UUID;\n import java.util.concurrent.CompletableFuture;\n import java.util.function.BiFunction;\n@@ -44,23 +43,18 @@ public class SimpleAckingTaskManagerGateway implements TaskManagerGateway {\n \n \tprivate final String address = UUID.randomUUID().toString();\n \n-\tprivate Optional<Consumer<ExecutionAttemptID>> optSubmitConsumer;\n+\tprivate Consumer<TaskDeploymentDescriptor> submitConsumer = ignore -> { };\n \n-\tprivate Optional<Consumer<ExecutionAttemptID>> optCancelConsumer;\n+\tprivate Consumer<ExecutionAttemptID> cancelConsumer = ignore -> { };\n \n \tprivate volatile BiFunction<AllocationID, Throwable, CompletableFuture<Acknowledge>> freeSlotFunction;\n \n-\tpublic SimpleAckingTaskManagerGateway() {\n-\t\toptSubmitConsumer = Optional.empty();\n-\t\toptCancelConsumer = Optional.empty();\n-\t}\n-\n-\tpublic void setSubmitConsumer(Consumer<ExecutionAttemptID> predicate) {\n-\t\toptSubmitConsumer = Optional.of(predicate);\n+\tpublic void setSubmitConsumer(Consumer<TaskDeploymentDescriptor> predicate) {\n+\t\tsubmitConsumer = predicate;\n \t}\n \n \tpublic void setCancelConsumer(Consumer<ExecutionAttemptID> predicate) {\n-\t\toptCancelConsumer = Optional.of(predicate);\n+\t\tcancelConsumer = predicate;\n \t}\n \n \tpublic void setFreeSlotFunction(BiFunction<AllocationID, Throwable, CompletableFuture<Acknowledge>> freeSlotFunction) {\n@@ -85,7 +79,7 @@ public CompletableFuture<StackTraceSampleResponse> requestStackTraceSample(\n \n \t@Override\n \tpublic CompletableFuture<Acknowledge> submitTask(TaskDeploymentDescriptor tdd, Time timeout) {\n-\t\toptSubmitConsumer.ifPresent(condition -> condition.accept(tdd.getExecutionAttemptId()));\n+\t\tsubmitConsumer.accept(tdd);\n \t\treturn CompletableFuture.completedFuture(Acknowledge.get());\n \t}\n \n@@ -96,7 +90,7 @@ public CompletableFuture<Acknowledge> stopTask(ExecutionAttemptID executionAttem\n \n \t@Override\n \tpublic CompletableFuture<Acknowledge> cancelTask(ExecutionAttemptID executionAttemptID, Time timeout) {\n-\t\toptCancelConsumer.ifPresent(condition -> condition.accept(executionAttemptID));\n+\t\tcancelConsumer.accept(executionAttemptID);\n \t\treturn CompletableFuture.completedFuture(Acknowledge.get());\n \t}\n ",
      "parent_sha": "69f91d88749da9b0e60414394d024c9ccc3ab9ca"
    }
  },
  {
    "oid": "d718342ad0311ea7481d1a1bd87c395aeff25928",
    "message": "[hotfix][tests] Make LocalRecoveryITCase fail when allocations don't match\n\nCurrently, wrong allocation fails the task causing a restart,\nwhich eventually allows to fix the allocation by picking the right TM.\nThis prevents the test from failure and hides the wrong allocation.",
    "date": "2023-03-02T21:39:04Z",
    "url": "https://github.com/apache/flink/commit/d718342ad0311ea7481d1a1bd87c395aeff25928",
    "details": {
      "sha": "c8e505c9c9f1856b5c123a16d56fc65da0840ff0",
      "filename": "flink-tests/src/test/java/org/apache/flink/test/recovery/LocalRecoveryITCase.java",
      "status": "modified",
      "additions": 21,
      "deletions": 7,
      "changes": 28,
      "blob_url": "https://github.com/apache/flink/blob/d718342ad0311ea7481d1a1bd87c395aeff25928/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Frecovery%2FLocalRecoveryITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/d718342ad0311ea7481d1a1bd87c395aeff25928/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Frecovery%2FLocalRecoveryITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Ftest%2Frecovery%2FLocalRecoveryITCase.java?ref=d718342ad0311ea7481d1a1bd87c395aeff25928",
      "patch": "@@ -19,6 +19,7 @@\n package org.apache.flink.test.recovery;\n \n import org.apache.flink.api.common.JobID;\n+import org.apache.flink.api.common.accumulators.ListAccumulator;\n import org.apache.flink.api.common.state.ListState;\n import org.apache.flink.api.common.state.ListStateDescriptor;\n import org.apache.flink.configuration.CheckpointingOptions;\n@@ -67,12 +68,14 @@\n import java.util.Optional;\n import java.util.concurrent.TimeUnit;\n \n-import static org.assertj.core.api.Assertions.assertThat;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n \n /** Tests local recovery by restarting Flink processes. */\n @ExtendWith(TestLoggerExtension.class)\n class LocalRecoveryITCase {\n \n+    private static final String ALLOCATION_FAILURES_ACCUMULATOR_NAME = \"acc\";\n+\n     @TempDir private File tmpDirectory;\n \n     @Test\n@@ -108,7 +111,12 @@ public void testRecoverLocallyFromProcessCrashWithWorkingDirectory() throws Exce\n \n             restartTaskManagerProcesses(taskManagerProcesses, parallelism - 1);\n \n-            jobClient.getJobExecutionResult().get(waitingTimeInSeconds, TimeUnit.SECONDS);\n+            List<String> allocFailures =\n+                    jobClient\n+                            .getJobExecutionResult()\n+                            .get(waitingTimeInSeconds, TimeUnit.SECONDS)\n+                            .getAccumulatorResult(ALLOCATION_FAILURES_ACCUMULATOR_NAME);\n+            assertTrue(allocFailures.isEmpty(), allocFailures.toString());\n \n             success = true;\n         } finally {\n@@ -307,11 +315,17 @@ public void initializeState(FunctionInitializationContext context) throws Except\n                                         new IllegalStateException(\n                                                 \"Could not find corresponding TaskNameAllocationID information.\"));\n \n-                assertThat(myTaskNameAllocationId.getAllocationId())\n-                        .withFailMessage(\n-                                \"The task was deployed to AllocationID(%s) but it should have been deployed to AllocationID(%s) for local recovery.\",\n-                                allocationId, myTaskNameAllocationId.getAllocationId())\n-                        .isEqualTo(allocationId);\n+                runtimeContext.addAccumulator(\n+                        ALLOCATION_FAILURES_ACCUMULATOR_NAME, new ListAccumulator<String>());\n+                if (!allocationId.equals(myTaskNameAllocationId.getAllocationId())) {\n+                    runtimeContext\n+                            .getAccumulator(ALLOCATION_FAILURES_ACCUMULATOR_NAME)\n+                            .add(\n+                                    String.format(\n+                                            \"The task was deployed to AllocationID(%s) but it should have been deployed to AllocationID(%s) for local recovery.\",\n+                                            allocationId,\n+                                            myTaskNameAllocationId.getAllocationId()));\n+                }\n                 // terminate\n                 running = false;\n             }",
      "parent_sha": "18c73f1488bd7f735ed626cde97f72a5cf7df863"
    }
  },
  {
    "oid": "e48a5f19e82ea71ad9c5bf94173299352233ed17",
    "message": "[hotfix] [java] Allow setting DataSink parallelism to default value\n\nThis was missed in FLINK-3589.",
    "date": "2016-04-21T20:40:39Z",
    "url": "https://github.com/apache/flink/commit/e48a5f19e82ea71ad9c5bf94173299352233ed17",
    "details": {
      "sha": "2e2d237d5adbbcfaf73cf1818e44e2a6f81e26f0",
      "filename": "flink-java/src/main/java/org/apache/flink/api/java/operators/DataSink.java",
      "status": "modified",
      "additions": 6,
      "deletions": 5,
      "changes": 11,
      "blob_url": "https://github.com/apache/flink/blob/e48a5f19e82ea71ad9c5bf94173299352233ed17/flink-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fjava%2Foperators%2FDataSink.java",
      "raw_url": "https://github.com/apache/flink/raw/e48a5f19e82ea71ad9c5bf94173299352233ed17/flink-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fjava%2Foperators%2FDataSink.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fapi%2Fjava%2Foperators%2FDataSink.java?ref=e48a5f19e82ea71ad9c5bf94173299352233ed17",
      "patch": "@@ -269,12 +269,13 @@ public int getParallelism() {\n \t * @return This data sink with set parallelism.\n \t */\n \tpublic DataSink<T> setParallelism(int parallelism) {\n-\t\t\n-\t\tif(parallelism < 1) {\n-\t\t\tthrow new IllegalArgumentException(\"The parallelism of an operator must be at least 1.\");\n+\t\tif (parallelism != ExecutionConfig.PARALLELISM_UNKNOWN) {\n+\t\t\tif (parallelism < 1 && parallelism != ExecutionConfig.PARALLELISM_DEFAULT) {\n+\t\t\t\tthrow new IllegalArgumentException(\"The parallelism of an operator must be at least 1.\");\n+\t\t\t}\n+\t\t\tthis.parallelism = parallelism;\n \t\t}\n-\t\tthis.parallelism = parallelism;\n-\t\t\n+\n \t\treturn this;\n \t}\n }",
      "parent_sha": "00888853b06edda2e8e47fb551cc4bd6c93ee97b"
    }
  },
  {
    "oid": "2e374954b9bfa69e30624dfb27ff3762749da725",
    "message": "[FLINK-23359][test] Fix the number of available slots in testResourceCanBeAllocatedForDifferentJobAfterFree\n\nThis closes #16469",
    "date": "2021-07-13T02:12:52Z",
    "url": "https://github.com/apache/flink/commit/2e374954b9bfa69e30624dfb27ff3762749da725",
    "details": {
      "sha": "6fab2b13b893df179d15acf7d66484f84e344f1c",
      "filename": "flink-runtime/src/test/java/org/apache/flink/runtime/resourcemanager/slotmanager/AbstractFineGrainedSlotManagerITCase.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/2e374954b9bfa69e30624dfb27ff3762749da725/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fresourcemanager%2Fslotmanager%2FAbstractFineGrainedSlotManagerITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/2e374954b9bfa69e30624dfb27ff3762749da725/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fresourcemanager%2Fslotmanager%2FAbstractFineGrainedSlotManagerITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-runtime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fruntime%2Fresourcemanager%2Fslotmanager%2FAbstractFineGrainedSlotManagerITCase.java?ref=2e374954b9bfa69e30624dfb27ff3762749da725",
      "patch": "@@ -315,7 +315,7 @@ private void testResourceCanBeAllocatedForDifferentJobAfterFree(\n                                                 .registerTaskManager(\n                                                         taskManagerConnection,\n                                                         slotReport,\n-                                                        DEFAULT_TOTAL_RESOURCE_PROFILE,\n+                                                        DEFAULT_SLOT_RESOURCE_PROFILE,\n                                                         DEFAULT_SLOT_RESOURCE_PROFILE);\n                                         getSlotManager()\n                                                 .processResourceRequirements(resourceRequirements1);",
      "parent_sha": "2268baf211f1b367e56c8f8d7cd8ee8dee355cab"
    }
  },
  {
    "oid": "55f880bdbe4b61003646c0f7d37f241f458c1e87",
    "message": "[streaming] javadoc added to fault tolerance buffer",
    "date": "2014-08-18T13:56:42Z",
    "url": "https://github.com/apache/flink/commit/55f880bdbe4b61003646c0f7d37f241f458c1e87",
    "details": {
      "sha": "28a57b682e864fef9be057a71fd67bd5cacf163c",
      "filename": "flink-addons/flink-streaming/src/main/java/eu/stratosphere/streaming/api/FaultTolerancyBuffer.java",
      "status": "modified",
      "additions": 55,
      "deletions": 2,
      "changes": 57,
      "blob_url": "https://github.com/apache/flink/blob/55f880bdbe4b61003646c0f7d37f241f458c1e87/flink-addons%2Fflink-streaming%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fstreaming%2Fapi%2FFaultTolerancyBuffer.java",
      "raw_url": "https://github.com/apache/flink/raw/55f880bdbe4b61003646c0f7d37f241f458c1e87/flink-addons%2Fflink-streaming%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fstreaming%2Fapi%2FFaultTolerancyBuffer.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-addons%2Fflink-streaming%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fstreaming%2Fapi%2FFaultTolerancyBuffer.java?ref=55f880bdbe4b61003646c0f7d37f241f458c1e87",
      "patch": "@@ -26,10 +26,13 @@\n \n import eu.stratosphere.nephele.io.RecordWriter;\n \n+/** An object to provide fault tolerance for Stratosphere stream processing.\n+ * It works as a buffer to hold StreamRecords for a task for re-emitting failed, or timed out records.\n+ */\n public class FaultTolerancyBuffer {\n \n \tprivate long TIMEOUT = 1000;\n-\n+\t\n \tprivate Long timeOfLastUpdate;\n \tprivate Map<String, StreamRecord> recordBuffer;\n \tprivate Map<String, Integer> ackCounter;\n@@ -41,6 +44,13 @@ public class FaultTolerancyBuffer {\n \n \tprivate int numberOfOutputs;\n \n+\t/**Creates fault tolerance buffer object for the given output channels and channel ID\n+\t * \n+\t * @param outputs\n+\t * List of outputs\n+\t * @param channelID\n+\t * ID of the task object that uses this buffer\n+\t */\n \tpublic FaultTolerancyBuffer(List<RecordWriter<StreamRecord>> outputs,\n \t\t\tString channelID) {\n \t\tthis.timeOfLastUpdate = System.currentTimeMillis();\n@@ -53,13 +63,23 @@ public FaultTolerancyBuffer(List<RecordWriter<StreamRecord>> outputs,\n \t\tthis.recordTimestamps = new HashMap<String, Long>();\n \t}\n \n+/** Adds the record to the fault tolerance buffer. This record will be monitored for acknowledgements and timeout.\n+ * \n+ */\n \tpublic void addRecord(StreamRecord streamRecord) {\n \n \t\trecordBuffer.put(streamRecord.getId(), streamRecord);\n \t\tackCounter.put(streamRecord.getId(), numberOfOutputs);\n \t\taddTimestamp(streamRecord.getId());\n \t}\n \n+\t/** Checks for records that have timed out since the last check and fails them.\n+\t * \n+\t * @param currentTime\n+\t * Time when the check should be made, usually current system time.\n+\t * @return\n+\t * Returns the list of the records that have timed out.\n+\t */\n \tList<String> timeoutRecords(Long currentTime) {\n \t\tif (timeOfLastUpdate + TIMEOUT < currentTime) {\n \t\t\tList<String> timedOutRecords = new LinkedList<String>();\n@@ -85,6 +105,14 @@ List<String> timeoutRecords(Long currentTime) {\n \t\treturn null;\n \t}\n \n+\t/**Stores time stamp for a record by recordID \n+\t * and also adds the record to a map which maps a time stamp to the IDs of records that were emitted at that time.\n+\t * <p>\n+\t * Later used for timeouts.\n+\t * \n+\t * @param recordID\n+\t * ID of the record\n+\t */\n \tpublic void addTimestamp(String recordID) {\n \t\tLong currentTime = System.currentTimeMillis();\n \t\trecordTimestamps.put(recordID, currentTime);\n@@ -98,13 +126,23 @@ public void addTimestamp(String recordID) {\n \t\t}\n \t}\n \n+\t/**Returns a StreamRecord after removing it from the buffer\n+\t * \n+\t * @param recordID\n+\t * \t   The ID of the record that will be popped\n+\t */\n \tpublic StreamRecord popRecord(String recordID) {\n \t\tSystem.out.println(\"Pop ID: \" + recordID);\n \t\tStreamRecord record = recordBuffer.get(recordID);\n \t\tremoveRecord(recordID);\n \t\treturn record;\n \t}\n \n+\t/** Removes a StreamRecord by ID from the fault tolerance buffer, further acks will have no effects for this record.\n+\t * @param recordID\n+\t *        The ID of the record that will be removed\n+\t * \n+\t */\n \tvoid removeRecord(String recordID) {\n \t\trecordBuffer.remove(recordID);\n \t\tackCounter.remove(recordID);\n@@ -119,6 +157,11 @@ void removeRecord(String recordID) {\n \t\t}\n \t}\n \n+\t/**Acknowledges the record of the given ID, if all the outputs have sent acknowledgments, removes it from the buffer\n+\t * \n+\t * @param recordID\n+\t * ID of the record that has been acknowledged\n+\t */\n \t//TODO: find a place to call timeoutRecords\n \tpublic void ackRecord(String recordID) {\n \t\tif (ackCounter.containsKey(recordID)) {\n@@ -133,14 +176,24 @@ public void ackRecord(String recordID) {\n \t\t//timeoutRecords(System.currentTimeMillis());\n \t}\n \n+\t/**Re-emits the failed record for the given ID, removes the old record and stores it with a new ID.\n+\t * \n+\t * @param recordID\n+\t * ID of the record that has been failed\n+\t */\n \tpublic void failRecord(String recordID) {\n \t\t// Create new id to avoid double counting acks\n \t\tSystem.out.println(\"Fail ID: \" + recordID);\n \t\tStreamRecord newRecord = popRecord(recordID).setId(channelID);\n \t\taddRecord(newRecord);\n \t\treEmit(newRecord);\n \t}\n-\n+\t\n+/**\n+ * Emit give record to all output channels\n+ * @param record\n+ * Record to be re-emitted\n+ */\n \tpublic void reEmit(StreamRecord record) {\n \t\tfor (RecordWriter<StreamRecord> output : outputs) {\n \t\t\ttry {",
      "parent_sha": "d1fc3385fcfaa0dc142be05a506a9c83dd03dcf6"
    }
  },
  {
    "oid": "2558c9ee1f1557b701ce5b8201c079ed5830c14f",
    "message": "[hotfix][task] Fix the comment typo in TwoInputStreamTask class",
    "date": "2020-02-11T10:03:55Z",
    "url": "https://github.com/apache/flink/commit/2558c9ee1f1557b701ce5b8201c079ed5830c14f",
    "details": {
      "sha": "5100a13527202df9bcdb6aeaf68b7fb9cabc7eb1",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TwoInputStreamTask.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/2558c9ee1f1557b701ce5b8201c079ed5830c14f/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FTwoInputStreamTask.java",
      "raw_url": "https://github.com/apache/flink/raw/2558c9ee1f1557b701ce5b8201c079ed5830c14f/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FTwoInputStreamTask.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FTwoInputStreamTask.java?ref=2558c9ee1f1557b701ce5b8201c079ed5830c14f",
      "patch": "@@ -57,7 +57,7 @@ protected void createInputProcessor(\n \t\tInputGate unionedInputGate1 = InputGateUtil.createInputGate(inputGates1.toArray(new InputGate[0]));\n \t\tInputGate unionedInputGate2 = InputGateUtil.createInputGate(inputGates2.toArray(new InputGate[0]));\n \n-\t\t// create a Input instance for each input\n+\t\t// create an input instance for each input\n \t\tCheckpointedInputGate[] checkpointedInputGates = InputProcessorUtil.createCheckpointedInputGatePair(\n \t\t\tthis,\n \t\t\tgetConfiguration().getCheckpointMode(),",
      "parent_sha": "57c33961a55cff1068345198cb4669d9f1313bf8"
    }
  },
  {
    "oid": "4536e9cbe1925c0a6dd24f74c2b73a675afb2625",
    "message": "[FLINK-8830] [yarn] Log reading of Hadoop's token file\n\nThis closes #5629.",
    "date": "2018-03-20T09:18:02Z",
    "url": "https://github.com/apache/flink/commit/4536e9cbe1925c0a6dd24f74c2b73a675afb2625",
    "details": {
      "sha": "b9f7fac29786231c9e2944e60e173e76e13f9a98",
      "filename": "flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java",
      "status": "modified",
      "additions": 12,
      "deletions": 10,
      "changes": 22,
      "blob_url": "https://github.com/apache/flink/blob/4536e9cbe1925c0a6dd24f74c2b73a675afb2625/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FUtils.java",
      "raw_url": "https://github.com/apache/flink/raw/4536e9cbe1925c0a6dd24f74c2b73a675afb2625/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FUtils.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FUtils.java?ref=4536e9cbe1925c0a6dd24f74c2b73a675afb2625",
      "patch": "@@ -489,14 +489,15 @@ static ContainerLaunchContext createTaskExecutorContext(\n \n \t\tctx.setEnvironment(containerEnv);\n \n-\t\ttry (DataOutputBuffer dob = new DataOutputBuffer()) {\n-\t\t\tlog.debug(\"Adding security tokens to Task Executor Container launch Context....\");\n+\t\t// For TaskManager YARN container context, read the tokens from the jobmanager yarn container local file.\n+\t\t// NOTE: must read the tokens from the local file, not from the UGI context, because if UGI is login\n+\t\t// using Kerberos keytabs, there is no HDFS delegation token in the UGI context.\n+\t\tfinal String fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);\n+\n+\t\tif (fileLocation != null) {\n+\t\t\tlog.debug(\"Adding security tokens to TaskExecutor's container launch context.\");\n \n-\t\t\t// For TaskManager YARN container context, read the tokens from the jobmanager yarn container local flie.\n-\t\t\t// NOTE: must read the tokens from the local file, not from the UGI context, because if UGI is login\n-\t\t\t// using Kerberos keytabs, there is no HDFS delegation token in the UGI context.\n-\t\t\tString fileLocation = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);\n-\t\t\tif (fileLocation != null) {\n+\t\t\ttry (DataOutputBuffer dob = new DataOutputBuffer()) {\n \t\t\t\tMethod readTokenStorageFileMethod = Credentials.class.getMethod(\n \t\t\t\t\t\"readTokenStorageFile\", File.class, org.apache.hadoop.conf.Configuration.class);\n \n@@ -509,10 +510,11 @@ static ContainerLaunchContext createTaskExecutorContext(\n \t\t\t\tcred.writeTokenStorageToStream(dob);\n \t\t\t\tByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n \t\t\t\tctx.setTokens(securityTokens);\n+\t\t\t} catch (Throwable t) {\n+\t\t\t\tlog.error(\"Failed to add Hadoop's security tokens.\", t);\n \t\t\t}\n-\t\t}\n-\t\tcatch (Throwable t) {\n-\t\t\tlog.error(\"Getting current user info failed when trying to launch the container\", t);\n+\t\t} else {\n+\t\t\tlog.info(\"Could not set security tokens because Hadoop's token file location is unknown.\");\n \t\t}\n \n \t\treturn ctx;",
      "parent_sha": "bcb0f324f50adc74fb6122621794d6d7e37bc933"
    }
  },
  {
    "oid": "b9ec89b16a28a3cfc94d312c827f16e6c17c4b0c",
    "message": "[FLINK-13029][table-planner] Ported GROUP BY expression to new type system",
    "date": "2019-07-02T12:19:26Z",
    "url": "https://github.com/apache/flink/commit/b9ec89b16a28a3cfc94d312c827f16e6c17c4b0c",
    "details": {
      "sha": "5b2dfcfe47637ac99ba656562cfc1b0eea0396f6",
      "filename": "flink-table/flink-table-planner/src/main/java/org/apache/flink/table/operations/AggregateOperationFactory.java",
      "status": "modified",
      "additions": 24,
      "deletions": 11,
      "changes": 35,
      "blob_url": "https://github.com/apache/flink/blob/b9ec89b16a28a3cfc94d312c827f16e6c17c4b0c/flink-table%2Fflink-table-planner%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Foperations%2FAggregateOperationFactory.java",
      "raw_url": "https://github.com/apache/flink/raw/b9ec89b16a28a3cfc94d312c827f16e6c17c4b0c/flink-table%2Fflink-table-planner%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Foperations%2FAggregateOperationFactory.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-table%2Fflink-table-planner%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Foperations%2FAggregateOperationFactory.java?ref=b9ec89b16a28a3cfc94d312c827f16e6c17c4b0c",
      "patch": "@@ -49,7 +49,11 @@\n import org.apache.flink.table.functions.TableAggregateFunction;\n import org.apache.flink.table.functions.TableAggregateFunctionDefinition;\n import org.apache.flink.table.operations.WindowAggregateQueryOperation.ResolvedGroupWindow;\n+import org.apache.flink.table.types.logical.LegacyTypeInformationType;\n import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.LogicalTypeRoot;\n+import org.apache.flink.table.types.logical.StructuredType;\n+import org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor;\n import org.apache.flink.table.typeutils.FieldInfoUtils;\n import org.apache.flink.table.typeutils.TimeIndicatorTypeInfo;\n import org.apache.flink.table.typeutils.TimeIntervalTypeInfo;\n@@ -82,14 +86,14 @@\n  * Utility class for creating a valid {@link AggregateQueryOperation} or {@link WindowAggregateQueryOperation}.\n  */\n @Internal\n-public class AggregateOperationFactory {\n+public final class AggregateOperationFactory {\n \n \tprivate final boolean isStreaming;\n \tprivate final ExpressionBridge<PlannerExpression> expressionBridge;\n-\tprivate final GroupingExpressionValidator groupingExpressionValidator = new GroupingExpressionValidator();\n \tprivate final NoNestedAggregates noNestedAggregates = new NoNestedAggregates();\n \tprivate final ValidateDistinct validateDistinct = new ValidateDistinct();\n-\tprivate AggregationExpressionValidator aggregationsValidator = new AggregationExpressionValidator();\n+\tprivate final AggregationExpressionValidator aggregationsValidator = new AggregationExpressionValidator();\n+\tprivate final IsKeyTypeChecker isKeyTypeChecker = new IsKeyTypeChecker();\n \n \tpublic AggregateOperationFactory(ExpressionBridge<PlannerExpression> expressionBridge, boolean isStreaming) {\n \t\tthis.expressionBridge = expressionBridge;\n@@ -413,7 +417,7 @@ private List<PlannerExpression> bridge(List<ResolvedExpression> aggregates) {\n \t}\n \n \tprivate void validateGroupings(List<ResolvedExpression> groupings) {\n-\t\tgroupings.forEach(expr -> expr.accept(groupingExpressionValidator));\n+\t\tgroupings.forEach(expr -> expr.getOutputDataType().getLogicalType().accept(isKeyTypeChecker));\n \t}\n \n \tprivate void validateAggregates(List<ResolvedExpression> aggregates) {\n@@ -501,16 +505,25 @@ protected Void defaultMethod(ResolvedExpression expression) {\n \t\t}\n \t}\n \n-\tprivate class GroupingExpressionValidator extends ResolvedExpressionDefaultVisitor<Void> {\n+\tprivate static class IsKeyTypeChecker extends LogicalTypeDefaultVisitor<Boolean> {\n+\n \t\t@Override\n-\t\tprotected Void defaultMethod(ResolvedExpression expression) {\n-\t\t\tTypeInformation<?> groupingType = expressionBridge.bridge(expression).resultType();\n+\t\tpublic Boolean visit(StructuredType structuredType) {\n+\t\t\tStructuredType.StructuredComparision comparision = structuredType.getComparision();\n+\t\t\treturn comparision == StructuredType.StructuredComparision.FULL ||\n+\t\t\t\tcomparision == StructuredType.StructuredComparision.EQUALS;\n+\t\t}\n \n-\t\t\tif (!groupingType.isKeyType()) {\n-\t\t\t\tthrow new ValidationException(format(\"Expression %s cannot be used as a grouping expression \" +\n-\t\t\t\t\t\"because it's not a valid key type which must be hashable and comparable\", expression));\n+\t\t@Override\n+\t\tprotected Boolean defaultMethod(LogicalType logicalType) {\n+\t\t\tif (logicalType.getTypeRoot() == LogicalTypeRoot.ANY) {\n+\t\t\t\t// we don't know anything about the ANY type, we don't know if it is comparable and hashable.\n+\t\t\t\treturn false;\n+\t\t\t} else if (logicalType instanceof LegacyTypeInformationType) {\n+\t\t\t\treturn ((LegacyTypeInformationType) logicalType).getTypeInformation().isKeyType();\n \t\t\t}\n-\t\t\treturn null;\n+\n+\t\t\treturn logicalType.getChildren().stream().allMatch(c -> c.accept(this));\n \t\t}\n \t}\n ",
      "parent_sha": "036c5492a9bfe7636d5e7d5eb664af5e77952707"
    }
  },
  {
    "oid": "0d01c4606ce5472a7fb417c254fb102ece8ea71a",
    "message": "Implemented dialog to create new failure pattern",
    "date": "2012-01-09T17:07:33Z",
    "url": "https://github.com/apache/flink/commit/0d01c4606ce5472a7fb417c254fb102ece8ea71a",
    "details": {
      "sha": "bc7e83509c26dd420ef37b0d887052da720685b1",
      "filename": "nephele/nephele-visualization/src/main/java/eu/stratosphere/nephele/visualization/swt/SWTNewFailurePatternDialog.java",
      "status": "modified",
      "additions": 188,
      "deletions": 1,
      "changes": 189,
      "blob_url": "https://github.com/apache/flink/blob/0d01c4606ce5472a7fb417c254fb102ece8ea71a/nephele%2Fnephele-visualization%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Fvisualization%2Fswt%2FSWTNewFailurePatternDialog.java",
      "raw_url": "https://github.com/apache/flink/raw/0d01c4606ce5472a7fb417c254fb102ece8ea71a/nephele%2Fnephele-visualization%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Fvisualization%2Fswt%2FSWTNewFailurePatternDialog.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/nephele%2Fnephele-visualization%2Fsrc%2Fmain%2Fjava%2Feu%2Fstratosphere%2Fnephele%2Fvisualization%2Fswt%2FSWTNewFailurePatternDialog.java?ref=0d01c4606ce5472a7fb417c254fb102ece8ea71a",
      "patch": "@@ -1,5 +1,192 @@\n+/***********************************************************************************************************************\n+ *\n+ * Copyright (C) 2010 by the Stratosphere project (http://stratosphere.eu)\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+ * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ **********************************************************************************************************************/\n+\n package eu.stratosphere.nephele.visualization.swt;\n \n-public class SWTNewFailurePatternDialog {\n+import java.util.List;\n+\n+import org.eclipse.swt.SWT;\n+import org.eclipse.swt.layout.GridData;\n+import org.eclipse.swt.layout.GridLayout;\n+import org.eclipse.swt.layout.RowLayout;\n+import org.eclipse.swt.widgets.Button;\n+import org.eclipse.swt.widgets.Composite;\n+import org.eclipse.swt.widgets.Display;\n+import org.eclipse.swt.widgets.Event;\n+import org.eclipse.swt.widgets.Listener;\n+import org.eclipse.swt.widgets.MessageBox;\n+import org.eclipse.swt.widgets.Shell;\n+import org.eclipse.swt.events.KeyAdapter;\n+import org.eclipse.swt.events.KeyEvent;\n+\n+/**\n+ * This class implements a dialogue for creating a new failure pattern.\n+ * \n+ * @author warneke\n+ */\n+public final class SWTNewFailurePatternDialog {\n+\n+\t/**\n+\t * The width of the dialog.\n+\t */\n+\tprivate static final int WIDTH = 300;\n+\n+\t/**\n+\t * The height of the dialog.\n+\t */\n+\tprivate static final int HEIGHT = 100;\n+\n+\t/**\n+\t * The shell for this dialog.\n+\t */\n+\tprivate final Shell shell;\n+\n+\t/**\n+\t * The auto-completion combo box.\n+\t */\n+\tprivate final AutoCompletionCombo input;\n+\n+\t/**\n+\t * The return value of the <code>showDialog</code> method.\n+\t */\n+\tprivate String returnValue = null;\n+\n+\t/**\n+\t * Constructs a new dialog for creating a new failure pattern.\n+\t * \n+\t * @param parent\n+\t *        the parent of this dialog\n+\t * @param nameSuggestions\n+\t *        name suggestions to be displayed inside auto-completion combo box\n+\t */\n+\tpublic SWTNewFailurePatternDialog(final Shell parent, final List<String> nameSuggestions) {\n+\n+\t\tthis.shell = new Shell(parent);\n+\t\tthis.shell.setSize(WIDTH, HEIGHT);\n+\t\tthis.shell.setText(\"Create New Failure Pattern\");\n+\t\tthis.shell.setLayout(new GridLayout(1, false));\n+\n+\t\tGridData gridData = new GridData();\n+\t\tgridData.horizontalAlignment = GridData.FILL;\n+\t\tgridData.grabExcessHorizontalSpace = true;\n+\t\tgridData.grabExcessVerticalSpace = false;\n+\n+\t\tthis.input = new AutoCompletionCombo(this.shell, SWT.NONE, nameSuggestions);\n+\t\tthis.input.setLayoutData(gridData);\n+\t\tthis.input.addKeyListener(new KeyAdapter() {\n+\n+\t\t\t@Override\n+\t\t\tpublic void keyReleased(final KeyEvent arg0) {\n+\n+\t\t\t\tif (arg0.character != SWT.CR) {\n+\t\t\t\t\treturn;\n+\t\t\t\t}\n+\n+\t\t\t\tif (isInputValid()) {\n+\t\t\t\t\treturnValue = input.getText();\n+\t\t\t\t\tshell.dispose();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\n+\t\tgridData = new GridData();\n+\t\tgridData.horizontalAlignment = SWT.RIGHT;\n+\t\tgridData.verticalAlignment = SWT.BOTTOM;\n+\t\tgridData.grabExcessHorizontalSpace = true;\n+\t\tgridData.grabExcessVerticalSpace = true;\n+\n+\t\tfinal Composite buttonComposite = new Composite(this.shell, SWT.RIGHT_TO_LEFT);\n+\t\tfinal RowLayout rowLayout = new RowLayout(SWT.HORIZONTAL);\n+\t\trowLayout.marginBottom = 0;\n+\t\trowLayout.marginHeight = 0;\n+\t\trowLayout.marginLeft = 0;\n+\t\trowLayout.marginRight = 0;\n+\t\trowLayout.marginTop = 0;\n+\t\trowLayout.marginWidth = 0;\n+\t\trowLayout.pack = false;\n+\n+\t\tbuttonComposite.setLayoutData(gridData);\n+\t\tbuttonComposite.setLayout(rowLayout);\n+\n+\t\tfinal Button ok = new Button(buttonComposite, SWT.PUSH);\n+\t\tok.setText(\"OK\");\n+\t\tok.addListener(SWT.Selection, new Listener() {\n+\n+\t\t\t@Override\n+\t\t\tpublic void handleEvent(final Event arg0) {\n+\n+\t\t\t\tif (isInputValid()) {\n+\t\t\t\t\treturnValue = input.getText();\n+\t\t\t\t\tshell.dispose();\n+\t\t\t\t}\n+\t\t\t}\n+\t\t});\n+\n+\t\tfinal Button cancel = new Button(buttonComposite, SWT.PUSH);\n+\t\tcancel.setText(\"Cancel\");\n+\t\tcancel.addListener(SWT.Selection, new Listener() {\n+\n+\t\t\t@Override\n+\t\t\tpublic void handleEvent(final Event arg0) {\n+\n+\t\t\t\treturnValue = null;\n+\t\t\t\tshell.dispose();\n+\t\t\t}\n+\t\t});\n+\t}\n+\n+\t/**\n+\t * Checks whether the input is valid and displays an error message box if not.\n+\t * \n+\t * @return <code>true</code> if the input is valid, <code>false</code> otherwise\n+\t */\n+\tprivate boolean isInputValid() {\n+\n+\t\tfinal String text = this.input.getText();\n+\t\tif (text.isEmpty()) {\n+\t\t\tfinal MessageBox messageBox = new MessageBox(this.shell, SWT.ICON_ERROR);\n+\n+\t\t\tmessageBox.setText(\"Invalid Input\");\n+\t\t\tmessageBox.setMessage(\"Name for failure pattern must not be empty.\");\n+\t\t\tmessageBox.open();\n+\n+\t\t\tthis.input.setFocus();\n+\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t/**\n+\t * Opens the dialog.\n+\t * \n+\t * @return the name for the new failure pattern or <code>null</code> if the user has canceled the dialog\n+\t */\n+\tpublic String showDialog() {\n+\n+\t\tthis.shell.open();\n+\n+\t\tfinal Display display = this.shell.getDisplay();\n+\n+\t\twhile (!this.shell.isDisposed()) {\n+\t\t\tif (!display.readAndDispatch()) {\n+\t\t\t\tdisplay.sleep();\n+\t\t\t}\n+\t\t}\n \n+\t\treturn this.returnValue;\n+\t}\n }",
      "parent_sha": "b5a0004c43b140465b3bf30162308b8385e7503e"
    }
  },
  {
    "oid": "e20229c0d5ad1cc59d51270407f4d3a2a811a144",
    "message": "[hotfix] Consider INITIALIZING JobState also as scheduled in CliFrontend",
    "date": "2020-09-16T09:18:06Z",
    "url": "https://github.com/apache/flink/commit/e20229c0d5ad1cc59d51270407f4d3a2a811a144",
    "details": {
      "sha": "7481b9dbc583e69bb1f75b5a9d7c49e55224f27f",
      "filename": "flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontend.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/e20229c0d5ad1cc59d51270407f4d3a2a811a144/flink-clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fclient%2Fcli%2FCliFrontend.java",
      "raw_url": "https://github.com/apache/flink/raw/e20229c0d5ad1cc59d51270407f4d3a2a811a144/flink-clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fclient%2Fcli%2FCliFrontend.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fclient%2Fcli%2FCliFrontend.java?ref=e20229c0d5ad1cc59d51270407f4d3a2a811a144",
      "patch": "@@ -420,7 +420,7 @@ private <ClusterID> void listJobs(\n \t\tfinal List<JobStatusMessage> scheduledJobs = new ArrayList<>();\n \t\tfinal List<JobStatusMessage> terminatedJobs = new ArrayList<>();\n \t\tjobDetails.forEach(details -> {\n-\t\t\tif (details.getJobState() == JobStatus.CREATED) {\n+\t\t\tif (details.getJobState() == JobStatus.CREATED || details.getJobState() == JobStatus.INITIALIZING) {\n \t\t\t\tscheduledJobs.add(details);\n \t\t\t} else if (!details.getJobState().isGloballyTerminalState()) {\n \t\t\t\trunningJobs.add(details);",
      "parent_sha": "12967c8721b5515d55eff87cfdc4b53633b6f1a8"
    }
  },
  {
    "oid": "4c86985dae04f0cb4daa7204f8dabc462d275041",
    "message": "[hotfix][test] Deduplicate code in BarrerTrackerTest",
    "date": "2019-06-05T12:58:59Z",
    "url": "https://github.com/apache/flink/commit/4c86985dae04f0cb4daa7204f8dabc462d275041",
    "details": {
      "sha": "df8c503ef28ca5f2f04daadb2408a580ca50eb73",
      "filename": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/io/BarrierTrackerTest.java",
      "status": "modified",
      "additions": 30,
      "deletions": 62,
      "changes": 92,
      "blob_url": "https://github.com/apache/flink/blob/4c86985dae04f0cb4daa7204f8dabc462d275041/flink-streaming-java%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Fio%2FBarrierTrackerTest.java",
      "raw_url": "https://github.com/apache/flink/raw/4c86985dae04f0cb4daa7204f8dabc462d275041/flink-streaming-java%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Fio%2FBarrierTrackerTest.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Fio%2FBarrierTrackerTest.java?ref=4c86985dae04f0cb4daa7204f8dabc462d275041",
      "patch": "@@ -30,6 +30,7 @@\n import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;\n import org.apache.flink.runtime.operators.testutils.DummyEnvironment;\n \n+import org.junit.After;\n import org.junit.Test;\n \n import java.util.Arrays;\n@@ -51,19 +52,23 @@ public class BarrierTrackerTest {\n \n \tprivate static final int PAGE_SIZE = 512;\n \n+\tprivate BarrierTracker tracker;\n+\n+\t@After\n+\tpublic void ensureEmpty() throws Exception {\n+\t\tassertNull(tracker.getNextNonBlocked());\n+\t\tassertNull(tracker.getNextNonBlocked());\n+\t\tassertTrue(tracker.isEmpty());\n+\t}\n+\n \t@Test\n \tpublic void testSingleChannelNoBarriers() throws Exception {\n \t\tBufferOrEvent[] sequence = { createBuffer(0), createBuffer(0), createBuffer(0) };\n-\n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 1, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n+\t\ttracker = createBarrierTracker(1, sequence);\n \n \t\tfor (BufferOrEvent boe : sequence) {\n \t\t\tassertEquals(boe, tracker.getNextNonBlocked());\n \t\t}\n-\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\t\tassertNull(tracker.getNextNonBlocked());\n \t}\n \n \t@Test\n@@ -72,16 +77,11 @@ public void testMultiChannelNoBarriers() throws Exception {\n \t\t\t\tcreateBuffer(1), createBuffer(0), createBuffer(3),\n \t\t\t\tcreateBuffer(1), createBuffer(1), createBuffer(2)\n \t\t};\n-\n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 4, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n+\t\ttracker = createBarrierTracker(4, sequence);\n \n \t\tfor (BufferOrEvent boe : sequence) {\n \t\t\tassertEquals(boe, tracker.getNextNonBlocked());\n \t\t}\n-\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\t\tassertNull(tracker.getNextNonBlocked());\n \t}\n \n \t@Test\n@@ -95,9 +95,7 @@ public void testSingleChannelWithBarriers() throws Exception {\n \t\t\t\tcreateBarrier(4, 0), createBarrier(5, 0), createBarrier(6, 0),\n \t\t\t\tcreateBuffer(0)\n \t\t};\n-\n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 1, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n+\t\ttracker = createBarrierTracker(1, sequence);\n \n \t\tCheckpointSequenceValidator validator =\n \t\t\t\tnew CheckpointSequenceValidator(1, 2, 3, 4, 5, 6);\n@@ -108,9 +106,6 @@ public void testSingleChannelWithBarriers() throws Exception {\n \t\t\t\tassertEquals(boe, tracker.getNextNonBlocked());\n \t\t\t}\n \t\t}\n-\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\t\tassertNull(tracker.getNextNonBlocked());\n \t}\n \n \t@Test\n@@ -124,9 +119,7 @@ public void testSingleChannelWithSkippedBarriers() throws Exception {\n \t\t\t\tcreateBarrier(7, 0), createBuffer(0), createBarrier(10, 0),\n \t\t\t\tcreateBuffer(0)\n \t\t};\n-\n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 1, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n+\t\ttracker = createBarrierTracker(1, sequence);\n \n \t\tCheckpointSequenceValidator validator =\n \t\t\t\tnew CheckpointSequenceValidator(1, 3, 4, 6, 7, 10);\n@@ -137,9 +130,6 @@ public void testSingleChannelWithSkippedBarriers() throws Exception {\n \t\t\t\tassertEquals(boe, tracker.getNextNonBlocked());\n \t\t\t}\n \t\t}\n-\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\t\tassertNull(tracker.getNextNonBlocked());\n \t}\n \n \t@Test\n@@ -162,9 +152,7 @@ public void testMultiChannelWithBarriers() throws Exception {\n \n \t\t\t\tcreateBuffer(0)\n \t\t};\n-\n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 3, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n+\t\ttracker = createBarrierTracker(3, sequence);\n \n \t\tCheckpointSequenceValidator validator =\n \t\t\t\tnew CheckpointSequenceValidator(1, 2, 3, 4);\n@@ -175,9 +163,6 @@ public void testMultiChannelWithBarriers() throws Exception {\n \t\t\t\tassertEquals(boe, tracker.getNextNonBlocked());\n \t\t\t}\n \t\t}\n-\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\t\tassertNull(tracker.getNextNonBlocked());\n \t}\n \n \t@Test\n@@ -204,9 +189,7 @@ public void testMultiChannelSkippingCheckpoints() throws Exception {\n \n \t\t\t\tcreateBuffer(0)\n \t\t};\n-\n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 3, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n+\t\ttracker = createBarrierTracker(3, sequence);\n \n \t\tCheckpointSequenceValidator validator =\n \t\t\t\tnew CheckpointSequenceValidator(1, 2, 4);\n@@ -217,9 +200,6 @@ public void testMultiChannelSkippingCheckpoints() throws Exception {\n \t\t\t\tassertEquals(boe, tracker.getNextNonBlocked());\n \t\t\t}\n \t\t}\n-\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\t\tassertNull(tracker.getNextNonBlocked());\n \t}\n \n \t/**\n@@ -280,24 +260,22 @@ public void testCompleteCheckpointsOnLateBarriers() throws Exception {\n \t\t\t\tcreateBuffer(1), createBuffer(2), createBarrier(9, 0),\n \n \t\t\t\t// trailing data\n-\t\t\t\tcreateBuffer(1), createBuffer(0), createBuffer(2)\n-\t\t};\n+\t\t\t\tcreateBuffer(1), createBuffer(0), createBuffer(2),\n \n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 3, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n+\t\t\t\t// complete checkpoint 10\n+\t\t\t\tcreateBarrier(10, 0), createBarrier(10, 1),\n+\t\t};\n+\t\ttracker = createBarrierTracker(3, sequence);\n \n \t\tCheckpointSequenceValidator validator =\n-\t\t\t\tnew CheckpointSequenceValidator(2, 3, 4, 5, 7, 8, 9);\n+\t\t\t\tnew CheckpointSequenceValidator(2, 3, 4, 5, 7, 8, 9, 10);\n \t\ttracker.registerCheckpointEventHandler(validator);\n \n \t\tfor (BufferOrEvent boe : sequence) {\n \t\t\tif (boe.isBuffer() || boe.getEvent().getClass() != CheckpointBarrier.class) {\n \t\t\t\tassertEquals(boe, tracker.getNextNonBlocked());\n \t\t\t}\n \t\t}\n-\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\t\tassertNull(tracker.getNextNonBlocked());\n \t}\n \n \t@Test\n@@ -313,9 +291,7 @@ public void testSingleChannelAbortCheckpoint() throws Exception {\n \t\t\t\tcreateCancellationBarrier(6, 0),\n \t\t\t\tcreateBuffer(0)\n \t\t};\n-\n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 1, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n+\t\ttracker = createBarrierTracker(1, sequence);\n \n \t\t// negative values mean an expected cancellation call!\n \t\tCheckpointSequenceValidator validator =\n@@ -328,9 +304,6 @@ public void testSingleChannelAbortCheckpoint() throws Exception {\n \t\t\t}\n \t\t\tassertTrue(tracker.isEmpty());\n \t\t}\n-\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\t\tassertNull(tracker.getNextNonBlocked());\n \t}\n \n \t@Test\n@@ -369,9 +342,7 @@ public void testMultiChannelAbortCheckpoint() throws Exception {\n \n \t\t\t\tcreateBuffer(0)\n \t\t};\n-\n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 3, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n+\t\ttracker = createBarrierTracker(3, sequence);\n \n \t\t// negative values mean an expected cancellation call!\n \t\tCheckpointSequenceValidator validator =\n@@ -383,13 +354,6 @@ public void testMultiChannelAbortCheckpoint() throws Exception {\n \t\t\t\tassertEquals(boe, tracker.getNextNonBlocked());\n \t\t\t}\n \t\t}\n-\n-\t\tassertTrue(tracker.isEmpty());\n-\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\t\tassertNull(tracker.getNextNonBlocked());\n-\n-\t\tassertTrue(tracker.isEmpty());\n \t}\n \n \t/**\n@@ -407,9 +371,8 @@ public void testInterleavedCancellationBarriers() throws Exception {\n \t\t\tcreateCancellationBarrier(2L, 2),\n \t\t\tcreateBuffer(0)\n \t\t};\n+\t\ttracker = createBarrierTracker(3, sequence);\n \n-\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, 3, Arrays.asList(sequence));\n-\t\tBarrierTracker tracker = new BarrierTracker(gate);\n \t\tAbstractInvokable statefulTask = mock(AbstractInvokable.class);\n \n \t\ttracker.registerCheckpointEventHandler(statefulTask);\n@@ -428,6 +391,11 @@ public void testInterleavedCancellationBarriers() throws Exception {\n \t//  Utils\n \t// ------------------------------------------------------------------------\n \n+\tprivate static BarrierTracker createBarrierTracker(int numberOfChannels, BufferOrEvent[] sequence) {\n+\t\tMockInputGate gate = new MockInputGate(PAGE_SIZE, numberOfChannels, Arrays.asList(sequence));\n+\t\treturn new BarrierTracker(gate);\n+\t}\n+\n \tprivate static BufferOrEvent createBarrier(long id, int channel) {\n \t\treturn new BufferOrEvent(new CheckpointBarrier(id, System.currentTimeMillis(), CheckpointOptions.forCheckpointWithDefaultLocation()), channel);\n \t}",
      "parent_sha": "ce729d757e7272471fd75f3174badfbb2ab839b8"
    }
  },
  {
    "oid": "3ae6801f6e35bf2fa9d73d4c227f20a84c432241",
    "message": "[FLINK-22625][connectors] Waiting for all tasks running before triggering savepoint in FileSinkMigrationITCase",
    "date": "2021-06-18T08:14:24Z",
    "url": "https://github.com/apache/flink/commit/3ae6801f6e35bf2fa9d73d4c227f20a84c432241",
    "details": {
      "sha": "8835145b87ea4ee7286dafb43deadd00aff9aa8a",
      "filename": "flink-connectors/flink-connector-files/src/test/java/org/apache/flink/connector/file/sink/writer/FileSinkMigrationITCase.java",
      "status": "modified",
      "additions": 3,
      "deletions": 76,
      "changes": 79,
      "blob_url": "https://github.com/apache/flink/blob/3ae6801f6e35bf2fa9d73d4c227f20a84c432241/flink-connectors%2Fflink-connector-files%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fconnector%2Ffile%2Fsink%2Fwriter%2FFileSinkMigrationITCase.java",
      "raw_url": "https://github.com/apache/flink/raw/3ae6801f6e35bf2fa9d73d4c227f20a84c432241/flink-connectors%2Fflink-connector-files%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fconnector%2Ffile%2Fsink%2Fwriter%2FFileSinkMigrationITCase.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors%2Fflink-connector-files%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fflink%2Fconnector%2Ffile%2Fsink%2Fwriter%2FFileSinkMigrationITCase.java?ref=3ae6801f6e35bf2fa9d73d4c227f20a84c432241",
      "patch": "@@ -20,9 +20,6 @@\n \n import org.apache.flink.api.common.JobID;\n import org.apache.flink.api.common.JobSubmissionResult;\n-import org.apache.flink.api.common.functions.IterationRuntimeContext;\n-import org.apache.flink.api.common.functions.RichFunction;\n-import org.apache.flink.api.common.functions.RuntimeContext;\n import org.apache.flink.api.common.state.CheckpointListener;\n import org.apache.flink.api.common.state.ListState;\n import org.apache.flink.api.common.state.ListStateDescriptor;\n@@ -40,7 +37,6 @@\n import org.apache.flink.streaming.api.CheckpointingMode;\n import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n-import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink;\n import org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.OnCheckpointRollingPolicy;\n import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;\n@@ -54,7 +50,6 @@\n import org.junit.Test;\n import org.junit.rules.TemporaryFolder;\n \n-import java.io.Serializable;\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Collections;\n@@ -63,6 +58,7 @@\n import java.util.stream.Collectors;\n import java.util.stream.LongStream;\n \n+import static org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning;\n import static org.junit.Assert.assertEquals;\n \n /**\n@@ -87,14 +83,10 @@ public class FileSinkMigrationITCase extends TestLogger {\n \n     private static final int NUM_BUCKETS = 4;\n \n-    private SharedReference<CountDownLatch> savepointLatch;\n-\n     private SharedReference<CountDownLatch> finalCheckpointLatch;\n \n     @Before\n     public void setup() {\n-        savepointLatch = sharedObjects.add(new CountDownLatch(NUM_SOURCES));\n-\n         // We wait for two successful checkpoints in sources before shutting down. This ensures that\n         // the sink can commit its data.\n         // We need to keep a \"static\" latch here because all sources need to be kept running\n@@ -157,7 +149,7 @@ private JobGraph createStreamingFileSinkJobGraph(String outputPath) {\n         env.addSource(new StatefulSource(true, finalCheckpointLatch))\n                 .uid(SOURCE_UID)\n                 .setParallelism(NUM_SOURCES)\n-                .addSink(new WaitingRunningSink<>(savepointLatch, sink))\n+                .addSink(sink)\n                 .setParallelism(NUM_SINKS)\n                 .uid(SINK_UID);\n         return env.getStreamGraph().getJobGraph();\n@@ -193,8 +185,7 @@ private String executeAndTakeSavepoint(\n                     miniCluster.submitJob(jobGraph);\n             JobID jobId = jobSubmissionResultFuture.get().getJobID();\n \n-            // wait till we can taking savepoint\n-            savepointLatch.get().await();\n+            waitForAllTaskRunning(miniCluster, jobId);\n \n             CompletableFuture<String> savepointResultFuture =\n                     miniCluster.triggerSavepoint(jobId, savepointBasePath, true);\n@@ -214,70 +205,6 @@ private void loadSavepointAndExecute(\n         }\n     }\n \n-    private static class WaitingRunningSink<T>\n-            implements RichFunction,\n-                    Serializable,\n-                    SinkFunction<T>,\n-                    CheckpointedFunction,\n-                    CheckpointListener {\n-        private final SharedReference<CountDownLatch> savepointLatch;\n-        private final StreamingFileSink<T> streamingFileSink;\n-\n-        /**\n-         * Creates a new {@code StreamingFileSink} that writes files to the given base directory\n-         * with the give buckets properties.\n-         */\n-        protected WaitingRunningSink(\n-                SharedReference<CountDownLatch> savepointLatch,\n-                StreamingFileSink<T> streamingFileSink) {\n-            this.savepointLatch = savepointLatch;\n-            this.streamingFileSink = streamingFileSink;\n-        }\n-\n-        public void setRuntimeContext(RuntimeContext t) {\n-            streamingFileSink.setRuntimeContext(t);\n-        }\n-\n-        public RuntimeContext getRuntimeContext() {\n-            return streamingFileSink.getRuntimeContext();\n-        }\n-\n-        public IterationRuntimeContext getIterationRuntimeContext() {\n-            return streamingFileSink.getIterationRuntimeContext();\n-        }\n-\n-        public void open(Configuration parameters) throws Exception {\n-            streamingFileSink.open(parameters);\n-        }\n-\n-        public void close() throws Exception {\n-            streamingFileSink.close();\n-        }\n-\n-        public void initializeState(FunctionInitializationContext context) throws Exception {\n-            streamingFileSink.initializeState(context);\n-        }\n-\n-        public void notifyCheckpointComplete(long checkpointId) throws Exception {\n-            streamingFileSink.notifyCheckpointComplete(checkpointId);\n-        }\n-\n-        public void notifyCheckpointAborted(long checkpointId) {\n-            streamingFileSink.notifyCheckpointAborted(checkpointId);\n-        }\n-\n-        public void snapshotState(FunctionSnapshotContext context) throws Exception {\n-            streamingFileSink.snapshotState(context);\n-        }\n-\n-        @Override\n-        public void invoke(T value, Context context) throws Exception {\n-            savepointLatch.get().countDown();\n-\n-            streamingFileSink.invoke(value, context);\n-        }\n-    }\n-\n     private static class StatefulSource extends RichParallelSourceFunction<Integer>\n             implements CheckpointedFunction, CheckpointListener {\n ",
      "parent_sha": "3fa698295175946085500e2f7971611b868eaed2"
    }
  },
  {
    "oid": "8c8c02887a27cdc87bb019626f82ec03392ca8ce",
    "message": "[FLINK-5237] Don't Fire Processing-Time Timer in registerTimer()\n\nImmediately firing the timer can lead to endless recursion if the\nonTimer() method sets a timer for the past.",
    "date": "2017-01-11T09:35:47Z",
    "url": "https://github.com/apache/flink/commit/8c8c02887a27cdc87bb019626f82ec03392ca8ce",
    "details": {
      "sha": "b4e7e9722da0e4628e25bd3cc874b0c17455ffe2",
      "filename": "flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/tasks/TestProcessingTimeService.java",
      "status": "modified",
      "additions": 2,
      "deletions": 10,
      "changes": 12,
      "blob_url": "https://github.com/apache/flink/blob/8c8c02887a27cdc87bb019626f82ec03392ca8ce/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FTestProcessingTimeService.java",
      "raw_url": "https://github.com/apache/flink/raw/8c8c02887a27cdc87bb019626f82ec03392ca8ce/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FTestProcessingTimeService.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-streaming-java%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fstreaming%2Fruntime%2Ftasks%2FTestProcessingTimeService.java?ref=8c8c02887a27cdc87bb019626f82ec03392ca8ce",
      "patch": "@@ -53,7 +53,7 @@ public int compare(Tuple2<Long, CallbackTask> o1, Tuple2<Long, CallbackTask> o2)\n \t\t\t}\n \t\t});\n \t}\n-\t\n+\n \tpublic void setCurrentTime(long timestamp) throws Exception {\n \t\tthis.currentTime = timestamp;\n \n@@ -90,15 +90,7 @@ public ScheduledFuture<?> registerTimer(long timestamp, ProcessingTimeCallback t\n \n \t\tCallbackTask callbackTask = new CallbackTask(target);\n \n-\t\tif (timestamp <= currentTime) {\n-\t\t\ttry {\n-\t\t\t\tcallbackTask.onProcessingTime(timestamp);\n-\t\t\t} catch (Exception e) {\n-\t\t\t\tthrow new RuntimeException(e);\n-\t\t\t}\n-\t\t} else {\n-\t\t\tpriorityQueue.offer(Tuple2.of(timestamp, callbackTask));\n-\t\t}\n+\t\tpriorityQueue.offer(Tuple2.of(timestamp, callbackTask));\n \n \t\treturn callbackTask;\n \t}",
      "parent_sha": "274cc41a6a43e95243d6e7b3110e16ff6f0ad633"
    }
  },
  {
    "oid": "a3e3bedfef879e5e8659e78ae06cbca22d53beba",
    "message": "[hotfix] Replace String concatenation with Slf4j placeholders.",
    "date": "2018-04-30T19:20:55Z",
    "url": "https://github.com/apache/flink/commit/a3e3bedfef879e5e8659e78ae06cbca22d53beba",
    "details": {
      "sha": "fb9a478b3a7bc93ed9077d16d3ad446e6104f1b8",
      "filename": "flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/flink/blob/a3e3bedfef879e5e8659e78ae06cbca22d53beba/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FUtils.java",
      "raw_url": "https://github.com/apache/flink/raw/a3e3bedfef879e5e8659e78ae06cbca22d53beba/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FUtils.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-yarn%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Fyarn%2FUtils.java?ref=a3e3bedfef879e5e8659e78ae06cbca22d53beba",
      "patch": "@@ -156,7 +156,7 @@ static Tuple2<Path, LocalResource> setupLocalResource(\n \n \t\tPath dst = new Path(homedir, suffix);\n \n-\t\tLOG.debug(\"Copying from \" + localSrcPath + \" to \" + dst);\n+\t\tLOG.debug(\"Copying from {} to {}\", localSrcPath, dst);\n \n \t\tfs.copyFromLocalFile(false, true, localSrcPath, dst);\n ",
      "parent_sha": "ad7a4ac9d82f6d3a96191936a3ac1557a78458d6"
    }
  },
  {
    "oid": "39acdb762295f6b744c0af0e0449faaef8e73652",
    "message": "[FLINK-23168][hive] HiveCatalog shouldn't merge properties for alter DB operation\n\nThis closes #16335",
    "date": "2021-07-02T06:36:16Z",
    "url": "https://github.com/apache/flink/commit/39acdb762295f6b744c0af0e0449faaef8e73652",
    "details": {
      "sha": "e6bb77e865b541396a11a1e55d81003fc6c5d01d",
      "filename": "flink-connectors/flink-connector-hive/src/main/java/org/apache/flink/table/catalog/hive/HiveCatalog.java",
      "status": "modified",
      "additions": 1,
      "deletions": 6,
      "changes": 7,
      "blob_url": "https://github.com/apache/flink/blob/39acdb762295f6b744c0af0e0449faaef8e73652/flink-connectors%2Fflink-connector-hive%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fcatalog%2Fhive%2FHiveCatalog.java",
      "raw_url": "https://github.com/apache/flink/raw/39acdb762295f6b744c0af0e0449faaef8e73652/flink-connectors%2Fflink-connector-hive%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fcatalog%2Fhive%2FHiveCatalog.java",
      "contents_url": "https://api.github.com/repos/apache/flink/contents/flink-connectors%2Fflink-connector-hive%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fflink%2Ftable%2Fcatalog%2Fhive%2FHiveCatalog.java?ref=39acdb762295f6b744c0af0e0449faaef8e73652",
      "patch": "@@ -1758,7 +1758,6 @@ public static boolean isEmbeddedMetastore(HiveConf hiveConf) {\n     }\n \n     private static Database alterDatabase(Database hiveDB, CatalogDatabase newDatabase) {\n-        Map<String, String> params = hiveDB.getParameters();\n         Map<String, String> newParams = newDatabase.getProperties();\n         String opStr = newParams.remove(ALTER_DATABASE_OP);\n         if (opStr == null) {\n@@ -1770,11 +1769,7 @@ private static Database alterDatabase(Database hiveDB, CatalogDatabase newDataba\n                 SqlAlterHiveDatabase.AlterHiveDatabaseOp.valueOf(opStr);\n         switch (op) {\n             case CHANGE_PROPS:\n-                if (params == null) {\n-                    hiveDB.setParameters(newParams);\n-                } else {\n-                    params.putAll(newParams);\n-                }\n+                hiveDB.setParameters(newParams);\n                 break;\n             case CHANGE_LOCATION:\n                 hiveDB.setLocationUri(newLocation);",
      "parent_sha": "37cd345c729016379a84b6e8083186561e3beb76"
    }
  }
]