[
  {
    "oid": "c0d5c4e8687f91af932c4f1177958d7f2d5c4a77",
    "message": "replace decreasing timeout with fixed timeout",
    "date": "2017-11-14T21:28:25Z",
    "url": "https://github.com/elastic/logstash/commit/c0d5c4e8687f91af932c4f1177958d7f2d5c4a77",
    "details": {
      "sha": "36ddef0a3aa5772fd5b9d9271011a600327687d1",
      "filename": "logstash-core/src/main/java/org/logstash/common/LsQueueUtils.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/elastic/logstash/blob/c0d5c4e8687f91af932c4f1177958d7f2d5c4a77/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2FLsQueueUtils.java",
      "raw_url": "https://github.com/elastic/logstash/raw/c0d5c4e8687f91af932c4f1177958d7f2d5c4a77/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2FLsQueueUtils.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2FLsQueueUtils.java?ref=c0d5c4e8687f91af932c4f1177958d7f2d5c4a77",
      "patch": "@@ -77,13 +77,12 @@ public static Collection<JrubyEventExtLibrary.RubyEvent> drain(\n     private static int drain(final BlockingQueue<JrubyEventExtLibrary.RubyEvent> queue,\n         final Collection<JrubyEventExtLibrary.RubyEvent> collection, final int count,\n         final long nanos) throws InterruptedException {\n-        final long deadline = System.nanoTime() + nanos;\n         int added = 0;\n         do {\n             added += queue.drainTo(collection, count - added);\n             if (added < count) {\n                 final JrubyEventExtLibrary.RubyEvent event =\n-                    queue.poll(deadline - System.nanoTime(), TimeUnit.NANOSECONDS);\n+                    queue.poll(nanos, TimeUnit.NANOSECONDS);\n                 if (event == null) {\n                     break;\n                 }",
      "parent_sha": "0371abad1de48110dcd85c65bde51112bd028ef2"
    }
  },
  {
    "oid": "a98983aa10e047d575d77ad2c41c9173365951b0",
    "message": "add explit fsync on checkpoint write\n\nFixes #6430",
    "date": "2016-12-23T19:34:43Z",
    "url": "https://github.com/elastic/logstash/commit/a98983aa10e047d575d77ad2c41c9173365951b0",
    "details": {
      "sha": "b20753e558bff594a18d501d27cfb6db8b70f2bf",
      "filename": "logstash-core/src/main/java/org/logstash/common/io/FileCheckpointIO.java",
      "status": "modified",
      "additions": 7,
      "deletions": 1,
      "changes": 8,
      "blob_url": "https://github.com/elastic/logstash/blob/a98983aa10e047d575d77ad2c41c9173365951b0/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FFileCheckpointIO.java",
      "raw_url": "https://github.com/elastic/logstash/raw/a98983aa10e047d575d77ad2c41c9173365951b0/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FFileCheckpointIO.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FFileCheckpointIO.java?ref=a98983aa10e047d575d77ad2c41c9173365951b0",
      "patch": "@@ -5,8 +5,13 @@\n import java.io.IOException;\n import java.io.InputStream;\n import java.nio.file.Files;\n+import java.nio.file.OpenOption;\n import java.nio.file.Path;\n import java.nio.file.Paths;\n+import static java.nio.file.StandardOpenOption.CREATE;\n+import static java.nio.file.StandardOpenOption.WRITE;\n+import static java.nio.file.StandardOpenOption.TRUNCATE_EXISTING;\n+import static java.nio.file.StandardOpenOption.DSYNC;\n \n public class FileCheckpointIO  implements CheckpointIO {\n //    Checkpoint file structure\n@@ -50,10 +55,11 @@ public Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, l\n \n     @Override\n     public void write(String fileName, Checkpoint checkpoint) throws IOException {\n+        OpenOption[] options = new OpenOption[] { WRITE, CREATE, TRUNCATE_EXISTING, DSYNC };\n         Path path = Paths.get(dirPath, fileName);\n         final byte[] buffer = new byte[BUFFER_SIZE];\n         write(checkpoint, buffer);\n-        Files.write(path, buffer);\n+        Files.write(path, buffer, options);\n     }\n \n     @Override",
      "parent_sha": "bec118ca5703dd7c974f622ddff3d5705676a3d2"
    }
  },
  {
    "oid": "e433abdbc14a721318a53e99a5056cc488b551ab",
    "message": "move options as constant\n\nFixes #6430",
    "date": "2016-12-23T19:34:44Z",
    "url": "https://github.com/elastic/logstash/commit/e433abdbc14a721318a53e99a5056cc488b551ab",
    "details": {
      "sha": "5388003fc41c0e5410e69741f6b9714475860b07",
      "filename": "logstash-core/src/main/java/org/logstash/common/io/FileCheckpointIO.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/elastic/logstash/blob/e433abdbc14a721318a53e99a5056cc488b551ab/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FFileCheckpointIO.java",
      "raw_url": "https://github.com/elastic/logstash/raw/e433abdbc14a721318a53e99a5056cc488b551ab/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FFileCheckpointIO.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FFileCheckpointIO.java?ref=e433abdbc14a721318a53e99a5056cc488b551ab",
      "patch": "@@ -34,6 +34,7 @@ public class FileCheckpointIO  implements CheckpointIO {\n     private final String dirPath;\n     private final String HEAD_CHECKPOINT = \"checkpoint.head\";\n     private final String TAIL_CHECKPOINT = \"checkpoint.\";\n+    private final OpenOption[] WRITE_OPTIONS = new OpenOption[] { WRITE, CREATE, TRUNCATE_EXISTING, DSYNC };\n \n     public FileCheckpointIO(String dirPath) {\n         this.dirPath = dirPath;\n@@ -55,11 +56,10 @@ public Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, l\n \n     @Override\n     public void write(String fileName, Checkpoint checkpoint) throws IOException {\n-        OpenOption[] options = new OpenOption[] { WRITE, CREATE, TRUNCATE_EXISTING, DSYNC };\n         Path path = Paths.get(dirPath, fileName);\n         final byte[] buffer = new byte[BUFFER_SIZE];\n         write(checkpoint, buffer);\n-        Files.write(path, buffer, options);\n+        Files.write(path, buffer, WRITE_OPTIONS);\n     }\n \n     @Override",
      "parent_sha": "a98983aa10e047d575d77ad2c41c9173365951b0"
    }
  },
  {
    "oid": "61982bcc5b40e44a5667b1b79c7202a8325e3f66",
    "message": "MINOR: Use more optimal loop in draining QUeue\n\nFixes #8101",
    "date": "2017-08-30T07:39:42Z",
    "url": "https://github.com/elastic/logstash/commit/61982bcc5b40e44a5667b1b79c7202a8325e3f66",
    "details": {
      "sha": "c85fa90d1271e0bc57e7c4dfd4f0721706a5bb0c",
      "filename": "logstash-core/src/main/java/org/logstash/common/LsQueueUtils.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/elastic/logstash/blob/61982bcc5b40e44a5667b1b79c7202a8325e3f66/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2FLsQueueUtils.java",
      "raw_url": "https://github.com/elastic/logstash/raw/61982bcc5b40e44a5667b1b79c7202a8325e3f66/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2FLsQueueUtils.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2FLsQueueUtils.java?ref=61982bcc5b40e44a5667b1b79c7202a8325e3f66",
      "patch": "@@ -37,13 +37,13 @@ public static Collection<JrubyEventExtLibrary.RubyEvent> drain(\n         int left = count;\n         final Collection<JrubyEventExtLibrary.RubyEvent> collection =\n             new HashSet<>(4 * count / 3 + 1);\n-        while (left > 0) {\n+        do {\n             final int drained = drain(queue, collection, left, nanos);\n             if (drained == 0) {\n                 break;\n             }\n             left -= drained;\n-        }\n+        } while (left > 0);\n         return collection;\n     }\n \n@@ -65,7 +65,7 @@ private static int drain(final BlockingQueue<JrubyEventExtLibrary.RubyEvent> que\n         final long nanos) throws InterruptedException {\n         final long deadline = System.nanoTime() + nanos;\n         int added = 0;\n-        while (added < count) {\n+        do {\n             added += queue.drainTo(collection, count - added);\n             if (added < count) {\n                 final JrubyEventExtLibrary.RubyEvent event =\n@@ -76,7 +76,7 @@ private static int drain(final BlockingQueue<JrubyEventExtLibrary.RubyEvent> que\n                 collection.add(event);\n                 added++;\n             }\n-        }\n+        } while (added < count);\n         return added;\n     }\n ",
      "parent_sha": "2fc04c9eddc9e1ffc53bf9a2663b66f1b64e83fc"
    }
  },
  {
    "oid": "b99c2f956dcb9bdb929d71abddccd994e97e3556",
    "message": "Mention the path of DLQ to indicate DLQ if full for which pipeline\n\nFixes #11280",
    "date": "2019-11-13T15:54:52Z",
    "url": "https://github.com/elastic/logstash/commit/b99c2f956dcb9bdb929d71abddccd994e97e3556",
    "details": {
      "sha": "d9159e201b346cce5305e37113fb835ac37f06bf",
      "filename": "logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/b99c2f956dcb9bdb929d71abddccd994e97e3556/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "raw_url": "https://github.com/elastic/logstash/raw/b99c2f956dcb9bdb929d71abddccd994e97e3556/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java?ref=b99c2f956dcb9bdb929d71abddccd994e97e3556",
      "patch": "@@ -131,7 +131,7 @@ private void innerWriteEntry(DLQEntry entry) throws IOException {\n         byte[] record = entry.serialize();\n         int eventPayloadSize = RECORD_HEADER_SIZE + record.length;\n         if (currentQueueSize.longValue() + eventPayloadSize > maxQueueSize) {\n-            logger.error(\"cannot write event to DLQ: reached maxQueueSize of \" + maxQueueSize);\n+            logger.error(\"cannot write event to DLQ(path: \" + this.queuePath + \"): reached maxQueueSize of \" + maxQueueSize);\n             return;\n         } else if (currentWriter.getPosition() + eventPayloadSize > maxSegmentSize) {\n             currentWriter.close();",
      "parent_sha": "ef9b0d2db5f1a30cd0cef7920991b3b309323019"
    }
  },
  {
    "oid": "d9f3d8b7db776a54fbe21127ba24f8a508e52468",
    "message": "Fix: missed 'equal' part in time comparison test (#11862)\n\nIn time comparison of LocalDateTime the isBefore is strict, so in case two instants has the same millisecond, it fails in test (happens in Windows tests)\r\n\r\nClose: 11862",
    "date": "2020-05-04T15:19:09Z",
    "url": "https://github.com/elastic/logstash/commit/d9f3d8b7db776a54fbe21127ba24f8a508e52468",
    "details": {
      "sha": "fba436f70c388f51e678f68e38f2b7e5601d30f7",
      "filename": "logstash-core/src/test/java/org/logstash/config/ir/PipelineConfigTest.java",
      "status": "modified",
      "additions": 26,
      "deletions": 1,
      "changes": 27,
      "blob_url": "https://github.com/elastic/logstash/blob/d9f3d8b7db776a54fbe21127ba24f8a508e52468/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FPipelineConfigTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/d9f3d8b7db776a54fbe21127ba24f8a508e52468/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FPipelineConfigTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FPipelineConfigTest.java?ref=d9f3d8b7db776a54fbe21127ba24f8a508e52468",
      "patch": "@@ -19,6 +19,8 @@\n \n package org.logstash.config.ir;\n \n+import org.hamcrest.Description;\n+import org.hamcrest.TypeSafeMatcher;\n import org.jruby.*;\n import org.jruby.runtime.builtin.IRubyObject;\n import org.junit.Before;\n@@ -35,6 +37,25 @@\n \n public class PipelineConfigTest extends RubyEnvTestCase {\n \n+    private static class IsBeforeOrSameMatcher extends TypeSafeMatcher<LocalDateTime> {\n+\n+        private LocalDateTime after;\n+\n+        IsBeforeOrSameMatcher(LocalDateTime after) {\n+            this.after = after;\n+        }\n+\n+        @Override\n+        protected boolean matchesSafely(LocalDateTime item) {\n+            return item.isBefore(after) || item.isEqual(after);\n+        }\n+\n+        @Override\n+        public void describeTo(Description description) {\n+            description.appendText(\" is before \" + after);\n+        }\n+    }\n+\n     public static final String PIPELINE_ID = \"main\";\n     private RubyClass source;\n     private RubySymbol pipelineIdSym;\n@@ -87,7 +108,11 @@ public void testReturnsTheSource() {\n         assertEquals(\"returns the pipeline id\", PIPELINE_ID, sut.getPipelineId());\n         assertNotNull(\"returns the config_hash\", sut.configHash());\n         assertEquals(\"returns the merged `ConfigPart#config_string`\", configMerged, sut.configString());\n-        assertTrue(\"records when the config was read\", sut.getReadAt().isBefore(LocalDateTime.now()));\n+        assertThat(\"records when the config was read\", sut.getReadAt(), isBeforeOrSame(LocalDateTime.now()));\n+    }\n+\n+    private static IsBeforeOrSameMatcher isBeforeOrSame(LocalDateTime after) {\n+        return new IsBeforeOrSameMatcher(after);\n     }\n \n     @SuppressWarnings(\"rawtypes\")",
      "parent_sha": "0856f7ddc939deb3c93171b7b9541bcd6e3b9608"
    }
  },
  {
    "oid": "66f0b52318936f10c8bce54621d362152f8fc1a9",
    "message": "Fix broken Gradle tests\n\nFixes #7114",
    "date": "2017-05-16T15:59:34Z",
    "url": "https://github.com/elastic/logstash/commit/66f0b52318936f10c8bce54621d362152f8fc1a9",
    "details": {
      "sha": "b58aad0c4817272dc6cdb2bca5af13a259ecf68e",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 31,
      "deletions": 31,
      "changes": 62,
      "blob_url": "https://github.com/elastic/logstash/blob/66f0b52318936f10c8bce54621d362152f8fc1a9/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/66f0b52318936f10c8bce54621d362152f8fc1a9/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=66f0b52318936f10c8bce54621d362152f8fc1a9",
      "patch": "@@ -19,6 +19,7 @@\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicInteger;\n \n import static org.hamcrest.CoreMatchers.equalTo;\n@@ -473,38 +474,37 @@ public void ackingMakesQueueNotFullAgainTest() throws IOException, InterruptedEx\n \n         // allow 10 elements per page but only 100 events in total\n         Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);\n-\n-        TestQueue q = new TestQueue(settings);\n-        q.open();\n-\n-        int ELEMENT_COUNT = 90; // should be able to write 90 events (9 pages) before getting full\n-        for (int i = 0; i < ELEMENT_COUNT; i++) {\n-            long seqNum = q.write(element);\n-        }\n-\n-        assertThat(q.isFull(), is(false));\n-\n-        // we expect this next write call to block so let's wrap it in a Future\n-        Callable<Long> write = () -> {\n-            return q.write(element);\n-        };\n         ExecutorService executor = Executors.newFixedThreadPool(1);\n-        Future<Long> future = executor.submit(write);\n-        assertThat(future.isDone(), is(false));\n-\n-        while (!q.isFull()) { Thread.sleep(10); }\n-        assertThat(q.isFull(), is(true));\n-\n-        Batch b = q.readBatch(10); // read 1 page (10 events)\n-        b.close();  // purge 1 page\n-\n-        while (q.isFull()) { Thread.sleep(10); }\n-        assertThat(q.isFull(), is(false));\n-\n-        // will not complete because write will not unblock until the page is purge with a batch close/acking.\n-        assertThat(future.isDone(), is(false));\n-\n-        q.close();\n+        try (TestQueue q = new TestQueue(settings)) {\n+            q.open();\n+            // should be able to write 90 events (9 pages) before getting full\n+            final long ELEMENT_COUNT = 90;\n+            for (int i = 0; i < ELEMENT_COUNT; i++) {\n+                q.write(element);\n+            }\n+            assertThat(q.isFull(), is(false));\n+            \n+            // we expect this next write call to block so let's wrap it in a Future\n+            Callable<Long> write = () -> q.write(element);\n+            Future<Long> future = executor.submit(write);\n+            assertThat(future.isDone(), is(false));\n+            \n+            while (!q.isFull()) {\n+                Thread.sleep(10);\n+            }\n+            assertThat(q.isFull(), is(true));\n+            \n+            Batch b = q.readBatch(10); // read 1 page (10 events)\n+            b.close();  // purge 1 page\n+            \n+            while (q.isFull()) { Thread.sleep(10); }\n+            assertThat(q.isFull(), is(false));\n+            \n+            assertThat(future.get(), is(ELEMENT_COUNT + 1));\n+        } finally {\n+            executor.shutdownNow();\n+            executor.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n+        }\n     }\n \n     @Test(timeout = 5000)",
      "parent_sha": "c9700220bd67b3bdd30bc33c133e66a68670179f"
    }
  },
  {
    "oid": "9f5506db078a17c584a86c02009759b45ead59c3",
    "message": "single element write can only really wakeup a single thread\n\nFixes #6800",
    "date": "2017-03-20T15:44:11Z",
    "url": "https://github.com/elastic/logstash/commit/9f5506db078a17c584a86c02009759b45ead59c3",
    "details": {
      "sha": "cb6fbfa90dad624c9013d36ffd375a220e60143c",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/elastic/logstash/blob/9f5506db078a17c584a86c02009759b45ead59c3/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "raw_url": "https://github.com/elastic/logstash/raw/9f5506db078a17c584a86c02009759b45ead59c3/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java?ref=9f5506db078a17c584a86c02009759b45ead59c3",
      "patch": "@@ -330,7 +330,9 @@ public long write(Queueable element) throws IOException {\n             this.unreadCount++;\n \n             // if the queue was empty before write, signal non emptiness\n-            if (wasEmpty) { notEmpty.signalAll(); }\n+            // a simple signal and not signalAll is necessary here since writing a single element\n+            // can only really enable a single thread to read a batch\n+            if (wasEmpty) { notEmpty.signal(); }\n \n             // now check if we reached a queue full state and block here until it is not full\n             // for the next write or the queue was closed.",
      "parent_sha": "6848798dcf76351473c5f3b607851d0305c88073"
    }
  },
  {
    "oid": "81a339e746d0c81879a5744eea5f44d984b01f9a",
    "message": "Fix: use l/w match-ing (which does not depend on frames)\n\nthis was wrong on LS' end - String#match impl expects a frame (due $~)\n... started failing due JRuby reducing frame usage (for blocks)\n\nFixes #11653",
    "date": "2020-03-04T16:39:10Z",
    "url": "https://github.com/elastic/logstash/commit/81a339e746d0c81879a5744eea5f44d984b01f9a",
    "details": {
      "sha": "829167b9ef44a9796b65747f88e0fd89f3c2d8f5",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java",
      "status": "modified",
      "additions": 17,
      "deletions": 11,
      "changes": 28,
      "blob_url": "https://github.com/elastic/logstash/blob/81a339e746d0c81879a5744eea5f44d984b01f9a/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FEventCondition.java",
      "raw_url": "https://github.com/elastic/logstash/raw/81a339e746d0c81879a5744eea5f44d984b01f9a/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FEventCondition.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FEventCondition.java?ref=81a339e746d0c81879a5744eea5f44d984b01f9a",
      "patch": "@@ -7,6 +7,7 @@\n import java.util.function.Predicate;\n \n import org.jruby.Ruby;\n+import org.jruby.RubyRegexp;\n import org.jruby.RubyString;\n import org.jruby.runtime.builtin.IRubyObject;\n import org.jruby.util.ByteList;\n@@ -467,35 +468,40 @@ private static boolean valueIsTruthy(Object object) {\n                     !Boolean.toString(false).equals(other);\n         }\n \n+        private static RubyRegexp newRegexp(String pattern) {\n+            final Ruby runtime = RubyUtil.RUBY;\n+            return RubyRegexp.newRegexpFromStr(runtime, runtime.newString(pattern), 0);\n+        }\n+\n+        private static boolean matches(RubyString str, RubyRegexp regexp) {\n+            return regexp.match_p(RubyUtil.RUBY.getCurrentContext(), str).isTrue(); // match? returns true/false\n+        }\n+\n         private static final class FieldMatches implements EventCondition {\n \n             private final FieldReference field;\n \n-            private final RubyString regex;\n+            private final RubyRegexp regexp;\n \n-            private FieldMatches(final String field, final String regex) {\n+            private FieldMatches(final String field, final String pattern) {\n                 this.field = FieldReference.from(field);\n-                this.regex = RubyUtil.RUBY.newString(regex);\n+                this.regexp = newRegexp(pattern);\n             }\n \n             @Override\n             public boolean fulfilled(final JrubyEventExtLibrary.RubyEvent event) {\n-                final Object tomatch = event.getEvent().getUnconvertedField(field);\n-                return tomatch instanceof RubyString &&\n-                    !((RubyString) tomatch).match(RubyUtil.RUBY.getCurrentContext(), regex).isNil();\n+                final Object toMatch = event.getEvent().getUnconvertedField(field);\n+                return toMatch instanceof RubyString && matches((RubyString) toMatch, regexp);\n             }\n         }\n \n         private static final class ConstantMatches implements EventCondition {\n \n             private final boolean matches;\n \n-            private ConstantMatches(final Object constant, final String regex) {\n-                final Ruby runtime = RubyUtil.RUBY;\n+            private ConstantMatches(final Object constant, final String pattern) {\n                 this.matches = constant instanceof String &&\n-                        !(runtime.newString((String) constant).match(\n-                                runtime.getCurrentContext(),\n-                                runtime.newString(regex)).isNil());\n+                        matches(RubyUtil.RUBY.newString((String) constant), newRegexp(pattern));\n             }\n \n             @Override",
      "parent_sha": "fc0ced5708f67686edb1848c9898a8b5773961e3"
    }
  },
  {
    "oid": "235dc12c09c0dbc5be70191748c37d4f95373eb0",
    "message": "Allow negative array indexes to mean an offset from the end of the array, where -1 means the last element.\n\nFixes #6226",
    "date": "2016-12-06T23:33:44Z",
    "url": "https://github.com/elastic/logstash/commit/235dc12c09c0dbc5be70191748c37d4f95373eb0",
    "details": {
      "sha": "00da2b9f11f5abce7ce0dfc0b23301861612a0ca",
      "filename": "logstash-core-event-java/src/main/java/org/logstash/Accessors.java",
      "status": "modified",
      "additions": 22,
      "deletions": 7,
      "changes": 29,
      "blob_url": "https://github.com/elastic/logstash/blob/235dc12c09c0dbc5be70191748c37d4f95373eb0/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FAccessors.java",
      "raw_url": "https://github.com/elastic/logstash/raw/235dc12c09c0dbc5be70191748c37d4f95373eb0/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FAccessors.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FAccessors.java?ref=235dc12c09c0dbc5be70191748c37d4f95373eb0",
      "patch": "@@ -34,10 +34,15 @@ public Object del(String reference) {\n                 return ((Map<String, Object>) target).remove(field.getKey());\n             } else if (target instanceof List) {\n                 int i = Integer.parseInt(field.getKey());\n-                if (i < 0 || i >= ((List) target).size()) {\n-                    return null;\n+                int size = ((List) target).size();\n+                if (i >= size || i < -size) {\n+                  return null;\n+                } else if (i < 0) {\n+                  // Offset from the end of the array.\n+                  return ((List<Object>) target).remove(size + i);\n+                } else {\n+                  return ((List<Object>) target).remove(i);\n                 }\n-                return ((List<Object>) target).remove(i);\n             } else {\n                 throw newCollectionException(target);\n             }\n@@ -112,9 +117,15 @@ private Object findCreateTarget(FieldReference field) {\n     }\n \n     private boolean foundInList(List<Object> target, int index) {\n-        if (index < 0 || index >= target.size()) {\n+        int size = ((List) target).size();\n+        if (index >= size || index < -size) {\n             return false;\n         }\n+\n+        if (index < 0) {\n+            index = size + index;\n+        }\n+\n         return target.get(index) != null;\n     }\n \n@@ -128,11 +139,15 @@ private Object fetch(Object target, String key) {\n             return result;\n         } else if (target instanceof List) {\n             int i = Integer.parseInt(key);\n-            if (i < 0 || i >= ((List) target).size()) {\n+            int size = ((List) target).size();\n+            if (i >= size || i < -size) {\n                 return null;\n+            } else if (i < 0) {\n+                // Offset from the end of the array.\n+                return ((List<Object>) target).get(size + i);\n+            } else {\n+                return ((List<Object>) target).get(i);\n             }\n-            Object result = ((List<Object>) target).get(i);\n-            return result;\n         } else if (target == null) {\n             return null;\n         } else {",
      "parent_sha": "c8131b146a3fbad4c07a3075d9051d8ca8c9c959"
    }
  },
  {
    "oid": "669be49408301a5ed263209aa2ba78f1e3fcebf0",
    "message": "Add some safety to closing the writer and deleting the lock file\n\nFixes #7822",
    "date": "2017-07-26T20:55:14Z",
    "url": "https://github.com/elastic/logstash/commit/669be49408301a5ed263209aa2ba78f1e3fcebf0",
    "details": {
      "sha": "ac04ad910f63a79ba8bb8d40891303bb6a58b0cd",
      "filename": "logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java",
      "status": "modified",
      "additions": 24,
      "deletions": 8,
      "changes": 32,
      "blob_url": "https://github.com/elastic/logstash/blob/669be49408301a5ed263209aa2ba78f1e3fcebf0/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "raw_url": "https://github.com/elastic/logstash/raw/669be49408301a5ed263209aa2ba78f1e3fcebf0/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java?ref=669be49408301a5ed263209aa2ba78f1e3fcebf0",
      "patch": "@@ -146,17 +146,33 @@ private void innerWriteEntry(DLQEntry entry) throws IOException {\n \n     @Override\n     public synchronized void close() throws IOException {\n-        if (this.lock != null){\n-            this.lock.release();\n-            if (this.lock.channel() != null && this.lock.channel().isOpen()) {\n-                this.lock.channel().close();\n+        if (currentWriter != null) {\n+            try {\n+                currentWriter.close();\n+                open = false;\n+            }catch (Exception e){\n+                logger.debug(\"Unable to close dlq writer\", e);\n             }\n         }\n-        if (currentWriter != null) {\n-            currentWriter.close();\n+        releaseLock();\n+    }\n+\n+    private void releaseLock() {\n+        if (this.lock != null){\n+            try {\n+                this.lock.release();\n+                if (this.lock.channel() != null && this.lock.channel().isOpen()) {\n+                    this.lock.channel().close();\n+                }\n+            } catch (Exception e) {\n+                logger.debug(\"Unable to close lock channel\", e);\n+            }\n+            try {\n+                Files.deleteIfExists(queuePath.resolve(LOCK_FILE));\n+            } catch (IOException e){\n+                logger.debug(\"Unable to delete lock file\", e);\n+            }\n         }\n-        Files.deleteIfExists(queuePath.resolve(LOCK_FILE));\n-        open = false;\n     }\n \n     public boolean isOpen() {",
      "parent_sha": "435466b6b0149af5172ee61da3c109e17c2d3734"
    }
  },
  {
    "oid": "3fdc4c3aa7e1a5043da816320f432b444956a469",
    "message": "Fix unit test under Windows (#13656)\n\nUse the System.lineSeparator instead of \"\\n\" to make the test portable across platforms",
    "date": "2022-01-20T19:14:18Z",
    "url": "https://github.com/elastic/logstash/commit/3fdc4c3aa7e1a5043da816320f432b444956a469",
    "details": {
      "sha": "109e025b3da0c03e11aa3167d2263799e59bc347",
      "filename": "logstash-core/src/test/java/org/logstash/launchers/JvmOptionsParserTest.java",
      "status": "modified",
      "additions": 7,
      "deletions": 7,
      "changes": 14,
      "blob_url": "https://github.com/elastic/logstash/blob/3fdc4c3aa7e1a5043da816320f432b444956a469/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Flaunchers%2FJvmOptionsParserTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/3fdc4c3aa7e1a5043da816320f432b444956a469/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Flaunchers%2FJvmOptionsParserTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Flaunchers%2FJvmOptionsParserTest.java?ref=3fdc4c3aa7e1a5043da816320f432b444956a469",
      "patch": "@@ -40,7 +40,7 @@ public void test_LS_JAVA_OPTS_isUsedWhenNoJvmOptionsIsAvailable() throws IOExcep\n \n         // Verify\n         final String output = outputStreamCaptor.toString();\n-        assertEquals(\"Output MUST contains the options present in LS_JAVA_OPTS\", \"-Xblabla\\n\", output);\n+        assertEquals(\"Output MUST contains the options present in LS_JAVA_OPTS\", \"-Xblabla\" + System.lineSeparator(), output);\n     }\n \n     @SuppressWarnings({ \"unchecked\" })\n@@ -54,7 +54,7 @@ public static void updateEnv(String name, String val) throws ReflectiveOperation\n \n     @Test\n     public void testParseCommentLine() throws IOException {\n-        final BufferedReader options = asReader(\"# this is a comment\\n-XX:+UseConcMarkSweepGC\");\n+        final BufferedReader options = asReader(\"# this is a comment\" + System.lineSeparator() + \"-XX:+UseConcMarkSweepGC\");\n         final JvmOptionsParser.ParseResult res = JvmOptionsParser.parse(11, options);\n \n         assertTrue(\"no invalid lines can be present\", res.getInvalidLines().isEmpty());\n@@ -91,19 +91,19 @@ public void testParseOptionVersionRange() throws IOException {\n \n     @Test\n     public void testErrorLinesAreReportedCorrectly() throws IOException {\n-        final String jvmOptionsContent = \"10-11:-XX:+UseConcMarkSweepGC\\n\" +\n-                \"invalidOption\\n\" +\n-                \"-Duser.country=US\\n\" +\n+        final String jvmOptionsContent = \"10-11:-XX:+UseConcMarkSweepGC\" + System.lineSeparator() +\n+                \"invalidOption\" + System.lineSeparator() +\n+                \"-Duser.country=US\" + System.lineSeparator() +\n                 \"anotherInvalidOption\";\n         JvmOptionsParser.ParseResult res = JvmOptionsParser.parse(11, asReader(jvmOptionsContent));\n-        verifyOptions(\"Option must be present for Java 11\", \"-XX:+UseConcMarkSweepGC\\n-Duser.country=US\", res);\n+        verifyOptions(\"Option must be present for Java 11\", \"-XX:+UseConcMarkSweepGC\" + System.lineSeparator() + \"-Duser.country=US\", res);\n \n         assertEquals(\"invalidOption\", res.getInvalidLines().get(2));\n         assertEquals(\"anotherInvalidOption\", res.getInvalidLines().get(4));\n     }\n \n     private void verifyOptions(String message, String expected, JvmOptionsParser.ParseResult res) {\n-        assertEquals(message, expected, String.join(\"\\n\", res.getJvmOptions()));\n+        assertEquals(message, expected, String.join(System.lineSeparator(), res.getJvmOptions()));\n     }\n \n     private BufferedReader asReader(String s) {",
      "parent_sha": "2a5e54cd218895657a41f04202d815572aaeff8b"
    }
  },
  {
    "oid": "bd3adca7e2d85481a4ef78a86d3882277ef9c6a0",
    "message": "MINOR: Add some context to unclean shutdown exceptions\n\nFixes #9005",
    "date": "2018-01-22T20:20:31Z",
    "url": "https://github.com/elastic/logstash/commit/bd3adca7e2d85481a4ef78a86d3882277ef9c6a0",
    "details": {
      "sha": "847e4f2c2b19e3d2f8605bcc51f7dda7639f76a1",
      "filename": "logstash-core/src/main/java/org/logstash/Logstash.java",
      "status": "modified",
      "additions": 7,
      "deletions": 3,
      "changes": 10,
      "blob_url": "https://github.com/elastic/logstash/blob/bd3adca7e2d85481a4ef78a86d3882277ef9c6a0/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FLogstash.java",
      "raw_url": "https://github.com/elastic/logstash/raw/bd3adca7e2d85481a4ef78a86d3882277ef9c6a0/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FLogstash.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FLogstash.java?ref=bd3adca7e2d85481a4ef78a86d3882277ef9c6a0",
      "patch": "@@ -85,13 +85,13 @@ public void run() {\n                 final IRubyObject status =\n                     rexep.callMethod(ruby.getCurrentContext(), \"status\");\n                 if (status != null && !status.isNil() && RubyNumeric.fix2int(status) != 0) {\n-                    throw new IllegalStateException(ex);\n+                    uncleanShutdown(ex);\n                 }\n             } else {\n-                throw new IllegalStateException(ex);\n+                uncleanShutdown(ex);\n             }\n         } catch (final IOException ex) {\n-            throw new IllegalStateException(ex);\n+            uncleanShutdown(ex);\n         }\n     }\n \n@@ -137,4 +137,8 @@ private static String safePath(final Path home, final String... subs) {\n         }\n         return resolved.toString();\n     }\n+\n+    private static void uncleanShutdown(final Exception ex) {\n+        throw new IllegalStateException(\"Logstash stopped processing because of an error:\", ex);\n+    }\n }",
      "parent_sha": "28a2d1d48d60bc9e7c96f814df08b8afdb5f2d25"
    }
  },
  {
    "oid": "4c96422fff45b047ad093d85f572cfa616e25661",
    "message": "reduce Compiler Cache size to 100\n\nThe reduction from 500 to 100 is based on observations where 06d7f01fd\nreduced the number of generated classes by about an order of magnitude\nespecially on very large pipelines (e.g. from ~600 to ~30).",
    "date": "2020-07-01T08:58:08Z",
    "url": "https://github.com/elastic/logstash/commit/4c96422fff45b047ad093d85f572cfa616e25661",
    "details": {
      "sha": "9de1b8d1882d76bcc079da552ed94e801b3027c5",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/compiler/ComputeStepSyntaxElement.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/4c96422fff45b047ad093d85f572cfa616e25661/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FComputeStepSyntaxElement.java",
      "raw_url": "https://github.com/elastic/logstash/raw/4c96422fff45b047ad093d85f572cfa616e25661/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FComputeStepSyntaxElement.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FComputeStepSyntaxElement.java?ref=4c96422fff45b047ad093d85f572cfa616e25661",
      "patch": "@@ -59,7 +59,7 @@ public final class ComputeStepSyntaxElement<T extends Dataset> {\n      * across pipelines and workers.\n      */\n     private static final ConcurrentHashMap<ComputeStepSyntaxElement<?>, Class<? extends Dataset>> CLASS_CACHE\n-        = new ConcurrentHashMap<>(500);\n+        = new ConcurrentHashMap<>(100);\n \n     private static final AtomicLong DATASET_CLASS_INDEX = new AtomicLong(0);\n ",
      "parent_sha": "afd313b6b581027e6501c4eda517d4f720a83f12"
    }
  },
  {
    "oid": "54de04cb90d5e784016478bc88218fb9e59f4a93",
    "message": "cleanup & DRY Queue open(), addIO() and addPage()",
    "date": "2017-12-01T16:46:38Z",
    "url": "https://github.com/elastic/logstash/commit/54de04cb90d5e784016478bc88218fb9e59f4a93",
    "details": {
      "sha": "b6fb43fc37f66e0765751cd734ce3baeb9ec681f",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java",
      "status": "modified",
      "additions": 57,
      "deletions": 70,
      "changes": 127,
      "blob_url": "https://github.com/elastic/logstash/blob/54de04cb90d5e784016478bc88218fb9e59f4a93/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "raw_url": "https://github.com/elastic/logstash/raw/54de04cb90d5e784016478bc88218fb9e59f4a93/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java?ref=54de04cb90d5e784016478bc88218fb9e59f4a93",
      "patch": "@@ -183,12 +183,24 @@ public void open() throws IOException {\n             for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {\n \n                 // all tail checkpoints in the sequence should exist, if not abort mission with a NoSuchFileException\n-                Checkpoint cp = this.checkpointIO.read(this.checkpointIO.tailFileName(pageNum));\n+                final Checkpoint cp = this.checkpointIO.read(this.checkpointIO.tailFileName(pageNum));\n \n                 logger.debug(\"opening tail page: {}, in: {}, with checkpoint: {}\", pageNum, this.dirPath, cp.toString());\n \n                 PageIO pageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);\n-                addIO(cp, pageIO);\n+                // important to NOT pageIO.open() just yet, we must first verify if it is fully acked in which case\n+                // we can purge it and we don't care about its integrity for example if it is of zero-byte file size.\n+                if (cp.isFullyAcked()) {\n+                    purgeTailPage(cp, pageIO);\n+                } else {\n+                    pageIO.open(cp.getMinSeqNum(), cp.getElementCount());\n+                    addTailPage(cp, PageFactory.newTailPage(cp, this, pageIO));\n+                }\n+\n+                // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum\n+                if (cp.maxSeqNum() > this.seqNum) {\n+                    this.seqNum = cp.maxSeqNum();\n+                }\n             }\n \n             // transform the head page into a tail page only if the headpage is non-empty\n@@ -222,17 +234,23 @@ public void open() throws IOException {\n                 // but checkpoint it to update the firstUnackedPageNum if it changed\n                 this.headPage.checkpoint();\n             } else {\n-                // head page is non-empty, transform it into a tail page and create a new empty head page\n+                // head page is non-empty, transform it into a tail page\n                 this.headPage.behead();\n-                addPage(headCheckpoint, this.headPage);\n \n-                headPageNum = headCheckpoint.getPageNum() + 1;\n-                newCheckpointedHeadpage(headPageNum);\n+                if (headCheckpoint.isFullyAcked()) {\n+                    purgeTailPage(headCheckpoint, pageIO);\n+                } else {\n+                    addTailPage(headCheckpoint, this.headPage);\n+                }\n \n                 // track the seqNum as we add this new tail page, prevent empty tailPage with a minSeqNum of 0 to reset seqNum\n                 if (headCheckpoint.maxSeqNum() > this.seqNum) {\n                     this.seqNum = headCheckpoint.maxSeqNum();\n                 }\n+\n+                // create a new empty head page\n+                headPageNum = headCheckpoint.getPageNum() + 1;\n+                newCheckpointedHeadpage(headPageNum);\n             }\n \n             // only activate the first tail page\n@@ -250,75 +268,44 @@ public void open() throws IOException {\n         }\n     }\n \n-    // TODO: addIO and addPage are almost identical - we should refactor to DRY it up.\n-\n-    // addIO is basically the same as addPage except that it avoid calling PageIO.open\n-    // before actually purging the page if it is fully acked. This avoid dealing with\n-    // zero byte page files that are fully acked.\n-    // see issue #7809\n-    private void addIO(Checkpoint checkpoint, PageIO pageIO) throws IOException {\n-        if (checkpoint.isFullyAcked()) {\n-            // first make sure any fully acked page per the checkpoint is purged if not already\n-            try { pageIO.purge(); } catch (NoSuchFileException e) { /* ignore */ }\n-\n-            // we want to keep all the \"middle\" checkpoints between the first unacked tail page and the head page\n-            // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this\n-            // we will remove any prepended fully acked tail pages but keep all other checkpoints between the first\n-            // unacked tail page and the head page. we did however purge the data file to free disk resources.\n-\n-            if (this.tailPages.size() == 0) {\n-                // this is the first tail page and it is fully acked so just purge it\n-                this.checkpointIO.purge(this.checkpointIO.tailFileName(checkpoint.getPageNum()));\n-            }\n-        } else {\n-            pageIO.open(checkpoint.getMinSeqNum(), checkpoint.getElementCount());\n-            Page page = PageFactory.newTailPage(checkpoint, this, pageIO);\n-\n-            this.tailPages.add(page);\n-            this.unreadTailPages.add(page);\n-            this.unreadCount += page.unreadCount();\n-            this.currentByteSize += page.getPageIO().getCapacity();\n+    /**\n+     * delete files for the given page\n+     *\n+     * @param checkpoint the tail page {@link Checkpoint}\n+     * @param pageIO the tail page {@link PageIO}\n+     * @throws IOException\n+     */\n+    private void purgeTailPage(Checkpoint checkpoint, PageIO pageIO) throws IOException {\n+        try {\n+            pageIO.purge();\n+        } catch (NoSuchFileException e) { /* ignore */ }\n \n-            // for now deactivate all tail pages, we will only reactivate the first one at the end\n-            page.getPageIO().deactivate();\n-        }\n+        // we want to keep all the \"middle\" checkpoints between the first unacked tail page and the head page\n+        // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this\n+        // we will remove any prepended fully acked tail pages but keep all other checkpoints between the first\n+        // unacked tail page and the head page. we did however purge the data file to free disk resources.\n \n-        // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum\n-        if (checkpoint.maxSeqNum() > this.seqNum) {\n-            this.seqNum = checkpoint.maxSeqNum();\n+        if (this.tailPages.size() == 0) {\n+            // this is the first tail page and it is fully acked so just purge it\n+            this.checkpointIO.purge(this.checkpointIO.tailFileName(checkpoint.getPageNum()));\n         }\n     }\n \n-    // add a read tail page into this queue structures but also verify that this tail page\n-    // is not fully acked in which case it will be purged\n-    private void addPage(Checkpoint checkpoint, Page page) throws IOException {\n-        if (checkpoint.isFullyAcked()) {\n-            // first make sure any fully acked page per the checkpoint is purged if not already\n-            try { page.getPageIO().purge(); } catch (NoSuchFileException e) { /* ignore */ }\n-\n-            // we want to keep all the \"middle\" checkpoints between the first unacked tail page and the head page\n-            // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this\n-            // we will remove any prepended fully acked tail pages but keep all other checkpoints between the first\n-            // unacked tail page and the head page. we did however purge the data file to free disk resources.\n-\n-            if (this.tailPages.size() == 0) {\n-                // this is the first tail page and it is fully acked so just purge it\n-                this.checkpointIO.purge(this.checkpointIO.tailFileName(checkpoint.getPageNum()));\n-            }\n-        } else {\n-            this.tailPages.add(page);\n-            this.unreadTailPages.add(page);\n-            this.unreadCount += page.unreadCount();\n-            this.currentByteSize += page.getPageIO().getCapacity();\n-\n-            // for now deactivate all tail pages, we will only reactivate the first one at the end\n-            page.getPageIO().deactivate();\n-        }\n-\n-        // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum\n-        if (checkpoint.maxSeqNum() > this.seqNum) {\n-            this.seqNum = checkpoint.maxSeqNum();\n-        }\n+    /**\n+     * add a not fully-acked tail page into this queue structures and un-mmap it.\n+     *\n+     * @param checkpoint the tail page {@link Checkpoint}\n+     * @param page the tail {@link Page}\n+     * @throws IOException\n+     */\n+    private void addTailPage(Checkpoint checkpoint, Page page) throws IOException {\n+        this.tailPages.add(page);\n+        this.unreadTailPages.add(page);\n+        this.unreadCount += page.unreadCount();\n+        this.currentByteSize += page.getPageIO().getCapacity();\n+\n+        // for now deactivate all tail pages, we will only reactivate the first one at the end\n+        page.getPageIO().deactivate();\n     }\n \n     // create a new empty headpage for the given pageNum and immediately checkpoint it",
      "parent_sha": "9a31b7ded1cb0b9acf8b9a8d3c2831b75d5acfd8"
    }
  },
  {
    "oid": "c7028199fd5658018a29275298b92a6bdc1c7b27",
    "message": "MINOR: Save a few null checks in Valuefier\n\nFixes #7938",
    "date": "2017-08-08T16:27:53Z",
    "url": "https://github.com/elastic/logstash/commit/c7028199fd5658018a29275298b92a6bdc1c7b27",
    "details": {
      "sha": "fdaafb44dea534077a2caabe8412dde71bffb52e",
      "filename": "logstash-core/src/main/java/org/logstash/Valuefier.java",
      "status": "modified",
      "additions": 9,
      "deletions": 11,
      "changes": 20,
      "blob_url": "https://github.com/elastic/logstash/blob/c7028199fd5658018a29275298b92a6bdc1c7b27/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FValuefier.java",
      "raw_url": "https://github.com/elastic/logstash/raw/c7028199fd5658018a29275298b92a6bdc1c7b27/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FValuefier.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FValuefier.java?ref=c7028199fd5658018a29275298b92a6bdc1c7b27",
      "patch": "@@ -1,6 +1,7 @@\n package org.logstash;\n \n import java.io.Serializable;\n+import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n import org.joda.time.DateTime;\n@@ -19,22 +20,19 @@\n import org.logstash.bivalues.BiValues;\n import org.logstash.ext.JrubyTimestampExtLibrary;\n \n-public class Valuefier {\n+public final class Valuefier {\n     private static final String PROXY_ERR_TEMPLATE = \"Missing Valuefier handling for full class name=%s, simple name=%s, wrapped object=%s\";\n     private static final String ERR_TEMPLATE = \"Missing Valuefier handling for full class name=%s, simple name=%s\";\n \n     private Valuefier(){}\n \n-    private static Object convertJavaProxy(JavaProxy jp) {\n-        if(jp == null) {\n-            return BiValues.NULL_BI_VALUE;\n-        }\n-        Object obj = JavaUtil.unwrapJavaObject(jp);\n+    private static Object convertJavaProxy(final JavaProxy jp) {\n+        final Object obj = JavaUtil.unwrapJavaObject(jp);\n         if (obj instanceof IRubyObject[]) {\n             return ConvertedList.newFromRubyArray((IRubyObject[]) obj);\n         }\n         if (obj instanceof List) {\n-            return ConvertedList.newFromList((List<Object>) obj);\n+            return ConvertedList.newFromList((Collection<?>) obj);\n         }\n         try {\n             return BiValues.newBiValue(jp);\n@@ -44,16 +42,16 @@ private static Object convertJavaProxy(JavaProxy jp) {\n         }\n     }\n \n-    public static Object convertNonCollection(Object o) {\n+    private static Object convertNonCollection(Object o) {\n         try {\n-            return o == null ? BiValues.NULL_BI_VALUE : BiValues.newBiValue(o);\n+            return BiValues.newBiValue(o);\n         } catch (IllegalArgumentException e) {\n             final Class<?> cls = o.getClass();\n             throw new IllegalArgumentException(String.format(ERR_TEMPLATE, cls.getName(), cls.getSimpleName()), e);\n         }\n     }\n \n-    public static Object convert(Object o) throws IllegalArgumentException {\n+    public static Object convert(Object o) {\n         if (o instanceof RubyString) {\n             return o;\n         }\n@@ -100,6 +98,6 @@ public static Object convert(Object o) throws IllegalArgumentException {\n             Timestamp ts = new Timestamp((DateTime) o);\n             return convertNonCollection(ts);\n         }\n-        return convertNonCollection(o);\n+        return o == null ? BiValues.NULL_BI_VALUE : convertNonCollection(o);\n     }\n }",
      "parent_sha": "ec8015225be4ba1b8a248bc9141e62429a02310c"
    }
  },
  {
    "oid": "0680c9ec09ea014f78e889cd6f564d67d51f94ba",
    "message": "#9255 Drastically speed up pipeline compilation by making Vertex compilation more efficient\n\nFixes #9278",
    "date": "2018-03-22T20:36:29Z",
    "url": "https://github.com/elastic/logstash/commit/0680c9ec09ea014f78e889cd6f564d67d51f94ba",
    "details": {
      "sha": "4222a0c80fbcc318c01d1d5039b8dcc4b5d02e86",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/CompiledPipeline.java",
      "status": "modified",
      "additions": 16,
      "deletions": 17,
      "changes": 33,
      "blob_url": "https://github.com/elastic/logstash/blob/0680c9ec09ea014f78e889cd6f564d67d51f94ba/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FCompiledPipeline.java",
      "raw_url": "https://github.com/elastic/logstash/raw/0680c9ec09ea014f78e889cd6f564d67d51f94ba/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FCompiledPipeline.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FCompiledPipeline.java?ref=0680c9ec09ea014f78e889cd6f564d67d51f94ba",
      "patch": "@@ -7,6 +7,7 @@\n import java.util.Map;\n import java.util.Objects;\n import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.jruby.RubyHash;\n@@ -247,14 +248,14 @@ private Dataset compile() {\n          * Build a {@link Dataset} representing the {@link JrubyEventExtLibrary.RubyEvent}s after\n          * the application of the given filter.\n          * @param vertex Vertex of the filter to create this {@link Dataset} for\n-         * @param datasets All the datasets that pass through this filter\n+         * @param datasets All the datasets that have children passing into this filter\n          * @return Filter {@link Dataset}\n          */\n         private Dataset filterDataset(final Vertex vertex, final Collection<Dataset> datasets) {\n             return plugins.computeIfAbsent(\n                 vertex.getId(), v -> {\n                     final ComputeStepSyntaxElement<Dataset> prepared =\n-                        DatasetCompiler.filterDataset(datasets, filters.get(v));\n+                        DatasetCompiler.filterDataset(flatten(datasets, vertex), filters.get(v));\n                     LOGGER.debug(\"Compiled filter\\n {} \\n into \\n {}\", vertex, prepared);\n                     return prepared.instantiate();\n                 }\n@@ -265,15 +266,15 @@ private Dataset filterDataset(final Vertex vertex, final Collection<Dataset> dat\n          * Build a {@link Dataset} representing the {@link JrubyEventExtLibrary.RubyEvent}s after\n          * the application of the given output.\n          * @param vertex Vertex of the output to create this {@link Dataset} for\n-         * @param datasets All the datasets that are passed into this output\n+         * @param datasets All the datasets that have children passing into this output\n          * @return Output {@link Dataset}\n          */\n         private Dataset outputDataset(final Vertex vertex, final Collection<Dataset> datasets) {\n             return plugins.computeIfAbsent(\n                 vertex.getId(), v -> {\n                     final ComputeStepSyntaxElement<Dataset> prepared =\n                         DatasetCompiler.outputDataset(\n-                            datasets, outputs.get(v), outputs.size() == 1\n+                            flatten(datasets, vertex), outputs.get(v), outputs.size() == 1\n                         );\n                     LOGGER.debug(\"Compiled output\\n {} \\n into \\n {}\", vertex, prepared);\n                     return prepared.instantiate();\n@@ -284,7 +285,7 @@ private Dataset outputDataset(final Vertex vertex, final Collection<Dataset> dat\n         /**\n          * Split the given {@link Dataset}s and return the dataset half of their elements that contains\n          * the {@link JrubyEventExtLibrary.RubyEvent} that fulfil the given {@link EventCondition}.\n-         * @param datasets Datasets to split\n+         * @param datasets Datasets that are the parents of the datasets to split\n          * @param condition Condition that must be fulfilled\n          * @param vertex Vertex id to cache the resulting {@link Dataset} under\n          * @return The half of the datasets contents that fulfils the condition\n@@ -294,7 +295,7 @@ private SplitDataset split(final Collection<Dataset> datasets,\n             return iffs.computeIfAbsent(\n                 vertex.getId(), v -> {\n                     final ComputeStepSyntaxElement<SplitDataset> prepared =\n-                        DatasetCompiler.splitDataset(datasets, condition);\n+                        DatasetCompiler.splitDataset(flatten(datasets, vertex), condition);\n                     LOGGER.debug(\n                         \"Compiled conditional\\n {} \\n into \\n {}\", vertex, prepared\n                     );\n@@ -314,11 +315,10 @@ private SplitDataset split(final Collection<Dataset> datasets,\n          */\n         private Collection<Dataset> flatten(final Collection<Dataset> datasets,\n             final Vertex start) {\n-            final Collection<Vertex> dependencies = start.incomingVertices()\n-                .filter(v -> isFilter(v) || isOutput(v) || v instanceof IfVertex)\n-                .collect(Collectors.toList());\n-            return dependencies.isEmpty() ? datasets\n-                : compileDependencies(start, datasets, dependencies);\n+            final Collection<Dataset> result = compileDependencies(start, datasets,\n+                start.incomingVertices().filter(v -> isFilter(v) || isOutput(v) || v instanceof IfVertex)\n+            );\n+            return result.isEmpty() ? datasets : result;\n         }\n \n         /**\n@@ -329,20 +329,19 @@ private Collection<Dataset> flatten(final Collection<Dataset> datasets,\n          * @return Datasets compiled from vertex children\n          */\n         private Collection<Dataset> compileDependencies(final Vertex start,\n-            final Collection<Dataset> datasets, final Collection<Vertex> dependencies) {\n-            return dependencies.stream().map(\n+            final Collection<Dataset> datasets, final Stream<Vertex> dependencies) {\n+            return dependencies.map(\n                 dependency -> {\n-                    final Collection<Dataset> transientDependencies = flatten(datasets, dependency);\n                     if (isFilter(dependency)) {\n-                        return filterDataset(dependency, transientDependencies);\n+                        return filterDataset(dependency, datasets);\n                     } else if (isOutput(dependency)) {\n-                        return outputDataset(dependency, transientDependencies);\n+                        return outputDataset(dependency, datasets);\n                     } else {\n                         // We know that it's an if vertex since the the input children are either\n                         // output, filter or if in type.\n                         final IfVertex ifvert = (IfVertex) dependency;\n                         final SplitDataset ifDataset = split(\n-                            transientDependencies,\n+                            datasets,\n                             EventCondition.Compiler.buildCondition(ifvert.getBooleanExpression()),\n                             dependency\n                         );",
      "parent_sha": "32931b8c11d8c4f03befba0450ff45b0a09c130d"
    }
  },
  {
    "oid": "9e886a9682dafe4ef49a325436dedea904eca8ac",
    "message": "tests: support more-precise clocks in Java 11 (#10381)",
    "date": "2019-02-05T16:08:49Z",
    "url": "https://github.com/elastic/logstash/commit/9e886a9682dafe4ef49a325436dedea904eca8ac",
    "details": {
      "sha": "58959ab7fe13e2c91d022a67f2b103d68d5f7a35",
      "filename": "logstash-core/src/test/java/org/logstash/TimestampTest.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/elastic/logstash/blob/9e886a9682dafe4ef49a325436dedea904eca8ac/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2FTimestampTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/9e886a9682dafe4ef49a325436dedea904eca8ac/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2FTimestampTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2FTimestampTest.java?ref=9e886a9682dafe4ef49a325436dedea904eca8ac",
      "patch": "@@ -49,7 +49,9 @@ public void testMicroseconds() {\n         Instant i = Instant.now();\n         Timestamp t1 = new Timestamp(i.toEpochMilli());\n         long usec = t1.usec();\n-        Assert.assertEquals(i.getNano() / 1000, usec);\n+\n+        // since our Timestamp was created with epoch millis, it cannot be more precise.\n+        Assert.assertEquals(i.getNano() / 1_000_000, usec / 1_000);\n     }\n \n     @Test",
      "parent_sha": "583ec6b62574878c6ed6669271304e1f06ffa8e4"
    }
  },
  {
    "oid": "68b22280bc11845383f53f050a2cbcdd776884cc",
    "message": "#7549 fix unsafe concurreny in PQ count getters\n\nFixes #8336",
    "date": "2017-09-21T10:52:56Z",
    "url": "https://github.com/elastic/logstash/commit/68b22280bc11845383f53f050a2cbcdd776884cc",
    "details": {
      "sha": "0e63024490729472983e442c7433303609ea0409",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java",
      "status": "modified",
      "additions": 17,
      "deletions": 7,
      "changes": 24,
      "blob_url": "https://github.com/elastic/logstash/blob/68b22280bc11845383f53f050a2cbcdd776884cc/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "raw_url": "https://github.com/elastic/logstash/raw/68b22280bc11845383f53f050a2cbcdd776884cc/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java?ref=68b22280bc11845383f53f050a2cbcdd776884cc",
      "patch": "@@ -768,16 +768,26 @@ protected int firstUnackedPageNum() {\n     }\n \n     public long getAckedCount() {\n-        return headPage.ackedSeqNums.cardinality() + tailPages.stream()\n-                .mapToLong(page -> page.ackedSeqNums.cardinality())\n-                .sum();\n+        lock.lock();\n+        try {\n+            return headPage.ackedSeqNums.cardinality() + tailPages.stream()\n+                .mapToLong(page -> page.ackedSeqNums.cardinality()).sum();\n+        } finally {\n+            lock.unlock();\n+        }\n     }\n \n     public long getUnackedCount() {\n-        long headPageCount = (headPage.getElementCount() - headPage.ackedSeqNums.cardinality());\n-        long tailPagesCount = tailPages.stream()\n-                .mapToLong(page -> (page.getElementCount() - page.ackedSeqNums.cardinality())).sum();\n-        return headPageCount + tailPagesCount;\n+        lock.lock();\n+        try {\n+            long headPageCount = (headPage.getElementCount() - headPage.ackedSeqNums.cardinality());\n+            long tailPagesCount = tailPages.stream()\n+                .mapToLong(page -> (page.getElementCount() - page.ackedSeqNums.cardinality()))\n+                .sum();\n+            return headPageCount + tailPagesCount;\n+        } finally {\n+            lock.unlock();\n+        }\n     }\n \n     protected long nextSeqNum() {",
      "parent_sha": "6962fc2a06ce8577197babe31bb50dc0552c7392"
    }
  },
  {
    "oid": "b250b145a73be392e494e3f0d684c1f4f26a95b1",
    "message": "MINOR: Fix compiler warnings in Ingest Converter Test\n\nFixes #7703",
    "date": "2017-07-17T09:42:32Z",
    "url": "https://github.com/elastic/logstash/commit/b250b145a73be392e494e3f0d684c1f4f26a95b1",
    "details": {
      "sha": "3117df7d4d0f17e6122a9bed1a57f1832fe207eb",
      "filename": "tools/ingest-converter/src/test/java/org/logstash/ingest/IngestTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/b250b145a73be392e494e3f0d684c1f4f26a95b1/tools%2Fingest-converter%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fingest%2FIngestTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/b250b145a73be392e494e3f0d684c1f4f26a95b1/tools%2Fingest-converter%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fingest%2FIngestTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/tools%2Fingest-converter%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fingest%2FIngestTest.java?ref=b250b145a73be392e494e3f0d684c1f4f26a95b1",
      "patch": "@@ -39,7 +39,7 @@ public abstract class IngestTest {\n     @Parameter\n     public String testCase;\n \n-    protected final void assertCorrectConversion(final Class clazz) throws Exception {\n+    protected final void assertCorrectConversion(final Class<?> clazz) throws Exception {\n         final URL append = getResultPath(temp);\n         clazz.getMethod(\"main\", String[].class).invoke(\n             null,",
      "parent_sha": "cc81dab8c32ad1046f640d619a638a244b9936aa"
    }
  },
  {
    "oid": "a15116c96cb263fc42b14e522a17356adc2937f6",
    "message": "#7712 & #7945 BiValue Improvements\n\n* Turn Enum into HashMap for direct lookup\n* Use stateless Lambdas instead of named classes as converter functions for better performance\n* Remove redundant instanceof checks by splitting existing converter functions between Java and Ruby version\n* Use most restrictive instanceof check possible for Ruby converters for performance improvements\n\nFixes #8087",
    "date": "2017-08-29T19:21:43Z",
    "url": "https://github.com/elastic/logstash/commit/a15116c96cb263fc42b14e522a17356adc2937f6",
    "details": {
      "sha": "8a7d9c4aa977b448387f258df373082439ba77d9",
      "filename": "logstash-core/src/main/java/org/logstash/bivalues/BiValues.java",
      "status": "modified",
      "additions": 33,
      "deletions": 110,
      "changes": 143,
      "blob_url": "https://github.com/elastic/logstash/blob/a15116c96cb263fc42b14e522a17356adc2937f6/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fbivalues%2FBiValues.java",
      "raw_url": "https://github.com/elastic/logstash/raw/a15116c96cb263fc42b14e522a17356adc2937f6/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fbivalues%2FBiValues.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fbivalues%2FBiValues.java?ref=a15116c96cb263fc42b14e522a17356adc2937f6",
      "patch": "@@ -3,135 +3,58 @@\n import java.math.BigDecimal;\n import java.math.BigInteger;\n import java.util.HashMap;\n+import java.util.Map;\n import org.jruby.RubyBignum;\n+import org.jruby.RubyFixnum;\n import org.jruby.RubyInteger;\n+import org.jruby.RubyNil;\n import org.jruby.RubySymbol;\n import org.jruby.ext.bigdecimal.RubyBigDecimal;\n+import org.jruby.java.proxies.ConcreteJavaProxy;\n import org.jruby.java.proxies.JavaProxy;\n-import org.jruby.runtime.builtin.IRubyObject;\n \n-public enum BiValues {\n-    JAVA_LANG_INTEGER(BiValueType.INT),\n-    JAVA_LANG_LONG(BiValueType.LONG),\n-    JAVA_MATH_BIGDECIMAL(BiValueType.DECIMAL),\n-    JAVA_MATH_BIGINTEGER(BiValueType.BIGINT),\n-    ORG_JRUBY_EXT_BIGDECIMAL_RUBYBIGDECIMAL(BiValueType.DECIMAL),\n-    ORG_JRUBY_JAVA_PROXIES_CONCRETEJAVAPROXY(BiValueType.JAVAPROXY),\n-    ORG_JRUBY_RUBYBIGNUM(BiValueType.BIGINT),\n-    ORG_JRUBY_RUBYFIXNUM(BiValueType.LONG),\n-    ORG_JRUBY_RUBYINTEGER(BiValueType.LONG),\n-    ORG_JRUBY_RUBYNIL(BiValueType.NULL),\n-    ORG_JRUBY_RUBYSYMBOL(BiValueType.SYMBOL), // one way conversion, a Java string will use STRING\n-    NULL(BiValueType.NULL);\n-\n-    private static HashMap<String, String> initCache() {\n-        HashMap<String, String> hm = new HashMap<>();\n-        hm.put(\"java.lang.Integer\", \"JAVA_LANG_INTEGER\");\n-        hm.put(\"java.lang.Long\", \"JAVA_LANG_LONG\");\n-        hm.put(\"java.math.BigDecimal\", \"JAVA_MATH_BIGDECIMAL\");\n-        hm.put(\"java.math.BigInteger\", \"JAVA_MATH_BIGINTEGER\");\n-        hm.put(\"org.jruby.RubyBignum\", \"ORG_JRUBY_RUBYBIGNUM\");\n-        hm.put(\"org.jruby.RubyFixnum\", \"ORG_JRUBY_RUBYFIXNUM\");\n-        hm.put(\"org.jruby.RubyInteger\", \"ORG_JRUBY_RUBYINTEGER\");\n-        hm.put(\"org.jruby.RubyNil\", \"ORG_JRUBY_RUBYNIL\");\n-        hm.put(\"org.jruby.RubySymbol\", \"ORG_JRUBY_RUBYSYMBOL\");\n-        hm.put(\"org.jruby.ext.bigdecimal.RubyBigDecimal\", \"ORG_JRUBY_EXT_BIGDECIMAL_RUBYBIGDECIMAL\");\n-        hm.put(\"org.jruby.java.proxies.ConcreteJavaProxy\", \"ORG_JRUBY_JAVA_PROXIES_CONCRETEJAVAPROXY\");\n-        return hm;\n+public final class BiValues {\n+    private BiValues() {\n     }\n \n     public static final NullBiValue NULL_BI_VALUE = NullBiValue.newNullBiValue();\n \n-    private final BiValueType biValueType;\n-\n-    BiValues(BiValueType biValueType) {\n-        this.biValueType = biValueType;\n-    }\n-\n-    private static final HashMap<String, String> NAME_CACHE = initCache();\n-\n-    private BiValue build(Object value) {\n-        return biValueType.build(value);\n-    }\n+    private static final Map<Class<?>, BiValues.BiValueType> CONVERTER_CACHE = initCache();\n \n     public static BiValue newBiValue(Object o) {\n         if (o == null) {\n             return NULL_BI_VALUE;\n         }\n-        return valueOf(fetchName(o)).build(o);\n-    }\n-\n-    private static String fetchName(Object o) {\n-        final String cls = o.getClass().getName();\n-        final String name = NAME_CACHE.get(cls);\n-        if (name != null) {\n-            return name;\n+        final Class<?> cls = o.getClass();\n+        final BiValues.BiValueType type = CONVERTER_CACHE.get(cls);\n+        if (type == null) {\n+            throw new IllegalArgumentException(\"Unsupported class \" + cls);\n         }\n-        return cacheName(cls);\n+        return type.build(o);\n     }\n-    \n-    private static String cacheName(final String cls) {\n-        final String toCache = cls.toUpperCase().replace('.', '_');\n-        // TODO[Guy] log warn that we are seeing a uncached value\n-        NAME_CACHE.put(cls, toCache);\n-        return toCache;\n+\n+    private interface BiValueType {\n+        BiValue build(Object value);\n     }\n \n-    private enum BiValueType {\n-        SYMBOL {\n-            BiValue build(Object value) {\n-                if (value instanceof IRubyObject) {\n-                    return new SymbolBiValue((RubySymbol) value);\n-                }\n-                return new SymbolBiValue((String) value);\n-            }\n-        },\n-        LONG {\n-            BiValue build(Object value) {\n-                if (value instanceof IRubyObject) {\n-                    return new LongBiValue((RubyInteger) value);\n-                }\n-                return new LongBiValue((Long) value);\n-            }\n-        },\n-        INT {\n-            BiValue build(Object value) {\n-                if (value instanceof IRubyObject) {\n-                    return new IntegerBiValue((RubyInteger) value);\n-                }\n-                return new IntegerBiValue((Integer) value);\n-            }\n-        },\n-        DECIMAL {\n-            BiValue build(Object value) {\n-                if (value instanceof IRubyObject) {\n-                    return new BigDecimalBiValue((RubyBigDecimal) value);\n-                }\n-                return new BigDecimalBiValue((BigDecimal) value);\n-            }\n-        },\n-        NULL {\n-            NullBiValue build(Object value) {\n-                return NULL_BI_VALUE;\n+    private static Map<Class<?>, BiValues.BiValueType> initCache() {\n+        final Map<Class<?>, BiValues.BiValueType> hm = new HashMap<>(50, 0.2F);\n+        hm.put(Integer.class, value -> new IntegerBiValue((Integer) value));\n+        hm.put(Long.class, value -> new LongBiValue((Long) value));\n+        hm.put(BigDecimal.class, value -> new BigDecimalBiValue((BigDecimal) value));\n+        hm.put(BigInteger.class, value -> new BigIntegerBiValue((BigInteger) value));\n+        hm.put(RubyBignum.class, value -> new BigIntegerBiValue((RubyBignum) value));\n+        hm.put(RubyFixnum.class, value -> new LongBiValue((RubyInteger) value));\n+        hm.put(RubyInteger.class, value -> new IntegerBiValue((RubyInteger) value));\n+        hm.put(RubyNil.class, value -> NULL_BI_VALUE);\n+        hm.put(RubySymbol.class, value -> new SymbolBiValue((RubySymbol) value));\n+        hm.put(RubyBigDecimal.class, value -> new BigDecimalBiValue((RubyBigDecimal) value));\n+        hm.put(ConcreteJavaProxy.class, value -> {\n+            if (value instanceof JavaProxy) {\n+                return new JavaProxyBiValue((JavaProxy) value);\n             }\n-        },\n-        BIGINT {\n-            BiValue build(Object value) {\n-                if (value instanceof IRubyObject) {\n-                    return new BigIntegerBiValue((RubyBignum) value);\n-                }\n-                return new BigIntegerBiValue((BigInteger) value);\n-            }\n-        },\n-        JAVAPROXY {\n-            BiValue build(Object value) {\n-                if (value instanceof IRubyObject) {\n-                    return new JavaProxyBiValue((JavaProxy) value);\n-                }\n-                return new JavaProxyBiValue(value);\n-            }\n-        };\n-        abstract BiValue build(Object value);\n+            return new JavaProxyBiValue(value);\n+        });\n+        return hm;\n     }\n-\n }",
      "parent_sha": "69fc14f76504bd024ff198d8581b576d03430eba"
    }
  },
  {
    "oid": "62273f7f564587f2a98dc03862858c6e901d4b77",
    "message": "increase timeout for long-running PQ tests\n\nFixes #9926",
    "date": "2018-08-21T14:17:58Z",
    "url": "https://github.com/elastic/logstash/commit/62273f7f564587f2a98dc03862858c6e901d4b77",
    "details": {
      "sha": "f69c1c0c755458f678d1fa44204798489dbe2097",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 4,
      "changes": 6,
      "blob_url": "https://github.com/elastic/logstash/blob/62273f7f564587f2a98dc03862858c6e901d4b77/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/62273f7f564587f2a98dc03862858c6e901d4b77/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=62273f7f564587f2a98dc03862858c6e901d4b77",
      "patch": "@@ -397,8 +397,7 @@ public void randomAcking() throws IOException {\n         }\n     }\n \n-    @Ignore(\"This test timed out on Linux. Issue: https://github.com/elastic/logstash/issues/9910\")\n-    @Test(timeout = 50_000)\n+    @Test(timeout = 300_000)\n     public void reachMaxUnread() throws IOException, InterruptedException, ExecutionException {\n         Queueable element = new StringElement(\"foobarbaz\");\n         int singleElementCapacity = computeCapacityForMmapPageIO(element);\n@@ -687,8 +686,7 @@ public void testAckedCount() throws IOException {\n         }\n     }\n \n-    @Ignore(\"This test frequently times out on Windows and Linux. Issue: https://github.com/elastic/logstash/issues/9878\")\n-    @Test(timeout = 50_000)\n+    @Test(timeout = 300_000)\n     public void concurrentWritesTest() throws IOException, InterruptedException, ExecutionException {\n \n         final int WRITER_COUNT = 5;",
      "parent_sha": "b6e355d151b252097df0dd720f4175c8486a55d0"
    }
  },
  {
    "oid": "bb7ecc203f698a56f341fa538bdc1cd4da15b28c",
    "message": "Fix ConditionalEvaluationError to do not include the event that errored in its serialiaxed form, because it's not expected that this class is ever serialized. (#16429)\n\nMake inner field of ConditionalEvaluationError transient to be avoided during serialization.",
    "date": "2024-09-06T10:09:58Z",
    "url": "https://github.com/elastic/logstash/commit/bb7ecc203f698a56f341fa538bdc1cd4da15b28c",
    "details": {
      "sha": "0dd464fc2551c1a5184ddc8048cddd11282dea79",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/compiler/ConditionalEvaluationError.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/elastic/logstash/blob/bb7ecc203f698a56f341fa538bdc1cd4da15b28c/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FConditionalEvaluationError.java",
      "raw_url": "https://github.com/elastic/logstash/raw/bb7ecc203f698a56f341fa538bdc1cd4da15b28c/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FConditionalEvaluationError.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FConditionalEvaluationError.java?ref=bb7ecc203f698a56f341fa538bdc1cd4da15b28c",
      "patch": "@@ -7,7 +7,10 @@\n  * */\n public class ConditionalEvaluationError extends RuntimeException {\n     private static final long serialVersionUID = -8633589068902565868L;\n-    private final Event failedEvent;\n+\n+    // This class is serializable because of inheritance from Throwable, however it's not expected\n+    // to be ever transmitted on wire on stored in some binary storage.\n+    private final transient Event failedEvent;\n \n     ConditionalEvaluationError(Throwable cause, Event failedEvent) {\n         super(cause);",
      "parent_sha": "58b6a0ac77432df1d3cc0ab4bcd0f29a54f5adbf"
    }
  },
  {
    "oid": "58f7fa66e03761dd17d700a30d581b4c8e496ea4",
    "message": "MINOR: Factor out cold-path from org.logstash.Event#initTimestamp\n\nFixes #7709",
    "date": "2017-07-17T09:31:09Z",
    "url": "https://github.com/elastic/logstash/commit/58f7fa66e03761dd17d700a30d581b4c8e496ea4",
    "details": {
      "sha": "4f2bf87ca2f906b26c5280a76893b9b237289317",
      "filename": "logstash-core/src/main/java/org/logstash/Event.java",
      "status": "modified",
      "additions": 15,
      "deletions": 5,
      "changes": 20,
      "blob_url": "https://github.com/elastic/logstash/blob/58f7fa66e03761dd17d700a30d581b4c8e496ea4/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FEvent.java",
      "raw_url": "https://github.com/elastic/logstash/raw/58f7fa66e03761dd17d700a30d581b4c8e496ea4/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FEvent.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FEvent.java?ref=58f7fa66e03761dd17d700a30d581b4c8e496ea4",
      "patch": "@@ -290,11 +290,22 @@ public String toString() {\n     }\n \n     private static Timestamp initTimestamp(Object o) {\n+        if (o == null || o instanceof NullBiValue) {\n+            // most frequent\n+            return new Timestamp();\n+        } else {\n+            return parseTimestamp(o);\n+        }\n+    }\n+\n+    /**\n+     * Cold path of {@link Event#initTimestamp(Object)}.\n+     * @param o Object to parse Timestamp out of\n+     * @return Parsed {@link Timestamp} or {@code null} on failure\n+     */\n+    private static Timestamp parseTimestamp(final Object o) {\n         try {\n-            if (o == null || o instanceof NullBiValue) {\n-                // most frequent\n-                return new Timestamp();\n-            } else if (o instanceof String) {\n+            if (o instanceof String) {\n                 // second most frequent\n                 return new Timestamp((String) o);\n             } else if (o instanceof StringBiValue) {\n@@ -319,7 +330,6 @@ private static Timestamp initTimestamp(Object o) {\n         } catch (IllegalArgumentException e) {\n             logger.warn(\"Error parsing \" + TIMESTAMP + \" string value=\" + o.toString());\n         }\n-\n         return null;\n     }\n ",
      "parent_sha": "4c4668baff5770219b10b02858b16af68515de4e"
    }
  },
  {
    "oid": "9720dfba7341b5fa9fa7dcb21a209eab95479aae",
    "message": "Shorten Vertex generated IDs\n\nCurrently generated Vertex IDs are unwieldy concatenations of two sha256 hashes. This is just a pain to deal with.\n\nThis patch hashes that concatenation instead, yielding something far easier to deal with.\n\nFixes #7931",
    "date": "2017-08-08T15:34:41Z",
    "url": "https://github.com/elastic/logstash/commit/9720dfba7341b5fa9fa7dcb21a209eab95479aae",
    "details": {
      "sha": "c04d1435261a978c34c262711575e340d9cd0024",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/graph/Vertex.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/9720dfba7341b5fa9fa7dcb21a209eab95479aae/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fgraph%2FVertex.java",
      "raw_url": "https://github.com/elastic/logstash/raw/9720dfba7341b5fa9fa7dcb21a209eab95479aae/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fgraph%2FVertex.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fgraph%2FVertex.java?ref=9720dfba7341b5fa9fa7dcb21a209eab95479aae",
      "patch": "@@ -243,7 +243,7 @@ public String getId() {\n         // they have no source metadata. This might also be used in the future by alternate config languages which are\n         // willing to take the hit.\n         if (this.getSourceWithMetadata() != null) {\n-            generatedId = this.getGraph().uniqueHash() + \"|\" + this.getSourceWithMetadata().uniqueHash();\n+            generatedId = Util.digest(this.getGraph().uniqueHash() + \"|\" + this.getSourceWithMetadata().uniqueHash());\n         } else {\n             generatedId = this.uniqueHash();\n         }",
      "parent_sha": "fe7cd1fe0745cfc46a5ceaa67c0ffce370e1c474"
    }
  },
  {
    "oid": "9fe4e9b3b760bca1059116d9e7e8ae8466b15ea9",
    "message": "MINOR: Clean up redundant matchers in QueueTest\n\nFixes #7161",
    "date": "2017-05-19T08:54:42Z",
    "url": "https://github.com/elastic/logstash/commit/9fe4e9b3b760bca1059116d9e7e8ae8466b15ea9",
    "details": {
      "sha": "17f69a98553a0c53fc91252b4d6720b4f47a282f",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 109,
      "deletions": 116,
      "changes": 225,
      "blob_url": "https://github.com/elastic/logstash/blob/9fe4e9b3b760bca1059116d9e7e8ae8466b15ea9/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/9fe4e9b3b760bca1059116d9e7e8ae8466b15ea9/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=9fe4e9b3b760bca1059116d9e7e8ae8466b15ea9",
      "patch": "@@ -23,6 +23,7 @@\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.CoreMatchers.is;\n import static org.hamcrest.CoreMatchers.notNullValue;\n+import static org.hamcrest.CoreMatchers.nullValue;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.junit.Assert.fail;\n \n@@ -41,7 +42,7 @@ public void newQueue() throws IOException {\n         try (Queue q = new TestQueue(TestSettings.volatileQueueSettings(10))) {\n             q.open();\n \n-            assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));\n+            assertThat(q.nonBlockReadBatch(1), nullValue());\n         }\n     }\n \n@@ -55,9 +56,9 @@ public void singleWriteRead() throws IOException {\n \n             Batch b = q.nonBlockReadBatch(1);\n \n-            assertThat(b.getElements().size(), is(equalTo(1)));\n-            assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));\n-            assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));\n+            assertThat(b.getElements().size(), is(1));\n+            assertThat(b.getElements().get(0).toString(), is(element.toString()));\n+            assertThat(q.nonBlockReadBatch(1), nullValue());\n         }\n     }\n \n@@ -71,9 +72,9 @@ public void singleWriteMultiRead() throws IOException {\n \n             Batch b = q.nonBlockReadBatch(2);\n \n-            assertThat(b.getElements().size(), is(equalTo(1)));\n-            assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));\n-            assertThat(q.nonBlockReadBatch(2), is(equalTo(null)));\n+            assertThat(b.getElements().size(), is(1));\n+            assertThat(b.getElements().get(0).toString(), is(element.toString()));\n+            assertThat(q.nonBlockReadBatch(2), nullValue());\n         }\n     }\n \n@@ -91,14 +92,14 @@ public void multiWriteSamePage() throws IOException {\n \n             Batch b = q.nonBlockReadBatch(2);\n \n-            assertThat(b.getElements().size(), is(equalTo(2)));\n-            assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(0).toString())));\n-            assertThat(b.getElements().get(1).toString(), is(equalTo(elements.get(1).toString())));\n+            assertThat(b.getElements().size(), is(2));\n+            assertThat(b.getElements().get(0).toString(), is(elements.get(0).toString()));\n+            assertThat(b.getElements().get(1).toString(), is(elements.get(1).toString()));\n \n             b = q.nonBlockReadBatch(2);\n \n-            assertThat(b.getElements().size(), is(equalTo(1)));\n-            assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(2).toString())));\n+            assertThat(b.getElements().size(), is(1));\n+            assertThat(b.getElements().get(0).toString(), is(elements.get(2).toString()));\n         }\n     }\n \n@@ -115,33 +116,33 @@ public void writeMultiPage() throws IOException {\n             }\n \n             // total of 2 pages: 1 head and 1 tail\n-            assertThat(q.getTailPages().size(), is(equalTo(1)));\n+            assertThat(q.getTailPages().size(), is(1));\n \n-            assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(false)));\n-            assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));\n-            assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));\n-            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n+            assertThat(q.getTailPages().get(0).isFullyRead(), is(false));\n+            assertThat(q.getTailPages().get(0).isFullyAcked(), is(false));\n+            assertThat(q.getHeadPage().isFullyRead(), is(false));\n+            assertThat(q.getHeadPage().isFullyAcked(), is(false));\n \n             Batch b = q.nonBlockReadBatch(10);\n-            assertThat(b.getElements().size(), is(equalTo(2)));\n+            assertThat(b.getElements().size(), is(2));\n \n-            assertThat(q.getTailPages().size(), is(equalTo(1)));\n+            assertThat(q.getTailPages().size(), is(1));\n \n-            assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(true)));\n-            assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));\n-            assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));\n-            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n+            assertThat(q.getTailPages().get(0).isFullyRead(), is(true));\n+            assertThat(q.getTailPages().get(0).isFullyAcked(), is(false));\n+            assertThat(q.getHeadPage().isFullyRead(), is(false));\n+            assertThat(q.getHeadPage().isFullyAcked(), is(false));\n \n             b = q.nonBlockReadBatch(10);\n-            assertThat(b.getElements().size(), is(equalTo(2)));\n+            assertThat(b.getElements().size(), is(2));\n \n-            assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(true)));\n-            assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));\n-            assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));\n-            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n+            assertThat(q.getTailPages().get(0).isFullyRead(), is(true));\n+            assertThat(q.getTailPages().get(0).isFullyAcked(), is(false));\n+            assertThat(q.getHeadPage().isFullyRead(), is(true));\n+            assertThat(q.getHeadPage().isFullyAcked(), is(false));\n \n             b = q.nonBlockReadBatch(10);\n-            assertThat(b, is(equalTo(null)));\n+            assertThat(b, nullValue());\n         }\n     }\n \n@@ -160,30 +161,30 @@ public void writeMultiPageWithInOrderAcking() throws IOException {\n \n             Batch b = q.nonBlockReadBatch(10);\n \n-            assertThat(b.getElements().size(), is(equalTo(2)));\n-            assertThat(q.getTailPages().size(), is(equalTo(1)));\n+            assertThat(b.getElements().size(), is(2));\n+            assertThat(q.getTailPages().size(), is(1));\n \n             // lets keep a ref to that tail page before acking\n             TailPage tailPage = q.getTailPages().get(0);\n \n-            assertThat(tailPage.isFullyRead(), is(equalTo(true)));\n+            assertThat(tailPage.isFullyRead(), is(true));\n \n             // ack first batch which includes all elements from tailPages\n             b.close();\n \n-            assertThat(q.getTailPages().size(), is(equalTo(0)));\n-            assertThat(tailPage.isFullyRead(), is(equalTo(true)));\n-            assertThat(tailPage.isFullyAcked(), is(equalTo(true)));\n+            assertThat(q.getTailPages().size(), is(0));\n+            assertThat(tailPage.isFullyRead(), is(true));\n+            assertThat(tailPage.isFullyAcked(), is(true));\n \n             b = q.nonBlockReadBatch(10);\n \n-            assertThat(b.getElements().size(), is(equalTo(2)));\n-            assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));\n-            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n+            assertThat(b.getElements().size(), is(2));\n+            assertThat(q.getHeadPage().isFullyRead(), is(true));\n+            assertThat(q.getHeadPage().isFullyAcked(), is(false));\n \n             b.close();\n \n-            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(true)));\n+            assertThat(q.getHeadPage().isFullyAcked(), is(true));\n         }\n     }\n \n@@ -200,51 +201,51 @@ public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {\n         try (TestQueue q = new TestQueue(settings)) {\n             q.open();\n \n-            assertThat(q.getHeadPage().getPageNum(), is(equalTo(0)));\n+            assertThat(q.getHeadPage().getPageNum(), is(0));\n             Checkpoint c = q.getCheckpointIO().read(\"checkpoint.head\");\n-            assertThat(c.getPageNum(), is(equalTo(0)));\n-            assertThat(c.getElementCount(), is(equalTo(0)));\n-            assertThat(c.getMinSeqNum(), is(equalTo(0L)));\n-            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));\n-            assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n+            assertThat(c.getPageNum(), is(0));\n+            assertThat(c.getElementCount(), is(0));\n+            assertThat(c.getMinSeqNum(), is(0L));\n+            assertThat(c.getFirstUnackedSeqNum(), is(0L));\n+            assertThat(c.getFirstUnackedPageNum(), is(0));\n \n             for (Queueable e : elements1) {\n                 q.write(e);\n             }\n \n             c = q.getCheckpointIO().read(\"checkpoint.head\");\n-            assertThat(c.getPageNum(), is(equalTo(0)));\n-            assertThat(c.getElementCount(), is(equalTo(0)));\n-            assertThat(c.getMinSeqNum(), is(equalTo(0L)));\n-            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));\n-            assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n+            assertThat(c.getPageNum(), is(0));\n+            assertThat(c.getElementCount(), is(0));\n+            assertThat(c.getMinSeqNum(), is(0L));\n+            assertThat(c.getFirstUnackedSeqNum(), is(0L));\n+            assertThat(c.getFirstUnackedPageNum(), is(0));\n \n-        //  assertThat(elements1.get(1).getSeqNum(), is(equalTo(2L)));\n+        //  assertThat(elements1.get(1).getSeqNum(), is(2L));\n             q.ensurePersistedUpto(2);\n \n             c = q.getCheckpointIO().read(\"checkpoint.head\");\n-            assertThat(c.getPageNum(), is(equalTo(0)));\n-            assertThat(c.getElementCount(), is(equalTo(2)));\n-            assertThat(c.getMinSeqNum(), is(equalTo(1L)));\n-            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));\n-            assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n+            assertThat(c.getPageNum(), is(0));\n+            assertThat(c.getElementCount(), is(2));\n+            assertThat(c.getMinSeqNum(), is(1L));\n+            assertThat(c.getFirstUnackedSeqNum(), is(1L));\n+            assertThat(c.getFirstUnackedPageNum(), is(0));\n \n             for (Queueable e : elements2) {\n                 q.write(e);\n             }\n \n             c = q.getCheckpointIO().read(\"checkpoint.head\");\n-            assertThat(c.getPageNum(), is(equalTo(1)));\n-            assertThat(c.getElementCount(), is(equalTo(0)));\n-            assertThat(c.getMinSeqNum(), is(equalTo(0L)));\n-            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));\n-            assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n+            assertThat(c.getPageNum(), is(1));\n+            assertThat(c.getElementCount(), is(0));\n+            assertThat(c.getMinSeqNum(), is(0L));\n+            assertThat(c.getFirstUnackedSeqNum(), is(0L));\n+            assertThat(c.getFirstUnackedPageNum(), is(0));\n \n             c = q.getCheckpointIO().read(\"checkpoint.0\");\n-            assertThat(c.getPageNum(), is(equalTo(0)));\n-            assertThat(c.getElementCount(), is(equalTo(2)));\n-            assertThat(c.getMinSeqNum(), is(equalTo(1L)));\n-            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));\n+            assertThat(c.getPageNum(), is(0));\n+            assertThat(c.getElementCount(), is(2));\n+            assertThat(c.getMinSeqNum(), is(1L));\n+            assertThat(c.getFirstUnackedSeqNum(), is(1L));\n \n             Batch b = q.nonBlockReadBatch(10);\n             b.close();\n@@ -257,21 +258,21 @@ public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {\n             }\n \n             c = q.getCheckpointIO().read(\"checkpoint.head\");\n-            assertThat(c.getPageNum(), is(equalTo(1)));\n-            assertThat(c.getElementCount(), is(equalTo(2)));\n-            assertThat(c.getMinSeqNum(), is(equalTo(3L)));\n-            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(3L)));\n-            assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));\n+            assertThat(c.getPageNum(), is(1));\n+            assertThat(c.getElementCount(), is(2));\n+            assertThat(c.getMinSeqNum(), is(3L));\n+            assertThat(c.getFirstUnackedSeqNum(), is(3L));\n+            assertThat(c.getFirstUnackedPageNum(), is(1));\n \n             b = q.nonBlockReadBatch(10);\n             b.close();\n \n             c = q.getCheckpointIO().read(\"checkpoint.head\");\n-            assertThat(c.getPageNum(), is(equalTo(1)));\n-            assertThat(c.getElementCount(), is(equalTo(2)));\n-            assertThat(c.getMinSeqNum(), is(equalTo(3L)));\n-            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(5L)));\n-            assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));\n+            assertThat(c.getPageNum(), is(1));\n+            assertThat(c.getElementCount(), is(2));\n+            assertThat(c.getMinSeqNum(), is(3L));\n+            assertThat(c.getFirstUnackedSeqNum(), is(5L));\n+            assertThat(c.getFirstUnackedPageNum(), is(1));\n         }\n     }\n \n@@ -298,22 +299,22 @@ public void randomAcking() throws IOException {\n                     q.write(e);\n                 }\n \n-                assertThat(q.getTailPages().size(), is(equalTo(page_count - 1)));\n+                assertThat(q.getTailPages().size(), is(page_count - 1));\n \n                 // first read all elements\n                 List<Batch> batches = new ArrayList<>();\n                 for (Batch b = q.nonBlockReadBatch(1); b != null; b = q.nonBlockReadBatch(1)) {\n                     batches.add(b);\n                 }\n-                assertThat(batches.size(), is(equalTo(page_count)));\n+                assertThat(batches.size(), is(page_count));\n \n                 // then ack randomly\n                 Collections.shuffle(batches);\n                 for (Batch b : batches) {\n                     b.close();\n                 }\n                 \n-                assertThat(q.getTailPages().size(), is(equalTo(0)));\n+                assertThat(q.getTailPages().size(), is(0));\n             }\n         }\n     }\n@@ -331,7 +332,7 @@ public void reachMaxUnread() throws IOException, InterruptedException, Execution\n             q.open();\n             \n             long seqNum = q.write(element);\n-            assertThat(seqNum, is(equalTo(1L)));\n+            assertThat(seqNum, is(1L));\n             assertThat(q.isFull(), is(false));\n \n             int ELEMENT_COUNT = 1000;\n@@ -349,22 +350,22 @@ public void reachMaxUnread() throws IOException, InterruptedException, Execution\n                     // spin wait until data is written and write blocks\n                     Thread.sleep(1);\n                 }\n-                assertThat(q.unreadCount, is(equalTo(2L)));\n+                assertThat(q.unreadCount, is(2L));\n                 assertThat(future.isDone(), is(false));\n \n                 // read one element, which will unblock the last write\n                 Batch b = q.nonBlockReadBatch(1);\n-                assertThat(b.getElements().size(), is(equalTo(1)));\n+                assertThat(b.getElements().size(), is(1));\n \n                 // future result is the blocked write seqNum for the second element\n-                assertThat(future.get(), is(equalTo(2L + i)));\n+                assertThat(future.get(), is(2L + i));\n                 assertThat(q.isFull(), is(false));\n \n                 executor.shutdown();\n             }\n \n             // since we did not ack and pages hold a single item\n-            assertThat(q.getTailPages().size(), is(equalTo(ELEMENT_COUNT)));\n+            assertThat(q.getTailPages().size(), is(ELEMENT_COUNT));\n         }\n     }\n \n@@ -383,16 +384,14 @@ public void reachMaxUnreadWithAcking() throws IOException, InterruptedException,\n             // perform first non-blocking write\n             long seqNum = q.write(element);\n \n-            assertThat(seqNum, is(equalTo(1L)));\n+            assertThat(seqNum, is(1L));\n             assertThat(q.isFull(), is(false));\n \n             int ELEMENT_COUNT = 1000;\n             for (int i = 0; i < ELEMENT_COUNT; i++) {\n \n                 // we expect this next write call to block so let's wrap it in a Future\n-                Callable<Long> write = () -> {\n-                    return q.write(element);\n-                };\n+                Callable<Long> write = () -> q.write(element);\n \n                 ExecutorService executor = Executors.newFixedThreadPool(1);\n                 Future<Long> future = executor.submit(write);\n@@ -403,24 +402,24 @@ public void reachMaxUnreadWithAcking() throws IOException, InterruptedException,\n                 }\n                 // read one element, which will unblock the last write\n                 Batch b = q.nonBlockReadBatch(1);\n-                assertThat(b, is(notNullValue()));\n-                assertThat(b.getElements().size(), is(equalTo(1)));\n+                assertThat(b, notNullValue());\n+                assertThat(b.getElements().size(), is(1));\n                 b.close();\n \n                 // future result is the blocked write seqNum for the second element\n-                assertThat(future.get(), is(equalTo(2L + i)));\n+                assertThat(future.get(), is(2L + i));\n                 assertThat(q.isFull(), is(false));\n \n                 executor.shutdown();\n             }\n \n             // all batches are acked, no tail pages should exist\n-            assertThat(q.getTailPages().size(), is(equalTo(0)));\n+            assertThat(q.getTailPages().size(), is(0));\n \n             // the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page\n             assertThat(q.getHeadPage().getElementCount() > 0L, is(true));\n-            assertThat(q.getHeadPage().unreadCount(), is(equalTo(1L)));\n-            assertThat(q.unreadCount, is(equalTo(1L)));\n+            assertThat(q.getHeadPage().unreadCount(), is(1L));\n+            assertThat(q.unreadCount, is(1L));\n         }\n     }\n \n@@ -443,9 +442,7 @@ public void reachMaxSizeTest() throws IOException, InterruptedException, Executi\n             assertThat(q.isFull(), is(false));\n \n             // we expect this next write call to block so let's wrap it in a Future\n-            Callable<Long> write = () -> {\n-                return q.write(element);\n-            };\n+            Callable<Long> write = () -> q.write(element);\n \n             ExecutorService executor = Executors.newFixedThreadPool(1);\n             Future<Long> future = executor.submit(write);\n@@ -523,9 +520,7 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted\n             Batch b = q.readBatch(10);\n \n             // we expect this next write call to block so let's wrap it in a Future\n-            Callable<Long> write = () -> {\n-                return q.write(element);\n-            };\n+            Callable<Long> write = () -> q.write(element);\n             ExecutorService executor = Executors.newFixedThreadPool(1);\n             Future<Long> future = executor.submit(write);\n             assertThat(future.isDone(), is(false));\n@@ -537,7 +532,7 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted\n \n             b.close();  // purge 1 page\n \n-            assertThat(future.get(), is(equalTo(ELEMENT_COUNT + 1L)));\n+            assertThat(future.get(), is(ELEMENT_COUNT + 1L));\n \n             executor.shutdown();\n         }\n@@ -563,9 +558,7 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup\n             assertThat(q.isFull(), is(false));\n \n             // we expect this next write call to block so let's wrap it in a Future\n-            Callable<Long> write = () -> {\n-                return q.write(element);\n-            };\n+            Callable<Long> write = () -> q.write(element);\n \n             ExecutorService executor = Executors.newFixedThreadPool(1);\n             Future<Long> future = executor.submit(write);\n@@ -599,7 +592,7 @@ public void testAckedCount() throws IOException {\n             element3 = new StringElement(\"third\");\n             firstSeqNum = q.write(element1);\n             b = q.nonBlockReadBatch(1);\n-            assertThat(b.getElements().size(), is(equalTo(1)));\n+            assertThat(b.getElements().size(), is(1));\n         }\n \n         long secondSeqNum;\n@@ -611,13 +604,13 @@ public void testAckedCount() throws IOException {\n             thirdSeqNum = q.write(element3);\n \n             b = q.nonBlockReadBatch(1);\n-            assertThat(b.getElements().size(), is(equalTo(1)));\n-            assertThat(b.getElements().get(0), is(equalTo(element1)));\n+            assertThat(b.getElements().size(), is(1));\n+            assertThat(b.getElements().get(0), is(element1));\n \n             b = q.nonBlockReadBatch(2);\n-            assertThat(b.getElements().size(), is(equalTo(2)));\n-            assertThat(b.getElements().get(0), is(equalTo(element2)));\n-            assertThat(b.getElements().get(1), is(equalTo(element3)));\n+            assertThat(b.getElements().size(), is(2));\n+            assertThat(b.getElements().get(0), is(element2));\n+            assertThat(b.getElements().get(1), is(element3));\n \n             q.ack(Collections.singletonList(firstSeqNum));\n         }\n@@ -626,7 +619,7 @@ public void testAckedCount() throws IOException {\n             q.open();\n \n             b = q.nonBlockReadBatch(2);\n-            assertThat(b.getElements().size(), is(equalTo(2)));\n+            assertThat(b.getElements().size(), is(2));\n \n             q.ack(Arrays.asList(secondSeqNum, thirdSeqNum));\n \n@@ -673,7 +666,7 @@ public void concurrentWritesTest() throws IOException, InterruptedException, Exe\n \n             for (Future<Integer> future : futures) {\n                 int result = future.get();\n-                assertThat(result, is(equalTo(ELEMENT_COUNT)));\n+                assertThat(result, is(ELEMENT_COUNT));\n             }\n \n             assertThat(q.getTailPages().isEmpty(), is(true));\n@@ -694,12 +687,12 @@ public void fullyAckedHeadPageBeheadingTest() throws IOException {\n             Batch b;\n             q.write(element);\n             b = q.nonBlockReadBatch(1);\n-            assertThat(b.getElements().size(), is(equalTo(1)));\n+            assertThat(b.getElements().size(), is(1));\n             b.close();\n \n             q.write(element);\n             b = q.nonBlockReadBatch(1);\n-            assertThat(b.getElements().size(), is(equalTo(1)));\n+            assertThat(b.getElements().size(), is(1));\n             b.close();\n \n             // head page should be full and fully acked\n@@ -713,8 +706,8 @@ public void fullyAckedHeadPageBeheadingTest() throws IOException {\n             // since head page was fully acked it should not have created a new tail page\n \n             assertThat(q.getTailPages().isEmpty(), is(true));\n-            assertThat(q.getHeadPage().getPageNum(), is(equalTo(1)));\n-            assertThat(q.firstUnackedPageNum(), is(equalTo(1)));\n+            assertThat(q.getHeadPage().getPageNum(), is(1));\n+            assertThat(q.firstUnackedPageNum(), is(1));\n             assertThat(q.isFullyAcked(), is(false));\n         }\n     }\n@@ -723,7 +716,7 @@ public void fullyAckedHeadPageBeheadingTest() throws IOException {\n     public void getsPersistedByteSizeCorrectlyForUnopened() throws Exception {\n         Settings settings = TestSettings.persistedQueueSettings(100, dataPath);\n         try (Queue q = new Queue(settings)) {\n-            assertThat(q.getPersistedByteSize(), is(equalTo(0L)));\n+            assertThat(q.getPersistedByteSize(), is(0L));\n         }\n     }\n }",
      "parent_sha": "b7f4fd7ebf51647d17821b0422388764ffa63f53"
    }
  },
  {
    "oid": "b2a396bccbc2711801f57e01ef735230cca5b9b1",
    "message": "Add more information to UnexpectedTypeException (#12426)\n\nWhere available, this commit addS information from getSourceWithMetadata to the\r\nerror message of UnexpectedTypeException, dropping down to `toString`\r\nif not, giving more context to find where the issue is caused in the configuration.",
    "date": "2020-11-10T14:46:58Z",
    "url": "https://github.com/elastic/logstash/commit/b2a396bccbc2711801f57e01ef735230cca5b9b1",
    "details": {
      "sha": "8c8a91327e7d48c7d8772b2ae235a94aef36d719",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java",
      "status": "modified",
      "additions": 64,
      "deletions": 46,
      "changes": 110,
      "blob_url": "https://github.com/elastic/logstash/blob/b2a396bccbc2711801f57e01ef735230cca5b9b1/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FEventCondition.java",
      "raw_url": "https://github.com/elastic/logstash/raw/b2a396bccbc2711801f57e01ef735230cca5b9b1/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FEventCondition.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FEventCondition.java?ref=b2a396bccbc2711801f57e01ef735230cca5b9b1",
      "patch": "@@ -101,6 +101,7 @@ public Compiler() {\n          * All compilation is globally {@code synchronized} on {@link EventCondition.Compiler#cache}\n          * to minimize code size by avoiding compiling logically equivalent expressions in more than\n          * one instance.\n+         *\n          * @param expression BooleanExpress to compile\n          * @return Compiled {@link EventCondition}\n          */\n@@ -125,7 +126,7 @@ public EventCondition buildCondition(final BooleanExpression expression) {\n                 } else if (expression instanceof Not) {\n                     condition = not((Not) expression);\n                 } else if (expression instanceof Gt || expression instanceof Gte\n-                    || expression instanceof Lt || expression instanceof Lte) {\n+                        || expression instanceof Lt || expression instanceof Lte) {\n                     condition = comparison((BinaryBooleanExpression) expression);\n                 } else if (expression instanceof Neq) {\n                     condition = not(eq((BinaryBooleanExpression) expression));\n@@ -152,7 +153,7 @@ private EventCondition booleanCondition(final BinaryBooleanExpression expression\n                 first = buildCondition((BooleanExpression) left);\n                 second = truthy((EventValueExpression) right);\n             } else if (right instanceof BooleanExpression &&\n-                left instanceof EventValueExpression) {\n+                    left instanceof EventValueExpression) {\n                 first = truthy((EventValueExpression) left);\n                 second = buildCondition((BooleanExpression) right);\n             } else {\n@@ -183,28 +184,29 @@ private EventCondition not(final Not not) {\n         /**\n          * Checks if a {@link BinaryBooleanExpression} consists of a {@link ValueExpression} on the\n          * left and a {@link EventValueExpression} on the right.\n+         *\n          * @param expression Expression to check type for\n          * @return True if the left branch of the {@link BinaryBooleanExpression} is a\n          * {@link ValueExpression} and its right side is a {@link EventValueExpression}.\n          */\n         private static boolean vAndE(final BinaryBooleanExpression expression) {\n             return expression.getLeft() instanceof ValueExpression &&\n-                expression.getRight() instanceof EventValueExpression;\n+                    expression.getRight() instanceof EventValueExpression;\n         }\n \n         private static boolean vAndV(final BinaryBooleanExpression expression) {\n             return expression.getLeft() instanceof ValueExpression &&\n-                expression.getRight() instanceof ValueExpression;\n+                    expression.getRight() instanceof ValueExpression;\n         }\n \n         private static boolean eAndV(final BinaryBooleanExpression expression) {\n             return expression.getLeft() instanceof EventValueExpression &&\n-                expression.getRight() instanceof ValueExpression;\n+                    expression.getRight() instanceof ValueExpression;\n         }\n \n         private static boolean eAndE(final BinaryBooleanExpression expression) {\n             return expression.getLeft() instanceof EventValueExpression &&\n-                expression.getRight() instanceof EventValueExpression;\n+                    expression.getRight() instanceof EventValueExpression;\n         }\n \n         private static boolean vAndR(final BinaryBooleanExpression expression) {\n@@ -268,19 +270,19 @@ private static EventCondition comparison(final BinaryBooleanExpression expressio\n             final Expression uright = expression.getRight();\n             if (eAndV(expression)) {\n                 condition = compareFieldToConstant(\n-                    (EventValueExpression) uleft, (ValueExpression) uright, conditional\n+                        (EventValueExpression) uleft, (ValueExpression) uright, conditional\n                 );\n             } else if (vAndE(expression)) {\n                 condition = compareFieldToConstant(\n-                    (EventValueExpression) uright, (ValueExpression) uleft, converse\n+                        (EventValueExpression) uright, (ValueExpression) uleft, converse\n                 );\n             } else if (vAndV(expression)) {\n                 return compareConstants(\n-                    (ValueExpression) uleft, (ValueExpression) uright, conditional\n+                        (ValueExpression) uleft, (ValueExpression) uright, conditional\n                 );\n             } else {\n                 return compareFields(\n-                    (EventValueExpression) uleft, (EventValueExpression) uright, conditional\n+                        (EventValueExpression) uleft, (EventValueExpression) uright, conditional\n                 );\n             }\n             return condition;\n@@ -292,23 +294,23 @@ private static EventCondition in(final In in) {\n             final EventCondition condition;\n             if (eAndV(in) && isScalar((ValueExpression) in.getRight())) {\n                 condition = new EventCondition.Compiler.FieldInConstantScalar(\n-                    FieldReference.from(((EventValueExpression) left).getFieldName()),\n-                    ((ValueExpression) right).get().toString()\n+                        FieldReference.from(((EventValueExpression) left).getFieldName()),\n+                        ((ValueExpression) right).get().toString()\n                 );\n             } else if (vAndE(in) && isScalar((ValueExpression) in.getLeft())) {\n                 final Object leftv = ((ValueExpression) left).get();\n                 final FieldReference rfield =\n-                    FieldReference.from(((EventValueExpression) right).getFieldName());\n+                        FieldReference.from(((EventValueExpression) right).getFieldName());\n                 if (leftv instanceof String) {\n                     condition = new EventCondition.Compiler.ConstantStringInField(\n-                        rfield, (String) leftv\n+                            rfield, (String) leftv\n                     );\n                 } else {\n                     condition = new EventCondition.Compiler.ConstantScalarInField(rfield, leftv);\n                 }\n             } else if (eAndV(in) && listValueRight(in)) {\n                 condition = in(\n-                    (EventValueExpression) left, (List<?>) ((ValueExpression) right).get()\n+                        (EventValueExpression) left, (List<?>) ((ValueExpression) right).get()\n                 );\n             } else if (eAndE(in)) {\n                 condition = in((EventValueExpression) left, (EventValueExpression) right);\n@@ -322,14 +324,15 @@ private static EventCondition in(final In in) {\n \n         private static EventCondition in(final EventValueExpression left, final List<?> right) {\n             return new EventCondition.Compiler.FieldInConstantList(\n-                FieldReference.from(left.getFieldName()), right\n+                    FieldReference.from(left.getFieldName()), right\n             );\n         }\n \n         /**\n          * Compiles a constant (due to both of its sides being constant {@link ValueExpression})\n          * conditional.\n-         * @param left Constant left side {@link ValueExpression}\n+         *\n+         * @param left  Constant left side {@link ValueExpression}\n          * @param right Constant right side {@link ValueExpression}\n          * @return Constant {@link EventCondition}\n          */\n@@ -339,12 +342,12 @@ private static EventCondition in(final ValueExpression left, final ValueExpressi\n             final boolean res;\n             if (found instanceof ConvertedList && other instanceof RubyString) {\n                 res = ((ConvertedList) found).stream().anyMatch(item -> item.toString()\n-                    .equals(other.toString()));\n+                        .equals(other.toString()));\n             } else if (found instanceof RubyString && other instanceof RubyString) {\n                 res = found.toString().contains(other.toString());\n             } else if (found instanceof RubyString && other instanceof ConvertedList) {\n                 res = ((ConvertedList) other).stream()\n-                    .anyMatch(item -> item.toString().equals(found.toString()));\n+                        .anyMatch(item -> item.toString().equals(found.toString()));\n             } else {\n                 res = found != null && found.equals(other);\n             }\n@@ -361,18 +364,18 @@ private static boolean isScalar(final ValueExpression expression) {\n         }\n \n         private static EventCondition in(final EventValueExpression left,\n-            final EventValueExpression right) {\n+                                         final EventValueExpression right) {\n             return new EventCondition.Compiler.FieldInField(\n-                FieldReference.from(left.getFieldName()), FieldReference.from(right.getFieldName())\n+                    FieldReference.from(left.getFieldName()), FieldReference.from(right.getFieldName())\n             );\n         }\n \n         @SuppressWarnings(\"unchecked\")\n         private static EventCondition eq(final EventValueExpression evalE,\n-            final ValueExpression valE) {\n+                                         final ValueExpression valE) {\n             return rubyFieldEquals(\n-                (Comparable<IRubyObject>) Rubyfier.deep(RubyUtil.RUBY, valE.get()),\n-                evalE.getFieldName()\n+                    (Comparable<IRubyObject>) Rubyfier.deep(RubyUtil.RUBY, valE.get()),\n+                    evalE.getFieldName()\n             );\n         }\n \n@@ -388,14 +391,14 @@ private static EventCondition eq(final BinaryBooleanExpression equals) {\n                 condition = eq((EventValueExpression) left, (EventValueExpression) right);\n             } else {\n                 condition = constant(\n-                    ((ValueExpression) left).get().equals(((ValueExpression) right).get())\n+                        ((ValueExpression) left).get().equals(((ValueExpression) right).get())\n                 );\n             }\n             return condition;\n         }\n \n         private static EventCondition eq(final EventValueExpression first,\n-            final EventValueExpression second) {\n+                                         final EventValueExpression second) {\n             final FieldReference field1 = FieldReference.from(first.getFieldName());\n             final FieldReference field2 = FieldReference.from(second.getFieldName());\n             return event -> {\n@@ -415,30 +418,30 @@ private static EventCondition not(final EventCondition condition) {\n         }\n \n         private static EventCondition compareConstants(final ValueExpression left,\n-            final ValueExpression right, final Predicate<Integer> operator) {\n+                                                       final ValueExpression right, final Predicate<Integer> operator) {\n             return constant(operator.test(compare(left.get(), right.get())));\n         }\n \n         private static EventCondition compareFields(final EventValueExpression left,\n-            final EventValueExpression right, final Predicate<Integer> operator) {\n+                                                    final EventValueExpression right, final Predicate<Integer> operator) {\n             final FieldReference one = FieldReference.from(left.getFieldName());\n             final FieldReference other = FieldReference.from(right.getFieldName());\n             return event -> {\n                 final Event javaEvent = event.getEvent();\n                 return operator.test(\n-                    compare(\n-                        javaEvent.getUnconvertedField(one), javaEvent.getUnconvertedField(other)\n-                    )\n+                        compare(\n+                                javaEvent.getUnconvertedField(one), javaEvent.getUnconvertedField(other)\n+                        )\n                 );\n             };\n         }\n \n         @SuppressWarnings(\"unchecked\")\n         private static EventCondition compareFieldToConstant(final EventValueExpression left,\n-            final ValueExpression right, final Predicate<Integer> operator) {\n+                                                             final ValueExpression right, final Predicate<Integer> operator) {\n             final FieldReference one = FieldReference.from(left.getFieldName());\n             final Comparable<IRubyObject> other =\n-                (Comparable<IRubyObject>) Rubyfier.deep(RubyUtil.RUBY, right.get());\n+                    (Comparable<IRubyObject>) Rubyfier.deep(RubyUtil.RUBY, right.get());\n             return event -> {\n                 final Event javaEvent = event.getEvent();\n                 return operator.test(compare(javaEvent.getUnconvertedField(one), other));\n@@ -455,7 +458,8 @@ private static int compare(final Object left, final Object right) {\n \n         /**\n          * Contains function using Ruby equivalent comparison logic.\n-         * @param list List to find value in\n+         *\n+         * @param list  List to find value in\n          * @param value Value to find in list\n          * @return True iff value is in list\n          */\n@@ -469,7 +473,7 @@ private static boolean contains(final ConvertedList list, final Object value) {\n         }\n \n         private static EventCondition rubyFieldEquals(final Comparable<IRubyObject> left,\n-            final String field) {\n+                                                      final String field) {\n             final FieldReference reference = FieldReference.from(field);\n             return event ->\n                     left.equals(Rubyfier.deep(RubyUtil.RUBY, event.getEvent().getUnconvertedField(reference)));\n@@ -549,8 +553,8 @@ private ConstantStringInField(final FieldReference field, final String value) {\n             public boolean fulfilled(final JrubyEventExtLibrary.RubyEvent event) {\n                 final Object found = event.getEvent().getUnconvertedField(field);\n                 return found instanceof RubyString &&\n-                    ((RubyString) found).getByteList().indexOf(bytes) > -1\n-                    || found instanceof ConvertedList && contains((ConvertedList) found, string);\n+                        ((RubyString) found).getByteList().indexOf(bytes) > -1\n+                        || found instanceof ConvertedList && contains((ConvertedList) found, string);\n             }\n         }\n \n@@ -569,7 +573,7 @@ private ConstantScalarInField(final FieldReference field, final Object value) {\n             public boolean fulfilled(final JrubyEventExtLibrary.RubyEvent event) {\n                 final Object found = event.getEvent().getUnconvertedField(field);\n                 return found instanceof ConvertedList && contains((ConvertedList) found, value)\n-                    || Objects.equals(found, field);\n+                        || Objects.equals(found, field);\n             }\n         }\n \n@@ -588,7 +592,7 @@ private FieldInConstantScalar(final FieldReference field, final String value) {\n             public boolean fulfilled(final JrubyEventExtLibrary.RubyEvent event) {\n                 final Object found = event.getEvent().getUnconvertedField(field);\n                 return found instanceof RubyString &&\n-                    value.indexOf(((RubyString) found).getByteList()) > -1;\n+                        value.indexOf(((RubyString) found).getByteList()) > -1;\n             }\n         }\n \n@@ -611,7 +615,7 @@ public boolean fulfilled(final JrubyEventExtLibrary.RubyEvent event) {\n                     return false;\n                 } else if (lfound instanceof RubyString && rfound instanceof RubyString) {\n                     return ((RubyString) rfound).getByteList()\n-                        .indexOf(((RubyString) lfound).getByteList()) > -1;\n+                            .indexOf(((RubyString) lfound).getByteList()) > -1;\n                 } else if (rfound instanceof ConvertedList) {\n                     return contains((ConvertedList) rfound, lfound);\n                 } else {\n@@ -635,7 +639,7 @@ private FieldInConstantList(final FieldReference field, final List<?> value) {\n             public boolean fulfilled(final JrubyEventExtLibrary.RubyEvent event) {\n                 final Object found = event.getEvent().getUnconvertedField(field);\n                 return found != null &&\n-                    value.stream().anyMatch(val -> val.toString().equals(found.toString()));\n+                        value.stream().anyMatch(val -> val.toString().equals(found.toString()));\n             }\n         }\n \n@@ -665,21 +669,35 @@ private static final class UnexpectedTypeException extends IllegalArgumentExcept\n \n             UnexpectedTypeException(final Expression left, final Expression right) {\n                 super(\n-                    String.format(\"Unexpected input types %s %s\", left.getClass(), right.getClass())\n+                        String.format(\n+                                \"Unexpected input types left: %s, right: %s\", getUnexpectedTypeDetails(left), getUnexpectedTypeDetails(right)\n+                        )\n                 );\n             }\n \n             UnexpectedTypeException(final Object inner) {\n-                super(String.format(\"Unexpected input type %s\", inner.getClass()));\n+                super(String.format(\"Unexpected input type %s\", getUnexpectedTypeDetails(inner)));\n             }\n \n             UnexpectedTypeException(final Object left, final Object right) {\n                 super(\n-                    String.format(\n-                        \"Unexpected input type combination %s %s\", left.getClass(), right.getClass()\n-                    )\n+                        String.format(\n+                                \"Unexpected input type combination left %s, right %s\", getUnexpectedTypeDetails(left), getUnexpectedTypeDetails(right)\n+                        )\n                 );\n             }\n+\n+            private static String getUnexpectedTypeDetails(Object unexpected) {\n+                String details;\n+                if (unexpected instanceof Expression) {\n+                    Expression expression = (Expression)unexpected;\n+                    details = (expression.getSourceWithMetadata() != null) ? expression.getSourceWithMetadata().toString()\n+                                                                           : expression.toString();\n+                } else {\n+                    details = unexpected.toString();\n+                }\n+                return String.format(\"%s:%s\", unexpected.getClass(), details);\n+            }\n         }\n     }\n }",
      "parent_sha": "606cfe5dfbcc685369074c3b25d73559f20933ee"
    }
  },
  {
    "oid": "4863621840223dd730095c8d93dcf58b87971051",
    "message": "MINOR: Removed dead fields from Event\n\nFixes #7139",
    "date": "2017-05-17T18:16:28Z",
    "url": "https://github.com/elastic/logstash/commit/4863621840223dd730095c8d93dcf58b87971051",
    "details": {
      "sha": "4670e1ff6ef55792b4298be0e64f1776c4d06ee5",
      "filename": "logstash-core/src/main/java/org/logstash/Event.java",
      "status": "modified",
      "additions": 3,
      "deletions": 7,
      "changes": 10,
      "blob_url": "https://github.com/elastic/logstash/blob/4863621840223dd730095c8d93dcf58b87971051/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FEvent.java",
      "raw_url": "https://github.com/elastic/logstash/raw/4863621840223dd730095c8d93dcf58b87971051/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FEvent.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FEvent.java?ref=4863621840223dd730095c8d93dcf58b87971051",
      "patch": "@@ -1,6 +1,5 @@\n package org.logstash;\n \n-import com.fasterxml.jackson.databind.ObjectMapper;\n import org.logstash.bivalues.NullBiValue;\n import org.logstash.bivalues.StringBiValue;\n import org.logstash.bivalues.TimeBiValue;\n@@ -43,16 +42,13 @@ public class Event implements Cloneable, Serializable, Queueable {\n     public static final String VERSION_ONE = \"1\";\n     private static final String DATA_MAP_KEY = \"DATA\";\n     private static final String META_MAP_KEY = \"META\";\n-    private static final String SEQNUM_MAP_KEY = \"SEQUENCE_NUMBER\";\n-\n \n     private static final Logger logger = LogManager.getLogger(Event.class);\n-    private static final ObjectMapper mapper = new ObjectMapper();\n \n     public Event()\n     {\n-        this.metadata = new HashMap<String, Object>();\n-        this.data = new HashMap<String, Object>();\n+        this.metadata = new HashMap<>();\n+        this.data = new HashMap<>();\n         this.data.put(VERSION, VERSION_ONE);\n         this.cancelled = false;\n         this.timestamp = new Timestamp();\n@@ -72,7 +68,7 @@ public Event(Map data)\n         if (this.data.containsKey(METADATA)) {\n             this.metadata = (Map<String, Object>) this.data.remove(METADATA);\n         } else {\n-            this.metadata = new HashMap<String, Object>();\n+            this.metadata = new HashMap<>();\n         }\n         this.metadata_accessors = new Accessors(this.metadata);\n ",
      "parent_sha": "7e9529fe4701dce84f69a0ea11b2d97eb09537c8"
    }
  },
  {
    "oid": "cc81dab8c32ad1046f640d619a638a244b9936aa",
    "message": "MINOR: Cleanup compiler warnings in org.logstash.Timestamp\n\nFixes #7702",
    "date": "2017-07-17T09:41:49Z",
    "url": "https://github.com/elastic/logstash/commit/cc81dab8c32ad1046f640d619a638a244b9936aa",
    "details": {
      "sha": "8de86920c07bbb7385ea852b4f9ec06206f85ccb",
      "filename": "logstash-core/src/main/java/org/logstash/Timestamp.java",
      "status": "modified",
      "additions": 6,
      "deletions": 8,
      "changes": 14,
      "blob_url": "https://github.com/elastic/logstash/blob/cc81dab8c32ad1046f640d619a638a244b9936aa/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FTimestamp.java",
      "raw_url": "https://github.com/elastic/logstash/raw/cc81dab8c32ad1046f640d619a638a244b9936aa/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FTimestamp.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FTimestamp.java?ref=cc81dab8c32ad1046f640d619a638a244b9936aa",
      "patch": "@@ -1,25 +1,23 @@\n package org.logstash;\n \n import com.fasterxml.jackson.databind.annotation.JsonSerialize;\n+import java.util.Date;\n import org.joda.time.DateTime;\n import org.joda.time.DateTimeZone;\n import org.joda.time.Duration;\n import org.joda.time.LocalDateTime;\n import org.joda.time.format.DateTimeFormatter;\n import org.joda.time.format.ISODateTimeFormat;\n import org.logstash.ackedqueue.Queueable;\n-\n-import java.io.IOException;\n-import java.util.Date;\n import org.logstash.json.TimestampSerializer;\n \n @JsonSerialize(using = TimestampSerializer.class)\n-public class Timestamp implements Cloneable, Comparable, Queueable {\n+public final class Timestamp implements Cloneable, Comparable<Timestamp>, Queueable {\n \n     // all methods setting the time object must set it in the UTC timezone\n     private DateTime time;\n \n-    private static DateTimeFormatter iso8601Formatter = ISODateTimeFormat.dateTime();\n+    private static final DateTimeFormatter iso8601Formatter = ISODateTimeFormat.dateTime();\n \n     private static final LocalDateTime JAN_1_1970 = new LocalDateTime(1970, 1, 1, 0, 0);\n \n@@ -78,8 +76,8 @@ public long usec() {\n     }\n \n     @Override\n-    public int compareTo(Object other) {\n-        return getTime().compareTo(((Timestamp) other).getTime());\n+    public int compareTo(Timestamp other) {\n+        return getTime().compareTo(other.getTime());\n     }\n \n     @Override\n@@ -90,7 +88,7 @@ public Timestamp clone() throws CloneNotSupportedException {\n     }\n \n     @Override\n-    public byte[] serialize() throws IOException {\n+    public byte[] serialize() {\n         return toString().getBytes();\n     }\n }",
      "parent_sha": "766d10e1a70544e33add56da4f46ba8f47dfcc95"
    }
  },
  {
    "oid": "933919367aaa232a4ad654f9d0399fab4c907aba",
    "message": "MINOR: Extract hot constructor from RubyEvent\n\nFixes #7781",
    "date": "2017-07-24T08:18:56Z",
    "url": "https://github.com/elastic/logstash/commit/933919367aaa232a4ad654f9d0399fab4c907aba",
    "details": {
      "sha": "4ddb21e58738f2773bbdf8beea25135bb45b718f",
      "filename": "logstash-core/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java",
      "status": "modified",
      "additions": 40,
      "deletions": 19,
      "changes": 59,
      "blob_url": "https://github.com/elastic/logstash/blob/933919367aaa232a4ad654f9d0399fab4c907aba/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJrubyEventExtLibrary.java",
      "raw_url": "https://github.com/elastic/logstash/raw/933919367aaa232a4ad654f9d0399fab4c907aba/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJrubyEventExtLibrary.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJrubyEventExtLibrary.java?ref=933919367aaa232a4ad654f9d0399fab4c907aba",
      "patch": "@@ -1,7 +1,15 @@\n package org.logstash.ext;\n \n-import org.logstash.*;\n-import org.jruby.*;\n+import java.io.IOException;\n+import java.util.Map;\n+import org.jruby.Ruby;\n+import org.jruby.RubyArray;\n+import org.jruby.RubyBoolean;\n+import org.jruby.RubyClass;\n+import org.jruby.RubyHash;\n+import org.jruby.RubyModule;\n+import org.jruby.RubyObject;\n+import org.jruby.RubyString;\n import org.jruby.anno.JRubyClass;\n import org.jruby.anno.JRubyMethod;\n import org.jruby.exceptions.RaiseException;\n@@ -12,9 +20,11 @@\n import org.jruby.runtime.ThreadContext;\n import org.jruby.runtime.builtin.IRubyObject;\n import org.jruby.runtime.load.Library;\n-\n-import java.io.IOException;\n-import java.util.Map;\n+import org.logstash.ConvertedMap;\n+import org.logstash.Event;\n+import org.logstash.PathCache;\n+import org.logstash.Rubyfier;\n+import org.logstash.Valuefier;\n \n public class JrubyEventExtLibrary implements Library {\n \n@@ -57,8 +67,8 @@ public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {\n         }\n     }\n \n-    @JRubyClass(name = \"Event\", parent = \"Object\")\n-    public static class RubyEvent extends RubyObject {\n+    @JRubyClass(name = \"Event\")\n+    public static final class RubyEvent extends RubyObject {\n         private Event event;\n \n         public RubyEvent(Ruby runtime, RubyClass klass) {\n@@ -88,23 +98,14 @@ public void setEvent(Event event) {\n \n         // def initialize(data = {})\n         @JRubyMethod(name = \"initialize\", optional = 1)\n-        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)\n-        {\n+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args) {\n             args = Arity.scanArgs(context.runtime, args, 0, 1);\n             IRubyObject data = args[0];\n-\n-            if (data == null || data.isNil()) {\n-                this.event = new Event();\n-            } else if (data instanceof RubyHash) {\n+            if (data instanceof RubyHash) {\n                 this.event = new Event(ConvertedMap.newFromRubyHash((RubyHash) data));\n-            } else if (data instanceof MapJavaProxy) {\n-                this.event = new Event(ConvertedMap.newFromMap(\n-                    (Map)((MapJavaProxy)data).getObject())\n-                );\n             } else {\n-                throw context.runtime.newTypeError(\"wrong argument type \" + data.getMetaClass() + \" (expected Hash)\");\n+                initializeFallback(context, data);\n             }\n-\n             return context.nil;\n         }\n \n@@ -293,5 +294,25 @@ public IRubyObject ruby_set_timestamp(ThreadContext context, IRubyObject value)\n             this.event.setTimestamp(((JrubyTimestampExtLibrary.RubyTimestamp)value).getTimestamp());\n             return value;\n         }\n+\n+        /**\n+         * Cold path for the Ruby constructor\n+         * {@link JrubyEventExtLibrary.RubyEvent#ruby_initialize(ThreadContext, IRubyObject[])} for\n+         * when its argument is not a {@link RubyHash}.\n+         * @param context Ruby {@link ThreadContext}\n+         * @param data Either {@code null}, {@link org.jruby.RubyNil} or an instance of\n+         * {@link MapJavaProxy}\n+         */\n+        private void initializeFallback(final ThreadContext context, final IRubyObject data) {\n+            if (data == null || data.isNil()) {\n+                this.event = new Event();\n+            } else if (data instanceof MapJavaProxy) {\n+                this.event = new Event(ConvertedMap.newFromMap(\n+                    (Map)((MapJavaProxy)data).getObject())\n+                );\n+            } else {\n+                throw context.runtime.newTypeError(\"wrong argument type \" + data.getMetaClass() + \" (expected Hash)\");\n+            }\n+        }\n     }\n }",
      "parent_sha": "6355c839ab503fb5a78b91c62d108f8d622e1458"
    }
  },
  {
    "oid": "6e5ea14c0be933823ff8515ffafe94f497692b98",
    "message": "Handle Windows delete pending files (#12335)\n\nWhen deleting temporary files created by the DLQ writer to store data before moving to their\r\nfinal location, Windows may leave these files in a \"delete pending\" state, where the files\r\nare somewhat in a state of limbo, where they result of `Files.exist(filename)` is `false`,\r\nbut the result of `filename.toFile().exists()` is true. When files are in this state, a new\r\nfile with the same name cannot be created, which causes the DLQ test used to ensure that\r\nclosing and reopening the DLQ (in such events as a pipeline restart) to fail.\r\n\r\nThis commit moves the temporary file to an alternative location before deletion, ensuring that\r\nthe \"pending delete\" status does not interrupt with the DLQ startup",
    "date": "2020-10-13T14:23:44Z",
    "url": "https://github.com/elastic/logstash/commit/6e5ea14c0be933823ff8515ffafe94f497692b98",
    "details": {
      "sha": "54edca12fe9396cee129b68a080235491a1d3890",
      "filename": "logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java",
      "status": "modified",
      "additions": 27,
      "deletions": 7,
      "changes": 34,
      "blob_url": "https://github.com/elastic/logstash/blob/6e5ea14c0be933823ff8515ffafe94f497692b98/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "raw_url": "https://github.com/elastic/logstash/raw/6e5ea14c0be933823ff8515ffafe94f497692b98/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java?ref=6e5ea14c0be933823ff8515ffafe94f497692b98",
      "patch": "@@ -295,8 +295,8 @@ private static Stream<Path> listFiles(Path path, String suffix) throws IOExcepti\n     // check if there is a corresponding .log file - if yes delete the temp file, if no atomic move the\n     // temp file to be a new segment file..\n     private void cleanupTempFile(final Path tempFile) {\n-        String tempFilename = tempFile.getFileName().toString().split(\"\\\\.\")[0];\n-        Path segmentFile = queuePath.resolve(String.format(\"%s.log\", tempFilename));\n+        String segmentName = tempFile.getFileName().toString().split(\"\\\\.\")[0];\n+        Path segmentFile = queuePath.resolve(String.format(\"%s.log\", segmentName));\n         try {\n             if (Files.exists(segmentFile)) {\n                 Files.delete(tempFile);\n@@ -305,16 +305,15 @@ private void cleanupTempFile(final Path tempFile) {\n                 SegmentStatus segmentStatus = RecordIOReader.getSegmentStatus(tempFile);\n                 switch (segmentStatus){\n                     case VALID:\n-                        logger.debug(\"Moving temp file {} to segment file {}\", tempFilename, segmentFile);\n+                        logger.debug(\"Moving temp file {} to segment file {}\", tempFile, segmentFile);\n                         Files.move(tempFile, segmentFile, StandardCopyOption.ATOMIC_MOVE);\n                         break;\n                     case EMPTY:\n-                        logger.debug(\"Removing unused temp file {}\", tempFilename);\n-                        Files.delete(tempFile);\n+                        deleteTemporaryFile(tempFile, segmentName);\n                         break;\n                     case INVALID:\n-                        Path errorFile = queuePath.resolve(String.format(\"%s.err\", tempFilename));\n-                        logger.warn(\"Segment file {} is in an error state, saving as {}\", tempFilename, errorFile);\n+                        Path errorFile = queuePath.resolve(String.format(\"%s.err\", segmentName));\n+                        logger.warn(\"Segment file {} is in an error state, saving as {}\", segmentFile, errorFile);\n                         Files.move(tempFile, errorFile, StandardCopyOption.ATOMIC_MOVE);\n                         break;\n                     default:\n@@ -325,4 +324,25 @@ private void cleanupTempFile(final Path tempFile) {\n             throw new IllegalStateException(\"Unable to clean up temp file: \" + tempFile, e);\n         }\n     }\n+\n+    // Windows can leave files in a \"Delete pending\" state, where the file presents as existing to certain\n+    // methods, and not to others, and actively prevents a new file being created with the same file name,\n+    // throwing AccessDeniedException. This method moves the temporary file to a .del file before\n+    // deletion, enabling a new temp file to be created in its place.\n+    private void deleteTemporaryFile(Path tempFile, String segmentName) throws IOException {\n+        Path deleteTarget;\n+        if (isWindows()) {\n+            Path deletedFile = queuePath.resolve(String.format(\"%s.del\", segmentName));\n+            logger.debug(\"Moving temp file {} to {}\", tempFile, deletedFile);\n+            deleteTarget = deletedFile;\n+            Files.move(tempFile, deletedFile, StandardCopyOption.ATOMIC_MOVE);\n+        } else {\n+            deleteTarget = tempFile;\n+        }\n+        Files.delete(deleteTarget);\n+    }\n+\n+    private static boolean isWindows(){\n+        return System.getProperty(\"os.name\").startsWith(\"Windows\");\n+    }\n }",
      "parent_sha": "ecbad2a2305a6fb92544d75b8b46f4200ef205b8"
    }
  },
  {
    "oid": "ff9ccb3e59ad8ba3ce30746d7be9fbd099bf58d7",
    "message": "#8345 adjust test stacktrace size example to work in all cases by acting on clearly defined thread\n\nFixes #8350",
    "date": "2017-09-21T14:45:13Z",
    "url": "https://github.com/elastic/logstash/commit/ff9ccb3e59ad8ba3ce30746d7be9fbd099bf58d7",
    "details": {
      "sha": "3334269f4cad57546019d66ab5f9ff03e0617031",
      "filename": "logstash-core/src/test/java/org/logstash/instruments/monitors/HotThreadMonitorTest.java",
      "status": "modified",
      "additions": 38,
      "deletions": 14,
      "changes": 52,
      "blob_url": "https://github.com/elastic/logstash/blob/ff9ccb3e59ad8ba3ce30746d7be9fbd099bf58d7/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Finstruments%2Fmonitors%2FHotThreadMonitorTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/ff9ccb3e59ad8ba3ce30746d7be9fbd099bf58d7/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Finstruments%2Fmonitors%2FHotThreadMonitorTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Finstruments%2Fmonitors%2FHotThreadMonitorTest.java?ref=ff9ccb3e59ad8ba3ce30746d7be9fbd099bf58d7",
      "patch": "@@ -1,14 +1,17 @@\n package org.logstash.instruments.monitors;\n \n \n-import org.junit.Test;\n-import org.logstash.instrument.monitors.HotThreadsMonitor;\n-\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+import org.junit.Test;\n+import org.logstash.instrument.monitors.HotThreadsMonitor;\n \n-import static org.hamcrest.CoreMatchers.*;\n+import static org.hamcrest.CoreMatchers.hasItem;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n import static org.hamcrest.MatcherAssert.assertThat;\n \n \n@@ -45,17 +48,38 @@ public void testAllThreadsHaveStackTraces(){\n     }\n \n     @Test\n-    public void testStackTraceSizeOption(){\n+    public void testStackTraceSizeOption() throws InterruptedException {\n         final String testStackSize = \"4\";\n-        Map<String, String> options = new HashMap<>();\n-        options.put(\"stacktrace_size\", testStackSize);\n-        HotThreadsMonitor.detect(options).stream().filter(tr -> !tr.getThreadName().equals(\"Signal Dispatcher\") &&\n-                                                                      !tr.getThreadName().equals(\"Reference Handler\") &&\n-                                                                            !tr.getThreadName().equals(\"Attach Listener\"))\n-                                                        .forEach(tr -> {\n-            List stackTrace = (List)tr.toMap().get(\"thread.stacktrace\");\n-            assertThat(stackTrace.size(), is(Integer.valueOf(testStackSize)));\n-        });\n+        final CountDownLatch latch = new CountDownLatch(1);\n+        final Thread thread = new Thread() {\n+            @Override\n+            public void run() {\n+                waitEnd();\n+            }\n+\n+            void waitEnd() {\n+                try {\n+                    latch.await();\n+                } catch (final InterruptedException ex) {\n+                    throw new IllegalArgumentException(ex);\n+                }\n+            }\n+        };\n+        try {\n+            thread.start();\n+            TimeUnit.MILLISECONDS.sleep(300L);\n+            final Map<String, String> options = new HashMap<>();\n+            options.put(\"stacktrace_size\", testStackSize);\n+            assertThat(\n+                ((List) HotThreadsMonitor.detect(options).stream()\n+                    .filter(tr -> thread.getName().equals(tr.getThreadName())).findFirst()\n+                    .get().toMap().get(\"thread.stacktrace\")).size(),\n+                is(Integer.parseInt(testStackSize))\n+            );\n+        } finally {\n+            latch.countDown();\n+            thread.join();\n+        }\n     }\n \n     @Test",
      "parent_sha": "68b22280bc11845383f53f050a2cbcdd776884cc"
    }
  },
  {
    "oid": "4993c37e024e49d7ef14dcb3bca95986739becfa",
    "message": "Changed the deletion of log files to use retry mechanism and avoid flaky errors on Windows builds (related to #11307)\n\nFixes #11387",
    "date": "2019-12-04T10:50:37Z",
    "url": "https://github.com/elastic/logstash/commit/4993c37e024e49d7ef14dcb3bca95986739becfa",
    "details": {
      "sha": "f7bd39fbbc3c77410789d232cf3b9e6f4d81c6f2",
      "filename": "logstash-core/src/test/java/org/logstash/log/LogTestUtils.java",
      "status": "modified",
      "additions": 27,
      "deletions": 1,
      "changes": 28,
      "blob_url": "https://github.com/elastic/logstash/blob/4993c37e024e49d7ef14dcb3bca95986739becfa/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Flog%2FLogTestUtils.java",
      "raw_url": "https://github.com/elastic/logstash/raw/4993c37e024e49d7ef14dcb3bca95986739becfa/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Flog%2FLogTestUtils.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Flog%2FLogTestUtils.java?ref=4993c37e024e49d7ef14dcb3bca95986739becfa",
      "patch": "@@ -3,6 +3,7 @@\n import org.apache.logging.log4j.core.LoggerContext;\n \n import java.io.IOException;\n+import java.nio.file.FileSystemException;\n import java.nio.file.FileSystems;\n import java.nio.file.Files;\n import java.nio.file.Path;\n@@ -29,6 +30,31 @@ static void reloadLogConfiguration() {\n     static void deleteLogFile(String logfileName) throws IOException {\n         Path path = FileSystems.getDefault()\n                 .getPath(System.getProperty(\"user.dir\"), System.getProperty(\"ls.logs\"), logfileName);\n-        Files.deleteIfExists(path);\n+        pollingDelete(path, 5, TimeUnit.SECONDS);\n+    }\n+\n+    static void pollingDelete(Path path, int sleep, TimeUnit timeUnit) throws IOException {\n+        final int maxRetries = 5;\n+        int retries = 0;\n+        do {\n+            try {\n+                Files.deleteIfExists(path);\n+                break;\n+            } catch (FileSystemException fsex) {\n+                System.out.println(\"FS access error while deleting, \" + fsex.getReason() + \" deleting: \" + fsex.getOtherFile());\n+            }\n+\n+            try {\n+                Thread.sleep(timeUnit.toMillis(sleep));\n+            } catch (InterruptedException e) {\n+                // follows up\n+                Thread.currentThread().interrupt();\n+                break;\n+            }\n+\n+            retries++;\n+        } while (retries < maxRetries);\n+\n+        assertTrue(\"Exhausted 5 retries to delete the file: \" + path,retries < maxRetries);\n     }\n }",
      "parent_sha": "cbfc945d745bc7eb6e317b59b26eff236b185694"
    }
  },
  {
    "oid": "fa2d18a6c611bfc379ac3a44c4b5af332919553c",
    "message": "bck\n\nFixes #9538",
    "date": "2018-05-11T16:54:54Z",
    "url": "https://github.com/elastic/logstash/commit/fa2d18a6c611bfc379ac3a44c4b5af332919553c",
    "details": {
      "sha": "7e919d28753e7974e782606c1d0291080efaff61",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/QueueUpgrade.java",
      "status": "modified",
      "additions": 75,
      "deletions": 52,
      "changes": 127,
      "blob_url": "https://github.com/elastic/logstash/blob/fa2d18a6c611bfc379ac3a44c4b5af332919553c/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java",
      "raw_url": "https://github.com/elastic/logstash/raw/fa2d18a6c611bfc379ac3a44c4b5af332919553c/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java?ref=fa2d18a6c611bfc379ac3a44c4b5af332919553c",
      "patch": "@@ -8,19 +8,20 @@\n import java.nio.file.Files;\n import java.nio.file.Path;\n import java.nio.file.StandardOpenOption;\n-import java.util.ArrayList;\n import java.util.Collection;\n-import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.logstash.Event;\n import org.logstash.ackedqueue.io.CheckpointIO;\n import org.logstash.ackedqueue.io.FileCheckpointIO;\n import org.logstash.ackedqueue.io.MmapPageIOV1;\n import org.logstash.ackedqueue.io.MmapPageIOV2;\n+import org.logstash.ackedqueue.io.PageIO;\n \n-public final class QueueUpgrade {\n+final class QueueUpgrade {\n \n     private static final Logger LOGGER = LogManager.getLogger(QueueUpgrade.class);\n \n@@ -32,64 +33,86 @@ private QueueUpgrade() {\n \n     public static void upgradeQueueDirectoryToV2(final Path path) throws IOException {\n         final File upgradeFile = path.resolve(\".queue-version\").toFile();\n-        if (!upgradeFile.exists()) {\n+        if (upgradeFile.exists()) {\n+            if (Ints.fromByteArray(Files.readAllBytes(upgradeFile.toPath())) != 2) {\n+                throw new IllegalStateException(\n+                    \"Unexpected upgrade file contents found.\"\n+                );\n+            }\n+            LOGGER.debug(\"PQ version file with correct version information (v2) found.\");\n+        } else {\n             LOGGER.info(\"No PQ version file found, upgrading to PQ v2.\");\n             try (final DirectoryStream<Path> files = Files.newDirectoryStream(path)) {\n-                final Collection<File> oldFiles = new ArrayList<>();\n-                files.forEach(file -> {\n-                    final Matcher matcher = PAGE_NAME_PATTERN.matcher(file.getFileName().toString());\n-                    if (matcher.matches()) {\n-                        oldFiles.add(file.toFile());\n-                    }\n-                });\n+                final Collection<File> pageFiles = StreamSupport.stream(\n+                    files.spliterator(), false\n+                ).filter(\n+                    file -> PAGE_NAME_PATTERN.matcher(file.getFileName().toString()).matches()\n+                ).map(Path::toFile).collect(Collectors.toList());\n                 final CheckpointIO cpIo = new FileCheckpointIO(path);\n-                for (final File v1PageFile : oldFiles) {\n-                    final int num =\n-                        Integer.parseInt(v1PageFile.getName().substring(\"page.\".length()));\n-                    try (final MmapPageIOV1 iov1 = new MmapPageIOV1(\n-                        num, Ints.checkedCast(v1PageFile.length()), path\n-                    )) {\n-                        final String cpFilename = cpIo.tailFileName(num);\n-                        final Checkpoint cp;\n-                        if (path.resolve(cpFilename).toFile().exists()) {\n-                            cp = cpIo.read(cpFilename);\n-                        } else {\n-                            cp = cpIo.read(\"checkpoint.head\");\n-                        }\n-                        final int count = cp.getElementCount();\n-                        final long minSeqNum = cp.getMinSeqNum();\n-                        iov1.open(minSeqNum, count);\n-                        for (int i = 0; i < count; ++i) {\n-                            try {\n-                                Event.deserialize(\n-                                    iov1.read(minSeqNum + 1L, 1).getElements().get(0)\n-                                );\n-                            } catch (final IOException ex) {\n-                                throw new IllegalStateException(\n-                                    \"Logstash was unable to upgrade your persistent queue.\" +\n-                                        \"Please either downgrade to version 6.2.3 and fully drain \" +\n-                                        \"your persistent queue or delete your queue data.dir if you \" +\n-                                        \"don't need to retain the data currently in your queue.\", ex\n-                                );\n-                            }\n-                        }\n-                    }\n-                }\n-                for (final File v1PageFile : oldFiles) {\n-                    try (final RandomAccessFile raf = new RandomAccessFile(v1PageFile, \"rw\")) {\n-                        raf.seek(0L);\n-                        raf.writeByte((int) MmapPageIOV2.VERSION_TWO);\n-                    }\n-                }\n+                pageFiles.forEach(p -> validatePageFile(path, cpIo, p));\n+                pageFiles.forEach(QueueUpgrade::setV2);\n             }\n             Files.write(upgradeFile.toPath(), Ints.toByteArray(2), StandardOpenOption.CREATE);\n+        }\n+    }\n+\n+    private static void validatePageFile(final Path path, final CheckpointIO cpIo, final File v1PageFile) {\n+        final int num =\n+            Integer.parseInt(v1PageFile.getName().substring(\"page.\".length()));\n+        try (final MmapPageIOV1 iov1 = new MmapPageIOV1(\n+            num, Ints.checkedCast(v1PageFile.length()), path\n+        )) {\n+            final Checkpoint cp = loadCheckpoint(path, cpIo, num);\n+            final int count = cp.getElementCount();\n+            final long minSeqNum = cp.getMinSeqNum();\n+            iov1.open(minSeqNum, count);\n+            for (int i = 0; i < count; ++i) {\n+                verifyEvent(iov1, minSeqNum + i);\n+            }\n+        } catch (final IOException ex) {\n+            throw new IllegalStateException(ex);\n+        }\n+    }\n+\n+    private static void verifyEvent(final PageIO iov1, final long seqNum) {\n+        try {\n+            Event.deserialize(iov1.read(seqNum, 1).getElements().get(0));\n+        } catch (final IOException ex) {\n+            failValidation(ex);\n+        }\n+    }\n+\n+    private static void setV2(final File v1PageFile) {\n+        try (final RandomAccessFile raf = new RandomAccessFile(v1PageFile, \"rw\")) {\n+            raf.seek(0L);\n+            raf.writeByte((int) MmapPageIOV2.VERSION_TWO);\n+        } catch (final IOException ex) {\n+            throw new IllegalStateException(ex);\n+        }\n+    }\n+\n+    private static Checkpoint loadCheckpoint(final Path path, final CheckpointIO cpIo,\n+        final int num) throws IOException {\n+        final String cpFilename = cpIo.tailFileName(num);\n+        final Checkpoint cp;\n+        if (path.resolve(cpFilename).toFile().exists()) {\n+            cp = cpIo.read(cpFilename);\n         } else {\n-            if (Ints.fromByteArray(Files.readAllBytes(upgradeFile.toPath())) != 2) {\n+            cp = cpIo.read(\"checkpoint.head\");\n+            if (cp.getPageNum() != num) {\n                 throw new IllegalStateException(\n-                    \"Unexpected upgrade file contents found.\"\n+                    String.format(\"No checkpoint file found for page %d\", num)\n                 );\n             }\n-            LOGGER.debug(\"PQ version file with correct version information (v2) found.\");\n         }\n+        return cp;\n+    }\n+\n+    private static void failValidation(final Throwable ex) {\n+        LOGGER.error(\"Logstash was unable to upgrade your persistent queue data.\" +\n+            \"Please either downgrade to version 6.2.3 and fully drain \" +\n+            \"your persistent queue or delete your queue data.dir if you \" +\n+            \"don't need to retain the data currently in your queue.\");\n+        throw new IllegalStateException(ex);\n     }\n }",
      "parent_sha": "ad79d7c884064f8b45ded278ae928fd490c335fe"
    }
  },
  {
    "oid": "dbf241285e8cb19e0b13fdba6fc8803c1103126b",
    "message": "generalize method parameters\n\nFixes #10133",
    "date": "2018-11-08T22:20:53Z",
    "url": "https://github.com/elastic/logstash/commit/dbf241285e8cb19e0b13fdba6fc8803c1103126b",
    "details": {
      "sha": "8165258e568a02f3253b4b1a9539eada5189fa13",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/compiler/Utils.java",
      "status": "modified",
      "additions": 3,
      "deletions": 4,
      "changes": 7,
      "blob_url": "https://github.com/elastic/logstash/blob/dbf241285e8cb19e0b13fdba6fc8803c1103126b/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FUtils.java",
      "raw_url": "https://github.com/elastic/logstash/raw/dbf241285e8cb19e0b13fdba6fc8803c1103126b/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FUtils.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FUtils.java?ref=dbf241285e8cb19e0b13fdba6fc8803c1103126b",
      "patch": "@@ -1,18 +1,17 @@\n package org.logstash.config.ir.compiler;\n \n-import org.jruby.RubyArray;\n import org.logstash.ext.JrubyEventExtLibrary;\n \n-import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.List;\n \n /**\n  * Static utility methods that replace common blocks of generated code in the Java execution.\n  */\n public class Utils {\n \n     // has field1.compute(batchArg, flushArg, shutdownArg) passed as input\n-    public static void copyNonCancelledEvents(Collection<JrubyEventExtLibrary.RubyEvent> input, RubyArray output) {\n+    public static void copyNonCancelledEvents(Collection<JrubyEventExtLibrary.RubyEvent> input, List output) {\n         for (JrubyEventExtLibrary.RubyEvent e : input) {\n             if (!(e.getEvent().isCancelled())) {\n                 output.add(e);\n@@ -21,7 +20,7 @@ public static void copyNonCancelledEvents(Collection<JrubyEventExtLibrary.RubyEv\n     }\n \n     public static void filterEvents(Collection<JrubyEventExtLibrary.RubyEvent> input, EventCondition filter,\n-                                    ArrayList fulfilled, ArrayList unfulfilled) {\n+                                    List fulfilled, List unfulfilled) {\n         for (JrubyEventExtLibrary.RubyEvent e : input) {\n             if (filter.fulfilled(e)) {\n                 fulfilled.add(e);",
      "parent_sha": "4bdff7a9fc784ed9b55d20ef6fa0e2b656eefe5c"
    }
  },
  {
    "oid": "435466b6b0149af5172ee61da3c109e17c2d3734",
    "message": "Close channel associated with lock\n\nFix Windows build issue where .lock file cannot be deleted due to\nunderling channel associated with lock not being closed. This\nwill stop the file from being deleted until the JVM exits, despite\nreporting through Java that the file *has* been deleted and does not\nexist. It also blocks new files from being created with the same\nfilename, which caused the test failure here.\n\nFixes #7822",
    "date": "2017-07-26T20:55:14Z",
    "url": "https://github.com/elastic/logstash/commit/435466b6b0149af5172ee61da3c109e17c2d3734",
    "details": {
      "sha": "68733241881a6bea446d7bb364086b2d674f95d8",
      "filename": "logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java",
      "status": "modified",
      "additions": 6,
      "deletions": 1,
      "changes": 7,
      "blob_url": "https://github.com/elastic/logstash/blob/435466b6b0149af5172ee61da3c109e17c2d3734/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "raw_url": "https://github.com/elastic/logstash/raw/435466b6b0149af5172ee61da3c109e17c2d3734/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java?ref=435466b6b0149af5172ee61da3c109e17c2d3734",
      "patch": "@@ -146,7 +146,12 @@ private void innerWriteEntry(DLQEntry entry) throws IOException {\n \n     @Override\n     public synchronized void close() throws IOException {\n-        this.lock.release();\n+        if (this.lock != null){\n+            this.lock.release();\n+            if (this.lock.channel() != null && this.lock.channel().isOpen()) {\n+                this.lock.channel().close();\n+            }\n+        }\n         if (currentWriter != null) {\n             currentWriter.close();\n         }",
      "parent_sha": "291518d74146bf167cfda3e18f92f25202c0ed3a"
    }
  },
  {
    "oid": "f1b6d7c2611fbe6edf2b1209662ef677525cc754",
    "message": "Updating remaining unit tests to use more accurate+concise assertions\n\nFixes #9314",
    "date": "2018-04-04T17:50:02Z",
    "url": "https://github.com/elastic/logstash/commit/f1b6d7c2611fbe6edf2b1209662ef677525cc754",
    "details": {
      "sha": "0275f8e6e3edbfcb15d7ea504dbea3d99f5346c1",
      "filename": "logstash-core/src/test/java/org/logstash/config/ir/imperative/IfStatementTest.java",
      "status": "modified",
      "additions": 26,
      "deletions": 33,
      "changes": 59,
      "blob_url": "https://github.com/elastic/logstash/blob/f1b6d7c2611fbe6edf2b1209662ef677525cc754/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fimperative%2FIfStatementTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/f1b6d7c2611fbe6edf2b1209662ef677525cc754/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fimperative%2FIfStatementTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fimperative%2FIfStatementTest.java?ref=f1b6d7c2611fbe6edf2b1209662ef677525cc754",
      "patch": "@@ -62,8 +62,10 @@ public void testIfWithOneTrueStatement() throws InvalidIRException {\n \n     @Test\n     public void testIfWithOneFalseStatement() throws InvalidIRException {\n+        PluginDefinition pluginDef = testPluginDefinition();\n         Statement trueStatement = new NoopStatement(randMeta());\n-        Statement falseStatement = new PluginStatement(randMeta(), testPluginDefinition());\n+        Statement falseStatement = new PluginStatement(randMeta(), pluginDef);\n+        BooleanExpression ifExpression = createTestExpression();\n         IfStatement ifStatement = new IfStatement(\n                 randMeta(),\n                 createTestExpression(),\n@@ -74,27 +76,22 @@ public void testIfWithOneFalseStatement() throws InvalidIRException {\n         Graph ifStatementGraph = ifStatement.toGraph();\n         assertFalse(ifStatementGraph.isEmpty());\n \n-        Stream<Vertex> trueVertices = ifStatementGraph\n-                .edges()\n-                .filter(e -> e instanceof BooleanEdge)\n-                .map(e -> (BooleanEdge) e)\n-                .filter(e -> e.getEdgeType() == true)\n-                .map(e -> e.getTo());\n-        assertEquals(0, trueVertices.count());\n-\n-        Stream<Vertex> falseVertices = ifStatementGraph\n-                .edges()\n-                .filter(e -> e instanceof BooleanEdge)\n-                .map(e -> (BooleanEdge) e)\n-                .filter(e -> e.getEdgeType() == false)\n-                .map(e -> e.getTo());\n-        assertEquals(1, falseVertices.count());\n+        Graph expected = new Graph();\n+        IfVertex expectedIf = DSL.gIf(randMeta(), ifExpression);\n+        expected.addVertex(expectedIf);\n+\n+        PluginVertex expectedF = DSL.gPlugin(randMeta(), pluginDef);\n+        expected.chainVertices(false, expectedIf, expectedF);\n+\n+        assertSyntaxEquals(expected, ifStatementGraph);\n     }\n \n     @Test\n     public void testIfWithOneTrueOneFalseStatement() throws InvalidIRException {\n-        Statement trueStatement = new PluginStatement(randMeta(), testPluginDefinition());\n-        Statement falseStatement = new PluginStatement(randMeta(), testPluginDefinition());\n+        PluginDefinition pluginDef = testPluginDefinition();\n+        Statement trueStatement = new PluginStatement(randMeta(), pluginDef);\n+        Statement falseStatement = new PluginStatement(randMeta(), pluginDef);\n+        BooleanExpression ifExpression = createTestExpression();\n         IfStatement ifStatement = new IfStatement(\n                 randMeta(),\n                 createTestExpression(),\n@@ -105,20 +102,16 @@ public void testIfWithOneTrueOneFalseStatement() throws InvalidIRException {\n         Graph ifStatementGraph = ifStatement.toGraph();\n         assertFalse(ifStatementGraph.isEmpty());\n \n-        Stream<Vertex> trueVertices = ifStatementGraph\n-                .edges()\n-                .filter(e -> e instanceof BooleanEdge)\n-                .map(e -> (BooleanEdge) e)\n-                .filter(e -> e.getEdgeType() == true)\n-                .map(e -> e.getTo());\n-        assertEquals(1, trueVertices.count());\n-\n-        Stream<Vertex> falseVertices = ifStatementGraph\n-                .edges()\n-                .filter(e -> e instanceof BooleanEdge)\n-                .map(e -> (BooleanEdge) e)\n-                .filter(e -> e.getEdgeType() == false)\n-                .map(e -> e.getTo());\n-        assertEquals(1, falseVertices.count());\n+        Graph expected = new Graph();\n+        IfVertex expectedIf = DSL.gIf(randMeta(), ifExpression);\n+        expected.addVertex(expectedIf);\n+\n+        PluginVertex expectedT = DSL.gPlugin(randMeta(), pluginDef);\n+        expected.chainVertices(true, expectedIf, expectedT);\n+\n+        PluginVertex expectedF = DSL.gPlugin(randMeta(), pluginDef);\n+        expected.chainVertices(false, expectedIf, expectedF);\n+\n+        assertSyntaxEquals(expected, ifStatementGraph);\n     }\n }",
      "parent_sha": "cf16429885eeaedef3a377e50a98be024012778b"
    }
  },
  {
    "oid": "39a0df1723ea3f0de79e9ea59e054a14dbec4c28",
    "message": "MINOR: Removed redundant contant\n\nFixes #7289",
    "date": "2017-06-02T09:41:57Z",
    "url": "https://github.com/elastic/logstash/commit/39a0df1723ea3f0de79e9ea59e054a14dbec4c28",
    "details": {
      "sha": "55e7ac8b867be68e7afe86796dc148fc28086a06",
      "filename": "logstash-core/src/main/java/org/logstash/PathCache.java",
      "status": "modified",
      "additions": 2,
      "deletions": 4,
      "changes": 6,
      "blob_url": "https://github.com/elastic/logstash/blob/39a0df1723ea3f0de79e9ea59e054a14dbec4c28/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FPathCache.java",
      "raw_url": "https://github.com/elastic/logstash/raw/39a0df1723ea3f0de79e9ea59e054a14dbec4c28/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FPathCache.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FPathCache.java?ref=39a0df1723ea3f0de79e9ea59e054a14dbec4c28",
      "patch": "@@ -9,13 +9,11 @@ public class PathCache {\n \n     private FieldReference timestamp;\n \n-    // TODO: dry with Event\n-    public static final String TIMESTAMP = \"@timestamp\";\n-    public static final String BRACKETS_TIMESTAMP = \"[\" + TIMESTAMP + \"]\";\n+    private static final String BRACKETS_TIMESTAMP = \"[\" + Event.TIMESTAMP + \"]\";\n \n     protected PathCache() {\n         // inject @timestamp\n-        this.timestamp = cache(TIMESTAMP);\n+        this.timestamp = cache(Event.TIMESTAMP);\n         cache(BRACKETS_TIMESTAMP, this.timestamp);\n     }\n ",
      "parent_sha": "46bfec52fe0b43eb7121637dfbb4f6a3d51bfd9a"
    }
  },
  {
    "oid": "a3a077c34818bbba113386f66e7abe43bee45f3d",
    "message": "#7184 avoid broken format being used in test\n\nFixes #7262",
    "date": "2017-06-02T21:43:59Z",
    "url": "https://github.com/elastic/logstash/commit/a3a077c34818bbba113386f66e7abe43bee45f3d",
    "details": {
      "sha": "f6d6fee429eedc3cbe89b75562b1c503f05817e0",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/elastic/logstash/blob/a3a077c34818bbba113386f66e7abe43bee45f3d/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/a3a077c34818bbba113386f66e7abe43bee45f3d/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=a3a077c34818bbba113386f66e7abe43bee45f3d",
      "patch": "@@ -297,7 +297,9 @@ public void randomAcking() throws IOException {\n         // 10 tests of random queue sizes\n         for (int loop = 0; loop < 10; loop++) {\n             int page_count = random.nextInt(10000) + 1;\n-            int digits = new Double(Math.ceil(Math.log10(page_count))).intValue();\n+\n+            // String format call below needs to at least print one digit\n+            final int digits = Math.max((int) Math.ceil(Math.log10(page_count)), 1);\n \n             // create a queue with a single element per page\n             List<Queueable> elements = new ArrayList<>();",
      "parent_sha": "280722ea70e3ddca96204d44cd14bb37d789fc7e"
    }
  },
  {
    "oid": "e8d6ae5e4c20ec6df1158518279203b493a483ab",
    "message": "test: no-op refactor to avoid repeating implementation in test",
    "date": "2020-08-06T18:11:47Z",
    "url": "https://github.com/elastic/logstash/commit/e8d6ae5e4c20ec6df1158518279203b493a483ab",
    "details": {
      "sha": "685f9bbe8fad7313c9c62fdb5ba83778b6d7e8e3",
      "filename": "logstash-core/src/test/java/org/logstash/config/ir/PipelineConfigTest.java",
      "status": "modified",
      "additions": 30,
      "deletions": 11,
      "changes": 41,
      "blob_url": "https://github.com/elastic/logstash/blob/e8d6ae5e4c20ec6df1158518279203b493a483ab/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FPipelineConfigTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/e8d6ae5e4c20ec6df1158518279203b493a483ab/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FPipelineConfigTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FPipelineConfigTest.java?ref=e8d6ae5e4c20ec6df1158518279203b493a483ab",
      "patch": "@@ -77,23 +77,42 @@ public void describeTo(Description description) {\n             \"  generator1\\n\" +\n             \"}\";\n \n+    static class SourceCollector {\n+        private final StringBuilder compositeSource = new StringBuilder();\n+        private final List<SourceWithMetadata> orderedConfigParts = new ArrayList<>();\n+\n+        void appendSource(final String protocol, final String id, final int line, final int column, final String text) throws IncompleteSourceWithMetadataException {\n+            orderedConfigParts.add(new SourceWithMetadata(protocol, id, line, column, text));\n+            compositeSource.append(text);\n+            compositeSource.append(\"\\n\");\n+        }\n+\n+        String compositeSource() {\n+            return this.compositeSource.toString();\n+        }\n+\n+        SourceWithMetadata[] orderedConfigParts() {\n+            return this.orderedConfigParts.toArray(new SourceWithMetadata[]{});\n+        }\n+    }\n+\n     @Before\n     public void setUp() throws IncompleteSourceWithMetadataException {\n \n         source = RubyUtil.RUBY.getClass(\"LogStash::Config::Source::Local\");\n         pipelineIdSym = RubySymbol.newSymbol(RubyUtil.RUBY, PIPELINE_ID);\n \n-        orderedConfigParts = new SourceWithMetadata[]{\n-                new SourceWithMetadata(\"file\", \"/tmp/1\", 0, 0, \"input { generator1 }\"),\n-                new SourceWithMetadata(\"file\", \"/tmp/2\", 0, 0, \"input { generator2 }\"),\n-                new SourceWithMetadata(\"file\", \"/tmp/3\", 0, 0, \"input { generator3 }\"),\n-                new SourceWithMetadata(\"file\", \"/tmp/4\", 0, 0, \"input { generator4 }\"),\n-                new SourceWithMetadata(\"file\", \"/tmp/5\", 0, 0, \"input { generator5 }\"),\n-                new SourceWithMetadata(\"file\", \"/tmp/6\", 0, 0, \"input { generator6 }\"),\n-                new SourceWithMetadata(\"string\", \"config_string\", 0, 0, \"input { generator1 }\"),\n-        };\n-\n-        configMerged = Arrays.stream(orderedConfigParts).map(SourceWithMetadata::getText).collect(Collectors.joining(\"\\n\"));\n+        final SourceCollector sourceCollector = new SourceCollector();\n+        sourceCollector.appendSource(\"file\", \"/tmp/1\", 0, 0, \"input { generator1 }\");\n+        sourceCollector.appendSource(\"file\", \"/tmp/2\", 0, 0, \"input { generator2 }\");\n+        sourceCollector.appendSource(\"file\", \"/tmp/3\", 0, 0, \"input { generator3 }\");\n+        sourceCollector.appendSource(\"file\", \"/tmp/4\", 0, 0, \"input { generator4 }\");\n+        sourceCollector.appendSource(\"file\", \"/tmp/5\", 0, 0, \"input { generator5 }\");\n+        sourceCollector.appendSource(\"file\", \"/tmp/6\", 0, 0, \"input { generator6 }\");\n+        sourceCollector.appendSource(\"string\", \"config_string\", 0, 0, \"input { generator1 }\");\n+\n+        orderedConfigParts = sourceCollector.orderedConfigParts();\n+        configMerged = sourceCollector.compositeSource();\n \n         List<SourceWithMetadata> unorderedList = Arrays.asList(orderedConfigParts);\n         Collections.shuffle(unorderedList);",
      "parent_sha": "95e386e4158a3ab09907d4ebf22adb3f79bff720"
    }
  },
  {
    "oid": "c2a10417c49b82b1102c2a426cce94027c5d32af",
    "message": "Fix: Logstash fails to run if data.path is a symlink\n\nFixes #9706",
    "date": "2018-06-20T12:13:06Z",
    "url": "https://github.com/elastic/logstash/commit/c2a10417c49b82b1102c2a426cce94027c5d32af",
    "details": {
      "sha": "8b41315e12117d6cbb58dba5daaa7551771ce6ec",
      "filename": "logstash-core/src/main/java/org/logstash/FileLockFactory.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/c2a10417c49b82b1102c2a426cce94027c5d32af/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FFileLockFactory.java",
      "raw_url": "https://github.com/elastic/logstash/raw/c2a10417c49b82b1102c2a426cce94027c5d32af/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FFileLockFactory.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FFileLockFactory.java?ref=c2a10417c49b82b1102c2a426cce94027c5d32af",
      "patch": "@@ -49,7 +49,7 @@ private FileLockFactory() {}\n     private static final Map<FileLock, String> LOCK_MAP =  Collections.synchronizedMap(new HashMap<>());\n \n     public static FileLock obtainLock(Path dirPath, String lockName) throws IOException {\n-        Files.createDirectories(dirPath);\n+        if (!Files.isDirectory(dirPath)) { Files.createDirectories(dirPath); }\n         Path lockPath = dirPath.resolve(lockName);\n \n         try {",
      "parent_sha": "4bfbdb28b8b3d4373f0534e9f63752dbba3d585a"
    }
  },
  {
    "oid": "637f447b8850e380b72c0a16b446564a59f0a217",
    "message": "allow concurrent Batch deserialization (#17050)\n\nCurrently the deserialization is behind the readBatch's lock, so any large batch will take time deserializing, causing any other Queue writer (e.g. netty executor threads) and any other Queue reader (pipeline worker) to block.\r\n\r\nThis commit moves the deserialization out of the lock, allowing multiple pipeline workers to deserialize batches concurrently.\r\n\r\n- add intermediate batch-holder from `Queue` methods\r\n- make the intermediate batch-holder a private inner class of `Queue` with a descriptive name `SerializedBatchHolder`\r\n\r\nCo-authored-by: Ry Biesemeyer <yaauie@users.noreply.github.com>",
    "date": "2025-02-17T19:01:44Z",
    "url": "https://github.com/elastic/logstash/commit/637f447b8850e380b72c0a16b446564a59f0a217",
    "details": {
      "sha": "e232c99a5f8e7b722a18b53509aae44a20baa96b",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java",
      "status": "modified",
      "additions": 28,
      "deletions": 5,
      "changes": 33,
      "blob_url": "https://github.com/elastic/logstash/blob/637f447b8850e380b72c0a16b446564a59f0a217/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "raw_url": "https://github.com/elastic/logstash/raw/637f447b8850e380b72c0a16b446564a59f0a217/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java?ref=637f447b8850e380b72c0a16b446564a59f0a217",
      "patch": "@@ -590,13 +590,18 @@ public void ensurePersistedUpto(long seqNum) throws IOException{\n      * @throws IOException if an IO error occurs\n      */\n     public synchronized Batch nonBlockReadBatch(int limit) throws IOException {\n+        final SerializedBatchHolder serializedBatchHolder;\n         lock.lock();\n         try {\n             Page p = nextReadPage();\n-            return (isHeadPage(p) && p.isFullyRead()) ? null : readPageBatch(p, limit, 0L);\n+            if (isHeadPage(p) && p.isFullyRead()) {\n+                return null;\n+            }\n+            serializedBatchHolder = readPageBatch(p, limit, 0L);\n         } finally {\n             lock.unlock();\n         }\n+        return serializedBatchHolder.deserialize();\n     }\n \n     /**\n@@ -607,7 +612,11 @@ public synchronized Batch nonBlockReadBatch(int limit) throws IOException {\n      * @throws QueueRuntimeException if queue is closed\n      * @throws IOException if an IO error occurs\n      */\n-    public synchronized Batch readBatch(int limit, long timeout) throws IOException {\n+    public Batch readBatch(int limit, long timeout) throws IOException {\n+        return readSerializedBatch(limit, timeout).deserialize();\n+    }\n+\n+    private synchronized SerializedBatchHolder readSerializedBatch(int limit, long timeout) throws IOException {\n         lock.lock();\n \n         try {\n@@ -618,15 +627,15 @@ public synchronized Batch readBatch(int limit, long timeout) throws IOException\n     }\n \n     /**\n-     * read a {@link Batch} from the given {@link Page}. If the page is a head page, try to maximize the\n+     * read a {@link SerializedBatchHolder} from the given {@link Page}. If the page is a head page, try to maximize the\n      * batch size by waiting for writes.\n      * @param p the {@link Page} to read from.\n      * @param limit size limit of the batch to read.\n      * @param timeout  the maximum time to wait in milliseconds on write operations.\n      * @return {@link Batch} with read elements or null if nothing was read\n      * @throws IOException if an IO error occurs\n      */\n-    private Batch readPageBatch(Page p, int limit, long timeout) throws IOException {\n+    private SerializedBatchHolder readPageBatch(Page p, int limit, long timeout) throws IOException {\n         int left = limit;\n         final List<byte[]> elements = new ArrayList<>(limit);\n \n@@ -678,7 +687,7 @@ private Batch readPageBatch(Page p, int limit, long timeout) throws IOException\n             removeUnreadPage(p);\n         }\n \n-        return new Batch(elements, firstSeqNum, this);\n+        return new SerializedBatchHolder(elements, firstSeqNum);\n     }\n \n     /**\n@@ -894,4 +903,18 @@ private static boolean containsSeq(final Page page, final long seqNum) {\n         final long pMaxSeq = pMinSeq + (long) page.getElementCount();\n         return seqNum >= pMinSeq && seqNum < pMaxSeq;\n     }\n+\n+    class SerializedBatchHolder {\n+        private final List<byte[]> elements;\n+        private final long firstSeqNum;\n+\n+        private SerializedBatchHolder(List<byte[]> elements, long firstSeqNum) {\n+            this.elements = elements;\n+            this.firstSeqNum = firstSeqNum;\n+        }\n+\n+        private Batch deserialize() {\n+            return new Batch(elements, firstSeqNum, Queue.this);\n+        }\n+    }\n }",
      "parent_sha": "e896cd727dda41a0bda9ce6e65c4fb391c3d1b4f"
    }
  },
  {
    "oid": "db9ab809a7ceaa0929a4fa50034ce013d699bfc4",
    "message": "update RecordIOWriter javadoc to further describe file-structure",
    "date": "2017-05-31T18:05:13Z",
    "url": "https://github.com/elastic/logstash/commit/db9ab809a7ceaa0929a4fa50034ce013d699bfc4",
    "details": {
      "sha": "7bd8f56d54382c11a17b97d1a74103b65cb8d30e",
      "filename": "logstash-core/src/main/java/org/logstash/common/io/RecordIOWriter.java",
      "status": "modified",
      "additions": 23,
      "deletions": 8,
      "changes": 31,
      "blob_url": "https://github.com/elastic/logstash/blob/db9ab809a7ceaa0929a4fa50034ce013d699bfc4/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FRecordIOWriter.java",
      "raw_url": "https://github.com/elastic/logstash/raw/db9ab809a7ceaa0929a4fa50034ce013d699bfc4/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FRecordIOWriter.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FRecordIOWriter.java?ref=db9ab809a7ceaa0929a4fa50034ce013d699bfc4",
      "patch": "@@ -34,19 +34,34 @@\n \n /**\n  *\n- * File Format\n- * | \u2014 magic number (4bytes) --|\n+ * RecordIO File Format: A file that is divided up into equal-sized blocks representing\n+ * parts of a sequence of Logstash Events so that it is easy to binary-search across to find\n+ * specific records based on some sort-value.\n  *\n- * [  32kbyte block....\n- *    --- 1 byte RecordHeader Type ---\n- *    --- 4 byte RecordHeader Size ---\n+ * At a high level, each recordIO file contains an initial version byte\n+ * and then 32kb record block sizes\n  *\n- * ]\n- * [ 32kbyte block...\n+ * |- VERSION (1 byte) -|- 32kb event block -|- 32kb event block -|...\n  *\n+ * Each 32kb event block contains different record types prepended by their\n+ * respective record headers\n  *\n+ * |- record header (13 bytes) -|- record type (varlength) -|\n  *\n- * ]\n+ * Record Header:\n+ *\n+ * |- record type -|- record size -|- total LS event size -|- checksum -|\n+ *\n+ * LS Events are split up into different record types because one event may be larger than the 32kb block\n+ * allotted. Therefore, we need to cut up the LS Event into different types so that we can more easily piece them\n+ * together when reading the RecordIO file.\n+ *\n+ * There are four different {@link RecordType} definitions:\n+ *   START: The start of an Event that was broken up into different records\n+ *   COMPLETE: A record representing the fully serialized LS Event\n+ *   MIDDLE: A middle record of one or multiple middle records representing a segment of an LS Event that will be proceeded\n+ *           by a final END record type.\n+ *   END: The final record segment of an LS Event, the final record representing the end of an LS Event.\n  */\n public class RecordIOWriter {\n ",
      "parent_sha": "e01d247d5ed2442dee678c2b30b71702b8198825"
    }
  },
  {
    "oid": "fbb167751e3e26f3dc5c094d22103ccc577a0d5d",
    "message": "Correctly calculate millisecond duration.\n\nFixes #9744.\n\nFixes #9753",
    "date": "2018-06-18T17:37:37Z",
    "url": "https://github.com/elastic/logstash/commit/fbb167751e3e26f3dc5c094d22103ccc577a0d5d",
    "details": {
      "sha": "5320ddcbf268fbf2337ca709b3a1b50ea5ef589a",
      "filename": "logstash-core/src/main/java/org/logstash/ext/JRubyWrappedWriteClientExt.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/elastic/logstash/blob/fbb167751e3e26f3dc5c094d22103ccc577a0d5d/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJRubyWrappedWriteClientExt.java",
      "raw_url": "https://github.com/elastic/logstash/raw/fbb167751e3e26f3dc5c094d22103ccc577a0d5d/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJRubyWrappedWriteClientExt.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJRubyWrappedWriteClientExt.java?ref=fbb167751e3e26f3dc5c094d22103ccc577a0d5d",
      "patch": "@@ -111,8 +111,8 @@ private void incrementCounters(final long count) {\n     }\n \n     private void incrementTimers(final long start) {\n-        final long increment = TimeUnit.NANOSECONDS.convert(\n-            System.nanoTime() - start, TimeUnit.MILLISECONDS\n+        final long increment = TimeUnit.MILLISECONDS.convert(\n+            System.nanoTime() - start, TimeUnit.NANOSECONDS\n         );\n         eventsMetricsTime.increment(increment);\n         pipelineMetricsTime.increment(increment);",
      "parent_sha": "99b5f87f86cf096328e0a3380224c05da07b39fc"
    }
  },
  {
    "oid": "398e64e2eb0d7bb663ceb8b0ed2dfd6686e67b03",
    "message": "Write generated Java files to disk only if debug flag is set\n\nFixes #11082",
    "date": "2019-08-28T14:24:12Z",
    "url": "https://github.com/elastic/logstash/commit/398e64e2eb0d7bb663ceb8b0ed2dfd6686e67b03",
    "details": {
      "sha": "bfd3bf56a408c82b4faec6889c5fe8e743a6009b",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/compiler/ComputeStepSyntaxElement.java",
      "status": "modified",
      "additions": 11,
      "deletions": 9,
      "changes": 20,
      "blob_url": "https://github.com/elastic/logstash/blob/398e64e2eb0d7bb663ceb8b0ed2dfd6686e67b03/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FComputeStepSyntaxElement.java",
      "raw_url": "https://github.com/elastic/logstash/raw/398e64e2eb0d7bb663ceb8b0ed2dfd6686e67b03/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FComputeStepSyntaxElement.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FComputeStepSyntaxElement.java?ref=398e64e2eb0d7bb663ceb8b0ed2dfd6686e67b03",
      "patch": "@@ -74,9 +74,13 @@ public T instantiate() {\n                 } else {\n                     final String name = String.format(\"CompiledDataset%d\", CLASS_CACHE.size());\n                     final String code = generateCode(name);\n-                    final Path sourceFile = SOURCE_DIR.resolve(String.format(\"%s.java\", name));\n-                    Files.write(sourceFile, code.getBytes(StandardCharsets.UTF_8));\n-                    COMPILER.cookFile(sourceFile.toFile());\n+                    if (SOURCE_DIR != null) {\n+                        final Path sourceFile = SOURCE_DIR.resolve(String.format(\"%s.java\", name));\n+                        Files.write(sourceFile, code.getBytes(StandardCharsets.UTF_8));\n+                        COMPILER.cookFile(sourceFile.toFile());\n+                    } else {\n+                        COMPILER.cook(code);\n+                    }\n                     COMPILER.setParentClassLoader(COMPILER.getClassLoader());\n                     clazz = (Class<T>) COMPILER.getClassLoader().loadClass(\n                         String.format(\"org.logstash.generated.%s\", name)\n@@ -125,17 +129,15 @@ private String generateCode(final String name) {\n     }\n \n     private static Path debugDir() {\n-        final Path sourceDir;\n+        Path sourceDir = null;\n         try {\n             final Path parentDir;\n             final String dir = System.getProperty(ICookable.SYSTEM_PROPERTY_SOURCE_DEBUGGING_DIR);\n-            if (dir == null) {\n-                parentDir = Files.createTempDirectory(\"logstash\");\n-            } else {\n+            if (dir != null) {\n                 parentDir = Paths.get(dir);\n+                sourceDir = parentDir.resolve(\"org\").resolve(\"logstash\").resolve(\"generated\");\n+                Files.createDirectories(sourceDir);\n             }\n-            sourceDir = parentDir.resolve(\"org\").resolve(\"logstash\").resolve(\"generated\");\n-            Files.createDirectories(sourceDir);\n         } catch (final IOException ex) {\n             throw new IllegalStateException(ex);\n         }",
      "parent_sha": "b66329f8ab49f34768de23c4f5d29b25ca0c41d1"
    }
  },
  {
    "oid": "c262d53765848406530cccd224435821ae058a1b",
    "message": "Move list offset calculation (for negative index values, etc) to a static method.\n\nFixes #6226",
    "date": "2016-12-06T23:33:44Z",
    "url": "https://github.com/elastic/logstash/commit/c262d53765848406530cccd224435821ae058a1b",
    "details": {
      "sha": "4b78c375123351637b6a8cebc33fd984f442889e",
      "filename": "logstash-core-event-java/src/main/java/org/logstash/Accessors.java",
      "status": "modified",
      "additions": 29,
      "deletions": 24,
      "changes": 53,
      "blob_url": "https://github.com/elastic/logstash/blob/c262d53765848406530cccd224435821ae058a1b/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FAccessors.java",
      "raw_url": "https://github.com/elastic/logstash/raw/c262d53765848406530cccd224435821ae058a1b/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FAccessors.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FAccessors.java?ref=c262d53765848406530cccd224435821ae058a1b",
      "patch": "@@ -34,14 +34,11 @@ public Object del(String reference) {\n                 return ((Map<String, Object>) target).remove(field.getKey());\n             } else if (target instanceof List) {\n                 int i = Integer.parseInt(field.getKey());\n-                int size = ((List) target).size();\n-                if (i >= size || i < -size) {\n-                  return null;\n-                } else if (i < 0) {\n-                  // Offset from the end of the array.\n-                  return ((List<Object>) target).remove(size + i);\n-                } else {\n-                  return ((List<Object>) target).remove(i);\n+                try {\n+                    int offset = listIndex(i, (List) target);\n+                    return ((List)target).remove(offset);\n+                } catch (IndexOutOfBoundsException e) {\n+                    return null;\n                 }\n             } else {\n                 throw newCollectionException(target);\n@@ -117,16 +114,13 @@ private Object findCreateTarget(FieldReference field) {\n     }\n \n     private boolean foundInList(List<Object> target, int index) {\n-        int size = ((List) target).size();\n-        if (index >= size || index < -size) {\n+        try {\n+            int offset = listIndex(index, target);\n+            return target.get(offset) != null;\n+        } catch (IndexOutOfBoundsException e) {\n             return false;\n         }\n \n-        if (index < 0) {\n-            index = size + index;\n-        }\n-\n-        return target.get(index) != null;\n     }\n \n     private boolean foundInMap(Map<String, Object> target, String key) {\n@@ -138,15 +132,11 @@ private Object fetch(Object target, String key) {\n             Object result = ((Map<String, Object>) target).get(key);\n             return result;\n         } else if (target instanceof List) {\n-            int i = Integer.parseInt(key);\n-            int size = ((List) target).size();\n-            if (i >= size || i < -size) {\n+            try {\n+                int offset = listIndex(Integer.parseInt(key), (List) target);\n+                return ((List<Object>) target).get(offset);\n+            } catch (IndexOutOfBoundsException e) {\n                 return null;\n-            } else if (i < 0) {\n-                // Offset from the end of the array.\n-                return ((List<Object>) target).get(size + i);\n-            } else {\n-                return ((List<Object>) target).get(i);\n             }\n         } else if (target == null) {\n             return null;\n@@ -171,7 +161,8 @@ private Object store(Object target, String key, Object value) {\n                 }\n                 ((List<Object>) target).add(value);\n             } else {\n-                ((List<Object>) target).set(i, value);\n+                int offset = listIndex(i, (List) target);\n+                ((List<Object>) target).set(offset, value);\n             }\n         } else {\n             throw newCollectionException(target);\n@@ -189,4 +180,18 @@ private boolean isCollection(Object target) {\n     private ClassCastException newCollectionException(Object target) {\n         return new ClassCastException(\"expecting List or Map, found \"  + target.getClass());\n     }\n+\n+    private static int listIndex(int i, List list) {\n+        int size = list.size();\n+\n+        if (i >= size || i < -size) {\n+            throw new IndexOutOfBoundsException(\"Index \" + i + \" is out of bounds for a list with size \" + size);\n+        }\n+\n+        if (i < 0) { // Offset from the end of the array.\n+            return size + i;\n+        } else {\n+            return i;\n+        }\n+    }\n }",
      "parent_sha": "235dc12c09c0dbc5be70191748c37d4f95373eb0"
    }
  },
  {
    "oid": "6886a14fb66139ca1118736a695eb115f1af2358",
    "message": "bck\n\nFixes #9538",
    "date": "2018-05-11T16:54:54Z",
    "url": "https://github.com/elastic/logstash/commit/6886a14fb66139ca1118736a695eb115f1af2358",
    "details": {
      "sha": "81ce3f09b52d68622fda590e258016956666136f",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/QueueUpgrade.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/6886a14fb66139ca1118736a695eb115f1af2358/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java",
      "raw_url": "https://github.com/elastic/logstash/raw/6886a14fb66139ca1118736a695eb115f1af2358/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java?ref=6886a14fb66139ca1118736a695eb115f1af2358",
      "patch": "@@ -21,7 +21,7 @@\n import org.logstash.ackedqueue.io.MmapPageIOV2;\n import org.logstash.ackedqueue.io.PageIO;\n \n-final class QueueUpgrade {\n+public final class QueueUpgrade {\n \n     private static final Logger LOGGER = LogManager.getLogger(QueueUpgrade.class);\n ",
      "parent_sha": "db8fadeccd06d686dd79cd6adf704086244c199a"
    }
  },
  {
    "oid": "58a3f887c50d40f560af7c52cf85752d86114a93",
    "message": "Added test for list append\n\nFixes #7247",
    "date": "2017-05-30T13:06:04Z",
    "url": "https://github.com/elastic/logstash/commit/58a3f887c50d40f560af7c52cf85752d86114a93",
    "details": {
      "sha": "acb902c532978f74a9bfdb2f255530bea6e452df",
      "filename": "logstash-core/src/test/java/org/logstash/EventTest.java",
      "status": "modified",
      "additions": 20,
      "deletions": 3,
      "changes": 23,
      "blob_url": "https://github.com/elastic/logstash/blob/58a3f887c50d40f560af7c52cf85752d86114a93/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2FEventTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/58a3f887c50d40f560af7c52cf85752d86114a93/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2FEventTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2FEventTest.java?ref=58a3f887c50d40f560af7c52cf85752d86114a93",
      "patch": "@@ -1,14 +1,12 @@\n package org.logstash;\n \n-import org.junit.Assert;\n-import org.junit.Test;\n-\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import org.junit.Test;\n \n import static net.javacrumbs.jsonunit.JsonAssert.assertJsonEquals;\n import static org.junit.Assert.assertEquals;\n@@ -200,6 +198,25 @@ public void testAppend() throws Exception {\n         assertEquals(\"original2\", e.getField(\"[field1][1]\"));\n     }\n \n+    @Test\n+    public void testAppendLists() throws Exception {\n+        Map data1 = new HashMap();\n+        data1.put(\"field1\", Arrays.asList(\"original1\", \"original2\"));\n+\n+        Map data2 = new HashMap();\n+        data2.put(\"field1\", Arrays.asList(\"original3\", \"original4\"));\n+\n+        Event e = new Event(data1);\n+        Event e2 = new Event(data2);\n+        e.append(e2);\n+\n+        assertEquals(4, ((List) e.getField(\"[field1]\")).size());\n+        assertEquals(\"original1\", e.getField(\"[field1][0]\"));\n+        assertEquals(\"original2\", e.getField(\"[field1][1]\"));\n+        assertEquals(\"original3\", e.getField(\"[field1][2]\"));\n+        assertEquals(\"original4\", e.getField(\"[field1][3]\"));\n+    }\n+\n     @Test\n     public void testFromJsonWithNull() throws Exception {\n         Event[] events = Event.fromJson(null);",
      "parent_sha": "d929a4daf61e854a5bfc55ef2a185de773904797"
    }
  },
  {
    "oid": "0bbcf9a921f859f8d417251e9f9feedd6f31674e",
    "message": "BUG: Fix incorrect pipeline shutdown logging\n\nFixes #9688",
    "date": "2018-05-31T08:40:40Z",
    "url": "https://github.com/elastic/logstash/commit/0bbcf9a921f859f8d417251e9f9feedd6f31674e",
    "details": {
      "sha": "6060d21f8152a4acb3bfa95fbde34b43383565fb",
      "filename": "logstash-core/src/main/java/org/logstash/execution/ShutdownWatcherExt.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/elastic/logstash/blob/0bbcf9a921f859f8d417251e9f9feedd6f31674e/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fexecution%2FShutdownWatcherExt.java",
      "raw_url": "https://github.com/elastic/logstash/raw/0bbcf9a921f859f8d417251e9f9feedd6f31674e/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fexecution%2FShutdownWatcherExt.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fexecution%2FShutdownWatcherExt.java?ref=0bbcf9a921f859f8d417251e9f9feedd6f31674e",
      "patch": "@@ -151,7 +151,8 @@ public IRubyObject start(final ThreadContext context) throws InterruptedExceptio\n                     reports.remove(0);\n                 }\n                 if (cycleNumber == reportEvery - 1) {\n-                    LOGGER.warn(reports.get(reports.size() - 1).anyToString().asJavaString());\n+                    LOGGER.warn(reports.get(reports.size() - 1).callMethod(context, \"to_s\")\n+                        .asJavaString());\n                     if (shutdownStalled(context).isTrue()) {\n                         if (stalledCount == 0) {\n                             LOGGER.error(",
      "parent_sha": "e75156ea546bbc3b5a675dbc368299a83bcab279"
    }
  },
  {
    "oid": "70bd491e3a8246d80e14f4ae349bb256ebd50c3f",
    "message": "MINOR: Upgrade Message in PQ Upgrade Error\n\nFixes #9612",
    "date": "2018-05-18T14:08:58Z",
    "url": "https://github.com/elastic/logstash/commit/70bd491e3a8246d80e14f4ae349bb256ebd50c3f",
    "details": {
      "sha": "7192a39ff9b096fc61de60d2e7d42fc729303d94",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/QueueUpgrade.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/70bd491e3a8246d80e14f4ae349bb256ebd50c3f/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java",
      "raw_url": "https://github.com/elastic/logstash/raw/70bd491e3a8246d80e14f4ae349bb256ebd50c3f/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java?ref=70bd491e3a8246d80e14f4ae349bb256ebd50c3f",
      "patch": "@@ -110,7 +110,7 @@ private static Checkpoint loadCheckpoint(final Path path, final CheckpointIO cpI\n \n     private static void failValidation(final Throwable ex) {\n         LOGGER.error(\"Logstash was unable to upgrade your persistent queue data.\" +\n-            \"Please either downgrade to version 6.2.3 and fully drain \" +\n+            \"Please either downgrade to your previous version of Logstash and fully drain \" +\n             \"your persistent queue or delete your queue data.dir if you \" +\n             \"don't need to retain the data currently in your queue.\");\n         throw new IllegalStateException(ex);",
      "parent_sha": "1fd34822b9480f93412f1f227b8e316898532797"
    }
  },
  {
    "oid": "6a7077c5348b8556bb460921b0b772732805a9f8",
    "message": "Add mandatory option to jvm configuration to handle logstash startup \u2026 (#14066)\n\n* Add mandatory option to jvm configuration to handle logstash startup issues\r\n\r\nOpenJDK versions 11.0.15+10, 17.0.3+7 introduced new functionality to allow Java to enable\r\nstrict path checking. This included disallowing the use of `:` in any place other than directly\r\nafter the drive letter. Unfortunately, this check had the side effect of breaking compatibility\r\nwith special device paths, such as `NUL:`, which in turn, prevents Logstash from starting.\r\nThis feature was gated by the use of the `jdk.io.File.enableADS` property with a value of `true`\r\ndisabling the check.\r\n\r\nThis property was introduced with the default value of `false`, which prevents logstash\r\nfrom starting in a Windows environment. While the next release is anticipated to set this\r\nvalue to `true`, this commit explicitly sets that value to enable Logstash to be able\r\nto start correctly.\r\n\r\nRelates: #14042",
    "date": "2022-05-10T14:36:22Z",
    "url": "https://github.com/elastic/logstash/commit/6a7077c5348b8556bb460921b0b772732805a9f8",
    "details": {
      "sha": "9368ad5d190312dce9a8822b4c9f082c8c824105",
      "filename": "logstash-core/src/main/java/org/logstash/launchers/JvmOptionsParser.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/elastic/logstash/blob/6a7077c5348b8556bb460921b0b772732805a9f8/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flaunchers%2FJvmOptionsParser.java",
      "raw_url": "https://github.com/elastic/logstash/raw/6a7077c5348b8556bb460921b0b772732805a9f8/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flaunchers%2FJvmOptionsParser.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flaunchers%2FJvmOptionsParser.java?ref=6a7077c5348b8556bb460921b0b772732805a9f8",
      "patch": "@@ -28,13 +28,15 @@\n import java.util.stream.Collectors;\n \n \n+\n /**\n  * Parse jvm.options file applying version conditional logic. Heavily inspired by same functionality in Elasticsearch.\n  * */\n public class JvmOptionsParser {\n \n     private static final String[] MANDATORY_JVM_OPTIONS = new String[]{\n             \"-Djruby.regexp.interruptible=true\",\n+            \"-Djdk.io.File.enableADS=true\",\n             \"16-:--add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED\",\n             \"16-:--add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED\",\n             \"16-:--add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED\",\n@@ -47,6 +49,7 @@ public class JvmOptionsParser {\n             \"11-:--add-opens=java.management/sun.management=ALL-UNNAMED\"\n     };\n \n+\n     static class JvmOptionsFileParserException extends Exception {\n \n         private static final long serialVersionUID = 2446165130736962758L;\n@@ -82,7 +85,7 @@ SortedMap<Integer, String> invalidLines() {\n      *\n      * @param args the args to the program which should consist of a single option, the path to LOGSTASH_HOME\n      */\n-    public static void main(final String[] args) throws InterruptedException, IOException {\n+    public static void main(final String[] args) {\n         if (args.length < 1 || args.length > 2) {\n             throw new IllegalArgumentException(\n                     \"Expected two arguments specifying path to LOGSTASH_HOME and an optional LS_JVM_OPTS, but was \" + Arrays.toString(args)",
      "parent_sha": "41cb3d368003808cef4fb926a8b1da3f9d790d9c"
    }
  },
  {
    "oid": "6a7295cbd0c59c236c140d397f6897c906f999cd",
    "message": "Fix logger handling of RubyNil in `#configure_logging`\n\nWe occasionally are getting errors (listed below) since refactoring this class into Java.\n\nThis skips the unnecessary cast and corrects the failing spec.\n\n```\n14:37:42       1) Test Monitoring API can configure logging\n14:37:42          Failure/Error: expect(result[\"acknowledged\"]).to be(true)\n14:37:42\n14:37:42            expected true\n14:37:42                 got nil\n14:37:42          # ./specs/monitoring_api_spec.rb:159:in `logging_put_assert'\n14:37:42          # ./specs/monitoring_api_spec.rb:114:in `block in /opt/logstash/qa/integration/specs/monitoring_api_spec.rb'\n14:37:42          # /opt/logstash/build/qa/integration/vendor/jruby/2.3.0/gems/stud-0.0.23/lib/stud/try.rb:79:in `block in try'\n14:37:42          # /opt/logstash/build/qa/integration/vendor/jruby/2.3.0/gems/stud-0.0.23/lib/stud/try.rb:95:in `block in try'\n14:37:42          # /opt/logstash/build/qa/integration/vendor/jruby/2.3.0/gems/stud-0.0.23/lib/stud/try.rb:91:in `try'\n14:37:42          # /opt/logstash/build/qa/integration/vendor/jruby/2.3.0/gems/stud-0.0.23/lib/stud/try.rb:123:in `try'\n14:37:42          # ./specs/monitoring_api_spec.rb:105:in `block in (root)'\n14:37:42          # /opt/logstash/build/qa/integration/vendor/jruby/2.3.0/gems/rspec-wait-0.0.9/lib/rspec/wait.rb:46:in `block in (root)'\n14:37:42          # ./rspec.rb:17:in `<main>'\n14:37:42\n14:37:42     Finished in 17 minutes 22 seconds (files took 3.23 seconds to load)\n14:37:42     23 examples, 1 failure\n14:37:42\n```\n\nFixes #9577",
    "date": "2018-05-15T17:05:53Z",
    "url": "https://github.com/elastic/logstash/commit/6a7295cbd0c59c236c140d397f6897c906f999cd",
    "details": {
      "sha": "cc2cc54759ac0387a12e9f70da0fff2874ed35ca",
      "filename": "logstash-core/src/main/java/org/logstash/log/LoggerExt.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/6a7295cbd0c59c236c140d397f6897c906f999cd/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flog%2FLoggerExt.java",
      "raw_url": "https://github.com/elastic/logstash/raw/6a7295cbd0c59c236c140d397f6897c906f999cd/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flog%2FLoggerExt.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flog%2FLoggerExt.java?ref=6a7295cbd0c59c236c140d397f6897c906f999cd",
      "patch": "@@ -130,7 +130,7 @@ public IRubyObject rubyTrace(final ThreadContext context, final IRubyObject[] ar\n     public static IRubyObject configureLogging(final ThreadContext context, final IRubyObject self,\n                                         final IRubyObject args[]) {\n         synchronized (CONFIG_LOCK) {\n-            RubyString path = args.length > 1 ? (RubyString) args[1] : null;\n+            IRubyObject path = args.length > 1 ? args[1] : null;\n             String level = args[0].asJavaString();\n             try {\n                 setLevel(level, (path == null || path.isNil()) ? null : path.asJavaString());",
      "parent_sha": "8ebde04ae87683fcffc165cc9d0292586e28d2ed"
    }
  },
  {
    "oid": "726061d5c5fe22e396208b12644cd1af92f22a2f",
    "message": "fix flushing upon empty batches with ordered execution\n\nwhen running a pipeline with ordered execution, flushes on the pipeline\nwere no longer being called when compute is called with an empty batch, causing\nissues with the aggregate filter, for example, not being able to push events on\ntimeout.",
    "date": "2020-08-26T19:07:01Z",
    "url": "https://github.com/elastic/logstash/commit/726061d5c5fe22e396208b12644cd1af92f22a2f",
    "details": {
      "sha": "092e1c0e578a3beb26472c218672b201c2834814",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/CompiledPipeline.java",
      "status": "modified",
      "additions": 21,
      "deletions": 9,
      "changes": 30,
      "blob_url": "https://github.com/elastic/logstash/blob/726061d5c5fe22e396208b12644cd1af92f22a2f/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FCompiledPipeline.java",
      "raw_url": "https://github.com/elastic/logstash/raw/726061d5c5fe22e396208b12644cd1af92f22a2f/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FCompiledPipeline.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FCompiledPipeline.java?ref=726061d5c5fe22e396208b12644cd1af92f22a2f",
      "patch": "@@ -306,23 +306,35 @@ private boolean isOutput(final Vertex vertex) {\n \n     public final class CompiledOrderedExecution extends CompiledExecution {\n \n+        @SuppressWarnings({\"unchecked\"})  private final RubyArray<RubyEvent> EMPTY_ARRAY = RubyUtil.RUBY.newEmptyArray();\n+\n         @Override\n         public void compute(final QueueBatch batch, final boolean flush, final boolean shutdown) {\n            compute(batch.events(), flush, shutdown);\n         }\n \n         @Override\n         public void compute(final Collection<RubyEvent> batch, final boolean flush, final boolean shutdown) {\n-            @SuppressWarnings({\"unchecked\"}) final RubyArray<RubyEvent> outputBatch = RubyUtil.RUBY.newArray();\n-            // send batch one-by-one as single-element batches down the filters\n-            @SuppressWarnings({\"unchecked\"}) final RubyArray<RubyEvent> filterBatch = RubyUtil.RUBY.newArray(1);\n-            for (final RubyEvent e : batch) {\n-                filterBatch.set(0, e);\n-                final Collection<RubyEvent> result = compiledFilters.compute(filterBatch, flush, shutdown);\n-                copyNonCancelledEvents(result, outputBatch);\n-                compiledFilters.clear();\n+            if (!batch.isEmpty()) {\n+                @SuppressWarnings({\"unchecked\"}) final RubyArray<RubyEvent> outputBatch = RubyUtil.RUBY.newArray();\n+                @SuppressWarnings({\"unchecked\"}) final RubyArray<RubyEvent> filterBatch = RubyUtil.RUBY.newArray(1);\n+                // send batch one-by-one as single-element batches down the filters\n+                for (final RubyEvent e : batch) {\n+                    filterBatch.set(0, e);\n+                    _compute(filterBatch, outputBatch, flush, shutdown);\n+                }\n+                compiledOutputs.compute(outputBatch, flush, shutdown);\n+            } else if (flush || shutdown) {\n+                @SuppressWarnings({\"unchecked\"}) final RubyArray<RubyEvent> outputBatch = RubyUtil.RUBY.newArray();\n+                _compute(EMPTY_ARRAY, outputBatch, flush, shutdown);\n+                compiledOutputs.compute(outputBatch, flush, shutdown);\n             }\n-            compiledOutputs.compute(outputBatch, flush, shutdown);\n+        }\n+\n+        private void _compute(final RubyArray<RubyEvent> batch, final RubyArray<RubyEvent> outputBatch, final boolean flush, final boolean shutdown) {\n+            final Collection<RubyEvent> result = compiledFilters.compute(batch, flush, shutdown);\n+            copyNonCancelledEvents(result, outputBatch);\n+            compiledFilters.clear();\n         }\n     }\n ",
      "parent_sha": "b6b85e1a1e533a871212c294274a0360de5af6af"
    }
  },
  {
    "oid": "a9c97d4b0eff74c493aa295d27bbad78a9a5326b",
    "message": "Trap startup exception about inaccessible temp directory and provide more helpful error message with link to more detail in documentation.\n\nFixes #9288\n\nFixes #9293",
    "date": "2018-04-10T14:54:22Z",
    "url": "https://github.com/elastic/logstash/commit/a9c97d4b0eff74c493aa295d27bbad78a9a5326b",
    "details": {
      "sha": "702c6c50e170e0390c6fa4a9fc3ed0bf32a98cce",
      "filename": "logstash-core/src/main/java/org/logstash/Logstash.java",
      "status": "modified",
      "additions": 25,
      "deletions": 4,
      "changes": 29,
      "blob_url": "https://github.com/elastic/logstash/blob/a9c97d4b0eff74c493aa295d27bbad78a9a5326b/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FLogstash.java",
      "raw_url": "https://github.com/elastic/logstash/raw/a9c97d4b0eff74c493aa295d27bbad78a9a5326b/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FLogstash.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FLogstash.java?ref=a9c97d4b0eff74c493aa295d27bbad78a9a5326b",
      "patch": "@@ -35,21 +35,42 @@ public static void main(final String... args) {\n         final String lsHome = System.getenv(\"LS_HOME\");\n         if (lsHome == null) {\n             throw new IllegalStateException(\n-                \"LS_HOME environment variable must be set. This is likely a bug that should be reported.\"\n+                    \"LS_HOME environment variable must be set. This is likely a bug that should be reported.\"\n             );\n         }\n         final Path home = Paths.get(lsHome).toAbsolutePath();\n         try (\n-            final Logstash logstash = new Logstash(home, args, System.out, System.err, System.in)\n+                final Logstash logstash = new Logstash(home, args, System.out, System.err, System.in)\n         ) {\n             logstash.run();\n+        } catch (final IllegalStateException e) {\n+            String errorMessage[] = null;\n+            if (e.getMessage().contains(\"Could not load FFI Provider\")) {\n+                errorMessage = new String[]{\n+                        \"\\nError accessing temp directory: \" + System.getProperty(\"java.io.tmpdir\"),\n+                        \"This often occurs because the temp directory has been mounted with NOEXEC or\",\n+                        \"the Logstash user has insufficient permissions on the directory. Possible\",\n+                        \"workarounds include setting the -Djava.io.tmpdir property in the jvm.options\",\n+                        \"file to an alternate directory or correcting the Logstash user's permissions.\"\n+                };\n+            }\n+            handleCriticalError(e, errorMessage);\n         } catch (final Throwable t) {\n-            LOGGER.error(\"Logstash encountered an unexpected fatal error!\", t);\n-            System.exit(1);\n+            handleCriticalError(t, null);\n         }\n         System.exit(0);\n     }\n \n+    private static void handleCriticalError(Throwable t, String[] errorMessage) {\n+        LOGGER.error(t);\n+        if (errorMessage != null) {\n+            for (String err : errorMessage) {\n+                System.err.println(err);\n+            }\n+        }\n+        System.exit(1);\n+    }\n+\n     /**\n      * Ctor.\n      * @param home Logstash Root Directory",
      "parent_sha": "06f697daecb70c435ccf0ef96fd322bad05b9a57"
    }
  },
  {
    "oid": "cd785581210bc44fa891061f5ea05aa02266cf44",
    "message": "fix NPE when getting native thread id (#15301)\n\nThis commit adds null guard to get the native thread when constructing pipeline report\r\n\r\nFix: #15300",
    "date": "2023-09-14T15:46:26Z",
    "url": "https://github.com/elastic/logstash/commit/cd785581210bc44fa891061f5ea05aa02266cf44",
    "details": {
      "sha": "7c455253ec28edb1596d802079764afd7d4907e0",
      "filename": "logstash-core/src/main/java/org/logstash/execution/PipelineReporterExt.java",
      "status": "modified",
      "additions": 11,
      "deletions": 3,
      "changes": 14,
      "blob_url": "https://github.com/elastic/logstash/blob/cd785581210bc44fa891061f5ea05aa02266cf44/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fexecution%2FPipelineReporterExt.java",
      "raw_url": "https://github.com/elastic/logstash/raw/cd785581210bc44fa891061f5ea05aa02266cf44/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fexecution%2FPipelineReporterExt.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fexecution%2FPipelineReporterExt.java?ref=cd785581210bc44fa891061f5ea05aa02266cf44",
      "patch": "@@ -37,6 +37,7 @@\n import org.logstash.config.ir.compiler.AbstractOutputDelegatorExt;\n \n import java.util.Collection;\n+import java.util.Optional;\n \n /**\n  * JRuby extension\n@@ -161,7 +162,7 @@ private RubyArray workerStates(final ThreadContext context, final RubyHash batch\n         final RubyArray result = context.runtime.newArray();\n         ((Iterable<IRubyObject>) pipeline.callMethod(context, \"worker_threads\"))\n             .forEach(thread -> {\n-                final long nativeThreadId = ((RubyThread) thread).getNativeThread().getId();\n+\n                 final RubyHash hash = RubyHash.newHash(context.runtime);\n                 IRubyObject status = thread.callMethod(context, \"status\");\n                 if (status.isNil()) {\n@@ -170,8 +171,15 @@ private RubyArray workerStates(final ThreadContext context, final RubyHash batch\n                 hash.op_aset(context, STATUS_KEY, status);\n                 hash.op_aset(context, ALIVE_KEY, thread.callMethod(context, \"alive?\"));\n                 hash.op_aset(context, INDEX_KEY, context.runtime.newFixnum(result.size()));\n-                final IRubyObject batch = batchMap.op_aref(context, context.runtime.newFixnum(nativeThreadId));\n-                hash.op_aset(context, INFLIGHT_COUNT_KEY, extractBatchSize(context, batch));\n+\n+                IRubyObject batchSize = Optional.of((RubyThread) thread)\n+                        .map(RubyThread::getNativeThread)\n+                        .map(Thread::getId)\n+                        .map(id -> batchMap.op_aref(context, context.runtime.newFixnum(id)))\n+                        .map(batch -> extractBatchSize(context, batch))\n+                        .orElse(context.runtime.newFixnum(0L));\n+\n+                hash.op_aset(context, INFLIGHT_COUNT_KEY, batchSize);\n                 result.add(hash);\n             });\n         return result;",
      "parent_sha": "4bddfd7d06101b8b2c20409c9e13264737e65c2c"
    }
  },
  {
    "oid": "19f3861545d60ec71f311e0bc167fd10e02b8dc5",
    "message": "PERFORMANCE: Correctly size pathcache backing map\n\nFixes #7848",
    "date": "2017-07-30T17:58:02Z",
    "url": "https://github.com/elastic/logstash/commit/19f3861545d60ec71f311e0bc167fd10e02b8dc5",
    "details": {
      "sha": "2d295fafa49df340024c8ade879e9f0b2a131ed5",
      "filename": "logstash-core/src/main/java/org/logstash/PathCache.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/elastic/logstash/blob/19f3861545d60ec71f311e0bc167fd10e02b8dc5/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FPathCache.java",
      "raw_url": "https://github.com/elastic/logstash/raw/19f3861545d60ec71f311e0bc167fd10e02b8dc5/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FPathCache.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FPathCache.java?ref=19f3861545d60ec71f311e0bc167fd10e02b8dc5",
      "patch": "@@ -1,10 +1,12 @@\n package org.logstash;\n \n+import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n \n public final class PathCache {\n \n-    private static final ConcurrentHashMap<String, FieldReference> cache = new ConcurrentHashMap<>();\n+    private static final Map<String, FieldReference> cache =\n+        new ConcurrentHashMap<>(64, 0.2F, 1);\n \n     private static final FieldReference timestamp = cache(Event.TIMESTAMP);\n ",
      "parent_sha": "9d6ecf92ac629b4a951bc940df148e2bbe21c306"
    }
  },
  {
    "oid": "eddd91454f35a996ea80185399897004d43a8771",
    "message": "Shutdown DLQ segments flusher only if it has been started (#15649)\n\nIn DLQ unit testing sometime the DLQ writer is started explicitly without starting the segments flushers. In such cases the test 's logs contains exceptions which could lead to think that the test fails silently.\r\n\r\nAvoid to invoke scheduledFlusher's shutdown when it's not started (such behaviour is present only in tests).",
    "date": "2023-12-05T08:06:24Z",
    "url": "https://github.com/elastic/logstash/commit/eddd91454f35a996ea80185399897004d43a8771",
    "details": {
      "sha": "b321005b49bed89b0b2c8276e79bc394ac38856a",
      "filename": "logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/elastic/logstash/blob/eddd91454f35a996ea80185399897004d43a8771/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "raw_url": "https://github.com/elastic/logstash/raw/eddd91454f35a996ea80185399897004d43a8771/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriter.java?ref=eddd91454f35a996ea80185399897004d43a8771",
      "patch": "@@ -254,7 +254,10 @@ public void close() {\n             }\n \n             try {\n-                flushScheduler.shutdown();\n+                // flushScheduler is null only if it's not explicitly started, which happens only in tests.\n+                if (flushScheduler != null) {\n+                    flushScheduler.shutdown();\n+                }\n             } catch (Exception e) {\n                 logger.warn(\"Unable shutdown flush scheduler, ignoring\", e);\n             }",
      "parent_sha": "a26b1d399f5fbc82fb57127cc46b392010da8342"
    }
  },
  {
    "oid": "a6e9a6bcfd400377d0ceb1449f0cc8e18af60825",
    "message": "Fix Reflections stack traces when process yml files in classpath and debug is enabled (#12991)\n\nAdds a filter to Reflections library initialization so that when it scan \"org.logstash.plugins\" it includes only .class files and avoid to load and process AliasRegistry.yml and plugin_aliases.yml\r\n\r\nFixes #12992",
    "date": "2021-06-16T10:15:02Z",
    "url": "https://github.com/elastic/logstash/commit/a6e9a6bcfd400377d0ceb1449f0cc8e18af60825",
    "details": {
      "sha": "4c7fd195c2db0f50c9b0eee67d6d0a55c98e6325",
      "filename": "logstash-core/src/main/java/org/logstash/plugins/discovery/PluginRegistry.java",
      "status": "modified",
      "additions": 10,
      "deletions": 1,
      "changes": 11,
      "blob_url": "https://github.com/elastic/logstash/blob/a6e9a6bcfd400377d0ceb1449f0cc8e18af60825/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fplugins%2Fdiscovery%2FPluginRegistry.java",
      "raw_url": "https://github.com/elastic/logstash/raw/a6e9a6bcfd400377d0ceb1449f0cc8e18af60825/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fplugins%2Fdiscovery%2FPluginRegistry.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fplugins%2Fdiscovery%2FPluginRegistry.java?ref=a6e9a6bcfd400377d0ceb1449f0cc8e18af60825",
      "patch": "@@ -20,8 +20,10 @@\n \n package org.logstash.plugins.discovery;\n \n+import com.google.common.base.Predicate;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n+import org.checkerframework.checker.nullness.compatqual.NullableDecl;\n import org.logstash.plugins.AliasRegistry;\n import co.elastic.logstash.api.Codec;\n import co.elastic.logstash.api.Configuration;\n@@ -32,6 +34,8 @@\n import co.elastic.logstash.api.Output;\n import org.logstash.plugins.PluginLookup.PluginType;\n import org.reflections.Reflections;\n+import org.reflections.util.ClasspathHelper;\n+import org.reflections.util.ConfigurationBuilder;\n \n import java.lang.annotation.Annotation;\n import java.lang.reflect.Constructor;\n@@ -80,7 +84,12 @@ public static PluginRegistry getInstance(AliasRegistry aliasRegistry) {\n     private void discoverPlugins() {\n         // the constructor of Reflection must be called only by one thread, else there is a\n         // risk that the first thread that completes close the Zip files for the others.\n-        Reflections reflections = new Reflections(\"org.logstash.plugins\");\n+        // scan all .class present in package classpath\n+        final ConfigurationBuilder configurationBuilder = new ConfigurationBuilder()\n+                .setUrls(ClasspathHelper.forPackage(\"org.logstash.plugins\"))\n+                .filterInputsBy(input -> input.endsWith(\".class\"));\n+        Reflections reflections = new Reflections(configurationBuilder);\n+\n         Set<Class<?>> annotated = reflections.getTypesAnnotatedWith(LogstashPlugin.class);\n         for (final Class<?> cls : annotated) {\n             for (final Annotation annotation : cls.getAnnotations()) {",
      "parent_sha": "1d6a3e4bb3705d43f94e3c0c939c9467237a5ace"
    }
  },
  {
    "oid": "416339f3c9d10d9531e574d5050e3823b283e5f0",
    "message": "MINOR: Cleanup RubyTimestamp Ctor\n\nFixes #8335",
    "date": "2017-09-21T15:56:13Z",
    "url": "https://github.com/elastic/logstash/commit/416339f3c9d10d9531e574d5050e3823b283e5f0",
    "details": {
      "sha": "1a60af8646c1d44247c0f0bb9a0472ce5d228fba",
      "filename": "logstash-core/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/elastic/logstash/blob/416339f3c9d10d9531e574d5050e3823b283e5f0/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJrubyTimestampExtLibrary.java",
      "raw_url": "https://github.com/elastic/logstash/raw/416339f3c9d10d9531e574d5050e3823b283e5f0/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJrubyTimestampExtLibrary.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJrubyTimestampExtLibrary.java?ref=416339f3c9d10d9531e574d5050e3823b283e5f0",
      "patch": "@@ -30,6 +30,8 @@ public RubyTimestamp allocate(Ruby runtime, RubyClass rubyClass) {\n         }\n     };\n \n+    private static final RubyClass TIMESTAMP_CLASS = createTimestamp(RubyUtil.RUBY);\n+\n     @Override\n     public void load(Ruby runtime, boolean wrap) {\n         createTimestamp(runtime);\n@@ -60,7 +62,7 @@ public RubyTimestamp(Ruby runtime, RubyClass klass, Timestamp timestamp) {\n         }\n \n         public RubyTimestamp(Ruby runtime, Timestamp timestamp) {\n-            this(runtime, runtime.getModule(RubyUtil.LS_MODULE_NAME).getClass(\"Timestamp\"), timestamp);\n+            this(runtime, TIMESTAMP_CLASS, timestamp);\n         }\n \n         public RubyTimestamp(Ruby runtime) {",
      "parent_sha": "ff9ccb3e59ad8ba3ce30746d7be9fbd099bf58d7"
    }
  },
  {
    "oid": "ad9698dc7bb6eaee3d27ae73929df2b8b1a090e5",
    "message": "Queue: fix unlock mutex on close()\n\nOn o.l.ackedqueue.Queue.close() method the\nlock.unlock() call might not execute, leaving\na mutex locked.\n\nThis change ensure that lock.unlock() is always\nexecute by moving its call to the sibling\ntry/catch's finally block.",
    "date": "2017-04-19T22:30:06Z",
    "url": "https://github.com/elastic/logstash/commit/ad9698dc7bb6eaee3d27ae73929df2b8b1a090e5",
    "details": {
      "sha": "060bfbb7d9cba1d24f1698b5b9442e82b5439ef6",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/elastic/logstash/blob/ad9698dc7bb6eaee3d27ae73929df2b8b1a090e5/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "raw_url": "https://github.com/elastic/logstash/raw/ad9698dc7bb6eaee3d27ae73929df2b8b1a090e5/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java?ref=ad9698dc7bb6eaee3d27ae73929df2b8b1a090e5",
      "patch": "@@ -668,8 +668,9 @@ public void close() throws IOException {\n                 } catch (IOException e) {\n                     // log error and ignore\n                     logger.error(\"Queue close releaseLock failed, error={}\", e.getMessage());\n+                } finally {\n+                    lock.unlock();\n                 }\n-                lock.unlock();\n             }\n         }\n     }",
      "parent_sha": "6c1230cb935bd9ae53b036ae883b5f6d3dcd1d54"
    }
  },
  {
    "oid": "0efc0d58d570522bd9a3cc7f28fb80e4650d3b8c",
    "message": "One more character encoding fix\n\nFixes #10452",
    "date": "2019-02-14T21:36:51Z",
    "url": "https://github.com/elastic/logstash/commit/0efc0d58d570522bd9a3cc7f28fb80e4650d3b8c",
    "details": {
      "sha": "c36eeaafb629a40911bead7b6cc1740fae43e375",
      "filename": "logstash-core/src/test/java/org/logstash/plugins/inputs/StdinTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/0efc0d58d570522bd9a3cc7f28fb80e4650d3b8c/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fplugins%2Finputs%2FStdinTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/0efc0d58d570522bd9a3cc7f28fb80e4650d3b8c/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fplugins%2Finputs%2FStdinTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fplugins%2Finputs%2FStdinTest.java?ref=0efc0d58d570522bd9a3cc7f28fb80e4650d3b8c",
      "patch": "@@ -51,7 +51,7 @@ public void testUtf8Events() throws IOException {\n                 new String(\"M\u00fcnchen3\".getBytes(), Charset.forName(\"UTF-8\"))\n         };\n         String testInput = String.join(Line.DEFAULT_DELIMITER, inputs) + Line.DEFAULT_DELIMITER;\n-        TestConsumer queueWriter = testStdin(testInput.getBytes());\n+        TestConsumer queueWriter = testStdin(testInput.getBytes(\"UTF-8\"));\n \n         List<Map<String, Object>> events = queueWriter.getEvents();\n         assertEquals(3, events.size());",
      "parent_sha": "7cb2cd631dea5378877fa67761d1cc1c42ce02ef"
    }
  },
  {
    "oid": "99be638e6c1e8f9654659a705accc3ebfefea397",
    "message": "MINOR: Simplify clear if done code generation\n\nFixes #9312",
    "date": "2018-04-02T18:30:17Z",
    "url": "https://github.com/elastic/logstash/commit/99be638e6c1e8f9654659a705accc3ebfefea397",
    "details": {
      "sha": "117b5689df358d9bd51b1a74f88d59332abaf880",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/compiler/DatasetCompiler.java",
      "status": "modified",
      "additions": 21,
      "deletions": 15,
      "changes": 36,
      "blob_url": "https://github.com/elastic/logstash/blob/99be638e6c1e8f9654659a705accc3ebfefea397/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FDatasetCompiler.java",
      "raw_url": "https://github.com/elastic/logstash/raw/99be638e6c1e8f9654659a705accc3ebfefea397/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FDatasetCompiler.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2Fcompiler%2FDatasetCompiler.java?ref=99be638e6c1e8f9654659a705accc3ebfefea397",
      "patch": "@@ -108,12 +108,8 @@ public static ComputeStepSyntaxElement<SplitDataset> splitDataset(final Collecti\n                         .add(SyntaxFactory.ret(ifData))\n                 ),\n                 MethodSyntaxElement.clear(\n-                    Closure.wrap(\n-                        SyntaxFactory.ifCondition(\n-                            done,\n-                            clearSyntax(parentFields).add(clear(ifData)).add(clear(elseData))\n-                                .add(SyntaxFactory.assignment(done, SyntaxFactory.FALSE))\n-                        )\n+                    clearIfDone(\n+                        clearSyntax(parentFields).add(clear(ifData)).add(clear(elseData)), done\n                     )\n                 ),\n                 MethodSyntaxElement.right(right)\n@@ -166,15 +162,8 @@ public static ComputeStepSyntaxElement<Dataset> filterDataset(final Collection<D\n         return prepare(\n             body.add(SyntaxFactory.assignment(done, SyntaxFactory.TRUE))\n                 .add(SyntaxFactory.ret(outputBuffer)),\n-            Closure.wrap(\n-                SyntaxFactory.ifCondition(\n-                    done,\n-                    Closure.wrap(\n-                        clearSyntax(parentFields), clear(outputBuffer),\n-                        SyntaxFactory.assignment(done, SyntaxFactory.FALSE)\n-                    )\n-                )\n-            ), fields\n+            clearIfDone(Closure.wrap(clearSyntax(parentFields), clear(outputBuffer)), done),\n+            fields\n         );\n     }\n \n@@ -264,6 +253,23 @@ public static ComputeStepSyntaxElement<Dataset> outputDataset(final Collection<D\n         );\n     }\n \n+    /**\n+     * Generates code that clears a {@link Dataset}'s internal buffers if the given boolean flag\n+     * is set to true, then sets the flag to false once done.\n+     * @param clearAction Action for clearing buffers\n+     * @param doneField Boolean field that must be true for the action to run\n+     * @return Closure wrapping the conditional clear action\n+     */\n+    private static Closure clearIfDone(final Closure clearAction,\n+        final MethodLevelSyntaxElement doneField) {\n+        return Closure.wrap(\n+            SyntaxFactory.ifCondition(\n+                doneField,\n+                Closure.wrap(clearAction, SyntaxFactory.assignment(doneField, SyntaxFactory.FALSE))\n+            )\n+        );\n+    }\n+\n     private static Closure returnIffBuffered(final MethodLevelSyntaxElement ifData,\n         final MethodLevelSyntaxElement done) {\n         return Closure.wrap(",
      "parent_sha": "1e37678db7e14a5ed3568c91339c6bd38a6d8aa9"
    }
  },
  {
    "oid": "c95e643209af54d955c694fd6867962220d568a9",
    "message": "#7239 fix timeunit on cpu time metric\n\nFixes #7240",
    "date": "2017-05-27T20:00:13Z",
    "url": "https://github.com/elastic/logstash/commit/c95e643209af54d955c694fd6867962220d568a9",
    "details": {
      "sha": "7498e0e3dca8221e2c4dd073aa925ceb001f60a0",
      "filename": "logstash-core/src/main/java/org/logstash/instrument/monitors/ProcessMonitor.java",
      "status": "modified",
      "additions": 5,
      "deletions": 5,
      "changes": 10,
      "blob_url": "https://github.com/elastic/logstash/blob/c95e643209af54d955c694fd6867962220d568a9/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Finstrument%2Fmonitors%2FProcessMonitor.java",
      "raw_url": "https://github.com/elastic/logstash/raw/c95e643209af54d955c694fd6867962220d568a9/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Finstrument%2Fmonitors%2FProcessMonitor.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Finstrument%2Fmonitors%2FProcessMonitor.java?ref=c95e643209af54d955c694fd6867962220d568a9",
      "patch": "@@ -1,13 +1,12 @@\n package org.logstash.instrument.monitors;\n \n import com.sun.management.UnixOperatingSystemMXBean;\n-\n-import javax.management.MBeanServer;\n import java.lang.management.ManagementFactory;\n import java.lang.management.OperatingSystemMXBean;\n import java.util.HashMap;\n import java.util.Map;\n-import java.util.Objects;\n+import java.util.concurrent.TimeUnit;\n+import javax.management.MBeanServer;\n \n /**\n  * Created by andrewvc on 5/12/16.\n@@ -35,8 +34,9 @@ public static class Report {\n \n                 this.openFds = unixOsBean.getOpenFileDescriptorCount();\n                 this.maxFds =  unixOsBean.getMaxFileDescriptorCount();\n-\n-                this.cpuMillisTotal = unixOsBean.getProcessCpuTime();\n+                this.cpuMillisTotal = TimeUnit.MILLISECONDS.convert(\n+                    unixOsBean.getProcessCpuTime(), TimeUnit.NANOSECONDS\n+                );\n                 this.cpuProcessPercent = scaleLoadToPercent(unixOsBean.getProcessCpuLoad());\n                 this.cpuSystemPercent = scaleLoadToPercent(unixOsBean.getSystemCpuLoad());\n ",
      "parent_sha": "34014fbb27a27addac728c8586f507015502f6b6"
    }
  },
  {
    "oid": "9aeb2a7d0e77c4b6fc436927fd4bec3a51f4db6a",
    "message": "ignore different EOLs when testing report output\n\nFixes #9515",
    "date": "2018-05-02T17:27:48Z",
    "url": "https://github.com/elastic/logstash/commit/9aeb2a7d0e77c4b6fc436927fd4bec3a51f4db6a",
    "details": {
      "sha": "353f637038446c9d7f21cf09d6ad459264a4f217",
      "filename": "tools/dependencies-report/src/test/java/org/logstash/dependencies/ReportGeneratorTest.java",
      "status": "modified",
      "additions": 5,
      "deletions": 1,
      "changes": 6,
      "blob_url": "https://github.com/elastic/logstash/blob/9aeb2a7d0e77c4b6fc436927fd4bec3a51f4db6a/tools%2Fdependencies-report%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fdependencies%2FReportGeneratorTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/9aeb2a7d0e77c4b6fc436927fd4bec3a51f4db6a/tools%2Fdependencies-report%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fdependencies%2FReportGeneratorTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/tools%2Fdependencies-report%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fdependencies%2FReportGeneratorTest.java?ref=9aeb2a7d0e77c4b6fc436927fd4bec3a51f4db6a",
      "patch": "@@ -22,7 +22,7 @@ public void testSuccessfulReport() throws IOException {\n         boolean result = runReportGenerator(\"/licenseMapping-good.csv\", output);\n \n         assertTrue(result);\n-        assertEquals(expectedOutput, output.toString());\n+        assertEquals(normalizeEol(expectedOutput), normalizeEol(output.toString()));\n     }\n \n     @Test\n@@ -69,5 +69,9 @@ private static boolean runReportGenerator(String licenseMappingPath, StringWrite\n     private static String getStringFromStream(InputStream stream) {\n         return new Scanner(stream, \"UTF-8\").useDelimiter(\"\\\\A\").next();\n     }\n+\n+    private static String normalizeEol(String s) {\n+        return s.replaceAll(\"\\\\r\\\\n\", \"\\n\");\n+    }\n }\n ",
      "parent_sha": "cb0edd6f215a7734d04bb0ce31b07696189cf700"
    }
  },
  {
    "oid": "130fcf46a0514700254db69019e7294900db746c",
    "message": "CLEANUP: Dry up RubyClass name handling in RubyUtil\n\nFixes #8625",
    "date": "2017-11-09T18:48:39Z",
    "url": "https://github.com/elastic/logstash/commit/130fcf46a0514700254db69019e7294900db746c",
    "details": {
      "sha": "eec176656040f49f8b6483f9d288e4078452bb52",
      "filename": "logstash-core/src/main/java/org/logstash/RubyUtil.java",
      "status": "modified",
      "additions": 12,
      "deletions": 16,
      "changes": 28,
      "blob_url": "https://github.com/elastic/logstash/blob/130fcf46a0514700254db69019e7294900db746c/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FRubyUtil.java",
      "raw_url": "https://github.com/elastic/logstash/raw/130fcf46a0514700254db69019e7294900db746c/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FRubyUtil.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FRubyUtil.java?ref=130fcf46a0514700254db69019e7294900db746c",
      "patch": "@@ -46,11 +46,10 @@ public final class RubyUtil {\n         RUBY = Ruby.getGlobalRuntime();\n         LOGSTASH_MODULE = RUBY.getOrCreateModule(\"LogStash\");\n         RUBY_TIMESTAMP_CLASS = setupLogstashClass(\n-            \"Timestamp\",\n             JrubyTimestampExtLibrary.RubyTimestamp::new, JrubyTimestampExtLibrary.RubyTimestamp.class\n         );\n         RUBY_EVENT_CLASS = setupLogstashClass(\n-            \"Event\", JrubyEventExtLibrary.RubyEvent::new, JrubyEventExtLibrary.RubyEvent.class\n+            JrubyEventExtLibrary.RubyEvent::new, JrubyEventExtLibrary.RubyEvent.class\n         );\n         final RubyModule json = LOGSTASH_MODULE.defineOrGetModuleUnder(\"Json\");\n         final RubyClass stdErr = RUBY.getStandardError();\n@@ -82,18 +81,15 @@ public final class RubyUtil {\n         RUBY_EVENT_CLASS.defineAnnotatedMethods(JrubyEventExtLibrary.RubyEvent.class);\n         RUBY_EVENT_CLASS.defineAnnotatedConstants(JrubyEventExtLibrary.RubyEvent.class);\n         final RubyClass abstractQueue = setupLogstashClass(\n-            \"AbstractAckedQueue\", ObjectAllocator.NOT_ALLOCATABLE_ALLOCATOR,\n-            AbstractJRubyQueue.class\n-        );\n-        RUBY_ACKED_BATCH_CLASS = setupLogstashClass(\n-            \"AckedBatch\", RubyAckedBatch::new, RubyAckedBatch.class\n+            ObjectAllocator.NOT_ALLOCATABLE_ALLOCATOR, AbstractJRubyQueue.class\n         );\n+        RUBY_ACKED_BATCH_CLASS = setupLogstashClass(RubyAckedBatch::new, RubyAckedBatch.class);\n         setupLogstashClass(\n-            \"AckedQueue\", abstractQueue, AbstractJRubyQueue.RubyAckedQueue::new,\n+            abstractQueue, AbstractJRubyQueue.RubyAckedQueue::new,\n             AbstractJRubyQueue.RubyAckedQueue.class\n         );\n         setupLogstashClass(\n-            \"AckedMemoryQueue\", abstractQueue, AbstractJRubyQueue.RubyAckedMemoryQueue::new,\n+            abstractQueue, AbstractJRubyQueue.RubyAckedMemoryQueue::new,\n             AbstractJRubyQueue.RubyAckedMemoryQueue.class\n         );\n     }\n@@ -115,27 +111,27 @@ public static RaiseException newRubyIOError(Ruby runtime, Throwable e) {\n \n     /**\n      * Sets up a Java-defined {@link RubyClass} in the Logstash Ruby module.\n-     * @param name Name of the class\n      * @param allocator Allocator of the class\n      * @param jclass Underlying Java class that is annotated by {@link JRubyClass}\n      * @return RubyClass\n      */\n-    private static RubyClass setupLogstashClass(final String name,\n-        final ObjectAllocator allocator, final Class<?> jclass) {\n-        return setupLogstashClass(name, RUBY.getObject(), allocator, jclass);\n+    private static RubyClass setupLogstashClass(final ObjectAllocator allocator,\n+        final Class<?> jclass) {\n+        return setupLogstashClass(RUBY.getObject(), allocator, jclass);\n     }\n \n     /**\n      * Sets up a Java-defined {@link RubyClass} in the Logstash Ruby module.\n-     * @param name Name of the class\n      * @param parent Parent RubyClass\n      * @param allocator Allocator of the class\n      * @param jclass Underlying Java class that is annotated by {@link JRubyClass}\n      * @return RubyClass\n      */\n-    private static RubyClass setupLogstashClass(final String name, final RubyClass parent,\n+    private static RubyClass setupLogstashClass(final RubyClass parent,\n         final ObjectAllocator allocator, final Class<?> jclass) {\n-        final RubyClass clazz = RUBY.defineClassUnder(name, parent, allocator, LOGSTASH_MODULE);\n+        final RubyClass clazz = RUBY.defineClassUnder(\n+            jclass.getAnnotation(JRubyClass.class).name()[0], parent, allocator, LOGSTASH_MODULE\n+        );\n         clazz.defineAnnotatedMethods(jclass);\n         return clazz;\n     }",
      "parent_sha": "d38e4ff665995e13407d2a1db02c73920a0bfc00"
    }
  },
  {
    "oid": "4926f5b6bb7395fa7f90c1993a36d1a3256eb6db",
    "message": "PERFORMANCE: Queue RW benchmark baseline\n\nFixes #7400",
    "date": "2017-06-09T17:22:39Z",
    "url": "https://github.com/elastic/logstash/commit/4926f5b6bb7395fa7f90c1993a36d1a3256eb6db",
    "details": {
      "sha": "27d19ea0a73b8721bcf7bb335b8e06bb7cc0784d",
      "filename": "logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java",
      "status": "modified",
      "additions": 26,
      "deletions": 2,
      "changes": 28,
      "blob_url": "https://github.com/elastic/logstash/blob/4926f5b6bb7395fa7f90c1993a36d1a3256eb6db/logstash-core%2Fbenchmarks%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fbenchmark%2FQueueRWBenchmark.java",
      "raw_url": "https://github.com/elastic/logstash/raw/4926f5b6bb7395fa7f90c1993a36d1a3256eb6db/logstash-core%2Fbenchmarks%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fbenchmark%2FQueueRWBenchmark.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fbenchmarks%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fbenchmark%2FQueueRWBenchmark.java?ref=4926f5b6bb7395fa7f90c1993a36d1a3256eb6db",
      "patch": "@@ -3,6 +3,7 @@\n import com.google.common.io.Files;\n import java.io.File;\n import java.io.IOException;\n+import java.util.concurrent.ArrayBlockingQueue;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n@@ -47,6 +48,8 @@ public class QueueRWBenchmark {\n     \n     private static final int BATCH_SIZE = 100;\n \n+    private static final int ACK_INTERVAL = 1024;\n+\n     private static final Event EVENT = new Event();\n \n     private Queue queue;\n@@ -100,6 +103,27 @@ public final void readFromPersistedQueue(final Blackhole blackhole) throws Excep\n         future.get();\n     }\n \n+    @Benchmark\n+    @OperationsPerInvocation(EVENTS_PER_INVOCATION)\n+    public final void readFromArrayBlockingQueue(final Blackhole blackhole) throws Exception {\n+        final ArrayBlockingQueue<Event> arrayBlockingQueue = new ArrayBlockingQueue<>(ACK_INTERVAL);\n+        final Future<?> future = exec.submit(() -> {\n+            for (int i = 0; i < EVENTS_PER_INVOCATION; ++i) {\n+                try {\n+                    final Event evnt = EVENT.clone();\n+                    evnt.setTimestamp(Timestamp.now());\n+                    arrayBlockingQueue.put(evnt);\n+                } catch (final CloneNotSupportedException | InterruptedException ex) {\n+                    throw new IllegalStateException(ex);\n+                }\n+            }\n+        });\n+        for (int i = 0; i < EVENTS_PER_INVOCATION; ++i) {\n+            blackhole.consume(arrayBlockingQueue.take());\n+        }\n+        future.get();\n+    }\n+\n     public static void main(final String... args) throws RunnerException {\n         Options opt = new OptionsBuilder()\n             .include(QueueRWBenchmark.class.getSimpleName())\n@@ -113,8 +137,8 @@ private static Settings settings() {\n             .capacity(256 * 1024 * 1024)\n             .queueMaxBytes(Long.MAX_VALUE)\n             .elementIOFactory(MmapPageIO::new)\n-            .checkpointMaxWrites(1024)\n-            .checkpointMaxAcks(1024)\n+            .checkpointMaxWrites(ACK_INTERVAL)\n+            .checkpointMaxAcks(ACK_INTERVAL)\n             .checkpointIOFactory(FileCheckpointIO::new)\n             .elementClass(Event.class).build();\n     }",
      "parent_sha": "2d2cee4d2e6280be80b1ccc360ad70f816983254"
    }
  },
  {
    "oid": "41a8adf1f8e7ed73b19da94bc2070507d9a67d43",
    "message": "MINOR: Simplify Java entrypoint\n\nFixes #8954",
    "date": "2018-01-16T16:39:12Z",
    "url": "https://github.com/elastic/logstash/commit/41a8adf1f8e7ed73b19da94bc2070507d9a67d43",
    "details": {
      "sha": "5f082d5b046992ad70cdcddd9ea1ddce97e89bd8",
      "filename": "logstash-core/src/main/java/org/logstash/Logstash.java",
      "status": "modified",
      "additions": 7,
      "deletions": 15,
      "changes": 22,
      "blob_url": "https://github.com/elastic/logstash/blob/41a8adf1f8e7ed73b19da94bc2070507d9a67d43/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FLogstash.java",
      "raw_url": "https://github.com/elastic/logstash/raw/41a8adf1f8e7ed73b19da94bc2070507d9a67d43/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FLogstash.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FLogstash.java?ref=41a8adf1f8e7ed73b19da94bc2070507d9a67d43",
      "patch": "@@ -1,5 +1,6 @@\n package org.logstash;\n \n+import java.io.IOException;\n import java.io.InputStream;\n import java.io.PrintStream;\n import java.nio.file.Path;\n@@ -20,21 +21,11 @@ public final class Logstash implements Runnable, AutoCloseable {\n \n     private static final Logger LOGGER = LogManager.getLogger(Logstash.class);\n \n-    /**\n-     * Configuration for {@link #ruby}.\n-     */\n-    private final RubyInstanceConfig config;\n-\n     /**\n      * JRuby Runtime Environment.\n      */\n     private final Ruby ruby;\n \n-    /**\n-     * Ruby Entrypoint Script.\n-     */\n-    private final InputStream script;\n-\n     /**\n      * Main Entrypoint.\n      * Requires environment {@code \"LS_HOME\"} to be set to the Logstash root directory.\n@@ -69,11 +60,10 @@ public static void main(final String... args) {\n      */\n     Logstash(final Path home, final String[] args, final PrintStream output,\n         final PrintStream error, final InputStream input) {\n-        config = buildConfig(home, args);\n+        final RubyInstanceConfig config = buildConfig(home, args);\n         config.setOutput(output);\n         config.setError(error);\n         config.setInput(input);\n-        script = config.getScriptSource();\n         ruby = Ruby.newInstance(config);\n     }\n \n@@ -85,7 +75,8 @@ public void run() {\n                 \"More than one JRuby Runtime detected in the current JVM!\"\n             );\n         }\n-        try {\n+        final RubyInstanceConfig config = ruby.getInstanceConfig();\n+        try (InputStream script = config.getScriptSource()) {\n             Thread.currentThread().setContextClassLoader(ruby.getJRubyClassLoader());\n             ruby.runFromMain(script, config.displayedFileName());\n         } catch (final RaiseException ex) {\n@@ -99,13 +90,14 @@ public void run() {\n             } else {\n                 throw new IllegalStateException(ex);\n             }\n+        } catch (final IOException ex) {\n+            throw new IllegalStateException(ex);\n         }\n     }\n \n     @Override\n-    public void close() throws Exception {\n+    public void close() {\n         ruby.tearDown(false);\n-        script.close();\n     }\n \n     /**",
      "parent_sha": "e653f7e7079389844bdaa0603c5a407352f416ea"
    }
  },
  {
    "oid": "9cc830fdf15535517ce34be2d46ecd77fcedbd86",
    "message": "Minor: Stop allocating byte[] to compute crc32\n\nFixes #7058",
    "date": "2017-05-15T20:07:12Z",
    "url": "https://github.com/elastic/logstash/commit/9cc830fdf15535517ce34be2d46ecd77fcedbd86",
    "details": {
      "sha": "32f350dfb1585d65520e0028d198ce50a8800976",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/io/AbstractByteBufferPageIO.java",
      "status": "modified",
      "additions": 10,
      "deletions": 9,
      "changes": 19,
      "blob_url": "https://github.com/elastic/logstash/blob/9cc830fdf15535517ce34be2d46ecd77fcedbd86/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2Fio%2FAbstractByteBufferPageIO.java",
      "raw_url": "https://github.com/elastic/logstash/raw/9cc830fdf15535517ce34be2d46ecd77fcedbd86/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2Fio%2FAbstractByteBufferPageIO.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2Fio%2FAbstractByteBufferPageIO.java?ref=9cc830fdf15535517ce34be2d46ecd77fcedbd86",
      "patch": "@@ -1,15 +1,13 @@\n package org.logstash.ackedqueue.io;\n \n-import org.apache.logging.log4j.LogManager;\n-import org.apache.logging.log4j.Logger;\n-import org.logstash.ackedqueue.SequencedList;\n-\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.zip.CRC32;\n-import java.util.zip.Checksum;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.logstash.ackedqueue.SequencedList;\n \n public abstract class AbstractByteBufferPageIO implements PageIO {\n \n@@ -44,7 +42,7 @@ public class PageIOInvalidVersionException extends IOException {\n     protected int elementCount;\n     protected int head; // head is the write position and is an int per ByteBuffer class position\n     protected byte version;\n-    protected Checksum checkSummer;\n+    private CRC32 checkSummer;\n \n     public AbstractByteBufferPageIO(int pageNum, int capacity) {\n         this.minSeqNum = 0;\n@@ -155,10 +153,13 @@ private void readNextElement(long expectedSeqNum, boolean verifyChecksum) throws\n \n         if (verifyChecksum) {\n             // read data and compute checksum;\n-            byte[] readBytes = new byte[length];\n-            buffer.get(readBytes);\n+            this.checkSummer.reset();\n+            final int prevLimit = buffer.limit();\n+            buffer.limit(buffer.position() + length);\n+            this.checkSummer.update(buffer);\n+            buffer.limit(prevLimit);\n             int checksum = buffer.getInt();\n-            int computedChecksum = checksum(readBytes);\n+            int computedChecksum = (int) this.checkSummer.getValue();\n             if (computedChecksum != checksum) { throw new PageIOInvalidElementException(\"Element invalid checksum\"); }\n         }\n ",
      "parent_sha": "4994bf3bc5e986fb1e910581a215573eb02bec52"
    }
  },
  {
    "oid": "0371abad1de48110dcd85c65bde51112bd028ef2",
    "message": "Test Fix: Better way to test CPU metric changes\n\nFixes #8578\n\nFixes #8665",
    "date": "2017-11-14T21:26:19Z",
    "url": "https://github.com/elastic/logstash/commit/0371abad1de48110dcd85c65bde51112bd028ef2",
    "details": {
      "sha": "8151f19f2237290cadd4ed5e4eb25dd4c5edfe08",
      "filename": "logstash-core/src/test/java/org/logstash/instrument/witness/process/ProcessWitnessTest.java",
      "status": "modified",
      "additions": 13,
      "deletions": 6,
      "changes": 19,
      "blob_url": "https://github.com/elastic/logstash/blob/0371abad1de48110dcd85c65bde51112bd028ef2/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Finstrument%2Fwitness%2Fprocess%2FProcessWitnessTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/0371abad1de48110dcd85c65bde51112bd028ef2/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Finstrument%2Fwitness%2Fprocess%2FProcessWitnessTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Finstrument%2Fwitness%2Fprocess%2FProcessWitnessTest.java?ref=0371abad1de48110dcd85c65bde51112bd028ef2",
      "patch": "@@ -5,6 +5,7 @@\n import org.junit.Test;\n \n import java.security.MessageDigest;\n+import java.time.Instant;\n import java.util.UUID;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n@@ -55,7 +56,7 @@ public void testRefreshChanges() throws InterruptedException {\n         ProcessWitness.Snitch snitch = witness.snitch();\n         assumeTrue(ProcessWitness.isUnix);\n         witness.refresh();\n-        long before = snitch.cpuProcessPercent();\n+        short before = snitch.cpuProcessPercent();\n \n         ScheduledExecutorService refresh = Executors.newSingleThreadScheduledExecutor();\n         refresh.scheduleAtFixedRate(() -> witness.refresh(), 0 , 100, TimeUnit.MILLISECONDS);\n@@ -76,11 +77,17 @@ public void testRefreshChanges() throws InterruptedException {\n                 }\n             }\n         });\n-        //give the threads some time up add measurable load\n-        Thread.sleep(3000);\n-        long after = snitch.cpuProcessPercent();\n-        //There is a slim chance that the stars align and the before and after are indeed equal, but should be very rare.\n-        assertThat(before).isNotEqualTo(after);\n+        //we only care that the value changes, give the load some time to change it\n+        boolean pass = false;\n+        Instant end = Instant.now().plusSeconds(10);\n+        do {\n+            Thread.sleep(100);\n+            if (before != snitch.cpuProcessPercent()) {\n+                pass = true;\n+                break;\n+            }\n+        } while (end.isAfter(Instant.now()));\n+        assertThat(pass).isTrue();\n \n         refresh.shutdownNow();\n         cpuLoad.shutdownNow();",
      "parent_sha": "62aecc570143b5416eb86b9207b8d2c41c63e96b"
    }
  },
  {
    "oid": "05bfaff799d3cb52b2d0af51d9a9a51f15a0501f",
    "message": "Avoid the wrapping of LogstashMessageFactory with log4j's MessageFactory2Adapter (#14727)\n\nStarting with Log4j2  2.6 if a subclass of MessageFactory associated with an Logger instance\r\nis not subclass of MessageFactory2, then it's wrapped with MessageFactory2Adapter.\r\n\r\nThis trigger a log4j warn log that, when a class subclasses LogStash::Plugin for example, is noisy and report about\r\na Logger is not associated with the default MessagedFactory (LogstashMessageFactory) every time a subclass of Plugins is instantiated.\r\n\r\nThis commit adapt LogstashMessageFactory to implement the MessagedFactory2 instead of the older MessageFactory to avoid the wrapping with the adapter class.",
    "date": "2022-11-09T09:16:34Z",
    "url": "https://github.com/elastic/logstash/commit/05bfaff799d3cb52b2d0af51d9a9a51f15a0501f",
    "details": {
      "sha": "28cf11fd9e947c396a5c5facf2022412729d1579",
      "filename": "logstash-core/src/main/java/org/logstash/log/LogstashMessageFactory.java",
      "status": "modified",
      "additions": 59,
      "deletions": 2,
      "changes": 61,
      "blob_url": "https://github.com/elastic/logstash/blob/05bfaff799d3cb52b2d0af51d9a9a51f15a0501f/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flog%2FLogstashMessageFactory.java",
      "raw_url": "https://github.com/elastic/logstash/raw/05bfaff799d3cb52b2d0af51d9a9a51f15a0501f/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flog%2FLogstashMessageFactory.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flog%2FLogstashMessageFactory.java?ref=05bfaff799d3cb52b2d0af51d9a9a51f15a0501f",
      "patch": "@@ -21,7 +21,7 @@\n package org.logstash.log;\n \n import org.apache.logging.log4j.message.Message;\n-import org.apache.logging.log4j.message.MessageFactory;\n+import org.apache.logging.log4j.message.MessageFactory2;\n import org.apache.logging.log4j.message.ObjectMessage;\n import org.apache.logging.log4j.message.ParameterizedMessage;\n import org.apache.logging.log4j.message.SimpleMessage;\n@@ -30,8 +30,10 @@\n \n /**\n  * Used in Log4j configuration.\n+ *\n+ * Requires Log4j 2.6 and above.\n  * */\n-public final class LogstashMessageFactory implements MessageFactory {\n+public final class LogstashMessageFactory implements MessageFactory2 {\n \n     public static final LogstashMessageFactory INSTANCE = new LogstashMessageFactory();\n \n@@ -53,4 +55,59 @@ public Message newMessage(String message, Object... params) {\n             return new ParameterizedMessage(message, params);\n         }\n     }\n+\n+    @Override\n+    public Message newMessage(CharSequence charSequence) {\n+        return new SimpleMessage(charSequence);\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0) {\n+        return newMessage(message, new Object[]{p0});\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0, Object p1) {\n+        return newMessage(message, new Object[]{p0, p1});\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0, Object p1, Object p2) {\n+        return newMessage(message, new Object[]{p0, p1, p2});\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0, Object p1, Object p2, Object p3) {\n+        return newMessage(message, new Object[]{p0, p1, p2, p3});\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0, Object p1, Object p2, Object p3, Object p4) {\n+        return newMessage(message, new Object[]{p0, p1, p2, p3, p4});\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0, Object p1, Object p2, Object p3, Object p4, Object p5) {\n+        return newMessage(message, new Object[]{p0, p1, p2, p3, p4, p5});\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0, Object p1, Object p2, Object p3, Object p4, Object p5, Object p6) {\n+        return newMessage(message, new Object[]{p0, p1, p2, p3, p4, p5, p6});\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0, Object p1, Object p2, Object p3, Object p4, Object p5, Object p6, Object p7) {\n+        return newMessage(message, new Object[]{p0, p1, p2, p3, p4, p5, p6, p7});\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0, Object p1, Object p2, Object p3, Object p4, Object p5, Object p6, Object p7, Object p8) {\n+        return newMessage(message, new Object[]{p0, p1, p2, p3, p4, p5, p6, p7, p8});\n+    }\n+\n+    @Override\n+    public Message newMessage(String message, Object p0, Object p1, Object p2, Object p3, Object p4, Object p5, Object p6, Object p7, Object p8, Object p9) {\n+        return newMessage(message, new Object[]{p0, p1, p2, p3, p4, p5, p6, p7, p8, p9});\n+    }\n }",
      "parent_sha": "90aae6a6f341896d83022eb3b6b7e0bf2a765874"
    }
  },
  {
    "oid": "386c263a7147a82a751b27f5831f1f67a78a5663",
    "message": "Fix: rounded to 1 second in nanos to avoid random NaN error\n\nConversion to seconds of values under 1_000_000_000 nanoseconds translates to value 0, and this led to NaN when used as denominator in a division.\nA value of 996_920_400 nanoseconds once converted to seconds is not rounded to 1 second by to 0, this manifest on Windows OS",
    "date": "2020-05-11T12:23:52Z",
    "url": "https://github.com/elastic/logstash/commit/386c263a7147a82a751b27f5831f1f67a78a5663",
    "details": {
      "sha": "09271c070c942b7a82bf0604450eec681096beb5",
      "filename": "tools/benchmark-cli/src/main/java/org/logstash/benchmark/cli/LsMetricsMonitor.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/386c263a7147a82a751b27f5831f1f67a78a5663/tools%2Fbenchmark-cli%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fbenchmark%2Fcli%2FLsMetricsMonitor.java",
      "raw_url": "https://github.com/elastic/logstash/raw/386c263a7147a82a751b27f5831f1f67a78a5663/tools%2Fbenchmark-cli%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fbenchmark%2Fcli%2FLsMetricsMonitor.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/tools%2Fbenchmark-cli%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fbenchmark%2Fcli%2FLsMetricsMonitor.java?ref=386c263a7147a82a751b27f5831f1f67a78a5663",
      "patch": "@@ -74,7 +74,7 @@ public EnumMap<LsMetricStats, ListStatistics> call() throws IOException {\n                 final long newstrt = System.nanoTime();\n                 stats.addValue(\n                     (double) (newcount - count) /\n-                        (double) TimeUnit.SECONDS.convert(newstrt - start, TimeUnit.NANOSECONDS)\n+                        (double) TimeUnit.SECONDS.convert(Math.max(newstrt - start, 1_000_000_000), TimeUnit.NANOSECONDS)\n                 );\n                 start = newstrt;\n                 count = newcount;",
      "parent_sha": "e2d48b4e61e561fb35db4002fe451bdfe4be8fc3"
    }
  },
  {
    "oid": "3a4c1ec7e4e8dc3b2e336ed563f785990bf7a9e4",
    "message": "use static Jackson mapper",
    "date": "2016-01-29T22:37:51Z",
    "url": "https://github.com/elastic/logstash/commit/3a4c1ec7e4e8dc3b2e336ed563f785990bf7a9e4",
    "details": {
      "sha": "cc51f011c2685717a27d4b313786c39d68204bf8",
      "filename": "logstash-core-event-java/src/main/java/com/logstash/Event.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/3a4c1ec7e4e8dc3b2e336ed563f785990bf7a9e4/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Fcom%2Flogstash%2FEvent.java",
      "raw_url": "https://github.com/elastic/logstash/raw/3a4c1ec7e4e8dc3b2e336ed563f785990bf7a9e4/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Fcom%2Flogstash%2FEvent.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Fcom%2Flogstash%2FEvent.java?ref=3a4c1ec7e4e8dc3b2e336ed563f785990bf7a9e4",
      "patch": "@@ -28,7 +28,7 @@ public class Event implements Cloneable, Serializable {\n     public static final String VERSION_ONE = \"1\";\n \n     private static final Logger DEFAULT_LOGGER = new StdioLogger();\n-    private transient final ObjectMapper mapper = new ObjectMapper();\n+    private static final ObjectMapper mapper = new ObjectMapper();\n \n     // logger is static since once set there is no point in changing it at runtime\n     // for other reasons than in tests/specs.",
      "parent_sha": "4e45dba8fddf8444120048c7c06ee2a381dd7daa"
    }
  },
  {
    "oid": "532a0304e91e5a3d763feff50ab09c56773ef831",
    "message": "PERFORMANCE: Better hashCode and equals for RubyEvent\n\nFixes #7902",
    "date": "2017-08-04T10:40:53Z",
    "url": "https://github.com/elastic/logstash/commit/532a0304e91e5a3d763feff50ab09c56773ef831",
    "details": {
      "sha": "bee50faaed23001cf7654e98391a78462a983abd",
      "filename": "logstash-core/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java",
      "status": "modified",
      "additions": 33,
      "deletions": 1,
      "changes": 34,
      "blob_url": "https://github.com/elastic/logstash/blob/532a0304e91e5a3d763feff50ab09c56773ef831/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJrubyEventExtLibrary.java",
      "raw_url": "https://github.com/elastic/logstash/raw/532a0304e91e5a3d763feff50ab09c56773ef831/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJrubyEventExtLibrary.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fext%2FJrubyEventExtLibrary.java?ref=532a0304e91e5a3d763feff50ab09c56773ef831",
      "patch": "@@ -2,6 +2,7 @@\n \n import java.io.IOException;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicLong;\n import org.jruby.Ruby;\n import org.jruby.RubyArray;\n import org.jruby.RubyBoolean;\n@@ -65,9 +66,21 @@ public void load(Ruby runtime, boolean wrap) throws IOException {\n \n     @JRubyClass(name = \"Event\")\n     public static final class RubyEvent extends RubyObject {\n+\n+        /**\n+         * Sequence number generator, for generating {@link RubyEvent#hash}.\n+         */\n+        private static final AtomicLong SEQUENCE_GENERATOR = new AtomicLong(1L);\n+\n+        /**\n+         * Hashcode of this instance. Used to avoid the more expensive {@link RubyObject#hashCode()}\n+         * since we only care about reference equality for this class anyway.\n+         */\n+        private final int hash = nextHash();\n+\n         private Event event;\n \n-        private RubyEvent(Ruby runtime, RubyClass klass) {\n+        private RubyEvent(final Ruby runtime, final RubyClass klass) {\n             super(runtime, klass);\n         }\n \n@@ -286,6 +299,16 @@ public IRubyObject ruby_set_timestamp(ThreadContext context, IRubyObject value)\n             return value;\n         }\n \n+        @Override\n+        public int hashCode() {\n+            return hash;\n+        }\n+\n+        @Override\n+        public boolean equals(final Object that) {\n+            return this == that;\n+        }\n+\n         /**\n          * Cold path for the Ruby constructor\n          * {@link JrubyEventExtLibrary.RubyEvent#ruby_initialize(ThreadContext, IRubyObject[])} for\n@@ -309,5 +332,14 @@ private void initializeFallback(final ThreadContext context, final IRubyObject d\n         private void setEvent(Event event) {\n             this.event = event;\n         }\n+\n+        /**\n+         * Generates a fixed hashcode.\n+         * @return HashCode value\n+         */\n+        private static int nextHash() {\n+            final long sequence = SEQUENCE_GENERATOR.incrementAndGet();\n+            return (int) (sequence ^ sequence >>> 32) + 31;\n+        }\n     }\n }",
      "parent_sha": "ce4b854b3d08988be6710d6b3a123347bb77c367"
    }
  },
  {
    "oid": "ad79d7c884064f8b45ded278ae928fd490c335fe",
    "message": "bck\n\nFixes #9538",
    "date": "2018-05-11T16:54:54Z",
    "url": "https://github.com/elastic/logstash/commit/ad79d7c884064f8b45ded278ae928fd490c335fe",
    "details": {
      "sha": "2b9122e0a5d8421c602e9b08abb144207a68b57e",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/QueueUpgrade.java",
      "status": "modified",
      "additions": 7,
      "deletions": 3,
      "changes": 10,
      "blob_url": "https://github.com/elastic/logstash/blob/ad79d7c884064f8b45ded278ae928fd490c335fe/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java",
      "raw_url": "https://github.com/elastic/logstash/raw/ad79d7c884064f8b45ded278ae928fd490c335fe/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueUpgrade.java?ref=ad79d7c884064f8b45ded278ae928fd490c335fe",
      "patch": "@@ -49,7 +49,13 @@ public static void upgradeQueueDirectoryToV2(final Path path) throws IOException\n                     try (final MmapPageIOV1 iov1 = new MmapPageIOV1(\n                         num, Ints.checkedCast(v1PageFile.length()), path\n                     )) {\n-                        final Checkpoint cp = cpIo.read(cpIo.tailFileName(num));\n+                        final String cpFilename = cpIo.tailFileName(num);\n+                        final Checkpoint cp;\n+                        if (path.resolve(cpFilename).toFile().exists()) {\n+                            cp = cpIo.read(cpFilename);\n+                        } else {\n+                            cp = cpIo.read(\"checkpoint.head\");\n+                        }\n                         final int count = cp.getElementCount();\n                         final long minSeqNum = cp.getMinSeqNum();\n                         iov1.open(minSeqNum, count);\n@@ -75,8 +81,6 @@ public static void upgradeQueueDirectoryToV2(final Path path) throws IOException\n                         raf.writeByte((int) MmapPageIOV2.VERSION_TWO);\n                     }\n                 }\n-            } catch (final Exception ex) {\n-                throw new IllegalStateException(\"Queue upgrade to V2 failed.\", ex);\n             }\n             Files.write(upgradeFile.toPath(), Ints.toByteArray(2), StandardOpenOption.CREATE);\n         } else {",
      "parent_sha": "9d4452bba44f46b3ad6d4649beadc80c1b1fc5b5"
    }
  },
  {
    "oid": "0615f1b097d95c9502957ade5cf521840d70b216",
    "message": "Fix using deprecated serializer\n\nFixes #8416",
    "date": "2017-10-02T14:31:47Z",
    "url": "https://github.com/elastic/logstash/commit/0615f1b097d95c9502957ade5cf521840d70b216",
    "details": {
      "sha": "63e300802c06db56c1beccbd096e1da6e198d028",
      "filename": "logstash-core/src/main/java/org/logstash/ObjectMappers.java",
      "status": "modified",
      "additions": 36,
      "deletions": 14,
      "changes": 50,
      "blob_url": "https://github.com/elastic/logstash/blob/0615f1b097d95c9502957ade5cf521840d70b216/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FObjectMappers.java",
      "raw_url": "https://github.com/elastic/logstash/raw/0615f1b097d95c9502957ade5cf521840d70b216/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FObjectMappers.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FObjectMappers.java?ref=0615f1b097d95c9502957ade5cf521840d70b216",
      "patch": "@@ -9,7 +9,7 @@\n import com.fasterxml.jackson.databind.deser.std.StdDeserializer;\n import com.fasterxml.jackson.databind.jsontype.TypeSerializer;\n import com.fasterxml.jackson.databind.module.SimpleModule;\n-import com.fasterxml.jackson.databind.ser.std.NonTypedScalarSerializerBase;\n+import com.fasterxml.jackson.databind.ser.std.StdScalarSerializer;\n import com.fasterxml.jackson.databind.ser.std.StdSerializer;\n import com.fasterxml.jackson.dataformat.cbor.CBORFactory;\n import com.fasterxml.jackson.dataformat.cbor.CBORGenerator;\n@@ -61,15 +61,35 @@ public final class ObjectMappers {\n     private ObjectMappers() {\n     }\n \n+    /**\n+     * Serializer for scalar types that does not write type information when called via\n+     * {@link ObjectMappers.NonTypedScalarSerializer#serializeWithType(\n+     * Object, JsonGenerator, SerializerProvider, TypeSerializer)}.\n+     * @param <T>\n+     */\n+    private abstract static class NonTypedScalarSerializer<T> extends StdScalarSerializer<T> {\n+\n+        NonTypedScalarSerializer(final Class<T> t) {\n+            super(t);\n+        }\n+\n+        @Override\n+        public final void serializeWithType(final T value, final JsonGenerator gen, \n+            final SerializerProvider provider, final TypeSerializer typeSer) throws IOException {\n+            serialize(value, gen, provider);\n+        }\n+    }\n+\n+\n     /**\n      * Serializer for {@link RubyString} since Jackson can't handle that type natively, so we\n      * simply serialize it as if it were a {@link String}.\n      */\n     private static final class RubyStringSerializer\n-        extends NonTypedScalarSerializerBase<RubyString> {\n+        extends ObjectMappers.NonTypedScalarSerializer<RubyString> {\n \n         RubyStringSerializer() {\n-            super(RubyString.class, true);\n+            super(RubyString.class);\n         }\n \n         @Override\n@@ -85,10 +105,10 @@ public void serialize(final RubyString value, final JsonGenerator generator,\n      * simply serialize it as if it were a {@link String}.\n      */\n     private static final class RubySymbolSerializer\n-        extends NonTypedScalarSerializerBase<RubySymbol> {\n+        extends ObjectMappers.NonTypedScalarSerializer<RubySymbol> {\n \n         RubySymbolSerializer() {\n-            super(RubySymbol.class, true);\n+            super(RubySymbol.class);\n         }\n \n         @Override\n@@ -104,7 +124,7 @@ public void serialize(final RubySymbol value, final JsonGenerator generator,\n      * simply serialize it as if it were a {@code double}.\n      */\n     private static final class RubyFloatSerializer\n-        extends NonTypedScalarSerializerBase<RubyFloat> {\n+        extends ObjectMappers.NonTypedScalarSerializer<RubyFloat> {\n \n         RubyFloatSerializer() {\n             super(RubyFloat.class);\n@@ -122,7 +142,7 @@ public void serialize(final RubyFloat value, final JsonGenerator generator,\n      * simply serialize it as if it were a {@code boolean}.\n      */\n     private static final class RubyBooleanSerializer\n-        extends NonTypedScalarSerializerBase<RubyBoolean> {\n+        extends ObjectMappers.NonTypedScalarSerializer<RubyBoolean> {\n \n         RubyBooleanSerializer() {\n             super(RubyBoolean.class);\n@@ -139,11 +159,11 @@ public void serialize(final RubyBoolean value, final JsonGenerator generator,\n      * Serializer for {@link RubyFixnum} since Jackson can't handle that type natively, so we\n      * simply serialize it as if it were a {@code long}.\n      */\n-    private static final class RubyFixnumSerializer\n-        extends NonTypedScalarSerializerBase<RubyFixnum> {\n+    private static final class RubyFixnumSerializer \n+        extends ObjectMappers.NonTypedScalarSerializer<RubyFixnum> {\n \n         RubyFixnumSerializer() {\n-            super(RubyFixnum.class, true);\n+            super(RubyFixnum.class);\n         }\n \n         @Override\n@@ -196,10 +216,11 @@ public Timestamp deserialize(final JsonParser p, final DeserializationContext ct\n      * Serializer for {@link RubyBignum} since Jackson can't handle that type natively, so we\n      * simply serialize it as if it were a {@link BigInteger}.\n      */\n-    private static final class RubyBignumSerializer extends NonTypedScalarSerializerBase<RubyBignum> {\n+    private static final class RubyBignumSerializer\n+        extends ObjectMappers.NonTypedScalarSerializer<RubyBignum> {\n \n         RubyBignumSerializer() {\n-            super(RubyBignum.class, true);\n+            super(RubyBignum.class);\n         }\n \n         @Override\n@@ -213,10 +234,11 @@ public void serialize(final RubyBignum value, final JsonGenerator jgen,\n      * Serializer for {@link BigDecimal} since Jackson can't handle that type natively, so we\n      * simply serialize it as if it were a {@link BigDecimal}.\n      */\n-    private static final class RubyBigDecimalSerializer extends NonTypedScalarSerializerBase<RubyBigDecimal> {\n+    private static final class RubyBigDecimalSerializer\n+        extends ObjectMappers.NonTypedScalarSerializer<RubyBigDecimal> {\n \n         RubyBigDecimalSerializer() {\n-            super(RubyBigDecimal.class, true);\n+            super(RubyBigDecimal.class);\n         }\n \n         @Override",
      "parent_sha": "36381dd4491de2feba70aedd2bfd3e485c72cb51"
    }
  },
  {
    "oid": "7c9ce4c2857fa25abe50577ce714c9ca9b6ff875",
    "message": "count unused space in page files towards current PQ size\n\nFixes #10052",
    "date": "2018-10-30T16:35:38Z",
    "url": "https://github.com/elastic/logstash/commit/7c9ce4c2857fa25abe50577ce714c9ca9b6ff875",
    "details": {
      "sha": "a498f447e3a9997033de2b2612bfe86f93337cea",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java",
      "status": "modified",
      "additions": 10,
      "deletions": 7,
      "changes": 17,
      "blob_url": "https://github.com/elastic/logstash/blob/7c9ce4c2857fa25abe50577ce714c9ca9b6ff875/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "raw_url": "https://github.com/elastic/logstash/raw/7c9ce4c2857fa25abe50577ce714c9ca9b6ff875/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java?ref=7c9ce4c2857fa25abe50577ce714c9ca9b6ff875",
      "patch": "@@ -158,7 +158,7 @@ public void open() throws IOException {\n \n                 logger.debug(\"No head checkpoint found at: {}, creating new head page\", checkpointIO.headFileName());\n \n-                this.ensureDiskAvailable(this.maxBytes);\n+                this.ensureDiskAvailable(this.maxBytes, 0);\n \n                 this.seqNum = 0;\n                 headPageNum = 0;\n@@ -172,7 +172,7 @@ public void open() throws IOException {\n             // at this point we have a head checkpoint to figure queue recovery\n \n             // as we load pages, compute actually disk needed substracting existing pages size to the required maxBytes\n-            long diskNeeded = this.maxBytes;\n+            long pqSizeBytes = 0;\n \n             // reconstruct all tail pages state upto but excluding the head page\n             for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {\n@@ -192,7 +192,7 @@ public void open() throws IOException {\n                 } else {\n                     pageIO.open(cp.getMinSeqNum(), cp.getElementCount());\n                     addTailPage(PageFactory.newTailPage(cp, this, pageIO));\n-                    diskNeeded -= (long)pageIO.getHead();\n+                    pqSizeBytes += pageIO.getCapacity();\n                 }\n \n                 // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum\n@@ -209,7 +209,8 @@ public void open() throws IOException {\n             PageIO pageIO = new MmapPageIOV2(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);\n             pageIO.recover(); // optimistically recovers the head page data file and set minSeqNum and elementCount to the actual read/recovered data\n \n-            ensureDiskAvailable(diskNeeded - (long)pageIO.getHead());\n+            pqSizeBytes += (long)pageIO.getHead();\n+            ensureDiskAvailable(this.maxBytes, pqSizeBytes);\n \n             if (pageIO.getMinSeqNum() != headCheckpoint.getMinSeqNum() || pageIO.getElementCount() != headCheckpoint.getElementCount()) {\n                 // the recovered page IO shows different minSeqNum or elementCount than the checkpoint, use the page IO attributes\n@@ -784,9 +785,11 @@ private boolean isTailPage(Page p) {\n         return !isHeadPage(p);\n     }\n \n-    private void ensureDiskAvailable(final long diskNeeded) throws IOException {\n-        if (!FsUtil.hasFreeSpace(this.dirPath, diskNeeded)) {\n-            throw new IOException(\"Not enough free disk space available to allocate persisted queue.\");\n+    private void ensureDiskAvailable(final long maxPqSize, long currentPqSize) throws IOException {\n+        if (!FsUtil.hasFreeSpace(this.dirPath, maxPqSize - currentPqSize)) {\n+            throw new IOException(\n+                    String.format(\"Unable to allocate %d more bytes for persisted queue on top of its current usage of %d bytes\",\n+                            maxPqSize - currentPqSize, currentPqSize));\n         }\n     }\n ",
      "parent_sha": "0adb99c3990097d21a496649ad4fdf0e3c75088e"
    }
  },
  {
    "oid": "0d04a1c542bed2437f6419a619edb0d42491b21c",
    "message": "queue test cleanup\n\nFixes #6297",
    "date": "2016-11-29T10:10:42Z",
    "url": "https://github.com/elastic/logstash/commit/0d04a1c542bed2437f6419a619edb0d42491b21c",
    "details": {
      "sha": "47261bb9b0f5a1dc6f9687ba57f85f4117506d42",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 26,
      "changes": 28,
      "blob_url": "https://github.com/elastic/logstash/blob/0d04a1c542bed2437f6419a619edb0d42491b21c/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/0d04a1c542bed2437f6419a619edb0d42491b21c/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=0d04a1c542bed2437f6419a619edb0d42491b21c",
      "patch": "@@ -399,7 +399,7 @@ public void reachMaxSizeTest() throws IOException, InterruptedException, Executi\n         TestQueue q = new TestQueue(settings);\n         q.open();\n \n-        int ELEMENT_COUNT = 99; // should be able to write 100 events before getting full\n+        int ELEMENT_COUNT = 99; // should be able to write 99 events before getting full\n         for (int i = 0; i < ELEMENT_COUNT; i++) {\n             long seqNum = q.write(element);\n         }\n@@ -416,32 +416,8 @@ public void reachMaxSizeTest() throws IOException, InterruptedException, Executi\n \n         Thread.sleep(1);\n         assertThat(q.isFull(), is(true));\n-        // spin wait until data is written and write blocks\n-        //while (!q.isFull()) { Thread.sleep(1); }\n-            /*\n \n-\n-            // read one element, which will unblock the last write\n-            Batch b = q.nonBlockReadBatch(1);\n-            assertThat(b, is(notNullValue()));\n-            assertThat(b.getElements().size(), is(equalTo(1)));\n-            b.close();\n-\n-            // future result is the blocked write seqNum for the second element\n-            assertThat(future.get(), is(equalTo(2L + i)));\n-            assertThat(q.isFull(), is(false));\n-\n-            executor.shutdown();\n-        }\n-\n-        // all batches are acked, no tail pages should exist\n-        assertThat(q.getTailPages().size(), is(equalTo(0)));\n-\n-        // the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page\n-        assertThat(q.getHeadPage().getElementCount() > 0L, is(true));\n-        assertThat(q.getHeadPage().unreadCount(), is(equalTo(1L)));\n-        assertThat(q.unreadCount, is(equalTo(1L)));\n-        */\n+        executor.shutdown();\n     }\n \n }\n\\ No newline at end of file",
      "parent_sha": "100b846d36883f45affbadc1db928b3ab0ded46e"
    }
  },
  {
    "oid": "2ab35b61ea5b090f79f3fb9a20197041bf7e2b34",
    "message": "#7399 stress test full queue situation\n\nFixes #7405",
    "date": "2017-06-13T14:27:36Z",
    "url": "https://github.com/elastic/logstash/commit/2ab35b61ea5b090f79f3fb9a20197041bf7e2b34",
    "details": {
      "sha": "860275480ff4116fe6a6337355bf4a98e00ce786",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 54,
      "deletions": 45,
      "changes": 99,
      "blob_url": "https://github.com/elastic/logstash/blob/2ab35b61ea5b090f79f3fb9a20197041bf7e2b34/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/2ab35b61ea5b090f79f3fb9a20197041bf7e2b34/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=2ab35b61ea5b090f79f3fb9a20197041bf7e2b34",
      "patch": "@@ -563,51 +563,13 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup\n     }\n \n     @Test\n-    public void queueStableUnderStress() throws Exception {\n-        Settings settings = TestSettings.persistedQueueSettings(1000000, dataPath);\n-        final ExecutorService exec = Executors.newScheduledThreadPool(2);\n-        try (Queue queue = new Queue(settings)) {\n-            final int count = 20_000;\n-            final int concurrent = 2;\n-            queue.open();\n-            final Future<Integer>[] futures = new Future[concurrent];\n-            for (int c = 0; c < concurrent; ++c) {\n-                futures[c] = exec.submit(() -> {\n-                    int i = 0;\n-                    try {\n-                        while (i < count / concurrent) {\n-                            final Batch batch = queue.readBatch(1);\n-                            for (final Queueable elem : batch.getElements()) {\n-                                if (elem != null) {\n-                                    ++i;\n-                                }\n-                            }\n-                        }\n-                        return i;\n-                    } catch (final IOException ex) {\n-                        throw new IllegalStateException(ex);\n-                    }\n-                });\n-            }\n-            for (int i = 0; i < count; ++i) {\n-                try {\n-                    final Queueable evnt = new StringElement(\"foo\");\n-                    queue.write(evnt);\n-                } catch (final IOException ex) {\n-                    throw new IllegalStateException(ex);\n-                }\n-            }\n-            assertThat(\n-                Arrays.stream(futures).map(i -> {\n-                    try {\n-                        return i.get(10L, TimeUnit.SECONDS);\n-                    } catch (final InterruptedException | ExecutionException | TimeoutException ex) {\n-                        throw new IllegalStateException(ex);\n-                    }\n-                }).reduce((x, y) -> x + y).orElse(0),\n-                is(20_000)\n-            );\n-        }\n+    public void queueStableUnderStressHugeCapacity() throws Exception {\n+        stableUnderStress(100_000);\n+    }\n+    \n+    @Test\n+    public void queueStableUnderStressLowCapacity() throws Exception {\n+        stableUnderStress(50);\n     }\n \n     @Test\n@@ -755,4 +717,51 @@ public void getsPersistedByteSizeCorrectlyForUnopened() throws Exception {\n             assertThat(q.getPersistedByteSize(), is(0L));\n         }\n     }\n+    \n+    private void stableUnderStress(final int capacity) throws IOException {\n+        Settings settings = TestSettings.persistedQueueSettings(capacity, dataPath);\n+        final ExecutorService exec = Executors.newScheduledThreadPool(2);\n+        try (Queue queue = new Queue(settings)) {\n+            final int count = 20_000;\n+            final int concurrent = 2;\n+            queue.open();\n+            final Future<Integer>[] futures = new Future[concurrent];\n+            for (int c = 0; c < concurrent; ++c) {\n+                futures[c] = exec.submit(() -> {\n+                    int i = 0;\n+                    try {\n+                        while (i < count / concurrent) {\n+                            final Batch batch = queue.readBatch(1);\n+                            for (final Queueable elem : batch.getElements()) {\n+                                if (elem != null) {\n+                                    ++i;\n+                                }\n+                            }\n+                        }\n+                        return i;\n+                    } catch (final IOException ex) {\n+                        throw new IllegalStateException(ex);\n+                    }\n+                });\n+            }\n+            for (int i = 0; i < count; ++i) {\n+                try {\n+                    final Queueable evnt = new StringElement(\"foo\");\n+                    queue.write(evnt);\n+                } catch (final IOException ex) {\n+                    throw new IllegalStateException(ex);\n+                }\n+            }\n+            assertThat(\n+                Arrays.stream(futures).map(i -> {\n+                    try {\n+                        return i.get(2L, TimeUnit.MINUTES);\n+                    } catch (final InterruptedException | ExecutionException | TimeoutException ex) {\n+                        throw new IllegalStateException(ex);\n+                    }\n+                }).reduce((x, y) -> x + y).orElse(0),\n+                is(20_000)\n+            );\n+        }\n+    }\n }",
      "parent_sha": "f5c6c5a4b1147cf38ef4a75ef0c47eeee9719830"
    }
  },
  {
    "oid": "088838750513019daf600da52c0186949ac693d7",
    "message": "Update Queue.java\n\nFixes #8865",
    "date": "2017-12-21T17:57:37Z",
    "url": "https://github.com/elastic/logstash/commit/088838750513019daf600da52c0186949ac693d7",
    "details": {
      "sha": "6a7d540df952b7955d9470479ece21488c9dd29e",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/088838750513019daf600da52c0186949ac693d7/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "raw_url": "https://github.com/elastic/logstash/raw/088838750513019daf600da52c0186949ac693d7/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java?ref=088838750513019daf600da52c0186949ac693d7",
      "patch": "@@ -496,7 +496,7 @@ public void ensurePersistedUpto(long seqNum) throws IOException{\n     /**\n      * non-blocking queue read\n      *\n-     * @param limit read the next batch of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available\n+     * @param limit read the next batch of size up to this limit. the returned batch size can be smaller than the requested limit if fewer elements are available\n      * @return {@link Batch} the batch containing 1 or more element up to the required limit or null of no elements were available\n      * @throws IOException\n      */",
      "parent_sha": "29de8ecd53c337a136e7902e9b0a3ad2f6b6edfb"
    }
  },
  {
    "oid": "b7f4fd7ebf51647d17821b0422388764ffa63f53",
    "message": "Cleanup Queue not being safely closed in QueueTest\n\nFixes #7063",
    "date": "2017-05-19T05:04:37Z",
    "url": "https://github.com/elastic/logstash/commit/b7f4fd7ebf51647d17821b0422388764ffa63f53",
    "details": {
      "sha": "3bb3aa9806ef8f08bb91e90b00a63fee815b31eb",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 411,
      "deletions": 420,
      "changes": 831,
      "blob_url": "https://github.com/elastic/logstash/blob/b7f4fd7ebf51647d17821b0422388764ffa63f53/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/b7f4fd7ebf51647d17821b0422388764ffa63f53/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=b7f4fd7ebf51647d17821b0422388764ffa63f53",
      "patch": "@@ -38,158 +38,153 @@ public void setUp() throws Exception {\n \n     @Test\n     public void newQueue() throws IOException {\n-        Queue q = new TestQueue(TestSettings.volatileQueueSettings(10));\n-        q.open();\n-\n-        assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));\n+        try (Queue q = new TestQueue(TestSettings.volatileQueueSettings(10))) {\n+            q.open();\n \n-        q.close();\n+            assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));\n+        }\n     }\n \n     @Test\n     public void singleWriteRead() throws IOException {\n-        Queue q = new TestQueue(TestSettings.volatileQueueSettings(100));\n-        q.open();\n-\n-        Queueable element = new StringElement(\"foobarbaz\");\n-        q.write(element);\n+        try (Queue q = new TestQueue(TestSettings.volatileQueueSettings(100))) {\n+            q.open();\n \n-        Batch b = q.nonBlockReadBatch(1);\n+            Queueable element = new StringElement(\"foobarbaz\");\n+            q.write(element);\n \n-        assertThat(b.getElements().size(), is(equalTo(1)));\n-        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));\n-        assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));\n+            Batch b = q.nonBlockReadBatch(1);\n \n-        q.close();\n+            assertThat(b.getElements().size(), is(equalTo(1)));\n+            assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));\n+            assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));\n+        }\n     }\n \n     @Test\n     public void singleWriteMultiRead() throws IOException {\n-        Queue q = new TestQueue(TestSettings.volatileQueueSettings(100));\n-        q.open();\n-\n-        Queueable element = new StringElement(\"foobarbaz\");\n-        q.write(element);\n+        try (Queue q = new TestQueue(TestSettings.volatileQueueSettings(100))) {\n+            q.open();\n \n-        Batch b = q.nonBlockReadBatch(2);\n+            Queueable element = new StringElement(\"foobarbaz\");\n+            q.write(element);\n \n-        assertThat(b.getElements().size(), is(equalTo(1)));\n-        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));\n-        assertThat(q.nonBlockReadBatch(2), is(equalTo(null)));\n+            Batch b = q.nonBlockReadBatch(2);\n \n-        q.close();\n+            assertThat(b.getElements().size(), is(equalTo(1)));\n+            assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));\n+            assertThat(q.nonBlockReadBatch(2), is(equalTo(null)));\n+        }\n     }\n \n     @Test\n     public void multiWriteSamePage() throws IOException {\n-        Queue q = new TestQueue(TestSettings.volatileQueueSettings(100));\n-        q.open();\n-\n-        List<Queueable> elements = Arrays.asList(new StringElement(\"foobarbaz1\"), new StringElement(\"foobarbaz2\"), new StringElement(\"foobarbaz3\"));\n-\n-        for (Queueable e : elements) {\n-            q.write(e);\n-        }\n-\n-        Batch b = q.nonBlockReadBatch(2);\n+        try (Queue q = new TestQueue(TestSettings.volatileQueueSettings(100))) {\n+            q.open();\n+            List<Queueable> elements = Arrays\n+                .asList(new StringElement(\"foobarbaz1\"), new StringElement(\"foobarbaz2\"),\n+                    new StringElement(\"foobarbaz3\")\n+                );\n+            for (Queueable e : elements) {\n+                q.write(e);\n+            }\n \n-        assertThat(b.getElements().size(), is(equalTo(2)));\n-        assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(0).toString())));\n-        assertThat(b.getElements().get(1).toString(), is(equalTo(elements.get(1).toString())));\n+            Batch b = q.nonBlockReadBatch(2);\n \n-        b = q.nonBlockReadBatch(2);\n+            assertThat(b.getElements().size(), is(equalTo(2)));\n+            assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(0).toString())));\n+            assertThat(b.getElements().get(1).toString(), is(equalTo(elements.get(1).toString())));\n \n-        assertThat(b.getElements().size(), is(equalTo(1)));\n-        assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(2).toString())));\n+            b = q.nonBlockReadBatch(2);\n \n-        q.close();\n+            assertThat(b.getElements().size(), is(equalTo(1)));\n+            assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(2).toString())));\n+        }\n     }\n \n     @Test\n     public void writeMultiPage() throws IOException {\n         List<Queueable> elements = Arrays.asList(new StringElement(\"foobarbaz1\"), new StringElement(\"foobarbaz2\"), new StringElement(\"foobarbaz3\"), new StringElement(\"foobarbaz4\"));\n         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);\n+        try (TestQueue q = new TestQueue(\n+            TestSettings.volatileQueueSettings(2 * singleElementCapacity))) {\n+            q.open();\n \n-        TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(2 * singleElementCapacity));\n-        q.open();\n-\n-        for (Queueable e : elements) {\n-            q.write(e);\n-        }\n-\n-        // total of 2 pages: 1 head and 1 tail\n-        assertThat(q.getTailPages().size(), is(equalTo(1)));\n+            for (Queueable e : elements) {\n+                q.write(e);\n+            }\n \n-        assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(false)));\n-        assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));\n-        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));\n-        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n+            // total of 2 pages: 1 head and 1 tail\n+            assertThat(q.getTailPages().size(), is(equalTo(1)));\n \n-        Batch b = q.nonBlockReadBatch(10);\n-        assertThat(b.getElements().size(), is(equalTo(2)));\n+            assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(false)));\n+            assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));\n+            assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));\n+            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n \n-        assertThat(q.getTailPages().size(), is(equalTo(1)));\n+            Batch b = q.nonBlockReadBatch(10);\n+            assertThat(b.getElements().size(), is(equalTo(2)));\n \n-        assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(true)));\n-        assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));\n-        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));\n-        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n+            assertThat(q.getTailPages().size(), is(equalTo(1)));\n \n-        b = q.nonBlockReadBatch(10);\n-        assertThat(b.getElements().size(), is(equalTo(2)));\n+            assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(true)));\n+            assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));\n+            assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));\n+            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n \n-        assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(true)));\n-        assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));\n-        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));\n-        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n+            b = q.nonBlockReadBatch(10);\n+            assertThat(b.getElements().size(), is(equalTo(2)));\n \n-        b = q.nonBlockReadBatch(10);\n-        assertThat(b, is(equalTo(null)));\n+            assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(true)));\n+            assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));\n+            assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));\n+            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n \n-        q.close();\n+            b = q.nonBlockReadBatch(10);\n+            assertThat(b, is(equalTo(null)));\n+        }\n     }\n \n \n     @Test\n     public void writeMultiPageWithInOrderAcking() throws IOException {\n         List<Queueable> elements = Arrays.asList(new StringElement(\"foobarbaz1\"), new StringElement(\"foobarbaz2\"), new StringElement(\"foobarbaz3\"), new StringElement(\"foobarbaz4\"));\n         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);\n+        try (TestQueue q = new TestQueue(\n+            TestSettings.volatileQueueSettings(2 * singleElementCapacity))) {\n+            q.open();\n \n-        TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(2 * singleElementCapacity));\n-        q.open();\n-\n-        for (Queueable e : elements) {\n-            q.write(e);\n-        }\n-\n-        Batch b = q.nonBlockReadBatch(10);\n+            for (Queueable e : elements) {\n+                q.write(e);\n+            }\n \n-        assertThat(b.getElements().size(), is(equalTo(2)));\n-        assertThat(q.getTailPages().size(), is(equalTo(1)));\n+            Batch b = q.nonBlockReadBatch(10);\n \n-        // lets keep a ref to that tail page before acking\n-        TailPage tailPage = q.getTailPages().get(0);\n+            assertThat(b.getElements().size(), is(equalTo(2)));\n+            assertThat(q.getTailPages().size(), is(equalTo(1)));\n \n-        assertThat(tailPage.isFullyRead(), is(equalTo(true)));\n+            // lets keep a ref to that tail page before acking\n+            TailPage tailPage = q.getTailPages().get(0);\n \n-        // ack first batch which includes all elements from tailPages\n-        b.close();\n+            assertThat(tailPage.isFullyRead(), is(equalTo(true)));\n \n-        assertThat(q.getTailPages().size(), is(equalTo(0)));\n-        assertThat(tailPage.isFullyRead(), is(equalTo(true)));\n-        assertThat(tailPage.isFullyAcked(), is(equalTo(true)));\n+            // ack first batch which includes all elements from tailPages\n+            b.close();\n \n-        b = q.nonBlockReadBatch(10);\n+            assertThat(q.getTailPages().size(), is(equalTo(0)));\n+            assertThat(tailPage.isFullyRead(), is(equalTo(true)));\n+            assertThat(tailPage.isFullyAcked(), is(equalTo(true)));\n \n-        assertThat(b.getElements().size(), is(equalTo(2)));\n-        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));\n-        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n+            b = q.nonBlockReadBatch(10);\n \n-        b.close();\n+            assertThat(b.getElements().size(), is(equalTo(2)));\n+            assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));\n+            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));\n \n-        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(true)));\n+            b.close();\n \n-        q.close();\n+            assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(true)));\n+        }\n     }\n \n     @Test\n@@ -202,83 +197,82 @@ public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {\n             TestSettings.volatileQueueSettings(2 * singleElementCapacity)\n         ).checkpointMaxWrites(1024) // arbitrary high enough threshold so that it's not reached (default for TestSettings is 1)\n         .build();\n-        TestQueue q = new TestQueue(settings);\n-        q.open();\n-\n-        assertThat(q.getHeadPage().getPageNum(), is(equalTo(0)));\n-        Checkpoint c = q.getCheckpointIO().read(\"checkpoint.head\");\n-        assertThat(c.getPageNum(), is(equalTo(0)));\n-        assertThat(c.getElementCount(), is(equalTo(0)));\n-        assertThat(c.getMinSeqNum(), is(equalTo(0L)));\n-        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));\n-        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n-\n-        for (Queueable e : elements1) {\n-            q.write(e);\n-        }\n+        try (TestQueue q = new TestQueue(settings)) {\n+            q.open();\n \n-        c = q.getCheckpointIO().read(\"checkpoint.head\");\n-        assertThat(c.getPageNum(), is(equalTo(0)));\n-        assertThat(c.getElementCount(), is(equalTo(0)));\n-        assertThat(c.getMinSeqNum(), is(equalTo(0L)));\n-        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));\n-        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n-\n-//        assertThat(elements1.get(1).getSeqNum(), is(equalTo(2L)));\n-        q.ensurePersistedUpto(2);\n-\n-        c = q.getCheckpointIO().read(\"checkpoint.head\");\n-        assertThat(c.getPageNum(), is(equalTo(0)));\n-        assertThat(c.getElementCount(), is(equalTo(2)));\n-        assertThat(c.getMinSeqNum(), is(equalTo(1L)));\n-        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));\n-        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n-\n-        for (Queueable e : elements2) {\n-            q.write(e);\n-        }\n+            assertThat(q.getHeadPage().getPageNum(), is(equalTo(0)));\n+            Checkpoint c = q.getCheckpointIO().read(\"checkpoint.head\");\n+            assertThat(c.getPageNum(), is(equalTo(0)));\n+            assertThat(c.getElementCount(), is(equalTo(0)));\n+            assertThat(c.getMinSeqNum(), is(equalTo(0L)));\n+            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));\n+            assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n \n-        c = q.getCheckpointIO().read(\"checkpoint.head\");\n-        assertThat(c.getPageNum(), is(equalTo(1)));\n-        assertThat(c.getElementCount(), is(equalTo(0)));\n-        assertThat(c.getMinSeqNum(), is(equalTo(0L)));\n-        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));\n-        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n-\n-        c = q.getCheckpointIO().read(\"checkpoint.0\");\n-        assertThat(c.getPageNum(), is(equalTo(0)));\n-        assertThat(c.getElementCount(), is(equalTo(2)));\n-        assertThat(c.getMinSeqNum(), is(equalTo(1L)));\n-        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));\n-\n-        Batch b = q.nonBlockReadBatch(10);\n-        b.close();\n-\n-        try {\n-            q.getCheckpointIO().read(\"checkpoint.0\");\n-            fail(\"expected NoSuchFileException thrown\");\n-        } catch (NoSuchFileException e) {\n-            // nothing\n-        }\n+            for (Queueable e : elements1) {\n+                q.write(e);\n+            }\n+\n+            c = q.getCheckpointIO().read(\"checkpoint.head\");\n+            assertThat(c.getPageNum(), is(equalTo(0)));\n+            assertThat(c.getElementCount(), is(equalTo(0)));\n+            assertThat(c.getMinSeqNum(), is(equalTo(0L)));\n+            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));\n+            assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n+\n+        //  assertThat(elements1.get(1).getSeqNum(), is(equalTo(2L)));\n+            q.ensurePersistedUpto(2);\n+\n+            c = q.getCheckpointIO().read(\"checkpoint.head\");\n+            assertThat(c.getPageNum(), is(equalTo(0)));\n+            assertThat(c.getElementCount(), is(equalTo(2)));\n+            assertThat(c.getMinSeqNum(), is(equalTo(1L)));\n+            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));\n+            assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n+\n+            for (Queueable e : elements2) {\n+                q.write(e);\n+            }\n+\n+            c = q.getCheckpointIO().read(\"checkpoint.head\");\n+            assertThat(c.getPageNum(), is(equalTo(1)));\n+            assertThat(c.getElementCount(), is(equalTo(0)));\n+            assertThat(c.getMinSeqNum(), is(equalTo(0L)));\n+            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));\n+            assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));\n+\n+            c = q.getCheckpointIO().read(\"checkpoint.0\");\n+            assertThat(c.getPageNum(), is(equalTo(0)));\n+            assertThat(c.getElementCount(), is(equalTo(2)));\n+            assertThat(c.getMinSeqNum(), is(equalTo(1L)));\n+            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));\n \n-        c = q.getCheckpointIO().read(\"checkpoint.head\");\n-        assertThat(c.getPageNum(), is(equalTo(1)));\n-        assertThat(c.getElementCount(), is(equalTo(2)));\n-        assertThat(c.getMinSeqNum(), is(equalTo(3L)));\n-        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(3L)));\n-        assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));\n+            Batch b = q.nonBlockReadBatch(10);\n+            b.close();\n+\n+            try {\n+                q.getCheckpointIO().read(\"checkpoint.0\");\n+                fail(\"expected NoSuchFileException thrown\");\n+            } catch (NoSuchFileException e) {\n+                // nothing\n+            }\n \n-        b = q.nonBlockReadBatch(10);\n-        b.close();\n+            c = q.getCheckpointIO().read(\"checkpoint.head\");\n+            assertThat(c.getPageNum(), is(equalTo(1)));\n+            assertThat(c.getElementCount(), is(equalTo(2)));\n+            assertThat(c.getMinSeqNum(), is(equalTo(3L)));\n+            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(3L)));\n+            assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));\n \n-        c = q.getCheckpointIO().read(\"checkpoint.head\");\n-        assertThat(c.getPageNum(), is(equalTo(1)));\n-        assertThat(c.getElementCount(), is(equalTo(2)));\n-        assertThat(c.getMinSeqNum(), is(equalTo(3L)));\n-        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(5L)));\n-        assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));\n+            b = q.nonBlockReadBatch(10);\n+            b.close();\n \n-        q.close();\n+            c = q.getCheckpointIO().read(\"checkpoint.head\");\n+            assertThat(c.getPageNum(), is(equalTo(1)));\n+            assertThat(c.getElementCount(), is(equalTo(2)));\n+            assertThat(c.getMinSeqNum(), is(equalTo(3L)));\n+            assertThat(c.getFirstUnackedSeqNum(), is(equalTo(5L)));\n+            assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));\n+        }\n     }\n \n     @Test\n@@ -296,32 +290,31 @@ public void randomAcking() throws IOException {\n                 elements.add(new StringElement(String.format(\"%0\" + digits + \"d\", i)));\n             }\n             int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);\n-\n-            TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(singleElementCapacity));\n-            q.open();\n-\n-            for (Queueable e : elements) {\n-                q.write(e);\n+            try (TestQueue q = new TestQueue(\n+                TestSettings.volatileQueueSettings(singleElementCapacity))) {\n+                q.open();\n+\n+                for (Queueable e : elements) {\n+                    q.write(e);\n+                }\n+\n+                assertThat(q.getTailPages().size(), is(equalTo(page_count - 1)));\n+\n+                // first read all elements\n+                List<Batch> batches = new ArrayList<>();\n+                for (Batch b = q.nonBlockReadBatch(1); b != null; b = q.nonBlockReadBatch(1)) {\n+                    batches.add(b);\n+                }\n+                assertThat(batches.size(), is(equalTo(page_count)));\n+\n+                // then ack randomly\n+                Collections.shuffle(batches);\n+                for (Batch b : batches) {\n+                    b.close();\n+                }\n+                \n+                assertThat(q.getTailPages().size(), is(equalTo(0)));\n             }\n-\n-            assertThat(q.getTailPages().size(), is(equalTo(page_count - 1)));\n-\n-            // first read all elements\n-            List<Batch> batches = new ArrayList<>();\n-            for (Batch b = q.nonBlockReadBatch(1); b != null; b = q.nonBlockReadBatch(1)) {\n-                batches.add(b);\n-            }\n-            assertThat(batches.size(), is(equalTo(page_count)));\n-\n-            // then ack randomly\n-            Collections.shuffle(batches);\n-            for (Batch b : batches) {\n-                b.close();\n-            }\n-\n-            assertThat(q.getTailPages().size(), is(equalTo(0)));\n-\n-            q.close();\n         }\n     }\n \n@@ -334,47 +327,45 @@ public void reachMaxUnread() throws IOException, InterruptedException, Execution\n             TestSettings.volatileQueueSettings(singleElementCapacity)\n         ).maxUnread(2) // 2 so we know the first write should not block and the second should\n         .build();\n-        TestQueue q = new TestQueue(settings);\n-        q.open();\n-\n+        try (TestQueue q = new TestQueue(settings)) {\n+            q.open();\n+            \n+            long seqNum = q.write(element);\n+            assertThat(seqNum, is(equalTo(1L)));\n+            assertThat(q.isFull(), is(false));\n \n-        long seqNum = q.write(element);\n-        assertThat(seqNum, is(equalTo(1L)));\n-        assertThat(q.isFull(), is(false));\n+            int ELEMENT_COUNT = 1000;\n+            for (int i = 0; i < ELEMENT_COUNT; i++) {\n \n-        int ELEMENT_COUNT = 1000;\n-        for (int i = 0; i < ELEMENT_COUNT; i++) {\n+                // we expect the next write call to block so let's wrap it in a Future\n+                Callable<Long> write = () -> {\n+                    return q.write(element);\n+                };\n \n-            // we expect the next write call to block so let's wrap it in a Future\n-            Callable<Long> write = () -> {\n-                return q.write(element);\n-            };\n+                ExecutorService executor = Executors.newFixedThreadPool(1);\n+                Future<Long> future = executor.submit(write);\n \n-            ExecutorService executor = Executors.newFixedThreadPool(1);\n-            Future<Long> future = executor.submit(write);\n+                while (!q.isFull()) {\n+                    // spin wait until data is written and write blocks\n+                    Thread.sleep(1);\n+                }\n+                assertThat(q.unreadCount, is(equalTo(2L)));\n+                assertThat(future.isDone(), is(false));\n \n-            while (!q.isFull()) {\n-                // spin wait until data is written and write blocks\n-                Thread.sleep(1);\n-            }\n-            assertThat(q.unreadCount, is(equalTo(2L)));\n-            assertThat(future.isDone(), is(false));\n+                // read one element, which will unblock the last write\n+                Batch b = q.nonBlockReadBatch(1);\n+                assertThat(b.getElements().size(), is(equalTo(1)));\n \n-            // read one element, which will unblock the last write\n-            Batch b = q.nonBlockReadBatch(1);\n-            assertThat(b.getElements().size(), is(equalTo(1)));\n+                // future result is the blocked write seqNum for the second element\n+                assertThat(future.get(), is(equalTo(2L + i)));\n+                assertThat(q.isFull(), is(false));\n \n-            // future result is the blocked write seqNum for the second element\n-            assertThat(future.get(), is(equalTo(2L + i)));\n-            assertThat(q.isFull(), is(false));\n+                executor.shutdown();\n+            }\n \n-            executor.shutdown();\n+            // since we did not ack and pages hold a single item\n+            assertThat(q.getTailPages().size(), is(equalTo(ELEMENT_COUNT)));\n         }\n-\n-        // since we did not ack and pages hold a single item\n-        assertThat(q.getTailPages().size(), is(equalTo(ELEMENT_COUNT)));\n-\n-        q.close();\n     }\n \n     @Test\n@@ -386,51 +377,51 @@ public void reachMaxUnreadWithAcking() throws IOException, InterruptedException,\n             TestSettings.volatileQueueSettings(256) // 256 is arbitrary, large enough to hold a few elements\n         ).maxUnread(2)\n         .build(); // 2 so we know the first write should not block and the second should\n-        TestQueue q = new TestQueue(settings);\n-        q.open();\n-\n-        // perform first non-blocking write\n-        long seqNum = q.write(element);\n-\n-        assertThat(seqNum, is(equalTo(1L)));\n-        assertThat(q.isFull(), is(false));\n+        try (TestQueue q = new TestQueue(settings)) {\n+            q.open();\n \n-        int ELEMENT_COUNT = 1000;\n-        for (int i = 0; i < ELEMENT_COUNT; i++) {\n+            // perform first non-blocking write\n+            long seqNum = q.write(element);\n \n-            // we expect this next write call to block so let's wrap it in a Future\n-            Callable<Long> write = () -> {\n-                return q.write(element);\n-            };\n+            assertThat(seqNum, is(equalTo(1L)));\n+            assertThat(q.isFull(), is(false));\n \n-            ExecutorService executor = Executors.newFixedThreadPool(1);\n-            Future<Long> future = executor.submit(write);\n+            int ELEMENT_COUNT = 1000;\n+            for (int i = 0; i < ELEMENT_COUNT; i++) {\n \n-            // spin wait until data is written and write blocks\n-            while (!q.isFull()) { Thread.sleep(1); }\n+                // we expect this next write call to block so let's wrap it in a Future\n+                Callable<Long> write = () -> {\n+                    return q.write(element);\n+                };\n \n-            // read one element, which will unblock the last write\n-            Batch b = q.nonBlockReadBatch(1);\n-            assertThat(b, is(notNullValue()));\n-            assertThat(b.getElements().size(), is(equalTo(1)));\n-            b.close();\n+                ExecutorService executor = Executors.newFixedThreadPool(1);\n+                Future<Long> future = executor.submit(write);\n \n-            // future result is the blocked write seqNum for the second element\n-            assertThat(future.get(), is(equalTo(2L + i)));\n-            assertThat(q.isFull(), is(false));\n+                // spin wait until data is written and write blocks\n+                while (!q.isFull()) {\n+                    Thread.sleep(1);\n+                }\n+                // read one element, which will unblock the last write\n+                Batch b = q.nonBlockReadBatch(1);\n+                assertThat(b, is(notNullValue()));\n+                assertThat(b.getElements().size(), is(equalTo(1)));\n+                b.close();\n \n-            executor.shutdown();\n-        }\n+                // future result is the blocked write seqNum for the second element\n+                assertThat(future.get(), is(equalTo(2L + i)));\n+                assertThat(q.isFull(), is(false));\n \n-        // all batches are acked, no tail pages should exist\n-        assertThat(q.getTailPages().size(), is(equalTo(0)));\n+                executor.shutdown();\n+            }\n \n-        // the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page\n-        assertThat(q.getHeadPage().getElementCount() > 0L, is(true));\n-        assertThat(q.getHeadPage().unreadCount(), is(equalTo(1L)));\n-        assertThat(q.unreadCount, is(equalTo(1L)));\n+            // all batches are acked, no tail pages should exist\n+            assertThat(q.getTailPages().size(), is(equalTo(0)));\n \n-        q.close();\n+            // the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page\n+            assertThat(q.getHeadPage().getElementCount() > 0L, is(true));\n+            assertThat(q.getHeadPage().unreadCount(), is(equalTo(1L)));\n+            assertThat(q.unreadCount, is(equalTo(1L)));\n+        }\n     }\n \n     @Test(timeout = 5000)\n@@ -441,31 +432,30 @@ public void reachMaxSizeTest() throws IOException, InterruptedException, Executi\n \n         // allow 10 elements per page but only 100 events in total\n         Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);\n+        try (TestQueue q = new TestQueue(settings)) {\n+            q.open();\n \n-        TestQueue q = new TestQueue(settings);\n-        q.open();\n-\n-        int ELEMENT_COUNT = 90; // should be able to write 99 events before getting full\n-        for (int i = 0; i < ELEMENT_COUNT; i++) {\n-            long seqNum = q.write(element);\n-        }\n-\n-        assertThat(q.isFull(), is(false));\n-\n-        // we expect this next write call to block so let's wrap it in a Future\n-        Callable<Long> write = () -> {\n-            return q.write(element);\n-        };\n+            int ELEMENT_COUNT = 90; // should be able to write 99 events before getting full\n+            for (int i = 0; i < ELEMENT_COUNT; i++) {\n+                long seqNum = q.write(element);\n+            }\n \n-        ExecutorService executor = Executors.newFixedThreadPool(1);\n-        Future<Long> future = executor.submit(write);\n+            assertThat(q.isFull(), is(false));\n \n-        while (!q.isFull()) { Thread.sleep(10); }\n+            // we expect this next write call to block so let's wrap it in a Future\n+            Callable<Long> write = () -> {\n+                return q.write(element);\n+            };\n \n-        assertThat(q.isFull(), is(true));\n+            ExecutorService executor = Executors.newFixedThreadPool(1);\n+            Future<Long> future = executor.submit(write);\n+            while (!q.isFull()) {\n+                Thread.sleep(10);\n+            }\n+            assertThat(q.isFull(), is(true));\n \n-        executor.shutdown();\n-        q.close();\n+            executor.shutdown();\n+        }\n     }\n \n     @Test(timeout = 5000)\n@@ -518,39 +508,39 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted\n \n         // allow 10 elements per page but only 100 events in total\n         Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);\n+        try (TestQueue q = new TestQueue(settings)) {\n+            q.open();\n+            int ELEMENT_COUNT =\n+                90; // should be able to write 90 events (9 pages) before getting full\n+            for (int i = 0; i < ELEMENT_COUNT; i++) {\n+                long seqNum = q.write(element);\n+            }\n \n-        TestQueue q = new TestQueue(settings);\n-        q.open();\n-\n-        int ELEMENT_COUNT = 90; // should be able to write 90 events (9 pages) before getting full\n-        for (int i = 0; i < ELEMENT_COUNT; i++) {\n-            long seqNum = q.write(element);\n-        }\n-\n-        assertThat(q.isFull(), is(false));\n-\n-        // read 1 page (10 events) here while not full yet so that the read will not singal the not full state\n-        // we want the batch closing below to signal the not full state\n-        Batch b = q.readBatch(10);\n+            assertThat(q.isFull(), is(false));\n \n-        // we expect this next write call to block so let's wrap it in a Future\n-        Callable<Long> write = () -> {\n-            return q.write(element);\n-        };\n-        ExecutorService executor = Executors.newFixedThreadPool(1);\n-        Future<Long> future = executor.submit(write);\n-        assertThat(future.isDone(), is(false));\n+            // read 1 page (10 events) here while not full yet so that the read will not singal the not full state\n+            // we want the batch closing below to signal the not full state\n+            Batch b = q.readBatch(10);\n \n-        while (!q.isFull()) { Thread.sleep(10); }\n-        assertThat(q.isFull(), is(true));\n-        assertThat(future.isDone(), is(false));\n+            // we expect this next write call to block so let's wrap it in a Future\n+            Callable<Long> write = () -> {\n+                return q.write(element);\n+            };\n+            ExecutorService executor = Executors.newFixedThreadPool(1);\n+            Future<Long> future = executor.submit(write);\n+            assertThat(future.isDone(), is(false));\n+            while (!q.isFull()) {\n+                Thread.sleep(10);\n+            }\n+            assertThat(q.isFull(), is(true));\n+            assertThat(future.isDone(), is(false));\n \n-        b.close();  // purge 1 page\n+            b.close();  // purge 1 page\n \n-        assertThat(future.get(), is(equalTo(ELEMENT_COUNT + 1L)));\n+            assertThat(future.get(), is(equalTo(ELEMENT_COUNT + 1L)));\n \n-        executor.shutdown();\n-        q.close();\n+            executor.shutdown();\n+        }\n     }\n \n     @Test(timeout = 5000)\n@@ -562,170 +552,171 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup\n \n         // allow 10 elements per page but only 100 events in total\n         Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);\n+        try (TestQueue q = new TestQueue(settings)) {\n+            q.open();\n \n-        TestQueue q = new TestQueue(settings);\n-        q.open();\n-\n-        int ELEMENT_COUNT = 90; // should be able to write 99 events before getting full\n-        for (int i = 0; i < ELEMENT_COUNT; i++) {\n-            long seqNum = q.write(element);\n-        }\n-\n-        assertThat(q.isFull(), is(false));\n-\n-        // we expect this next write call to block so let's wrap it in a Future\n-        Callable<Long> write = () -> {\n-            return q.write(element);\n-        };\n+            int ELEMENT_COUNT = 90; // should be able to write 99 events before getting full\n+            for (int i = 0; i < ELEMENT_COUNT; i++) {\n+                long seqNum = q.write(element);\n+            }\n \n-        ExecutorService executor = Executors.newFixedThreadPool(1);\n-        Future<Long> future = executor.submit(write);\n+            assertThat(q.isFull(), is(false));\n \n-        while (!q.isFull()) { Thread.sleep(10); }\n+            // we expect this next write call to block so let's wrap it in a Future\n+            Callable<Long> write = () -> {\n+                return q.write(element);\n+            };\n \n-        assertThat(q.isFull(), is(true));\n+            ExecutorService executor = Executors.newFixedThreadPool(1);\n+            Future<Long> future = executor.submit(write);\n+            while (!q.isFull()) {\n+                Thread.sleep(10);\n+            }\n+            assertThat(q.isFull(), is(true));\n \n-        Batch b = q.readBatch(9); // read 90% of page (9 events)\n-        b.close();  // this should not purge a page\n+            Batch b = q.readBatch(9); // read 90% of page (9 events)\n+            b.close();  // this should not purge a page\n \n-        assertThat(q.isFull(), is(true)); // queue should still be full\n+            assertThat(q.isFull(), is(true)); // queue should still be full\n \n-        executor.shutdown();\n-        q.close();\n+            executor.shutdown();\n+        }\n     }\n \n     @Test\n     public void testAckedCount() throws IOException {\n         Settings settings = TestSettings.persistedQueueSettings(100, dataPath);\n-        Queue q = new Queue(settings);\n-        q.open();\n-\n-        Queueable element1 = new StringElement(\"foobarbaz\");\n-        Queueable element2 = new StringElement(\"wowza\");\n-        Queueable element3 = new StringElement(\"third\");\n-        long firstSeqNum = q.write(element1);\n-\n-        Batch b = q.nonBlockReadBatch(1);\n-        assertThat(b.getElements().size(), is(equalTo(1)));\n-\n-        q.close();\n+        Batch b;\n+        Queueable element1;\n+        Queueable element2;\n+        Queueable element3;\n+        long firstSeqNum;\n+        try(Queue q = new Queue(settings)) {\n+            q.open();\n \n-        q = new Queue(settings);\n-        q.open();\n+            element1 = new StringElement(\"foobarbaz\");\n+            element2 = new StringElement(\"wowza\");\n+            element3 = new StringElement(\"third\");\n+            firstSeqNum = q.write(element1);\n+            b = q.nonBlockReadBatch(1);\n+            assertThat(b.getElements().size(), is(equalTo(1)));\n+        }\n \n-        long secondSeqNum = q.write(element2);\n-        long thirdSeqNum = q.write(element3);\n+        long secondSeqNum;\n+        long thirdSeqNum;\n+        try(Queue q = new Queue(settings)){\n+            q.open();\n \n-        b = q.nonBlockReadBatch(1);\n-        assertThat(b.getElements().size(), is(equalTo(1)));\n-        assertThat(b.getElements().get(0), is(equalTo(element1)));\n+            secondSeqNum = q.write(element2);\n+            thirdSeqNum = q.write(element3);\n \n-        b = q.nonBlockReadBatch(2);\n-        assertThat(b.getElements().size(), is(equalTo(2)));\n-        assertThat(b.getElements().get(0), is(equalTo(element2)));\n-        assertThat(b.getElements().get(1), is(equalTo(element3)));\n+            b = q.nonBlockReadBatch(1);\n+            assertThat(b.getElements().size(), is(equalTo(1)));\n+            assertThat(b.getElements().get(0), is(equalTo(element1)));\n \n-        q.ack(Collections.singletonList(firstSeqNum));\n-        q.close();\n+            b = q.nonBlockReadBatch(2);\n+            assertThat(b.getElements().size(), is(equalTo(2)));\n+            assertThat(b.getElements().get(0), is(equalTo(element2)));\n+            assertThat(b.getElements().get(1), is(equalTo(element3)));\n \n-        q = new Queue(settings);\n-        q.open();\n+            q.ack(Collections.singletonList(firstSeqNum));\n+        }\n \n-        b = q.nonBlockReadBatch(2);\n-        assertThat(b.getElements().size(), is(equalTo(2)));\n+        try(Queue q = new Queue(settings)) {\n+            q.open();\n \n-        q.ack(Arrays.asList(secondSeqNum, thirdSeqNum));\n+            b = q.nonBlockReadBatch(2);\n+            assertThat(b.getElements().size(), is(equalTo(2)));\n \n-        assertThat(q.getAckedCount(), equalTo(0L));\n-        assertThat(q.getUnackedCount(), equalTo(0L));\n+            q.ack(Arrays.asList(secondSeqNum, thirdSeqNum));\n \n-        q.close();\n+            assertThat(q.getAckedCount(), equalTo(0L));\n+            assertThat(q.getUnackedCount(), equalTo(0L));\n+        }\n     }\n \n     @Test(timeout = 5000)\n     public void concurrentWritesTest() throws IOException, InterruptedException, ExecutionException {\n \n         // very small pages to maximize page creation\n         Settings settings = TestSettings.volatileQueueSettings(100);\n+        try (TestQueue q = new TestQueue(settings)) {\n+            q.open();\n \n-        TestQueue q = new TestQueue(settings);\n-        q.open();\n+            int ELEMENT_COUNT = 10000;\n+            int WRITER_COUNT = 5;\n+            AtomicInteger element_num = new AtomicInteger(0);\n \n-        int ELEMENT_COUNT = 10000;\n-        int WRITER_COUNT = 5;\n-        AtomicInteger element_num = new AtomicInteger(0);\n+            // we expect this next write call to block so let's wrap it in a Future\n+            Callable<Integer> writer = () -> {\n+                for (int i = 0; i < ELEMENT_COUNT; i++) {\n+                    int n = element_num.getAndIncrement();\n+                    q.write(new StringElement(\"\" + n));\n+                }\n+                return ELEMENT_COUNT;\n+            };\n \n-        // we expect this next write call to block so let's wrap it in a Future\n-        Callable<Integer> writer = () -> {\n-            for (int i = 0; i < ELEMENT_COUNT; i++) {\n-                int n = element_num.getAndIncrement();\n-                q.write(new StringElement(\"\" + n));\n+            List<Future<Integer>> futures = new ArrayList<>();\n+            ExecutorService executor = Executors.newFixedThreadPool(WRITER_COUNT);\n+            for (int i = 0; i < WRITER_COUNT; i++) {\n+                futures.add(executor.submit(writer));\n             }\n-            return ELEMENT_COUNT;\n-        };\n-\n-        List<Future<Integer>> futures = new ArrayList<>();\n-        ExecutorService executor = Executors.newFixedThreadPool(WRITER_COUNT);\n-        for  (int i = 0; i < WRITER_COUNT; i++) {\n-            futures.add(executor.submit(writer));\n-        }\n \n-        int BATCH_SIZE = 10;\n-        int read_count = 0;\n+            int BATCH_SIZE = 10;\n+            int read_count = 0;\n \n-        while (read_count < ELEMENT_COUNT * WRITER_COUNT) {\n-            Batch b = q.readBatch(BATCH_SIZE);\n-            read_count += b.size();\n-            b.close();\n-        }\n+            while (read_count < ELEMENT_COUNT * WRITER_COUNT) {\n+                Batch b = q.readBatch(BATCH_SIZE);\n+                read_count += b.size();\n+                b.close();\n+            }\n \n-        for (Future<Integer> future : futures) {\n-            int result = future.get();\n-            assertThat(result, is(equalTo(ELEMENT_COUNT)));\n-        }\n+            for (Future<Integer> future : futures) {\n+                int result = future.get();\n+                assertThat(result, is(equalTo(ELEMENT_COUNT)));\n+            }\n \n-        assertThat(q.getTailPages().isEmpty(), is(true));\n-        assertThat(q.isFullyAcked(), is(true));\n+            assertThat(q.getTailPages().isEmpty(), is(true));\n+            assertThat(q.isFullyAcked(), is(true));\n \n-        executor.shutdown();\n-        q.close();\n+            executor.shutdown();\n+        }\n     }\n \n     @Test\n     public void fullyAckedHeadPageBeheadingTest() throws IOException {\n         Queueable element = new StringElement(\"foobarbaz1\");\n         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);\n+        try (TestQueue q = new TestQueue(\n+            TestSettings.volatileQueueSettings(2 * singleElementCapacity))) {\n+            q.open();\n \n-        TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(2 * singleElementCapacity));\n-        q.open();\n-\n-        Batch b;\n-        q.write(element);\n-        b = q.nonBlockReadBatch(1);\n-        assertThat(b.getElements().size(), is(equalTo(1)));\n-        b.close();\n-\n-        q.write(element);\n-        b = q.nonBlockReadBatch(1);\n-        assertThat(b.getElements().size(), is(equalTo(1)));\n-        b.close();\n+            Batch b;\n+            q.write(element);\n+            b = q.nonBlockReadBatch(1);\n+            assertThat(b.getElements().size(), is(equalTo(1)));\n+            b.close();\n \n-        // head page should be full and fully acked\n-        assertThat(q.getHeadPage().isFullyAcked(), is(true));\n-        assertThat(q.getHeadPage().hasSpace(element.serialize().length), is(false));\n-        assertThat(q.isFullyAcked(), is(true));\n+            q.write(element);\n+            b = q.nonBlockReadBatch(1);\n+            assertThat(b.getElements().size(), is(equalTo(1)));\n+            b.close();\n \n-        // write extra element to trigger beheading\n-        q.write(element);\n+            // head page should be full and fully acked\n+            assertThat(q.getHeadPage().isFullyAcked(), is(true));\n+            assertThat(q.getHeadPage().hasSpace(element.serialize().length), is(false));\n+            assertThat(q.isFullyAcked(), is(true));\n \n-        // since head page was fully acked it should not have created a new tail page\n+            // write extra element to trigger beheading\n+            q.write(element);\n \n-        assertThat(q.getTailPages().isEmpty(), is(true));\n-        assertThat(q.getHeadPage().getPageNum(), is(equalTo(1)));\n-        assertThat(q.firstUnackedPageNum(), is(equalTo(1)));\n-        assertThat(q.isFullyAcked(), is(false));\n+            // since head page was fully acked it should not have created a new tail page\n \n-        q.close();\n+            assertThat(q.getTailPages().isEmpty(), is(true));\n+            assertThat(q.getHeadPage().getPageNum(), is(equalTo(1)));\n+            assertThat(q.firstUnackedPageNum(), is(equalTo(1)));\n+            assertThat(q.isFullyAcked(), is(false));\n+        }\n     }\n \n     @Test",
      "parent_sha": "64397c66e32577c6681a8e80926bd511a4246a12"
    }
  },
  {
    "oid": "99e309fe7b9958476773e58cfc53f053c7eca21a",
    "message": "Avoid to throw an exception from a finally block (#14192)\n\nProposes a code cleanup of the FileLockFactory.obtainLock method. It removes the nesting of ifs using a \"precondition style\" check, lifting up the error condition, and checking on top of the code flow.\r\n\r\nIt removed a throw exception from the clean-up finally block.\r\nIf an exception is raised from that point it hides the original cause of the problem.\r\nThis commit switches the finally block, used in obtaining a file lock, to a catch and re-throw.",
    "date": "2022-06-08T14:26:56Z",
    "url": "https://github.com/elastic/logstash/commit/99e309fe7b9958476773e58cfc53f053c7eca21a",
    "details": {
      "sha": "81b6dc5c1f349bdd911acbe2819ab8950276008e",
      "filename": "logstash-core/src/main/java/org/logstash/FileLockFactory.java",
      "status": "modified",
      "additions": 37,
      "deletions": 32,
      "changes": 69,
      "blob_url": "https://github.com/elastic/logstash/blob/99e309fe7b9958476773e58cfc53f053c7eca21a/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FFileLockFactory.java",
      "raw_url": "https://github.com/elastic/logstash/raw/99e309fe7b9958476773e58cfc53f053c7eca21a/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FFileLockFactory.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FFileLockFactory.java?ref=99e309fe7b9958476773e58cfc53f053c7eca21a",
      "patch": "@@ -69,59 +69,64 @@ private FileLockFactory() {}\n     private static final Map<FileLock, String> LOCK_MAP =  Collections.synchronizedMap(new HashMap<>());\n \n     public static FileLock obtainLock(Path dirPath, String lockName) throws IOException {\n-        if (!Files.isDirectory(dirPath)) { Files.createDirectories(dirPath); }\n+        if (!Files.isDirectory(dirPath)) {\n+            Files.createDirectories(dirPath);\n+        }\n         Path lockPath = dirPath.resolve(lockName);\n \n         try {\n             Files.createFile(lockPath);\n         } catch (IOException ignore) {\n             // we must create the file to have a truly canonical path.\n-            // if it's already created, we don't care. if it cant be created, it will fail below.\n+            // if it's already created, we don't care. if it can't be created, it will fail below.\n         }\n \n         // fails if the lock file does not exist\n         final Path realLockPath = lockPath.toRealPath();\n \n-        if (LOCK_HELD.add(realLockPath.toString())) {\n-            FileChannel channel = null;\n-            FileLock lock = null;\n+        if (!LOCK_HELD.add(realLockPath.toString())) {\n+            throw new LockException(\"Lock held by this virtual machine on lock path: \" + realLockPath);\n+        }\n+        FileChannel channel = null;\n+        FileLock lock;\n+        try {\n+            channel = FileChannel.open(realLockPath, StandardOpenOption.CREATE, StandardOpenOption.WRITE);\n+            lock = channel.tryLock();\n+            if (lock == null) {\n+                throw new LockException(\"Lock held by another program on lock path: \" + realLockPath);\n+            }\n+        } catch (IOException ex) {\n             try {\n-                channel = FileChannel.open(realLockPath, StandardOpenOption.CREATE, StandardOpenOption.WRITE);\n-                lock = channel.tryLock();\n-                if (lock != null) {\n-                    LOCK_MAP.put(lock, realLockPath.toString());\n-                    return lock;\n-                } else {\n-                    throw new LockException(\"Lock held by another program on lock path: \" + realLockPath);\n-                }\n-            } finally {\n-                if (lock == null) { // not successful - clear up and move out\n-                    try {\n-                        if (channel != null) {\n-                            channel.close();\n-                        }\n-                    } catch (Throwable t) {\n-                        // suppress any channel close exceptions\n-                    }\n-\n-                    boolean removed = LOCK_HELD.remove(realLockPath.toString());\n-                    if (removed == false) {\n-                        throw new LockException(\"Lock path was cleared but never marked as held: \" + realLockPath);\n-                    }\n+                if (channel != null) {\n+                    channel.close();\n                 }\n+            } catch (Throwable t) {\n+                // suppress any channel close exceptions\n             }\n-        } else {\n-            throw new LockException(\"Lock held by this virtual machine on lock path: \" + realLockPath);\n+\n+            boolean removed = LOCK_HELD.remove(realLockPath.toString());\n+            if (!removed) {\n+                throw new LockException(\"Lock path was cleared but never marked as held: \" + realLockPath, ex);\n+            }\n+\n+            throw ex;\n         }\n+\n+        LOCK_MAP.put(lock, realLockPath.toString());\n+        return lock;\n     }\n \n     public static void releaseLock(FileLock lock) throws IOException {\n         String lockPath = LOCK_MAP.remove(lock);\n-        if (lockPath == null) { throw new LockException(\"Cannot release unobtained lock\"); }\n+        if (lockPath == null) {\n+            throw new LockException(\"Cannot release unobtained lock\");\n+        }\n         lock.release();\n         lock.channel().close();\n-        Boolean removed = LOCK_HELD.remove(lockPath);\n-        if (removed == false) { throw new LockException(\"Lock path was not marked as held: \" + lockPath); }\n+        boolean removed = LOCK_HELD.remove(lockPath);\n+        if (!removed) {\n+            throw new LockException(\"Lock path was not marked as held: \" + lockPath);\n+        }\n     }\n \n }",
      "parent_sha": "d2b9b15bc1567adb14b27cc9c73cff6d464cf789"
    }
  },
  {
    "oid": "5505f7c9d578fe9a64c82de9f2119ec675dca717",
    "message": "fix Stdin tests to work on platforms that do not have UTF-8 as the default character encoding\n\nFixes #10449",
    "date": "2019-02-14T16:56:07Z",
    "url": "https://github.com/elastic/logstash/commit/5505f7c9d578fe9a64c82de9f2119ec675dca717",
    "details": {
      "sha": "5b558fef7b1a5f0467b49f692e1251c9df3cb37d",
      "filename": "logstash-core/src/test/java/org/logstash/plugins/inputs/StdinTest.java",
      "status": "modified",
      "additions": 6,
      "deletions": 1,
      "changes": 7,
      "blob_url": "https://github.com/elastic/logstash/blob/5505f7c9d578fe9a64c82de9f2119ec675dca717/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fplugins%2Finputs%2FStdinTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/5505f7c9d578fe9a64c82de9f2119ec675dca717/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fplugins%2Finputs%2FStdinTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fplugins%2Finputs%2FStdinTest.java?ref=5505f7c9d578fe9a64c82de9f2119ec675dca717",
      "patch": "@@ -10,6 +10,7 @@\n import java.io.RandomAccessFile;\n import java.nio.ByteBuffer;\n import java.nio.channels.FileChannel;\n+import java.nio.charset.Charset;\n import java.nio.file.Files;\n import java.nio.file.Path;\n import java.util.ArrayList;\n@@ -44,7 +45,11 @@ public void testEvents() throws IOException {\n \n     @Test\n     public void testUtf8Events() throws IOException {\n-        String[] inputs = {\"M\u00fcnchen1\", \"\u5b89\u88c5\u4e2d\u6587\u8f93\u5165\u6cd5\", \"M\u00fcnchen3\"};\n+        String[] inputs = {\n+                new String(\"M\u00fcnchen1\".getBytes(), Charset.forName(\"UTF-8\")),\n+                new String(\"\u5b89\u88c5\u4e2d\u6587\u8f93\u5165\u6cd5\".getBytes(), Charset.forName(\"UTF-8\")),\n+                new String(\"M\u00fcnchen3\".getBytes(), Charset.forName(\"UTF-8\"))\n+        };\n         String testInput = String.join(Line.DEFAULT_DELIMITER, inputs) + Line.DEFAULT_DELIMITER;\n         TestConsumer queueWriter = testStdin(testInput.getBytes());\n ",
      "parent_sha": "9eb2aac36a3a7da1e4ac4769d4c9a3f0ad16defc"
    }
  },
  {
    "oid": "ff37e1e0d3d19b605951c94263b72c5e5a053112",
    "message": "Fix failing DLQ test due to time scheduling (#15960)\n\nAdds a burning of time condition to avoid a collision of time which, under certain circumstances, would fail the test.\r\nThe sealing of a segment happens if the segment is considered as stale, which requires 2 conditions:\r\n\r\n- the segment must have received a write.\r\n- the time of the last write must exceed the flush interval.\r\n\r\nIn this failing test, the flush interval is set to ZERO because of the synchronicity of the test, to avoid time dependency. However, with coarse grain timer resolution, could happen that the last write coincide with the time of the stale check, so fail the seal condition.",
    "date": "2024-02-22T10:28:31Z",
    "url": "https://github.com/elastic/logstash/commit/ff37e1e0d3d19b605951c94263b72c5e5a053112",
    "details": {
      "sha": "971cff117093c5d92260290644b6f50c75866e4d",
      "filename": "logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterAgeRetentionTest.java",
      "status": "modified",
      "additions": 8,
      "deletions": 7,
      "changes": 15,
      "blob_url": "https://github.com/elastic/logstash/blob/ff37e1e0d3d19b605951c94263b72c5e5a053112/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriterAgeRetentionTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/ff37e1e0d3d19b605951c94263b72c5e5a053112/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriterAgeRetentionTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fcommon%2Fio%2FDeadLetterQueueWriterAgeRetentionTest.java?ref=ff37e1e0d3d19b605951c94263b72c5e5a053112",
      "patch": "@@ -335,15 +335,8 @@ public void testDLQWriterFlusherRemovesExpiredSegmentWhenCurrentWriterIsStale()\n         }\n     }\n \n-    private static boolean isWindows() {\n-        return System.getProperty(\"os.name\").startsWith(\"Windows\");\n-    }\n-\n     @Test\n     public void testDLQWriterFlusherRemovesExpiredSegmentWhenCurrentHeadSegmentIsEmpty() throws IOException {\n-        // https://github.com/elastic/logstash/issues/15768\n-        assumeThat(isWindows(), is(not(true)));\n-\n         final Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(\n                 Collections.singletonMap(\"message\", \"Not so important content\"));\n \n@@ -360,8 +353,16 @@ public void testDLQWriterFlusherRemovesExpiredSegmentWhenCurrentHeadSegmentIsEmp\n                 .build()) {\n \n             DLQEntry entry = new DLQEntry(event, \"\", \"\", \"00001\", DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(fakeClock));\n+            Instant beforeWriteEntry = Instant.now();\n             writeManager.writeEntry(entry);\n \n+            // WARN: writeEntry set the lastWrite instant which is later checked by isStale in RecordIOWriter\n+            // against now - flush period. Given that flush period is 0, it could be that now is the same\n+            // instant as lastWrite, while should be greater than, so put an artificial small delay\n+            Awaitility.await(\"Let the time flow a little so that last write is recognized to be in the past\")\n+                      .atMost(Duration.ofSeconds(1))\n+                      .until(() -> Instant.now().isAfter(beforeWriteEntry));\n+\n             triggerExecutionOfFlush();\n \n             // wait the flush interval so that the current head segment is sealed",
      "parent_sha": "14dc3d24a43648a035d92b9354f7dc67965a3c31"
    }
  },
  {
    "oid": "209a3aaa21d6ff91fdfa63e6f47772d2e6956eff",
    "message": "make some Queue tests less aggressive on Thread.sleep usage\n\nFixes #6297",
    "date": "2016-11-29T10:10:42Z",
    "url": "https://github.com/elastic/logstash/commit/209a3aaa21d6ff91fdfa63e6f47772d2e6956eff",
    "details": {
      "sha": "13a777a7d632795463c3566b441db33dda002905",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/elastic/logstash/blob/209a3aaa21d6ff91fdfa63e6f47772d2e6956eff/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/209a3aaa21d6ff91fdfa63e6f47772d2e6956eff/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=209a3aaa21d6ff91fdfa63e6f47772d2e6956eff",
      "patch": "@@ -417,7 +417,7 @@ public void reachMaxSizeTest() throws IOException, InterruptedException, Executi\n         ExecutorService executor = Executors.newFixedThreadPool(1);\n         Future<Long> future = executor.submit(write);\n \n-        while (!q.isFull()) { Thread.sleep(1); }\n+        while (!q.isFull()) { Thread.sleep(10); }\n \n         assertThat(q.isFull(), is(true));\n \n@@ -452,15 +452,15 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted\n         ExecutorService executor = Executors.newFixedThreadPool(1);\n         Future<Long> future = executor.submit(write);\n \n-        while (!q.isFull()) { Thread.sleep(1); }\n+        while (!q.isFull()) { Thread.sleep(10); }\n \n         assertThat(q.isFull(), is(true));\n \n         Batch b = q.readBatch(10); // read 1 page (10 events)\n         b.close();  // purge 1 page\n \n         // spin wait until data is written and write blocks\n-        while (q.isFull()) { Thread.sleep(1); }\n+        while (q.isFull()) { Thread.sleep(10); }\n \n         assertThat(q.isFull(), is(false));\n \n@@ -495,7 +495,7 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup\n         ExecutorService executor = Executors.newFixedThreadPool(1);\n         Future<Long> future = executor.submit(write);\n \n-        while (!q.isFull()) { Thread.sleep(1); }\n+        while (!q.isFull()) { Thread.sleep(10); }\n \n         assertThat(q.isFull(), is(true));\n ",
      "parent_sha": "bc3bcfde24a4c7421aa60487426782feba099752"
    }
  },
  {
    "oid": "aaaa6de51ac8081f41f37853d8923a78ef6b0c45",
    "message": "use sprintf un toString\n\nFixes #4325",
    "date": "2015-12-10T00:59:56Z",
    "url": "https://github.com/elastic/logstash/commit/aaaa6de51ac8081f41f37853d8923a78ef6b0c45",
    "details": {
      "sha": "16fcc1bc2b3df0a355f2ab45a818a9b7eab7d682",
      "filename": "logstash-core-event-java/src/main/java/com/logstash/Event.java",
      "status": "modified",
      "additions": 5,
      "deletions": 5,
      "changes": 10,
      "blob_url": "https://github.com/elastic/logstash/blob/aaaa6de51ac8081f41f37853d8923a78ef6b0c45/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Fcom%2Flogstash%2FEvent.java",
      "raw_url": "https://github.com/elastic/logstash/raw/aaaa6de51ac8081f41f37853d8923a78ef6b0c45/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Fcom%2Flogstash%2FEvent.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Fcom%2Flogstash%2FEvent.java?ref=aaaa6de51ac8081f41f37853d8923a78ef6b0c45",
      "patch": "@@ -196,13 +196,13 @@ public Event clone()\n     }\n \n     public String toString() {\n-        // TODO: until we have sprintf\n-        String host = (String)this.data.getOrDefault(\"host\", \"%{host}\");\n-        String message = (String)this.data.getOrDefault(\"message\", \"%{message}\");\n+        // TODO: (colin) clean this IOException handling, not sure why we bubble IOException here\n         try {\n-            return getTimestamp().toIso8601() + \" \" + host + \" \" + message;\n+            return (getTimestamp().toIso8601() + \" \" + this.sprintf(\"%{host} %{message}\"));\n         } catch (IOException e) {\n-            return host + \" \" + message;\n+            String host = (String)this.data.getOrDefault(\"host\", \"%{host}\");\n+            String message = (String)this.data.getOrDefault(\"message\", \"%{message}\");\n+            return (host + \" \" + message);\n         }\n     }\n ",
      "parent_sha": "aef6b5aad21039e6dedc7182124d114428966d53"
    }
  },
  {
    "oid": "71dc852b9d254e20481810ae146e75644d343ccc",
    "message": "MINOR: Prevent leaking threads from QueueTest\n\nFixes #7163",
    "date": "2017-05-19T10:33:16Z",
    "url": "https://github.com/elastic/logstash/commit/71dc852b9d254e20481810ae146e75644d343ccc",
    "details": {
      "sha": "3a585678c27da862004b539c8517699498d995d8",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 32,
      "deletions": 47,
      "changes": 79,
      "blob_url": "https://github.com/elastic/logstash/blob/71dc852b9d254e20481810ae146e75644d343ccc/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/71dc852b9d254e20481810ae146e75644d343ccc/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=71dc852b9d254e20481810ae146e75644d343ccc",
      "patch": "@@ -14,6 +14,7 @@\n import java.util.concurrent.Future;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicInteger;\n+import org.junit.After;\n import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -28,13 +29,26 @@\n import static org.junit.Assert.fail;\n \n public class QueueTest {\n-    @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();\n+\n+    @Rule\n+    public TemporaryFolder temporaryFolder = new TemporaryFolder();\n+\n+    private ExecutorService executor;\n \n     private String dataPath;\n \n     @Before\n     public void setUp() throws Exception {\n         dataPath = temporaryFolder.newFolder(\"data\").getPath();\n+        executor = Executors.newSingleThreadExecutor();\n+    }\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        executor.shutdownNow();\n+        if (!executor.awaitTermination(2L, TimeUnit.MINUTES)) {\n+            throw new IllegalStateException(\"Failed to shut down Executor\");\n+        }\n     }\n \n     @Test\n@@ -339,12 +353,7 @@ public void reachMaxUnread() throws IOException, InterruptedException, Execution\n             for (int i = 0; i < ELEMENT_COUNT; i++) {\n \n                 // we expect the next write call to block so let's wrap it in a Future\n-                Callable<Long> write = () -> {\n-                    return q.write(element);\n-                };\n-\n-                ExecutorService executor = Executors.newFixedThreadPool(1);\n-                Future<Long> future = executor.submit(write);\n+                Future<Long> future = executor.submit(() -> q.write(element));\n \n                 while (!q.isFull()) {\n                     // spin wait until data is written and write blocks\n@@ -360,8 +369,6 @@ public void reachMaxUnread() throws IOException, InterruptedException, Execution\n                 // future result is the blocked write seqNum for the second element\n                 assertThat(future.get(), is(2L + i));\n                 assertThat(q.isFull(), is(false));\n-\n-                executor.shutdown();\n             }\n \n             // since we did not ack and pages hold a single item\n@@ -391,10 +398,7 @@ public void reachMaxUnreadWithAcking() throws IOException, InterruptedException,\n             for (int i = 0; i < ELEMENT_COUNT; i++) {\n \n                 // we expect this next write call to block so let's wrap it in a Future\n-                Callable<Long> write = () -> q.write(element);\n-\n-                ExecutorService executor = Executors.newFixedThreadPool(1);\n-                Future<Long> future = executor.submit(write);\n+                Future<Long> future = executor.submit(() -> q.write(element));\n \n                 // spin wait until data is written and write blocks\n                 while (!q.isFull()) {\n@@ -409,8 +413,6 @@ public void reachMaxUnreadWithAcking() throws IOException, InterruptedException,\n                 // future result is the blocked write seqNum for the second element\n                 assertThat(future.get(), is(2L + i));\n                 assertThat(q.isFull(), is(false));\n-\n-                executor.shutdown();\n             }\n \n             // all batches are acked, no tail pages should exist\n@@ -436,22 +438,17 @@ public void reachMaxSizeTest() throws IOException, InterruptedException, Executi\n \n             int ELEMENT_COUNT = 90; // should be able to write 99 events before getting full\n             for (int i = 0; i < ELEMENT_COUNT; i++) {\n-                long seqNum = q.write(element);\n+                q.write(element);\n             }\n \n             assertThat(q.isFull(), is(false));\n \n             // we expect this next write call to block so let's wrap it in a Future\n-            Callable<Long> write = () -> q.write(element);\n-\n-            ExecutorService executor = Executors.newFixedThreadPool(1);\n-            Future<Long> future = executor.submit(write);\n+            executor.submit(() -> q.write(element));\n             while (!q.isFull()) {\n                 Thread.sleep(10);\n             }\n             assertThat(q.isFull(), is(true));\n-\n-            executor.shutdown();\n         }\n     }\n \n@@ -464,7 +461,6 @@ public void ackingMakesQueueNotFullAgainTest() throws IOException, InterruptedEx\n \n         // allow 10 elements per page but only 100 events in total\n         Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);\n-        ExecutorService executor = Executors.newFixedThreadPool(1);\n         try (TestQueue q = new TestQueue(settings)) {\n             q.open();\n             // should be able to write 90 events (9 pages) before getting full\n@@ -475,8 +471,7 @@ public void ackingMakesQueueNotFullAgainTest() throws IOException, InterruptedEx\n             assertThat(q.isFull(), is(false));\n             \n             // we expect this next write call to block so let's wrap it in a Future\n-            Callable<Long> write = () -> q.write(element);\n-            Future<Long> future = executor.submit(write);\n+            Future<Long> future = executor.submit(() -> q.write(element));\n             assertThat(future.isDone(), is(false));\n             \n             while (!q.isFull()) {\n@@ -491,9 +486,6 @@ public void ackingMakesQueueNotFullAgainTest() throws IOException, InterruptedEx\n             assertThat(q.isFull(), is(false));\n             \n             assertThat(future.get(), is(ELEMENT_COUNT + 1));\n-        } finally {\n-            executor.shutdownNow();\n-            executor.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n         }\n     }\n \n@@ -509,8 +501,8 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted\n             q.open();\n             int ELEMENT_COUNT =\n                 90; // should be able to write 90 events (9 pages) before getting full\n-            for (int i = 0; i < ELEMENT_COUNT; i++) {\n-                long seqNum = q.write(element);\n+            for (int i = 0; i < ELEMENT_COUNT; i++) { \n+                q.write(element);\n             }\n \n             assertThat(q.isFull(), is(false));\n@@ -520,9 +512,7 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted\n             Batch b = q.readBatch(10);\n \n             // we expect this next write call to block so let's wrap it in a Future\n-            Callable<Long> write = () -> q.write(element);\n-            ExecutorService executor = Executors.newFixedThreadPool(1);\n-            Future<Long> future = executor.submit(write);\n+            Future<Long> future = executor.submit(() -> q.write(element));\n             assertThat(future.isDone(), is(false));\n             while (!q.isFull()) {\n                 Thread.sleep(10);\n@@ -533,8 +523,6 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted\n             b.close();  // purge 1 page\n \n             assertThat(future.get(), is(ELEMENT_COUNT + 1L));\n-\n-            executor.shutdown();\n         }\n     }\n \n@@ -552,16 +540,13 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup\n \n             int ELEMENT_COUNT = 90; // should be able to write 99 events before getting full\n             for (int i = 0; i < ELEMENT_COUNT; i++) {\n-                long seqNum = q.write(element);\n+                q.write(element);\n             }\n \n             assertThat(q.isFull(), is(false));\n \n             // we expect this next write call to block so let's wrap it in a Future\n-            Callable<Long> write = () -> q.write(element);\n-\n-            ExecutorService executor = Executors.newFixedThreadPool(1);\n-            Future<Long> future = executor.submit(write);\n+            executor.submit(() -> q.write(element));\n             while (!q.isFull()) {\n                 Thread.sleep(10);\n             }\n@@ -571,8 +556,6 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup\n             b.close();  // this should not purge a page\n \n             assertThat(q.isFull(), is(true)); // queue should still be full\n-\n-            executor.shutdown();\n         }\n     }\n \n@@ -631,13 +614,15 @@ public void testAckedCount() throws IOException {\n     @Test(timeout = 5000)\n     public void concurrentWritesTest() throws IOException, InterruptedException, ExecutionException {\n \n+        final int WRITER_COUNT = 5;\n+\n+        final ExecutorService executorService = Executors.newFixedThreadPool(WRITER_COUNT);\n         // very small pages to maximize page creation\n         Settings settings = TestSettings.volatileQueueSettings(100);\n         try (TestQueue q = new TestQueue(settings)) {\n             q.open();\n \n             int ELEMENT_COUNT = 10000;\n-            int WRITER_COUNT = 5;\n             AtomicInteger element_num = new AtomicInteger(0);\n \n             // we expect this next write call to block so let's wrap it in a Future\n@@ -650,9 +635,8 @@ public void concurrentWritesTest() throws IOException, InterruptedException, Exe\n             };\n \n             List<Future<Integer>> futures = new ArrayList<>();\n-            ExecutorService executor = Executors.newFixedThreadPool(WRITER_COUNT);\n             for (int i = 0; i < WRITER_COUNT; i++) {\n-                futures.add(executor.submit(writer));\n+                futures.add(executorService.submit(writer));\n             }\n \n             int BATCH_SIZE = 10;\n@@ -671,8 +655,9 @@ public void concurrentWritesTest() throws IOException, InterruptedException, Exe\n \n             assertThat(q.getTailPages().isEmpty(), is(true));\n             assertThat(q.isFullyAcked(), is(true));\n-\n-            executor.shutdown();\n+        } finally {\n+            executorService.shutdownNow();\n+            executorService.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n         }\n     }\n ",
      "parent_sha": "9fe4e9b3b760bca1059116d9e7e8ae8466b15ea9"
    }
  },
  {
    "oid": "9c97879bf38fdd90725762c984cf79e937fd050b",
    "message": "Fix using deprecated type marker serialization calls\n\nFixes #8416",
    "date": "2017-10-02T14:31:47Z",
    "url": "https://github.com/elastic/logstash/commit/9c97879bf38fdd90725762c984cf79e937fd050b",
    "details": {
      "sha": "bf12ff19683e3ae74092dfe41753dd1eed80d32b",
      "filename": "logstash-core/src/main/java/org/logstash/ObjectMappers.java",
      "status": "modified",
      "additions": 21,
      "deletions": 15,
      "changes": 36,
      "blob_url": "https://github.com/elastic/logstash/blob/9c97879bf38fdd90725762c984cf79e937fd050b/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FObjectMappers.java",
      "raw_url": "https://github.com/elastic/logstash/raw/9c97879bf38fdd90725762c984cf79e937fd050b/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FObjectMappers.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FObjectMappers.java?ref=9c97879bf38fdd90725762c984cf79e937fd050b",
      "patch": "@@ -2,6 +2,8 @@\n \n import com.fasterxml.jackson.core.JsonGenerator;\n import com.fasterxml.jackson.core.JsonParser;\n+import com.fasterxml.jackson.core.JsonToken;\n+import com.fasterxml.jackson.core.type.WritableTypeId;\n import com.fasterxml.jackson.databind.DeserializationContext;\n import com.fasterxml.jackson.databind.JavaType;\n import com.fasterxml.jackson.databind.ObjectMapper;\n@@ -44,7 +46,7 @@ public final class ObjectMappers {\n         new SimpleModule(\"CborRubyDeserializers\")\n             .addDeserializer(RubyNil.class, new RubyNilDeserializer());\n \n-    public static final ObjectMapper JSON_MAPPER = \n+    public static final ObjectMapper JSON_MAPPER =\n         new ObjectMapper().registerModule(RUBY_SERIALIZERS);\n \n     public static final ObjectMapper CBOR_MAPPER = new ObjectMapper(\n@@ -63,9 +65,8 @@ private ObjectMappers() {\n \n     /**\n      * Serializer for scalar types that does not write type information when called via\n-     * {@link ObjectMappers.NonTypedScalarSerializer#serializeWithType(\n-     * Object, JsonGenerator, SerializerProvider, TypeSerializer)}.\n-     * @param <T>\n+     * {@link ObjectMappers.NonTypedScalarSerializer#serializeWithType(Object, JsonGenerator, SerializerProvider, TypeSerializer)}.\n+     * @param <T> Scalar Type\n      */\n     private abstract static class NonTypedScalarSerializer<T> extends StdScalarSerializer<T> {\n \n@@ -74,13 +75,12 @@ private abstract static class NonTypedScalarSerializer<T> extends StdScalarSeria\n         }\n \n         @Override\n-        public final void serializeWithType(final T value, final JsonGenerator gen, \n+        public final void serializeWithType(final T value, final JsonGenerator gen,\n             final SerializerProvider provider, final TypeSerializer typeSer) throws IOException {\n             serialize(value, gen, provider);\n         }\n     }\n \n-\n     /**\n      * Serializer for {@link RubyString} since Jackson can't handle that type natively, so we\n      * simply serialize it as if it were a {@link String}.\n@@ -159,7 +159,7 @@ public void serialize(final RubyBoolean value, final JsonGenerator generator,\n      * Serializer for {@link RubyFixnum} since Jackson can't handle that type natively, so we\n      * simply serialize it as if it were a {@code long}.\n      */\n-    private static final class RubyFixnumSerializer \n+    private static final class RubyFixnumSerializer\n         extends ObjectMappers.NonTypedScalarSerializer<RubyFixnum> {\n \n         RubyFixnumSerializer() {\n@@ -185,17 +185,19 @@ public static final class TimestampSerializer extends StdSerializer<Timestamp> {\n         }\n \n         @Override\n-        public void serialize(final Timestamp value, final JsonGenerator jgen, \n+        public void serialize(final Timestamp value, final JsonGenerator jgen,\n             final SerializerProvider provider) throws IOException {\n             jgen.writeString(value.toString());\n         }\n \n         @Override\n-        public void serializeWithType(final Timestamp value, final JsonGenerator jgen, \n+        public void serializeWithType(final Timestamp value, final JsonGenerator jgen,\n             final SerializerProvider serializers, final TypeSerializer typeSer) throws IOException {\n-            typeSer.writeTypePrefixForScalar(value, jgen, Timestamp.class);\n+            final WritableTypeId typeId =\n+                typeSer.typeId(value, Timestamp.class, JsonToken.VALUE_STRING);\n+            typeSer.writeTypePrefix(jgen, typeId);\n             jgen.writeString(value.toString());\n-            typeSer.writeTypeSuffixForScalar(value, jgen);\n+            typeSer.writeTypeSuffix(jgen, typeId);\n         }\n     }\n \n@@ -272,9 +274,11 @@ public void serializeWithType(final JrubyTimestampExtLibrary.RubyTimestamp value\n             final JsonGenerator jgen, final SerializerProvider serializers,\n             final TypeSerializer typeSer)\n             throws IOException {\n-            typeSer.writeTypePrefixForScalar(value, jgen, Timestamp.class);\n+            final WritableTypeId typeId =\n+                typeSer.typeId(value, Timestamp.class, JsonToken.VALUE_STRING);\n+            typeSer.writeTypePrefix(jgen, typeId);\n             jgen.writeObject(value.getTimestamp());\n-            typeSer.writeTypeSuffixForScalar(value, jgen);\n+            typeSer.writeTypeSuffix(jgen, typeId);\n         }\n     }\n \n@@ -297,9 +301,11 @@ public void serialize(final RubyNil value, final JsonGenerator jgen,\n         @Override\n         public void serializeWithType(final RubyNil value, final JsonGenerator jgen,\n             final SerializerProvider serializers, final TypeSerializer typeSer) throws IOException {\n-            typeSer.writeTypePrefixForScalar(value, jgen, RubyNil.class);\n+            final WritableTypeId typeId =\n+                typeSer.typeId(value, RubyNil.class, JsonToken.VALUE_NULL);\n+            typeSer.writeTypePrefix(jgen, typeId);\n             jgen.writeNull();\n-            typeSer.writeTypeSuffixForScalar(value, jgen);\n+            typeSer.writeTypeSuffix(jgen, typeId);\n         }\n     }\n ",
      "parent_sha": "0615f1b097d95c9502957ade5cf521840d70b216"
    }
  },
  {
    "oid": "a8185a2043d34afd9f5fe2c95655d11686f1f6b9",
    "message": "Timestamp.java reformatting, relates to #4191",
    "date": "2015-11-16T21:36:57Z",
    "url": "https://github.com/elastic/logstash/commit/a8185a2043d34afd9f5fe2c95655d11686f1f6b9",
    "details": {
      "sha": "7480e1aeb23908ce76742df2c5881100789deb3c",
      "filename": "src/main/java/com/logstash/Timestamp.java",
      "status": "modified",
      "additions": 18,
      "deletions": 18,
      "changes": 36,
      "blob_url": "https://github.com/elastic/logstash/blob/a8185a2043d34afd9f5fe2c95655d11686f1f6b9/src%2Fmain%2Fjava%2Fcom%2Flogstash%2FTimestamp.java",
      "raw_url": "https://github.com/elastic/logstash/raw/a8185a2043d34afd9f5fe2c95655d11686f1f6b9/src%2Fmain%2Fjava%2Fcom%2Flogstash%2FTimestamp.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/src%2Fmain%2Fjava%2Fcom%2Flogstash%2FTimestamp.java?ref=a8185a2043d34afd9f5fe2c95655d11686f1f6b9",
      "patch": "@@ -26,38 +26,38 @@ public Timestamp(String iso8601) {\n     }\n \n     public Timestamp(Timestamp t) {\n-    this.time = t.getTime();\n-  }\n+        this.time = t.getTime();\n+    }\n \n     public Timestamp(long epoch_milliseconds) {\n-    this.time = new DateTime(epoch_milliseconds, DateTimeZone.UTC);\n-  }\n+        this.time = new DateTime(epoch_milliseconds, DateTimeZone.UTC);\n+    }\n \n     public Timestamp(Long epoch_milliseconds) {\n-    this.time = new DateTime(epoch_milliseconds, DateTimeZone.UTC);\n-  }\n+        this.time = new DateTime(epoch_milliseconds, DateTimeZone.UTC);\n+    }\n \n     public Timestamp(Date date) {\n-    this.time = new DateTime(date, DateTimeZone.UTC);\n-  }\n+        this.time = new DateTime(date, DateTimeZone.UTC);\n+    }\n \n     public Timestamp(DateTime date) {\n-    this.time = date.toDateTime(DateTimeZone.UTC);\n-  }\n+        this.time = date.toDateTime(DateTimeZone.UTC);\n+    }\n \n     public DateTime getTime() {\n-    return time;\n-  }\n+        return time;\n+    }\n \n     public static Timestamp now() {\n-    return new Timestamp();\n-  }\n+        return new Timestamp();\n+    }\n \n     public String toIso8601() {\n-    return this.iso8601Formatter.print(this.time);\n-  }\n+        return this.iso8601Formatter.print(this.time);\n+    }\n \n     public String toString() {\n-    return toIso8601();\n-  }\n+        return toIso8601();\n+    }\n }",
      "parent_sha": "08c707fadad1a939e2f098adcdb684db6e0aa643"
    }
  },
  {
    "oid": "ffb72d3812ecd96ef7c64c2760340786357c1484",
    "message": "#8293 fix concurrent access to PQ persisted bytesize throwing\n\nFixes #8317",
    "date": "2017-09-19T18:16:44Z",
    "url": "https://github.com/elastic/logstash/commit/ffb72d3812ecd96ef7c64c2760340786357c1484",
    "details": {
      "sha": "f1fdbf2dce335fd95c03a8da7e040b111238069c",
      "filename": "logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java",
      "status": "modified",
      "additions": 12,
      "deletions": 7,
      "changes": 19,
      "blob_url": "https://github.com/elastic/logstash/blob/ffb72d3812ecd96ef7c64c2760340786357c1484/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "raw_url": "https://github.com/elastic/logstash/raw/ffb72d3812ecd96ef7c64c2760340786357c1484/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueue.java?ref=ffb72d3812ecd96ef7c64c2760340786357c1484",
      "patch": "@@ -139,14 +139,19 @@ public long getCurrentByteSize() {\n     }\n \n     public long getPersistedByteSize() {\n-        final long size;\n-        if (headPage == null) {\n-            size = 0L;\n-        } else {\n-            size = headPage.getPageIO().getHead()\n-                + tailPages.stream().mapToLong(p -> p.getPageIO().getHead()).sum();\n+        lock.lock();\n+        try {\n+            final long size;\n+            if (headPage == null) {\n+                size = 0L;\n+            } else {\n+                size = headPage.getPageIO().getHead()\n+                    + tailPages.stream().mapToLong(p -> p.getPageIO().getHead()).sum();\n+            }\n+            return size;\n+        } finally {\n+            lock.unlock();\n         }\n-        return size;\n     }\n \n     public int getPageCapacity() {",
      "parent_sha": "f7637058ea48f6a3e708a1f7e8ca5ef336751e7f"
    }
  },
  {
    "oid": "86ab26f818b0b35c7f45d23040d784afe544d866",
    "message": "keystore: instances of `CharsetEncoder` are stateful and cannot be shared\n\nFixes a crash that occurs on pipeline load and/or reload when using both the\njava keystore and the multi-pipeline feature, when more than one pipeline\nreferences `${}`-style variables.\n\nFixes #10408",
    "date": "2019-02-06T22:17:52Z",
    "url": "https://github.com/elastic/logstash/commit/86ab26f818b0b35c7f45d23040d784afe544d866",
    "details": {
      "sha": "b0424ff589f45656c9bcb8b183d164cbd216c88f",
      "filename": "logstash-core/src/main/java/org/logstash/secret/store/backend/JavaKeyStore.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/86ab26f818b0b35c7f45d23040d784afe544d866/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fsecret%2Fstore%2Fbackend%2FJavaKeyStore.java",
      "raw_url": "https://github.com/elastic/logstash/raw/86ab26f818b0b35c7f45d23040d784afe544d866/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fsecret%2Fstore%2Fbackend%2FJavaKeyStore.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fsecret%2Fstore%2Fbackend%2FJavaKeyStore.java?ref=86ab26f818b0b35c7f45d23040d784afe544d866",
      "patch": "@@ -41,7 +41,7 @@ public final class JavaKeyStore implements SecretStore {\n     private static final String KEYSTORE_TYPE = \"pkcs12\";\n     private static final Logger LOGGER = LogManager.getLogger(JavaKeyStore.class);\n     private static final String PATH_KEY = \"keystore.file\";\n-    private static final CharsetEncoder asciiEncoder = StandardCharsets.US_ASCII.newEncoder();\n+    private final CharsetEncoder asciiEncoder = StandardCharsets.US_ASCII.newEncoder();\n     private KeyStore keyStore;\n     private char[] keyStorePass;\n     private Path keyStorePath;",
      "parent_sha": "7b6d60c433dccfdc34ac931a200bb2d5fbca1b82"
    }
  },
  {
    "oid": "1eec33c3a80197b04264f4319270f2e165c4c341",
    "message": "Aliases static initialize method and adds info? method\n\nFixes #9536",
    "date": "2018-05-07T16:49:03Z",
    "url": "https://github.com/elastic/logstash/commit/1eec33c3a80197b04264f4319270f2e165c4c341",
    "details": {
      "sha": "f5a186d5e4545d1cbf865d1711cb9dbeaf3c591c",
      "filename": "logstash-core/src/main/java/org/logstash/log/LoggerExt.java",
      "status": "modified",
      "additions": 6,
      "deletions": 1,
      "changes": 7,
      "blob_url": "https://github.com/elastic/logstash/blob/1eec33c3a80197b04264f4319270f2e165c4c341/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flog%2FLoggerExt.java",
      "raw_url": "https://github.com/elastic/logstash/raw/1eec33c3a80197b04264f4319270f2e165c4c341/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flog%2FLoggerExt.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Flog%2FLoggerExt.java?ref=1eec33c3a80197b04264f4319270f2e165c4c341",
      "patch": "@@ -41,6 +41,11 @@ public RubyBoolean isDebug(final ThreadContext context) {\n         return logger.isDebugEnabled() ? context.tru : context.fals;\n     }\n \n+    @JRubyMethod(name = \"info?\")\n+    public RubyBoolean isInfo(final ThreadContext context) {\n+        return logger.isInfoEnabled() ? context.tru : context.fals;\n+    }\n+\n     @JRubyMethod(name = \"error?\")\n     public RubyBoolean isError(final ThreadContext context) {\n         return logger.isErrorEnabled() ? context.tru : context.fals;\n@@ -138,7 +143,7 @@ public static IRubyObject configureLogging(final ThreadContext context, final IR\n         return context.nil;\n     }\n \n-    @JRubyMethod(meta = true)\n+    @JRubyMethod(name = {\"reconfigure\", \"initialize\"}, meta = true)\n     public static IRubyObject reconfigure(final ThreadContext context, final IRubyObject self,\n                                           final IRubyObject configPath) {\n         synchronized (CONFIG_LOCK) {",
      "parent_sha": "3e14bc2741d4863aaf187117ffca2053f1146181"
    }
  },
  {
    "oid": "734405dcbec949a8c49fb1eafa76cb268620848f",
    "message": "Replace stack traces with logger in DSL. (#16159)",
    "date": "2024-05-13T20:45:56Z",
    "url": "https://github.com/elastic/logstash/commit/734405dcbec949a8c49fb1eafa76cb268620848f",
    "details": {
      "sha": "76a87dc8f89e90e2a0f465aba1144773ea890387",
      "filename": "logstash-core/src/main/java/org/logstash/config/ir/DSL.java",
      "status": "modified",
      "additions": 7,
      "deletions": 2,
      "changes": 9,
      "blob_url": "https://github.com/elastic/logstash/blob/734405dcbec949a8c49fb1eafa76cb268620848f/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FDSL.java",
      "raw_url": "https://github.com/elastic/logstash/raw/734405dcbec949a8c49fb1eafa76cb268620848f/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FDSL.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fconfig%2Fir%2FDSL.java?ref=734405dcbec949a8c49fb1eafa76cb268620848f",
      "patch": "@@ -25,6 +25,8 @@\n import java.util.Map;\n import java.util.UUID;\n \n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n import org.logstash.common.IncompleteSourceWithMetadataException;\n import org.logstash.common.SourceWithMetadata;\n import org.logstash.config.ir.expression.BooleanExpression;\n@@ -56,6 +58,9 @@\n import org.logstash.config.ir.imperative.Statement;\n \n public class DSL {\n+\n+    private static final Logger logger = LogManager.getLogger(DSL.class);\n+\n     public static EventValueExpression eEventValue(SourceWithMetadata meta, String fieldName) {\n         return new EventValueExpression(meta, fieldName);\n     }\n@@ -84,7 +89,7 @@ public static ValueExpression eValue(long value) {\n         try {\n             return eValue(null, value);\n         } catch (InvalidIRException e) {\n-            e.printStackTrace(); // Can't happen with an int\n+            logger.debug(e.getMessage(), e);\n             return null;\n         }\n     }\n@@ -93,7 +98,7 @@ public static ValueExpression eValue(double value) {\n         try {\n             return eValue(null, value);\n         } catch (InvalidIRException e) {\n-            e.printStackTrace(); // Can't happen with an int\n+            logger.debug(e.getMessage(), e);\n             return null;\n         }\n     }",
      "parent_sha": "2eebfd8f0eaa6baa2dc30b68212f75fc294c654e"
    }
  },
  {
    "oid": "c2b89d49d4a478077f70cc5b092c5085ef71a70c",
    "message": "fix broken resumeWriteOnNoLongerFullQueueTest\n\nFixes #6297",
    "date": "2016-11-29T10:10:42Z",
    "url": "https://github.com/elastic/logstash/commit/c2b89d49d4a478077f70cc5b092c5085ef71a70c",
    "details": {
      "sha": "bb54db0946162cd91978e55a58204db8cee65851",
      "filename": "logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/c2b89d49d4a478077f70cc5b092c5085ef71a70c/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/c2b89d49d4a478077f70cc5b092c5085ef71a70c/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fackedqueue%2FQueueTest.java?ref=c2b89d49d4a478077f70cc5b092c5085ef71a70c",
      "patch": "@@ -456,7 +456,7 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted\n \n         assertThat(q.isFull(), is(true));\n \n-        Batch b = q.readBatch(9); // read 1 page (10 events)\n+        Batch b = q.readBatch(10); // read 1 page (10 events)\n         b.close();  // purge 1 page\n \n         // spin wait until data is written and write blocks",
      "parent_sha": "094a1b2b6af532dfc56a9a7a9289d9d40c4842ba"
    }
  },
  {
    "oid": "a170e73571adc9d1c4c27a73323820fc6db6c5fc",
    "message": "Fix Javadoc warning\n\nAdds an @return value to fix the JavaDoc error\n\nFixes #7176",
    "date": "2017-05-22T23:57:20Z",
    "url": "https://github.com/elastic/logstash/commit/a170e73571adc9d1c4c27a73323820fc6db6c5fc",
    "details": {
      "sha": "8a7fb02d2a6738d493c2cbde196efbea16329675",
      "filename": "logstash-core/src/main/java/org/logstash/instrument/reports/MemoryReport.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/elastic/logstash/blob/a170e73571adc9d1c4c27a73323820fc6db6c5fc/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Finstrument%2Freports%2FMemoryReport.java",
      "raw_url": "https://github.com/elastic/logstash/raw/a170e73571adc9d1c4c27a73323820fc6db6c5fc/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Finstrument%2Freports%2FMemoryReport.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Finstrument%2Freports%2FMemoryReport.java?ref=a170e73571adc9d1c4c27a73323820fc6db6c5fc",
      "patch": "@@ -16,7 +16,9 @@ public class MemoryReport {\n \n     /**\n      * Build a report with current Memory information\n-     * @return\n+     * @return Returns a Map containing information about the\n+     *         current state of the Java memory pools\n+     *\n      */\n     public static Map<String, Map<String, Map<String, Object>>> generate() {\n         MemoryMonitor.Report report = generateReport(MemoryMonitor.Type.All);",
      "parent_sha": "30dc1b83a7107258ea5ec9acc71d65a19f0cf0d2"
    }
  },
  {
    "oid": "d236fa898ad49d5da69a4c8b1e032bc1df159037",
    "message": "move license URL check outside loop\n\nFixes #9895",
    "date": "2018-08-14T12:57:13Z",
    "url": "https://github.com/elastic/logstash/commit/d236fa898ad49d5da69a4c8b1e032bc1df159037",
    "details": {
      "sha": "71fb9266a9131cac439432092befee9c0b3fb640",
      "filename": "tools/dependencies-report/src/main/java/org/logstash/dependencies/ReportGenerator.java",
      "status": "modified",
      "additions": 5,
      "deletions": 4,
      "changes": 9,
      "blob_url": "https://github.com/elastic/logstash/blob/d236fa898ad49d5da69a4c8b1e032bc1df159037/tools%2Fdependencies-report%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fdependencies%2FReportGenerator.java",
      "raw_url": "https://github.com/elastic/logstash/raw/d236fa898ad49d5da69a4c8b1e032bc1df159037/tools%2Fdependencies-report%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fdependencies%2FReportGenerator.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/tools%2Fdependencies-report%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Fdependencies%2FReportGenerator.java?ref=d236fa898ad49d5da69a4c8b1e032bc1df159037",
      "patch": "@@ -143,10 +143,11 @@ private void checkDependencyLicense(Map<String, LicenseUrlPair> licenseMapping,\n \n             String[] dependencyLicenses = pair.license.split(\"\\\\|\");\n             boolean hasAcceptableLicense = false;\n-            for (int k = 0; k < dependencyLicenses.length && !hasAcceptableLicense; k++) {\n-                if (pair.url != null && !pair.url.equals(\"\") &&\n-                   (acceptableLicenses.stream().anyMatch(dependencyLicenses[k]::equalsIgnoreCase))) {\n-                    hasAcceptableLicense = true;\n+            if (pair.url != null && !pair.url.equals(\"\")) {\n+                for (int k = 0; k < dependencyLicenses.length && !hasAcceptableLicense; k++) {\n+                    if (acceptableLicenses.stream().anyMatch(dependencyLicenses[k]::equalsIgnoreCase)) {\n+                        hasAcceptableLicense = true;\n+                    }\n                 }\n             }\n ",
      "parent_sha": "95ea8e65c2151668550f64be19b7cf6e8d23419a"
    }
  },
  {
    "oid": "892a2d90db5c8a43de5edab99431241f989bd070",
    "message": "#7642 Fix windows line endings issues in ingest test\n\nFixes #7644",
    "date": "2017-07-11T15:17:50Z",
    "url": "https://github.com/elastic/logstash/commit/892a2d90db5c8a43de5edab99431241f989bd070",
    "details": {
      "sha": "2197208dc6b06c42d3af539fbaf4fe9eee376dd0",
      "filename": "tools/ingest-converter/src/test/java/org/logstash/ingest/IngestTest.java",
      "status": "modified",
      "additions": 25,
      "deletions": 2,
      "changes": 27,
      "blob_url": "https://github.com/elastic/logstash/blob/892a2d90db5c8a43de5edab99431241f989bd070/tools%2Fingest-converter%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fingest%2FIngestTest.java",
      "raw_url": "https://github.com/elastic/logstash/raw/892a2d90db5c8a43de5edab99431241f989bd070/tools%2Fingest-converter%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fingest%2FIngestTest.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/tools%2Fingest-converter%2Fsrc%2Ftest%2Fjava%2Forg%2Flogstash%2Fingest%2FIngestTest.java?ref=892a2d90db5c8a43de5edab99431241f989bd070",
      "patch": "@@ -5,6 +5,7 @@\n import java.io.InputStream;\n import java.net.URL;\n import java.nio.charset.StandardCharsets;\n+import java.util.regex.Pattern;\n import org.apache.commons.io.IOUtils;\n import org.junit.Rule;\n import org.junit.rules.TemporaryFolder;\n@@ -21,6 +22,17 @@\n @RunWith(Parameterized.class)\n public abstract class IngestTest {\n \n+    /**\n+     * Used to normalize line endings since static reference result files have Unix line endings.\n+     */\n+    private static final Pattern CR_LF =\n+        Pattern.compile(\"\\\\r\\\\n\");\n+\n+    /**\n+     * Used to normalize line endings since static reference result files have Unix line endings.\n+     */\n+    private static final Pattern CARRIAGE_RETURN = Pattern.compile(\"\\\\r\");\n+\n     @Rule\n     public TemporaryFolder temp = new TemporaryFolder();\n \n@@ -41,19 +53,30 @@ protected final void assertCorrectConversion(final Class clazz) throws Exception\n         );\n     }\n \n+    /**\n+     * Reads a file, normalizes line endings to Unix line endings and returns the whole content\n+     * as a String.\n+     * @param path Url to read\n+     * @return String content of the URL\n+     * @throws IOException On failure to read from given URL\n+     */\n     private static String utf8File(final URL path) throws IOException {\n         final ByteArrayOutputStream baos = new ByteArrayOutputStream();\n         try (final InputStream input = path.openStream()) {\n             IOUtils.copy(input, baos);\n         }\n-        return baos.toString(StandardCharsets.UTF_8.name());\n+        return CARRIAGE_RETURN.matcher(\n+            CR_LF.matcher(\n+                baos.toString(StandardCharsets.UTF_8.name())\n+            ).replaceAll(\"\\n\")\n+        ).replaceAll(\"\\n\");\n     }\n \n     private static URL resourcePath(final String name) {\n         return IngestTest.class.getResource(name);\n     }\n \n-    static URL getResultPath(TemporaryFolder temp) throws IOException {\n+    private static URL getResultPath(TemporaryFolder temp) throws IOException {\n         return temp.newFolder().toPath().resolve(\"converted\").toUri().toURL();\n     }\n }",
      "parent_sha": "62f827030833b9d0bf9801ef75115b51a1bc4723"
    }
  },
  {
    "oid": "a1ac7e699faa8a627e80acba6d4859650c3f3bf8",
    "message": "renamed newTimetsamp to newTimestamp",
    "date": "2016-03-17T22:54:41Z",
    "url": "https://github.com/elastic/logstash/commit/a1ac7e699faa8a627e80acba6d4859650c3f3bf8",
    "details": {
      "sha": "9748a815ccb4fcb26e937f2d8212874b7a2491b2",
      "filename": "logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/elastic/logstash/blob/a1ac7e699faa8a627e80acba6d4859650c3f3bf8/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Fcom%2Flogstash%2Fext%2FJrubyTimestampExtLibrary.java",
      "raw_url": "https://github.com/elastic/logstash/raw/a1ac7e699faa8a627e80acba6d4859650c3f3bf8/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Fcom%2Flogstash%2Fext%2FJrubyTimestampExtLibrary.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core-event-java%2Fsrc%2Fmain%2Fjava%2Fcom%2Flogstash%2Fext%2FJrubyTimestampExtLibrary.java?ref=a1ac7e699faa8a627e80acba6d4859650c3f3bf8",
      "patch": "@@ -140,7 +140,7 @@ public IRubyObject ruby_to_json(ThreadContext context, IRubyObject[] args)\n             return RubyString.newString(context.runtime,  \"\\\"\" + this.timestamp.toIso8601() + \"\\\"\");\n         }\n \n-        public static Timestamp newTimetsamp(IRubyObject time)\n+        public static Timestamp newTimestamp(IRubyObject time)\n         {\n             if (time.isNil()) {\n                 return new Timestamp();\n@@ -160,7 +160,7 @@ public static Timestamp newTimetsamp(IRubyObject time)\n         public static IRubyObject ruby_coerce(ThreadContext context, IRubyObject recv, IRubyObject time)\n         {\n             try {\n-                Timestamp ts = newTimetsamp(time);\n+                Timestamp ts = newTimestamp(time);\n                 return (ts == null) ? context.runtime.getNil() : RubyTimestamp.newRubyTimestamp(context.runtime, ts);\n              } catch (IllegalArgumentException e) {\n                 throw new RaiseException(\n@@ -178,7 +178,7 @@ public static IRubyObject ruby_parse_iso8601(ThreadContext context, IRubyObject\n         {\n             if (time instanceof RubyString) {\n                 try {\n-                    return RubyTimestamp.newRubyTimestamp(context.runtime, newTimetsamp(time));\n+                    return RubyTimestamp.newRubyTimestamp(context.runtime, newTimestamp(time));\n                 } catch (IllegalArgumentException e) {\n                     throw new RaiseException(\n                             context.runtime,",
      "parent_sha": "8d592240adea3fd963ed0df6f4d8a627e41393ee"
    }
  },
  {
    "oid": "8586f4aac222fdbf797bb918b3f0393bbad31c63",
    "message": "PERFORMANCE: Intern string keys from FieldReferences\n\nFixes #7894",
    "date": "2017-08-03T20:12:16Z",
    "url": "https://github.com/elastic/logstash/commit/8586f4aac222fdbf797bb918b3f0393bbad31c63",
    "details": {
      "sha": "a43d7ffdef5835a7afecc10a360e2877de6bcd92",
      "filename": "logstash-core/src/main/java/org/logstash/FieldReference.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/elastic/logstash/blob/8586f4aac222fdbf797bb918b3f0393bbad31c63/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FFieldReference.java",
      "raw_url": "https://github.com/elastic/logstash/raw/8586f4aac222fdbf797bb918b3f0393bbad31c63/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FFieldReference.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FFieldReference.java?ref=8586f4aac222fdbf797bb918b3f0393bbad31c63",
      "patch": "@@ -67,10 +67,10 @@ public static FieldReference parse(final CharSequence reference) {\n         final List<String> path = new ArrayList<>(parts.length);\n         for (final String part : parts) {\n             if (!part.isEmpty()) {\n-                path.add(part);\n+                path.add(part.intern());\n             }\n         }\n-        final String key = path.remove(path.size() - 1);\n+        final String key = path.remove(path.size() - 1).intern();\n         final boolean empty = path.isEmpty();\n         if (empty && key.equals(Event.METADATA)) {\n             return METADATA_PARENT_REFERENCE;",
      "parent_sha": "a0fea763703df4faa2d444ce553f317f3b7f0fc2"
    }
  },
  {
    "oid": "5c77bde6e44ba5e55a51835c8c94158bfdd11bf5",
    "message": "MINOR: Inline hot path code in Accessors\n\nFixes #7771",
    "date": "2017-07-23T18:34:05Z",
    "url": "https://github.com/elastic/logstash/commit/5c77bde6e44ba5e55a51835c8c94158bfdd11bf5",
    "details": {
      "sha": "f7197d1f260c39f20cb26ebd7762533021a1f2e3",
      "filename": "logstash-core/src/main/java/org/logstash/Accessors.java",
      "status": "modified",
      "additions": 42,
      "deletions": 50,
      "changes": 92,
      "blob_url": "https://github.com/elastic/logstash/blob/5c77bde6e44ba5e55a51835c8c94158bfdd11bf5/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FAccessors.java",
      "raw_url": "https://github.com/elastic/logstash/raw/5c77bde6e44ba5e55a51835c8c94158bfdd11bf5/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FAccessors.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2FAccessors.java?ref=5c77bde6e44ba5e55a51835c8c94158bfdd11bf5",
      "patch": "@@ -1,8 +1,8 @@\n package org.logstash;\n \n import java.util.HashMap;\n-import java.util.Map;\n import java.util.List;\n+import java.util.Map;\n \n public class Accessors {\n \n@@ -21,9 +21,36 @@ public Object get(String reference) {\n     }\n \n     public Object set(String reference, Object value) {\n-        FieldReference field = PathCache.cache(reference);\n-        Object target = findCreateTarget(field);\n-        return store(target, field.getKey(), value);\n+        final FieldReference field = PathCache.cache(reference);\n+        final Object target = findCreateTarget(field);\n+        final String key = field.getKey();\n+        if (target instanceof Map) {\n+            ((Map<String, Object>) target).put(key, value);\n+        } else if (target instanceof List) {\n+            int i;\n+            try {\n+                i = Integer.parseInt(key);\n+            } catch (NumberFormatException e) {\n+                return null;\n+            }\n+            int size = ((List<Object>) target).size();\n+            if (i >= size) {\n+                // grow array by adding trailing null items\n+                // this strategy reflects legacy Ruby impl behaviour and is backed by specs\n+                // TODO: (colin) this is potentially dangerous, and could produce OOM using arbitrary big numbers\n+                // TODO: (colin) should be guard against this?\n+                for (int j = size; j < i; j++) {\n+                    ((List<Object>) target).add(null);\n+                }\n+                ((List<Object>) target).add(value);\n+            } else {\n+                int offset = listIndex(i, ((List) target).size());\n+                ((List<Object>) target).set(offset, value);\n+            }\n+        } else {\n+            throw newCollectionException(target);\n+        }\n+        return value;\n     }\n \n     public Object del(String reference) {\n@@ -48,18 +75,17 @@ public Object del(String reference) {\n     }\n \n     public boolean includes(String reference) {\n-        FieldReference field = PathCache.cache(reference);\n-        Object target = findTarget(field);\n-        if (target instanceof Map && foundInMap((Map<String, Object>) target, field.getKey())) {\n-            return true;\n-        } else if (target instanceof List) {\n-            try {\n-                int i = Integer.parseInt(field.getKey());\n-                return (foundInList((List<Object>) target, i) ? true : false);\n-            } catch (NumberFormatException e) {\n-                return false;\n-            }\n-        } else {\n+        final FieldReference field = PathCache.cache(reference);\n+        final Object target = findTarget(field);\n+        final String key = field.getKey();\n+        return target instanceof Map && ((Map<String, Object>) target).containsKey(key) ||\n+            target instanceof List && foundInList(key, (List<Object>) target);\n+    }\n+\n+    private static boolean foundInList(final String key, final List<Object> target) {\n+        try {\n+            return foundInList(target, Integer.parseInt(key));\n+        } catch (NumberFormatException e) {\n             return false;\n         }\n     }\n@@ -132,10 +158,6 @@ private static boolean foundInList(List<Object> target, int index) {\n \n     }\n \n-    private static boolean foundInMap(Map<String, Object> target, String key) {\n-        return target.containsKey(key);\n-    }\n-\n     private static Object fetch(Object target, String key) {\n         if (target instanceof Map) {\n             Object result = ((Map<String, Object>) target).get(key);\n@@ -154,36 +176,6 @@ private static Object fetch(Object target, String key) {\n         }\n     }\n \n-    private static Object store(Object target, String key, Object value) {\n-        if (target instanceof Map) {\n-            ((Map<String, Object>) target).put(key, value);\n-        } else if (target instanceof List) {\n-            int i;\n-            try {\n-                i = Integer.parseInt(key);\n-            } catch (NumberFormatException e) {\n-                return null;\n-            }\n-            int size = ((List<Object>) target).size();\n-            if (i >= size) {\n-                // grow array by adding trailing null items\n-                // this strategy reflects legacy Ruby impl behaviour and is backed by specs\n-                // TODO: (colin) this is potentially dangerous, and could produce OOM using arbitrary big numbers\n-                // TODO: (colin) should be guard against this?\n-                for (int j = size; j < i; j++) {\n-                    ((List<Object>) target).add(null);\n-                }\n-                ((List<Object>) target).add(value);\n-            } else {\n-                int offset = listIndex(i, ((List) target).size());\n-                ((List<Object>) target).set(offset, value);\n-            }\n-        } else {\n-            throw newCollectionException(target);\n-        }\n-        return value;\n-    }\n-\n     private static boolean isCollection(Object target) {\n         if (target == null) {\n             return false;",
      "parent_sha": "a055b2b8546be37f94d214325c24aa76ecab0539"
    }
  },
  {
    "oid": "6486624ee7984b1fd94295646b6fe1f75b1f29e2",
    "message": "Fixed typo in guage metric of unknown type log.\n\nFixes #11689",
    "date": "2020-03-16T09:19:28Z",
    "url": "https://github.com/elastic/logstash/commit/6486624ee7984b1fd94295646b6fe1f75b1f29e2",
    "details": {
      "sha": "1e1d995a20d4507fdccba762d4156b5977be3d73",
      "filename": "logstash-core/src/main/java/org/logstash/instrument/metrics/gauge/LazyDelegatingGauge.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/elastic/logstash/blob/6486624ee7984b1fd94295646b6fe1f75b1f29e2/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Finstrument%2Fmetrics%2Fgauge%2FLazyDelegatingGauge.java",
      "raw_url": "https://github.com/elastic/logstash/raw/6486624ee7984b1fd94295646b6fe1f75b1f29e2/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Finstrument%2Fmetrics%2Fgauge%2FLazyDelegatingGauge.java",
      "contents_url": "https://api.github.com/repos/elastic/logstash/contents/logstash-core%2Fsrc%2Fmain%2Fjava%2Forg%2Flogstash%2Finstrument%2Fmetrics%2Fgauge%2FLazyDelegatingGauge.java?ref=6486624ee7984b1fd94295646b6fe1f75b1f29e2",
      "patch": "@@ -111,7 +111,7 @@ private synchronized void wakeMetric(Object value) {\n             } else if (value instanceof RubyTimestamp) {\n                 lazyMetric = new RubyTimeStampGauge(key, (RubyTimestamp) value);\n             } else {\n-                LOGGER.warn(\"A gauge metric of an unknown type ({}) has been create for key: {}. This may result in invalid serialization.  It is recommended to \" +\n+                LOGGER.warn(\"A gauge metric of an unknown type ({}) has been created for key: {}. This may result in invalid serialization.  It is recommended to \" +\n                         \"log an issue to the responsible developer/development team.\", value.getClass().getCanonicalName(), key);\n                 lazyMetric = new UnknownGauge(key, value);\n             }",
      "parent_sha": "c740b23375e97278cb8598c80e1f4e8f280898d0"
    }
  }
]