[
  {
    "oid": "1b0e16ba65fae1550315aedbcec93b03d12da8e0",
    "message": "KAFKA-17473 Speed Up ClientTelemetryTest (#17263)\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2024-09-29T17:10:29Z",
    "url": "https://github.com/apache/kafka/commit/1b0e16ba65fae1550315aedbcec93b03d12da8e0",
    "details": {
      "sha": "a2f57c7d0406d9266b05f8377189b477c05d7660",
      "filename": "core/src/test/java/kafka/admin/ClientTelemetryTest.java",
      "status": "modified",
      "additions": 7,
      "deletions": 3,
      "changes": 10,
      "blob_url": "https://github.com/apache/kafka/blob/1b0e16ba65fae1550315aedbcec93b03d12da8e0/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Fadmin%2FClientTelemetryTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/1b0e16ba65fae1550315aedbcec93b03d12da8e0/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Fadmin%2FClientTelemetryTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Fadmin%2FClientTelemetryTest.java?ref=1b0e16ba65fae1550315aedbcec93b03d12da8e0",
      "patch": "@@ -59,6 +59,7 @@\n import java.util.concurrent.ExecutionException;\n import java.util.stream.Collectors;\n \n+import static org.apache.kafka.clients.admin.AdminClientConfig.METRIC_REPORTER_CLASSES_CONFIG;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertNotNull;\n import static org.junit.jupiter.api.Assertions.assertNull;\n@@ -67,9 +68,12 @@\n @ExtendWith(value = ClusterTestExtensions.class)\n public class ClientTelemetryTest {\n \n-    @ClusterTest(types = Type.KRAFT,\n-            serverProperties = @ClusterConfigProperty(key = AdminClientConfig.METRIC_REPORTER_CLASSES_CONFIG,\n-                    value = \"kafka.admin.ClientTelemetryTest$GetIdClientTelemetry\"))\n+    @ClusterTest(\n+            types = Type.KRAFT, \n+            brokers = 3,\n+            serverProperties = {\n+                    @ClusterConfigProperty(key = METRIC_REPORTER_CLASSES_CONFIG, value = \"kafka.admin.ClientTelemetryTest$GetIdClientTelemetry\"),\n+            })\n     public void testClientInstanceId(ClusterInstance clusterInstance) throws InterruptedException, ExecutionException {\n         Map<String, Object> configs = new HashMap<>();\n         configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, clusterInstance.bootstrapServers());",
      "parent_sha": "05696037d330e5e355d62cb1b977bac9df78db82"
    }
  },
  {
    "oid": "b47d1950c73928b988a30ae08e55a82fad329096",
    "message": "MINOR: increase connectionMaxIdleMs to make test reliable (#13031)\n\nSaw this flaky test in recent builds: #1452, #1454,\r\n\r\norg.opentest4j.AssertionFailedError: Unexpected channel state EXPIRED ==> expected: <true> but was: <false>\r\n\tat app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)\r\n\tat app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)\r\n\tat app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)\r\n\tat app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)\r\n\tat app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:210)\r\n\tat app//org.apache.kafka.common.network.SslTransportLayerTest.testIOExceptionsDuringHandshake(SslTransportLayerTest.java:883)\r\n\tat app//org.apache.kafka.common.network.SslTransportLayerTest.testUngracefulRemoteCloseDuringHandshakeWrite(SslTransportLayerTest.java:833)\r\n\r\nWe expected the channel state to be AUTHENTICATE or READY, but got EXPIRED. Checking the test, we're not expecting expired state at all, but set a 5 secs idle timeout in selector, which is not enough when machine is busy. Increasing the connectionMaxIdleMs to 10 secs to make the test reliable.\r\n\r\nReviewers: Ismael Juma <ismael@juma.me.uk>",
    "date": "2022-12-22T02:29:13Z",
    "url": "https://github.com/apache/kafka/commit/b47d1950c73928b988a30ae08e55a82fad329096",
    "details": {
      "sha": "a391208faaf0ed26efde98aa327de0b11f484839",
      "filename": "clients/src/test/java/org/apache/kafka/common/network/SslTransportLayerTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/b47d1950c73928b988a30ae08e55a82fad329096/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FSslTransportLayerTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/b47d1950c73928b988a30ae08e55a82fad329096/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FSslTransportLayerTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FSslTransportLayerTest.java?ref=b47d1950c73928b988a30ae08e55a82fad329096",
      "patch": "@@ -867,7 +867,7 @@ private void testIOExceptionsDuringHandshake(Args args,\n             channelBuilder.flushFailureAction = flushFailureAction;\n             channelBuilder.failureIndex = i;\n             channelBuilder.configure(args.sslClientConfigs);\n-            this.selector = new Selector(5000, new Metrics(), time, \"MetricGroup\", channelBuilder, new LogContext());\n+            this.selector = new Selector(10000, new Metrics(), time, \"MetricGroup\", channelBuilder, new LogContext());\n \n             InetSocketAddress addr = new InetSocketAddress(\"localhost\", server.port());\n             selector.connect(node, addr, BUFFER_SIZE, BUFFER_SIZE);",
      "parent_sha": "e8232edd24c6dd31bc58d29967d552f3dc6cf831"
    }
  },
  {
    "oid": "bbbf431d6e82ce33ba77f378c173acedbe1e0d98",
    "message": "KAFKA-9384: Loop improvements (#7907)\n\nReviewers: Bruno Cadonna <bruno@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",
    "date": "2020-01-09T01:16:46Z",
    "url": "https://github.com/apache/kafka/commit/bbbf431d6e82ce33ba77f378c173acedbe1e0d98",
    "details": {
      "sha": "4790dc2abc9c0dc12088510adbc81f37e120975e",
      "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractSegments.java",
      "status": "modified",
      "additions": 5,
      "deletions": 12,
      "changes": 17,
      "blob_url": "https://github.com/apache/kafka/blob/bbbf431d6e82ce33ba77f378c173acedbe1e0d98/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fstate%2Finternals%2FAbstractSegments.java",
      "raw_url": "https://github.com/apache/kafka/raw/bbbf431d6e82ce33ba77f378c173acedbe1e0d98/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fstate%2Finternals%2FAbstractSegments.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fstate%2Finternals%2FAbstractSegments.java?ref=bbbf431d6e82ce33ba77f378c173acedbe1e0d98",
      "patch": "@@ -97,18 +97,11 @@ public void openExisting(final InternalProcessorContext context, final long stre\n             if (dir.exists()) {\n                 final String[] list = dir.list();\n                 if (list != null) {\n-                    final long[] segmentIds = new long[list.length];\n-                    for (int i = 0; i < list.length; i++) {\n-                        segmentIds[i] = segmentIdFromSegmentName(list[i], dir);\n-                    }\n-\n-                    // open segments in the id order\n-                    Arrays.sort(segmentIds);\n-                    for (final long segmentId : segmentIds) {\n-                        if (segmentId >= 0) {\n-                            getOrCreateSegment(segmentId, context);\n-                        }\n-                    }\n+                    Arrays.stream(list)\n+                            .map(segment -> segmentIdFromSegmentName(segment, dir))\n+                            .sorted() // open segments in the id order\n+                            .filter(segmentId -> segmentId >= 0)\n+                            .forEach(segmentId -> getOrCreateSegment(segmentId, context));\n                 }\n             } else {\n                 if (!dir.mkdir()) {",
      "parent_sha": "f6b36fe842d18dd8015ccb6f8490b84c7639336d"
    }
  },
  {
    "oid": "e71f6ebc81d24d5f6aad4ebc96d42e56c5eae3c6",
    "message": "MINOR: only log error when rack aware assignment is enabled (#14415)\n\nReviewers:  Lucas Brutschy <lbrutschy@confluent.io>, Matthias J. Sax <matthias@confluent.io>",
    "date": "2023-09-29T17:16:29Z",
    "url": "https://github.com/apache/kafka/commit/e71f6ebc81d24d5f6aad4ebc96d42e56c5eae3c6",
    "details": {
      "sha": "18e3d78d4d78e396ddc3f642988cd3e3ef222343",
      "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/RackAwareTaskAssignor.java",
      "status": "modified",
      "additions": 8,
      "deletions": 3,
      "changes": 11,
      "blob_url": "https://github.com/apache/kafka/blob/e71f6ebc81d24d5f6aad4ebc96d42e56c5eae3c6/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2Fassignment%2FRackAwareTaskAssignor.java",
      "raw_url": "https://github.com/apache/kafka/raw/e71f6ebc81d24d5f6aad4ebc96d42e56c5eae3c6/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2Fassignment%2FRackAwareTaskAssignor.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2Fassignment%2FRackAwareTaskAssignor.java?ref=e71f6ebc81d24d5f6aad4ebc96d42e56c5eae3c6",
      "patch": "@@ -212,8 +212,11 @@ private boolean validateClientRack(final Map<UUID, Map<String, Optional<String>>\n             KeyValue<String, String> previousRackInfo = null;\n             for (final Map.Entry<String, Optional<String>> rackEntry : entry.getValue().entrySet()) {\n                 if (!rackEntry.getValue().isPresent()) {\n-                    log.error(String.format(\"RackId doesn't exist for process %s and consumer %s\",\n-                        processId, rackEntry.getKey()));\n+                    if (!StreamsConfig.RACK_AWARE_ASSIGNMENT_STRATEGY_NONE.equals(assignmentConfigs.rackAwareAssignmentStrategy)) {\n+                        log.error(\n+                            String.format(\"RackId doesn't exist for process %s and consumer %s\",\n+                                processId, rackEntry.getKey()));\n+                    }\n                     return false;\n                 }\n                 if (previousRackInfo == null) {\n@@ -232,7 +235,9 @@ private boolean validateClientRack(final Map<UUID, Map<String, Optional<String>>\n                 }\n             }\n             if (previousRackInfo == null) {\n-                log.error(String.format(\"RackId doesn't exist for process %s\", processId));\n+                if (!StreamsConfig.RACK_AWARE_ASSIGNMENT_STRATEGY_NONE.equals(assignmentConfigs.rackAwareAssignmentStrategy)) {\n+                    log.error(String.format(\"RackId doesn't exist for process %s\", processId));\n+                }\n                 return false;\n             }\n             racksForProcess.put(entry.getKey(), previousRackInfo.value);",
      "parent_sha": "03259f6f414425c1c0b396eb31f0f4b17f93b424"
    }
  },
  {
    "oid": "e68a94056fa7eb5b8d69815bcfa522247568d3e9",
    "message": "KAFKA-5557: Using a logPrefix inside the StreamPartitionAssignor\n\nAdded logPrefix for avoiding stream thread name formatting replicated more times\n\nAuthor: ppatierno <ppatierno@live.com>\n\nReviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>\n\nCloses #3488 from ppatierno/kafka-5557",
    "date": "2017-07-06T05:20:24Z",
    "url": "https://github.com/apache/kafka/commit/e68a94056fa7eb5b8d69815bcfa522247568d3e9",
    "details": {
      "sha": "4eadb99e8fef247a422f2e19ba04597f96ed5258",
      "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java",
      "status": "modified",
      "additions": 28,
      "deletions": 25,
      "changes": 53,
      "blob_url": "https://github.com/apache/kafka/blob/e68a94056fa7eb5b8d69815bcfa522247568d3e9/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStreamPartitionAssignor.java",
      "raw_url": "https://github.com/apache/kafka/raw/e68a94056fa7eb5b8d69815bcfa522247568d3e9/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStreamPartitionAssignor.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStreamPartitionAssignor.java?ref=e68a94056fa7eb5b8d69815bcfa522247568d3e9",
      "patch": "@@ -62,6 +62,8 @@ public class StreamPartitionAssignor implements PartitionAssignor, Configurable\n     private final static int UNKNOWN = -1;\n     public final static int NOT_AVAILABLE = -2;\n \n+    private String logPrefix;\n+\n     private static class AssignedPartition implements Comparable<AssignedPartition> {\n         public final TaskId taskId;\n         public final TopicPartition partition;\n@@ -216,19 +218,21 @@ public void configure(Map<String, ?> configs) {\n         streamThread = (StreamThread) o;\n         streamThread.setPartitionAssignor(this);\n \n+        logPrefix = String.format(\"stream-thread [%s]\", streamThread.getName());\n+\n         String userEndPoint = (String) configs.get(StreamsConfig.APPLICATION_SERVER_CONFIG);\n         if (userEndPoint != null && !userEndPoint.isEmpty()) {\n             try {\n                 String host = getHost(userEndPoint);\n                 Integer port = getPort(userEndPoint);\n \n                 if (host == null || port == null)\n-                    throw new ConfigException(String.format(\"stream-thread [%s] Config %s isn't in the correct format. Expected a host:port pair\" +\n+                    throw new ConfigException(String.format(\"%s Config %s isn't in the correct format. Expected a host:port pair\" +\n                                     \" but received %s\",\n-                            streamThread.getName(), StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint));\n+                            logPrefix, StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint));\n             } catch (NumberFormatException nfe) {\n-                throw new ConfigException(String.format(\"stream-thread [%s] Invalid port supplied in %s for config %s\",\n-                        streamThread.getName(), userEndPoint, StreamsConfig.APPLICATION_SERVER_CONFIG));\n+                throw new ConfigException(String.format(\"%s Invalid port supplied in %s for config %s\",\n+                        logPrefix, userEndPoint, StreamsConfig.APPLICATION_SERVER_CONFIG));\n             }\n \n             this.userEndPoint = userEndPoint;\n@@ -271,8 +275,7 @@ public Subscription subscription(Set<String> topics) {\n \n     private void updateSubscribedTopics(Set<String> topics) {\n         SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();\n-        log.debug(\"stream-thread [{}] found {} topics possibly matching regex\",\n-                  streamThread.getName(), topics);\n+        log.debug(\"{} found {} topics possibly matching regex\", logPrefix, topics);\n         // update the topic groups with the returned subscription set for regex pattern subscriptions\n         subscriptionUpdates.updateTopics(topics);\n         streamThread.builder.updateSubscriptions(subscriptionUpdates, streamThread.getName());\n@@ -321,7 +324,7 @@ public Map<String, Assignment> assign(Cluster metadata, Map<String, Subscription\n             clientMetadata.addConsumer(consumerId, info);\n         }\n \n-        log.debug(\"stream-thread [{}] Constructed client metadata {} from the member subscriptions.\", streamThread.getName(), clientsMetadata);\n+        log.debug(\"{} Constructed client metadata {} from the member subscriptions.\", logPrefix, clientsMetadata);\n \n         // ---------------- Step Zero ---------------- //\n \n@@ -407,7 +410,7 @@ public Map<String, Assignment> assign(Cluster metadata, Map<String, Subscription\n \n         metadataWithInternalTopics = metadata.withPartitions(allRepartitionTopicPartitions);\n \n-        log.debug(\"stream-thread [{}] Created repartition topics {} from the parsed topology.\", streamThread.getName(), allRepartitionTopicPartitions.values());\n+        log.debug(\"{} Created repartition topics {} from the parsed topology.\", logPrefix, allRepartitionTopicPartitions.values());\n \n         // ---------------- Step One ---------------- //\n \n@@ -429,7 +432,7 @@ public Map<String, Assignment> assign(Cluster metadata, Map<String, Subscription\n             Set<TopicPartition> partitions = entry.getValue();\n             for (TopicPartition partition : partitions) {\n                 if (allAssignedPartitions.contains(partition)) {\n-                    log.warn(\"stream-thread [{}] Partition {} is assigned to more than one tasks: {}\", streamThread.getName(), partition, partitionsForTask);\n+                    log.warn(\"{} Partition {} is assigned to more than one tasks: {}\", logPrefix, partition, partitionsForTask);\n                 }\n             }\n             allAssignedPartitions.addAll(partitions);\n@@ -448,11 +451,11 @@ public Map<String, Assignment> assign(Cluster metadata, Map<String, Subscription\n                 for (PartitionInfo partitionInfo : partitionInfoList) {\n                     TopicPartition partition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());\n                     if (!allAssignedPartitions.contains(partition)) {\n-                        log.warn(\"stream-thread [{}] Partition {} is not assigned to any tasks: {}\", streamThread.getName(), partition, partitionsForTask);\n+                        log.warn(\"{} Partition {} is not assigned to any tasks: {}\", logPrefix, partition, partitionsForTask);\n                     }\n                 }\n             } else {\n-                log.warn(\"stream-thread [{}] No partitions found for topic {}\", streamThread.getName(), topic);\n+                log.warn(\"{} No partitions found for topic {}\", logPrefix, topic);\n             }\n         }\n \n@@ -475,14 +478,14 @@ public Map<String, Assignment> assign(Cluster metadata, Map<String, Subscription\n \n                     changelogTopicMetadata.put(topicConfig.name(), topicMetadata);\n                 } else {\n-                    log.debug(\"stream-thread [{}] No tasks found for topic group {}\", streamThread.getName(), topicGroupId);\n+                    log.debug(\"{} No tasks found for topic group {}\", logPrefix, topicGroupId);\n                 }\n             }\n         }\n \n         prepareTopic(changelogTopicMetadata);\n \n-        log.debug(\"stream-thread [{}] Created state changelog topics {} from the parsed topology.\", streamThread.getName(), changelogTopicMetadata.values());\n+        log.debug(\"{} Created state changelog topics {} from the parsed topology.\", logPrefix, changelogTopicMetadata.values());\n \n         // ---------------- Step Two ---------------- //\n \n@@ -492,13 +495,13 @@ public Map<String, Assignment> assign(Cluster metadata, Map<String, Subscription\n             states.put(entry.getKey(), entry.getValue().state);\n         }\n \n-        log.debug(\"stream-thread [{}] Assigning tasks {} to clients {} with number of replicas {}\",\n-                streamThread.getName(), partitionsForTask.keySet(), states, numStandbyReplicas);\n+        log.debug(\"{} Assigning tasks {} to clients {} with number of replicas {}\",\n+                logPrefix, partitionsForTask.keySet(), states, numStandbyReplicas);\n \n         final StickyTaskAssignor<UUID> taskAssignor = new StickyTaskAssignor<>(states, partitionsForTask.keySet());\n         taskAssignor.assign(numStandbyReplicas);\n \n-        log.info(\"stream-thread [{}] Assigned tasks to clients as {}.\", streamThread.getName(), states);\n+        log.info(\"{} Assigned tasks to clients as {}.\", logPrefix, states);\n \n         // ---------------- Step Three ---------------- //\n \n@@ -589,8 +592,8 @@ public void onAssignment(Assignment assignment) {\n         // could be duplicated if one task has more than one assigned partitions\n         if (partitions.size() != info.activeTasks.size()) {\n             throw new TaskAssignmentException(\n-                    String.format(\"stream-thread [%s] Number of assigned partitions %d is not equal to the number of active taskIds %d\" +\n-                            \", assignmentInfo=%s\", streamThread.getName(), partitions.size(), info.activeTasks.size(), info.toString())\n+                    String.format(\"%s Number of assigned partitions %d is not equal to the number of active taskIds %d\" +\n+                            \", assignmentInfo=%s\", logPrefix, partitions.size(), info.activeTasks.size(), info.toString())\n             );\n         }\n \n@@ -643,7 +646,7 @@ private void checkForNewTopicAssignments(Assignment assignment) {\n      * @param topicPartitions Map that contains the topic names to be created with the number of partitions\n      */\n     private void prepareTopic(final Map<String, InternalTopicMetadata> topicPartitions) {\n-        log.debug(\"stream-thread [{}] Starting to validate internal topics in partition assignor.\", streamThread.getName());\n+        log.debug(\"{} Starting to validate internal topics in partition assignor.\", logPrefix);\n \n         // first construct the topics to make ready\n         Map<InternalTopicConfig, Integer> topicsToMakeReady = new HashMap<>();\n@@ -657,7 +660,7 @@ private void prepareTopic(final Map<String, InternalTopicMetadata> topicPartitio\n                 continue;\n             }\n             if (numPartitions < 0) {\n-                throw new TopologyBuilderException(String.format(\"stream-thread [%s] Topic [%s] number of partitions not defined\", streamThread.getName(), topic.name()));\n+                throw new TopologyBuilderException(String.format(\"%s Topic [%s] number of partitions not defined\", logPrefix, topic.name()));\n             }\n \n             topicsToMakeReady.put(topic, numPartitions);\n@@ -677,7 +680,7 @@ private void prepareTopic(final Map<String, InternalTopicMetadata> topicPartitio\n             }\n         }\n \n-        log.debug(\"stream-thread [{}] Completed validating internal topics in partition assignor\", streamThread.getName());\n+        log.debug(\"{} Completed validating internal topics in partition assignor\", logPrefix);\n     }\n \n     private boolean allTopicsCreated(final Set<String> topicNamesToMakeReady, final Map<InternalTopicConfig, Integer> topicsToMakeReady) {\n@@ -765,10 +768,10 @@ public void close() {\n     }\n \n     static class CopartitionedTopicsValidator {\n-        private final String threadName;\n+        private final String logPrefix;\n \n         CopartitionedTopicsValidator(final String threadName) {\n-            this.threadName = threadName;\n+            this.logPrefix = String.format(\"stream-thread [%s]\", threadName);\n         }\n \n         void validate(final Set<String> copartitionGroup,\n@@ -781,15 +784,15 @@ void validate(final Set<String> copartitionGroup,\n                     final Integer partitions = metadata.partitionCountForTopic(topic);\n \n                     if (partitions == null) {\n-                        throw new TopologyBuilderException(String.format(\"stream-thread [%s] Topic not found: %s\", threadName, topic));\n+                        throw new TopologyBuilderException(String.format(\"%s Topic not found: %s\", logPrefix, topic));\n                     }\n \n                     if (numPartitions == UNKNOWN) {\n                         numPartitions = partitions;\n                     } else if (numPartitions != partitions) {\n                         final String[] topics = copartitionGroup.toArray(new String[copartitionGroup.size()]);\n                         Arrays.sort(topics);\n-                        throw new TopologyBuilderException(String.format(\"stream-thread [%s] Topics not co-partitioned: [%s]\", threadName, Utils.join(Arrays.asList(topics), \",\")));\n+                        throw new TopologyBuilderException(String.format(\"%s Topics not co-partitioned: [%s]\", logPrefix, Utils.join(Arrays.asList(topics), \",\")));\n                     }\n                 } else if (allRepartitionTopicsNumPartitions.get(topic).numPartitions == NOT_AVAILABLE) {\n                     numPartitions = NOT_AVAILABLE;",
      "parent_sha": "70e949d522eba72b22c2619c4fde372d0f1a26b3"
    }
  },
  {
    "oid": "106fd5f601df858ab0affd8967a07df0987e9ee1",
    "message": "KAFKA-18354 Use log4j2 APIs to refactor LogCaptureAppender (#18338)\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2024-12-29T14:50:53Z",
    "url": "https://github.com/apache/kafka/commit/106fd5f601df858ab0affd8967a07df0987e9ee1",
    "details": {
      "sha": "f4385d8acacd0fe1571afd0497a9c197e04afeb6",
      "filename": "clients/src/test/java/org/apache/kafka/common/utils/LogCaptureAppender.java",
      "status": "modified",
      "additions": 19,
      "deletions": 46,
      "changes": 65,
      "blob_url": "https://github.com/apache/kafka/blob/106fd5f601df858ab0affd8967a07df0987e9ee1/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Futils%2FLogCaptureAppender.java",
      "raw_url": "https://github.com/apache/kafka/raw/106fd5f601df858ab0affd8967a07df0987e9ee1/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Futils%2FLogCaptureAppender.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Futils%2FLogCaptureAppender.java?ref=106fd5f601df858ab0affd8967a07df0987e9ee1",
      "patch": "@@ -20,35 +20,26 @@\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n import org.apache.logging.log4j.core.LogEvent;\n-import org.apache.logging.log4j.core.LoggerContext;\n import org.apache.logging.log4j.core.appender.AbstractAppender;\n-import org.apache.logging.log4j.core.config.Configuration;\n-import org.apache.logging.log4j.core.config.LoggerConfig;\n+import org.apache.logging.log4j.core.config.Configurator;\n import org.apache.logging.log4j.core.config.Property;\n \n import java.io.PrintWriter;\n import java.io.StringWriter;\n import java.util.ArrayList;\n+import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Map;\n import java.util.Optional;\n+import java.util.UUID;\n import java.util.stream.Collectors;\n \n public class LogCaptureAppender extends AbstractAppender implements AutoCloseable {\n     private final List<LogEvent> events = new LinkedList<>();\n-    private final List<LogLevelChange> logLevelChanges = new LinkedList<>();\n+    private final Map<Class<?>, Level> logLevelChanges = new HashMap<>();\n     private final List<org.apache.logging.log4j.core.Logger> loggers = new ArrayList<>();\n-\n-    public static class LogLevelChange {\n-        private final Level originalLevel;\n-        private final Class<?> clazz;\n-\n-        public LogLevelChange(final Level originalLevel, final Class<?> clazz) {\n-            this.originalLevel = originalLevel;\n-            this.clazz = clazz;\n-        }\n-    }\n-\n+    \n     @SuppressWarnings(\"OptionalUsedAsFieldOrParameterType\")\n     public static class Event {\n         private final String level;\n@@ -80,19 +71,19 @@ public Optional<String> getThrowableClassName() {\n         }\n     }\n \n-    public LogCaptureAppender(String name) {\n-        super(name, null, null, true, Property.EMPTY_ARRAY);\n+    public LogCaptureAppender() {\n+        super(\"LogCaptureAppender-\" + UUID.randomUUID(), null, null, true, Property.EMPTY_ARRAY);\n     }\n \n     public static LogCaptureAppender createAndRegister() {\n-        final LogCaptureAppender logCaptureAppender = new LogCaptureAppender(\"LogCaptureAppender\");\n+        final LogCaptureAppender logCaptureAppender = new LogCaptureAppender();\n         Logger logger = LogManager.getRootLogger();\n         logCaptureAppender.addToLogger(logger);\n         return logCaptureAppender;\n     }\n \n     public static LogCaptureAppender createAndRegister(final Class<?> clazz) {\n-        final LogCaptureAppender logCaptureAppender = new LogCaptureAppender(\"LogCaptureAppender\");\n+        final LogCaptureAppender logCaptureAppender = new LogCaptureAppender();\n         Logger logger = LogManager.getLogger(clazz);\n         logCaptureAppender.addToLogger(logger);\n         return logCaptureAppender;\n@@ -106,21 +97,12 @@ public void addToLogger(Logger logger) {\n     }\n \n     public void setClassLogger(final Class<?> clazz, Level level) {\n-        LoggerContext ctx = (LoggerContext) LogManager.getContext(false);\n-        Configuration config = ctx.getConfiguration();\n-        String loggerName = clazz.getName();\n-        LoggerConfig loggerConfig = config.getLoggerConfig(loggerName);\n-\n-        Level originalLevel = loggerConfig.getLevel();\n-        logLevelChanges.add(new LogLevelChange(originalLevel, clazz));\n-\n-        if (!loggerConfig.getName().equals(loggerName)) {\n-            LoggerConfig newLoggerConfig = new LoggerConfig(loggerName, level, true);\n-            config.addLogger(loggerName, newLoggerConfig);\n-        } else {\n-            loggerConfig.setLevel(level);\n+        if (!logLevelChanges.containsKey(clazz)) {\n+            Level currentLevel = LogManager.getLogger(clazz).getLevel();\n+            logLevelChanges.put(clazz, currentLevel);\n         }\n-        ctx.updateLoggers();\n+\n+        Configurator.setLevel(clazz.getName(), level);\n     }\n \n     @Override\n@@ -177,21 +159,12 @@ public List<Event> getEvents() {\n \n     @Override\n     public void close() {\n-        LoggerContext ctx = (LoggerContext) LogManager.getContext(false);\n-        Configuration config = ctx.getConfiguration();\n-\n-        for (final LogLevelChange logLevelChange : logLevelChanges) {\n-            String loggerName = logLevelChange.clazz.getName();\n-            LoggerConfig loggerConfig = config.getLoggerConfig(loggerName);\n-            if (!loggerConfig.getName().equals(loggerName)) {\n-                LoggerConfig newLoggerConfig = new LoggerConfig(loggerName, logLevelChange.originalLevel, true);\n-                config.addLogger(loggerName, newLoggerConfig);\n-            } else {\n-                loggerConfig.setLevel(logLevelChange.originalLevel);\n-            }\n+        for (Map.Entry<Class<?>, Level> entry : logLevelChanges.entrySet()) {\n+            Class<?> clazz = entry.getKey();\n+            Level originalLevel = entry.getValue();\n+            Configurator.setLevel(clazz.getName(), originalLevel);\n         }\n         logLevelChanges.clear();\n-        ctx.updateLoggers();\n \n         unregister();\n     }",
      "parent_sha": "e9a03b3a86b423beca4bef0aa52811452f056ca7"
    }
  },
  {
    "oid": "667a6b2d2611200e8363d67185785412bb701d73",
    "message": "MINOR: fix record time in test shouldWipeOutStandbyStateDirectoryIfCheckpointIsMissing (#9948)\n\nReviewer: Matthias J. Sax <matthias@confluent.io>",
    "date": "2021-01-22T23:55:05Z",
    "url": "https://github.com/apache/kafka/commit/667a6b2d2611200e8363d67185785412bb701d73",
    "details": {
      "sha": "88d3ae57c9f6bbb9a4b6818b138ae814dc38ee13",
      "filename": "streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java",
      "status": "modified",
      "additions": 4,
      "deletions": 3,
      "changes": 7,
      "blob_url": "https://github.com/apache/kafka/blob/667a6b2d2611200e8363d67185785412bb701d73/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FStandbyTaskEOSIntegrationTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/667a6b2d2611200e8363d67185785412bb701d73/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FStandbyTaskEOSIntegrationTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FStandbyTaskEOSIntegrationTest.java?ref=667a6b2d2611200e8363d67185785412bb701d73",
      "patch": "@@ -176,6 +176,7 @@ private KafkaStreams buildStreamWithDirtyStateDir(final String stateDirPath,\n     @Test\n     @Deprecated\n     public void shouldWipeOutStandbyStateDirectoryIfCheckpointIsMissing() throws Exception {\n+        final long time = System.currentTimeMillis();\n         final String base = TestUtils.tempDirectory(appId).getPath();\n \n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n@@ -189,7 +190,7 @@ public void shouldWipeOutStandbyStateDirectoryIfCheckpointIsMissing() throws Exc\n                 IntegerSerializer.class,\n                 new Properties()\n             ),\n-            10L\n+            10L + time\n         );\n \n         try (\n@@ -248,7 +249,7 @@ public void shouldWipeOutStandbyStateDirectoryIfCheckpointIsMissing() throws Exc\n                     IntegerSerializer.class,\n                     new Properties()\n                 ),\n-                10L\n+                10L + time\n             );\n             waitForCondition(\n                 () -> streamInstanceOne.state() == KafkaStreams.State.ERROR,\n@@ -307,7 +308,7 @@ public void shouldWipeOutStandbyStateDirectoryIfCheckpointIsMissing() throws Exc\n                     IntegerSerializer.class,\n                     new Properties()\n                 ),\n-                10L\n+                10L + time\n             );\n             waitForCondition(\n                 () -> streamInstanceOneRecovery.state() == KafkaStreams.State.ERROR,",
      "parent_sha": "7b06a2417df0d098b8947327f77629f16d2f5e49"
    }
  },
  {
    "oid": "9e06767ffa80b26791c3bff6bc9b10b6612ce7d2",
    "message": "KAFKA-13898 Updated docs for metrics.recording.level (#16402)\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2024-07-30T20:29:33Z",
    "url": "https://github.com/apache/kafka/commit/9e06767ffa80b26791c3bff6bc9b10b6612ce7d2",
    "details": {
      "sha": "b4fd73e40de6454000a5082e6dff92aa63b32c7f",
      "filename": "clients/src/main/java/org/apache/kafka/clients/CommonClientConfigs.java",
      "status": "modified",
      "additions": 7,
      "deletions": 2,
      "changes": 9,
      "blob_url": "https://github.com/apache/kafka/blob/9e06767ffa80b26791c3bff6bc9b10b6612ce7d2/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FCommonClientConfigs.java",
      "raw_url": "https://github.com/apache/kafka/raw/9e06767ffa80b26791c3bff6bc9b10b6612ce7d2/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FCommonClientConfigs.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FCommonClientConfigs.java?ref=9e06767ffa80b26791c3bff6bc9b10b6612ce7d2",
      "patch": "@@ -118,8 +118,13 @@ public class CommonClientConfigs {\n     public static final String METRICS_NUM_SAMPLES_DOC = \"The number of samples maintained to compute metrics.\";\n \n     public static final String METRICS_RECORDING_LEVEL_CONFIG = \"metrics.recording.level\";\n-    public static final String METRICS_RECORDING_LEVEL_DOC = \"The highest recording level for metrics.\";\n-\n+    public static final String METRICS_RECORDING_LEVEL_DOC = \"The highest recording level for metrics. It has three levels for recording metrics - info, debug, and trace.\\n\" +\n+            \" \\n\" +\n+            \"INFO level records only essential metrics necessary for monitoring system performance and health. It collects vital data without gathering too much detail, making it suitable for production environments where minimal overhead is desired.\\n\" +\n+            \"\\n\" +\n+            \"DEBUG level records most metrics, providing more detailed information about the system's operation. It's useful for development and testing environments where you need deeper insights to debug and fine-tune the application.\\n\" +\n+            \"\\n\" +\n+            \"TRACE level records all possible metrics, capturing every detail about the system's performance and operation. It's best for controlled environments where in-depth analysis is required, though it can introduce significant overhead.\";\n     public static final String METRIC_REPORTER_CLASSES_CONFIG = \"metric.reporters\";\n     public static final String METRIC_REPORTER_CLASSES_DOC = \"A list of classes to use as metrics reporters. Implementing the <code>org.apache.kafka.common.metrics.MetricsReporter</code> interface allows plugging in classes that will be notified of new metric creation. The JmxReporter is always included to register JMX statistics.\";\n ",
      "parent_sha": "047b5aefd0299811cee364cf55dae8788ede7020"
    }
  },
  {
    "oid": "997dfa950ec58e789acafbb676313edaf0b7fa8b",
    "message": "MINOR: Fix typo in selector documentation (#12710)\n\nReviewers: David Jacot <djacot@confluent.io>",
    "date": "2022-10-04T11:05:26Z",
    "url": "https://github.com/apache/kafka/commit/997dfa950ec58e789acafbb676313edaf0b7fa8b",
    "details": {
      "sha": "e5787b94c7b5cbb6a10147690b39221fe0739107",
      "filename": "clients/src/main/java/org/apache/kafka/common/network/Selector.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/997dfa950ec58e789acafbb676313edaf0b7fa8b/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FSelector.java",
      "raw_url": "https://github.com/apache/kafka/raw/997dfa950ec58e789acafbb676313edaf0b7fa8b/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FSelector.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FSelector.java?ref=997dfa950ec58e789acafbb676313edaf0b7fa8b",
      "patch": "@@ -426,7 +426,7 @@ public void send(NetworkSend send) {\n      * buffers. If there are channels with buffered data that can by processed, we set \"timeout\" to 0 and process the data even\n      * if there is no more data to read from the socket.\n      *\n-     * Atmost one entry is added to \"completedReceives\" for a channel in each poll. This is necessary to guarantee that\n+     * At most one entry is added to \"completedReceives\" for a channel in each poll. This is necessary to guarantee that\n      * requests from a channel are processed on the broker in the order they are sent. Since outstanding requests added\n      * by SocketServer to the request queue may be processed by different request handler threads, requests on each\n      * channel must be processed one-at-a-time to guarantee ordering.",
      "parent_sha": "fb0ae71f3387401014b20991731dd3f54a721623"
    }
  },
  {
    "oid": "a0e0028b16ae1b7b4a1dca1715b5b130187b334a",
    "message": "MINOR: add test for repartition/source-topic/changelog optimization (#9668)\n\nIf topology optimization is enabled, KafkaStreams does not create store changelog topics but re-uses source input topics if possible. However, this optimization should not be applied to internal repartition topics, because those are actively purged.\r\n\r\nReviewers: A. Sophie Blee-Goldman <sophie@confluent.io>",
    "date": "2020-12-23T19:56:55Z",
    "url": "https://github.com/apache/kafka/commit/a0e0028b16ae1b7b4a1dca1715b5b130187b334a",
    "details": {
      "sha": "d687e1551921340514d529d149de8334228cba57",
      "filename": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java",
      "status": "modified",
      "additions": 30,
      "deletions": 1,
      "changes": 31,
      "blob_url": "https://github.com/apache/kafka/blob/a0e0028b16ae1b7b4a1dca1715b5b130187b334a/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FStreamsBuilderTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/a0e0028b16ae1b7b4a1dca1715b5b130187b334a/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FStreamsBuilderTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FStreamsBuilderTest.java?ref=a0e0028b16ae1b7b4a1dca1715b5b130187b334a",
      "patch": "@@ -16,7 +16,6 @@\n  */\n package org.apache.kafka.streams;\n \n-import java.util.regex.Pattern;\n import org.apache.kafka.common.serialization.LongSerializer;\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.common.serialization.StringSerializer;\n@@ -62,6 +61,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Properties;\n+import java.util.regex.Pattern;\n \n import static java.util.Arrays.asList;\n import static org.hamcrest.CoreMatchers.equalTo;\n@@ -462,6 +462,35 @@ public void shouldReuseSourceTopicAsChangelogsWithOptimization20() {\n             equalTo(true));\n     }\n \n+    @Test\n+    public void shouldNotReuseRepartitionTopicAsChangelogs() {\n+        final String topic = \"topic\";\n+        builder.<Long, String>stream(topic).repartition().toTable(Materialized.as(\"store\"));\n+        final Properties props = StreamsTestUtils.getStreamsConfig(\"appId\");\n+        props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, StreamsConfig.OPTIMIZE);\n+        final Topology topology = builder.build(props);\n+\n+        final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(topology);\n+        internalTopologyBuilder.rewriteTopology(new StreamsConfig(props));\n+\n+        assertThat(\n+            internalTopologyBuilder.buildTopology().storeToChangelogTopic(),\n+            equalTo(Collections.singletonMap(\"store\", \"appId-store-changelog\"))\n+        );\n+        assertThat(\n+            internalTopologyBuilder.stateStores().keySet(),\n+            equalTo(Collections.singleton(\"store\"))\n+        );\n+        assertThat(\n+            internalTopologyBuilder.stateStores().get(\"store\").loggingEnabled(),\n+            equalTo(true)\n+        );\n+        assertThat(\n+            internalTopologyBuilder.topicGroups().get(1).stateChangelogTopics.keySet(),\n+            equalTo(Collections.singleton(\"appId-store-changelog\"))\n+        );\n+    }\n+\n     @Test\n     public void shouldNotReuseSourceTopicAsChangelogsByDefault() {\n         final String topic = \"topic\";",
      "parent_sha": "5b06e9690b0fba37ac369149f573b27abc39a721"
    }
  },
  {
    "oid": "644ff6e60f9ee7126cf02ee839610dab82037ac9",
    "message": "KAFKA-7288 - Make sure no bytes buffered when relying on idle timeout in channel close test (#6390)\n\nSelectorTest.testCloseConnectionInClosingState sends and receives messages to get the channel into a state with staged receives and then waits for idle timeout to close the channel. When run with SSL, the channel may have buffered bytes that prevent the channel from being closed. Updated the test to wait until there are no buffered bytes as well.\r\n\r\nReviewers: Ismael Juma <ismael@juma.me.uk>",
    "date": "2019-03-07T18:09:50Z",
    "url": "https://github.com/apache/kafka/commit/644ff6e60f9ee7126cf02ee839610dab82037ac9",
    "details": {
      "sha": "9d48ab5264967eb3a8c4d72dea8f8d0b32800e26",
      "filename": "clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/644ff6e60f9ee7126cf02ee839610dab82037ac9/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FSelectorTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/644ff6e60f9ee7126cf02ee839610dab82037ac9/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FSelectorTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FSelectorTest.java?ref=644ff6e60f9ee7126cf02ee839610dab82037ac9",
      "patch": "@@ -493,8 +493,10 @@ private KafkaChannel createConnectionWithStagedReceives(int maxStagedReceives) t\n             do {\n                 selector.poll(1000);\n             } while (selector.completedReceives().isEmpty());\n-        } while (selector.numStagedReceives(channel) == 0 && --retries > 0);\n+        } while (selector.numStagedReceives(channel) == 0 && !channel.hasBytesBuffered() && --retries > 0);\n         assertTrue(\"No staged receives after 100 attempts\", selector.numStagedReceives(channel) > 0);\n+        // We want to return without any bytes buffered to ensure that channel will be closed after idle time\n+        assertFalse(\"Channel has bytes buffered\", channel.hasBytesBuffered());\n \n         return channel;\n     }",
      "parent_sha": "f708e782947c340b480882d61f28a98623226583"
    }
  },
  {
    "oid": "5792f2fb3db69333bfd22b57b00b42336dc16aa9",
    "message": "KAFKA-5980: FailOnInvalidTimestamp does not log error\n\nAuthor: Matthias J. Sax <matthias@confluent.io>\n\nReviewers: Damian Guy <damian.guy@gmail.com>, Ted Yu <yuzhihong@gmail.com>, Denis Bolshakov\n\nCloses #3966 from mjsax/kafka-5980-FailOnInvalidTimestamp-does-not-log-error",
    "date": "2017-10-04T22:10:59Z",
    "url": "https://github.com/apache/kafka/commit/5792f2fb3db69333bfd22b57b00b42336dc16aa9",
    "details": {
      "sha": "87cb0dec0e8c9e5de6cc97bf82901862318bd357",
      "filename": "streams/src/main/java/org/apache/kafka/streams/processor/FailOnInvalidTimestamp.java",
      "status": "modified",
      "additions": 11,
      "deletions": 4,
      "changes": 15,
      "blob_url": "https://github.com/apache/kafka/blob/5792f2fb3db69333bfd22b57b00b42336dc16aa9/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2FFailOnInvalidTimestamp.java",
      "raw_url": "https://github.com/apache/kafka/raw/5792f2fb3db69333bfd22b57b00b42336dc16aa9/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2FFailOnInvalidTimestamp.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2FFailOnInvalidTimestamp.java?ref=5792f2fb3db69333bfd22b57b00b42336dc16aa9",
      "patch": "@@ -19,6 +19,8 @@\n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.apache.kafka.common.annotation.InterfaceStability;\n import org.apache.kafka.streams.errors.StreamsException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n /**\n  * Retrieves embedded metadata timestamps from Kafka messages.\n@@ -45,6 +47,7 @@\n  */\n @InterfaceStability.Evolving\n public class FailOnInvalidTimestamp extends ExtractRecordMetadataTimestamp {\n+    private static final Logger log = LoggerFactory.getLogger(FailOnInvalidTimestamp.class);\n \n     /**\n      * Raises an exception on every call.\n@@ -60,10 +63,14 @@ public long onInvalidTimestamp(final ConsumerRecord<Object, Object> record,\n                                    final long recordTimestamp,\n                                    final long previousTimestamp)\n             throws StreamsException {\n-        throw new StreamsException(\"Input record \" + record + \" has invalid (negative) timestamp. \" +\n-            \"Possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding a timestamp, \" +\n-            \"or because the input topic was created before upgrading the Kafka cluster to 0.10+. \" +\n-            \"Use a different TimestampExtractor to process this data.\");\n+\n+        final String message = \"Input record \" + record + \" has invalid (negative) timestamp. \" +\n+            \"Possibly because a pre-0.10 producer client was used to write this record to Kafka without embedding \" +\n+            \"a timestamp, or because the input topic was created before upgrading the Kafka cluster to 0.10+. \" +\n+            \"Use a different TimestampExtractor to process this data.\";\n+\n+        log.error(message);\n+        throw new StreamsException(message);\n     }\n \n }",
      "parent_sha": "713a67fddaec3fa9cd7cce53dd6fef5ab6e0cdab"
    }
  },
  {
    "oid": "ac86a720b820448470fcc31290261626256aa27a",
    "message": "MINOR: Update Serdes.java (#10117)\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>",
    "date": "2021-02-14T18:19:33Z",
    "url": "https://github.com/apache/kafka/commit/ac86a720b820448470fcc31290261626256aa27a",
    "details": {
      "sha": "347bf8713ece8c995b36d132324ffdf8f33f28a9",
      "filename": "clients/src/main/java/org/apache/kafka/common/serialization/Serdes.java",
      "status": "modified",
      "additions": 11,
      "deletions": 11,
      "changes": 22,
      "blob_url": "https://github.com/apache/kafka/blob/ac86a720b820448470fcc31290261626256aa27a/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fserialization%2FSerdes.java",
      "raw_url": "https://github.com/apache/kafka/raw/ac86a720b820448470fcc31290261626256aa27a/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fserialization%2FSerdes.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fserialization%2FSerdes.java?ref=ac86a720b820448470fcc31290261626256aa27a",
      "patch": "@@ -189,77 +189,77 @@ static public <T> Serde<T> serdeFrom(final Serializer<T> serializer, final Deser\n         return new WrapperSerde<>(serializer, deserializer);\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code Long} type.\n      */\n     static public Serde<Long> Long() {\n         return new LongSerde();\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code Integer} type.\n      */\n     static public Serde<Integer> Integer() {\n         return new IntegerSerde();\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code Short} type.\n      */\n     static public Serde<Short> Short() {\n         return new ShortSerde();\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code Float} type.\n      */\n     static public Serde<Float> Float() {\n         return new FloatSerde();\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code Double} type.\n      */\n     static public Serde<Double> Double() {\n         return new DoubleSerde();\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code String} type.\n      */\n     static public Serde<String> String() {\n         return new StringSerde();\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code ByteBuffer} type.\n      */\n     static public Serde<ByteBuffer> ByteBuffer() {\n         return new ByteBufferSerde();\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code Bytes} type.\n      */\n     static public Serde<Bytes> Bytes() {\n         return new BytesSerde();\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code UUID} type\n      */\n     static public Serde<UUID> UUID() {\n         return new UUIDSerde();\n     }\n \n-    /*\n+    /**\n      * A serde for nullable {@code byte[]} type.\n      */\n     static public Serde<byte[]> ByteArray() {\n         return new ByteArraySerde();\n     }\n \n-    /*\n+    /**\n      * A serde for {@code Void} type.\n      */\n     static public Serde<Void> Void() {",
      "parent_sha": "744d05b12897267803f46549e8bca3d31d57be4c"
    }
  },
  {
    "oid": "18f70835a39381967b4d0d57effa14a91e44ac74",
    "message": "KAFKA-10502: Use Threadlocal.remote to avoid leak on TimestampRouter (#9304)\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>",
    "date": "2020-09-28T02:16:17Z",
    "url": "https://github.com/apache/kafka/commit/18f70835a39381967b4d0d57effa14a91e44ac74",
    "details": {
      "sha": "388a7b941fa6db02f59eac21b53476feda34794d",
      "filename": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/TimestampRouter.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/18f70835a39381967b4d0d57effa14a91e44ac74/connect%2Ftransforms%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Ftransforms%2FTimestampRouter.java",
      "raw_url": "https://github.com/apache/kafka/raw/18f70835a39381967b4d0d57effa14a91e44ac74/connect%2Ftransforms%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Ftransforms%2FTimestampRouter.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Ftransforms%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Ftransforms%2FTimestampRouter.java?ref=18f70835a39381967b4d0d57effa14a91e44ac74",
      "patch": "@@ -91,7 +91,7 @@ public R apply(R record) {\n \n     @Override\n     public void close() {\n-        timestampFormat = null;\n+        timestampFormat.remove();\n     }\n \n     @Override",
      "parent_sha": "c2789934e4e9598c3bb789ae189a67a2aeab383e"
    }
  },
  {
    "oid": "7bf1906132b84b3350bba0569da2d6fe94c75dbd",
    "message": "KAFKA-14809 Fix logging conditional on WorkerSourceTask (#13386)\n\nReviewers: Chris Egerton <chrise@aiven.io>",
    "date": "2023-03-16T12:39:31Z",
    "url": "https://github.com/apache/kafka/commit/7bf1906132b84b3350bba0569da2d6fe94c75dbd",
    "details": {
      "sha": "a6767675a3afecda13b23a6a407a48792c1f0312",
      "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java",
      "status": "modified",
      "additions": 7,
      "deletions": 7,
      "changes": 14,
      "blob_url": "https://github.com/apache/kafka/blob/7bf1906132b84b3350bba0569da2d6fe94c75dbd/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2FWorkerSourceTask.java",
      "raw_url": "https://github.com/apache/kafka/raw/7bf1906132b84b3350bba0569da2d6fe94c75dbd/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2FWorkerSourceTask.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2FWorkerSourceTask.java?ref=7bf1906132b84b3350bba0569da2d6fe94c75dbd",
      "patch": "@@ -216,7 +216,7 @@ public boolean commitOffsets() {\n             this.committableOffsets = CommittableOffsets.EMPTY;\n         }\n \n-        if (committableOffsets.isEmpty()) {\n+        if (offsetsToCommit.isEmpty()) {\n             log.debug(\"{} Either no records were produced by the task since the last offset commit, \" \n                     + \"or every record has been filtered out by a transformation \" \n                     + \"or dropped due to transformation or conversion errors.\",\n@@ -225,15 +225,15 @@ public boolean commitOffsets() {\n             // We continue with the offset commit process here instead of simply returning immediately\n             // in order to invoke SourceTask::commit and record metrics for a successful offset commit\n         } else {\n-            log.info(\"{} Committing offsets for {} acknowledged messages\", this, committableOffsets.numCommittableMessages());\n-            if (committableOffsets.hasPending()) {\n+            log.info(\"{} Committing offsets for {} acknowledged messages\", this, offsetsToCommit.numCommittableMessages());\n+            if (offsetsToCommit.hasPending()) {\n                 log.debug(\"{} There are currently {} pending messages spread across {} source partitions whose offsets will not be committed. \"\n                                 + \"The source partition with the most pending messages is {}, with {} pending messages\",\n                         this,\n-                        committableOffsets.numUncommittableMessages(),\n-                        committableOffsets.numDeques(),\n-                        committableOffsets.largestDequePartition(),\n-                        committableOffsets.largestDequeSize()\n+                        offsetsToCommit.numUncommittableMessages(),\n+                        offsetsToCommit.numDeques(),\n+                        offsetsToCommit.largestDequePartition(),\n+                        offsetsToCommit.largestDequeSize()\n                 );\n             } else {\n                 log.debug(\"{} There are currently no pending messages for this offset commit; \"",
      "parent_sha": "3137b75d40c1857a7bc5191093142ab0fa1e6582"
    }
  },
  {
    "oid": "8e5faca963cdcb082c882a5e726327b36e9701f0",
    "message": "Correct exception message in DistributedHerder (#7995)\n\nAuthor: Ted Yu <yuzhihong@gmail.com>\r\nReviewer: Randall Hauch <rhauch@gmail.com>",
    "date": "2020-01-24T19:56:08Z",
    "url": "https://github.com/apache/kafka/commit/8e5faca963cdcb082c882a5e726327b36e9701f0",
    "details": {
      "sha": "9202a946b754ec4840bc1033454f8dca9a6cec88",
      "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/8e5faca963cdcb082c882a5e726327b36e9701f0/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fdistributed%2FDistributedHerder.java",
      "raw_url": "https://github.com/apache/kafka/raw/8e5faca963cdcb082c882a5e726327b36e9701f0/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fdistributed%2FDistributedHerder.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fdistributed%2FDistributedHerder.java?ref=8e5faca963cdcb082c882a5e726327b36e9701f0",
      "patch": "@@ -699,7 +699,7 @@ public void deleteConnectorConfig(final String connName, final Callback<Created<\n                     public Void call() throws Exception {\n                         log.trace(\"Handling connector config request {}\", connName);\n                         if (!isLeader()) {\n-                            callback.onCompletion(new NotLeaderException(\"Only the leader can set connector configs.\", leaderUrl()), null);\n+                            callback.onCompletion(new NotLeaderException(\"Only the leader can delete connector configs.\", leaderUrl()), null);\n                             return null;\n                         }\n ",
      "parent_sha": "57b2f6807d332f4e6de4dc4ad18aa24686d304de"
    }
  },
  {
    "oid": "18181ca953b672e8856879e1daff530deccc2fec",
    "message": "HOTFIX: KAFKA-7097; Set create time default to -1L in VerifiableProducer\n\nReviewers: Anna Povzner <anna@confluent.io>, Ted Yu <yuzhihong@gmail.com>, Jason Gustafson <jason@confluent.io>",
    "date": "2018-06-29T20:50:28Z",
    "url": "https://github.com/apache/kafka/commit/18181ca953b672e8856879e1daff530deccc2fec",
    "details": {
      "sha": "fafa9e6c8c6f7b3716b74fe525e5e4bce6798dbc",
      "filename": "tools/src/main/java/org/apache/kafka/tools/VerifiableProducer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/18181ca953b672e8856879e1daff530deccc2fec/tools%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Ftools%2FVerifiableProducer.java",
      "raw_url": "https://github.com/apache/kafka/raw/18181ca953b672e8856879e1daff530deccc2fec/tools%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Ftools%2FVerifiableProducer.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/tools%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Ftools%2FVerifiableProducer.java?ref=18181ca953b672e8856879e1daff530deccc2fec",
      "patch": "@@ -164,7 +164,7 @@ private static ArgumentParser argParser() {\n         parser.addArgument(\"--message-create-time\")\n                 .action(store())\n                 .required(false)\n-                .setDefault(-1)\n+                .setDefault(-1L)\n                 .type(Long.class)\n                 .metavar(\"CREATETIME\")\n                 .dest(\"createTime\")",
      "parent_sha": "d44d5d7520d31f33feb03b0679f8c552254dac4e"
    }
  },
  {
    "oid": "f6f65a78684e41bcd6115438a76cd6c552afc043",
    "message": "KAFKA-6302: Improve AdmintClient JavaDocs\n\nReviewers: Colin P. McCabe <cmccabe@confluent.io>, Ismael Juma <ismael@juma.me.uk>\r\n\r\nCloses #4332",
    "date": "2018-01-17T22:01:43Z",
    "url": "https://github.com/apache/kafka/commit/f6f65a78684e41bcd6115438a76cd6c552afc043",
    "details": {
      "sha": "897e127d5577bbda0408a20d4e5340628b2e0c48",
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/AdminClient.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/f6f65a78684e41bcd6115438a76cd6c552afc043/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FAdminClient.java",
      "raw_url": "https://github.com/apache/kafka/raw/f6f65a78684e41bcd6115438a76cd6c552afc043/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FAdminClient.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FAdminClient.java?ref=f6f65a78684e41bcd6115438a76cd6c552afc043",
      "patch": "@@ -105,7 +105,7 @@ public CreateTopicsResult createTopics(Collection<NewTopic> newTopics) {\n      *\n      * This operation is not transactional so it may succeed for some topics while fail for others.\n      *\n-     * It may take several seconds after this method returns\n+     * It may take several seconds after {@code CreateTopicsResult} returns\n      * success for all the brokers to become aware that the topics have been created.\n      * During this time, {@link AdminClient#listTopics()} and {@link AdminClient#describeTopics(Collection)}\n      * may not return information about the new topics.\n@@ -138,7 +138,7 @@ public DeleteTopicsResult deleteTopics(Collection<String> topics) {\n      *\n      * This operation is not transactional so it may succeed for some topics while fail for others.\n      *\n-     * It may take several seconds after AdminClient#deleteTopics returns\n+     * It may take several seconds after the {@code DeleteTopicsResult} returns\n      * success for all the brokers to become aware that the topics are gone.\n      * During this time, AdminClient#listTopics and AdminClient#describeTopics\n      * may continue to return information about the deleted topics.",
      "parent_sha": "11f3db0b731739017174e488670e529af4dc22ae"
    }
  },
  {
    "oid": "b50a78b4acdcba14930e528162dd4ffcea4bedc0",
    "message": "TRIVIAL: fix JavaDocs formatting (#10134)\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>, Bill Bejeck <bill@confluent.io>",
    "date": "2021-02-19T00:02:25Z",
    "url": "https://github.com/apache/kafka/commit/b50a78b4acdcba14930e528162dd4ffcea4bedc0",
    "details": {
      "sha": "a3e5f30c2c46427b37eb0080dbe4be373d96165b",
      "filename": "streams/src/main/java/org/apache/kafka/streams/processor/ProcessorSupplier.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/b50a78b4acdcba14930e528162dd4ffcea4bedc0/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2FProcessorSupplier.java",
      "raw_url": "https://github.com/apache/kafka/raw/b50a78b4acdcba14930e528162dd4ffcea4bedc0/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2FProcessorSupplier.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2FProcessorSupplier.java?ref=b50a78b4acdcba14930e528162dd4ffcea4bedc0",
      "patch": "@@ -26,7 +26,7 @@\n  * It is used in {@link Topology} for adding new processor operators, whose generated\n  * topology can then be replicated (and thus creating one or more {@link Processor} instances)\n  * and distributed to multiple stream threads.\n- *\n+ * <p>\n  * The supplier should always generate a new instance each time {@link ProcessorSupplier#get()} gets called. Creating\n  * a single {@link Processor} object and returning the same object reference in {@link ProcessorSupplier#get()} would be\n  * a violation of the supplier pattern and leads to runtime exceptions.",
      "parent_sha": "97c9ae119ac12e9b3cf60d6639900a46ab30d062"
    }
  },
  {
    "oid": "333278d9bbf9c180d556b5c3676c1832c583bbd5",
    "message": "MINOR: Add actual state directory to related exceptions (#11751)\n\nFor debugging it is useful to see the actual state directory when\r\nan exception regarding the state directory is thrown.\r\n\r\nReviewer: Bill Bejeck <bbejeck@apache.org>",
    "date": "2022-02-16T19:32:00Z",
    "url": "https://github.com/apache/kafka/commit/333278d9bbf9c180d556b5c3676c1832c583bbd5",
    "details": {
      "sha": "2cacfbe0d1c9099dab7467064007c329d02c213c",
      "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java",
      "status": "modified",
      "additions": 6,
      "deletions": 5,
      "changes": 11,
      "blob_url": "https://github.com/apache/kafka/blob/333278d9bbf9c180d556b5c3676c1832c583bbd5/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStateDirectory.java",
      "raw_url": "https://github.com/apache/kafka/raw/333278d9bbf9c180d556b5c3676c1832c583bbd5/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStateDirectory.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStateDirectory.java?ref=333278d9bbf9c180d556b5c3676c1832c583bbd5",
      "patch": "@@ -175,9 +175,9 @@ private boolean lockStateDirectory() {\n             stateDirLock = tryLock(stateDirLockChannel);\n         } catch (final IOException e) {\n             log.error(\"Unable to lock the state directory due to unexpected exception\", e);\n-            throw new ProcessorStateException(\"Failed to lock the state directory during startup\", e);\n+            throw new ProcessorStateException(String.format(\"Failed to lock the state directory [%s] during startup\",\n+                stateDir.getAbsolutePath()), e);\n         }\n-\n         return stateDirLock != null;\n     }\n \n@@ -188,8 +188,9 @@ public UUID initializeProcessId() {\n \n         if (!lockStateDirectory()) {\n             log.error(\"Unable to obtain lock as state directory is already locked by another process\");\n-            throw new StreamsException(\"Unable to initialize state, this can happen if multiple instances of \" +\n-                                           \"Kafka Streams are running in the same state directory\");\n+            throw new StreamsException(String.format(\"Unable to initialize state, this can happen if multiple instances of \" +\n+                                           \"Kafka Streams are running in the same state directory \" +\n+                                           \"(current state directory is [%s]\", stateDir.getAbsolutePath()));\n         }\n \n         final File processFile = new File(stateDir, PROCESS_FILE_NAME);\n@@ -386,7 +387,7 @@ public void close() {\n                 stateDirLockChannel = null;\n             } catch (final IOException e) {\n                 log.error(\"Unexpected exception while unlocking the state dir\", e);\n-                throw new StreamsException(\"Failed to release the lock on the state directory\", e);\n+                throw new StreamsException(String.format(\"Failed to release the lock on the state directory [%s]\", stateDir.getAbsolutePath()), e);\n             }\n \n             // all threads should be stopped and cleaned up by now, so none should remain holding a lock",
      "parent_sha": "8047ba3800436d6162d0f8eb707e28857ab9eb68"
    }
  },
  {
    "oid": "a281fe17fe1dda9a41890648299b74fa2449552a",
    "message": "KAFKA-5215; Small Javadoc fixes for AdminClient#describeTopics\n\nAuthor: Colin P. Mccabe <cmccabe@confluent.io>\n\nReviewers: Roger Hoover <roger.hoover@gmail.com>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #3013 from cmccabe/KAFKA-5215",
    "date": "2017-05-16T10:56:02Z",
    "url": "https://github.com/apache/kafka/commit/a281fe17fe1dda9a41890648299b74fa2449552a",
    "details": {
      "sha": "7db5e6e028da4a0f85eeb7279a8c75908ec81e6a",
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/AdminClient.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/kafka/blob/a281fe17fe1dda9a41890648299b74fa2449552a/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FAdminClient.java",
      "raw_url": "https://github.com/apache/kafka/raw/a281fe17fe1dda9a41890648299b74fa2449552a/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FAdminClient.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FAdminClient.java?ref=a281fe17fe1dda9a41890648299b74fa2449552a",
      "patch": "@@ -118,7 +118,7 @@ public ListTopicsResults listTopics() {\n     public abstract ListTopicsResults listTopics(ListTopicsOptions options);\n \n     /**\n-     * Descripe an individual topic in the cluster, with the default options.\n+     * Describe some topics in the cluster, with the default options.\n      *\n      * See {@link AdminClient#describeTopics(Collection<String>, DescribeTopicsOptions)}\n      *\n@@ -131,10 +131,10 @@ public DescribeTopicsResults describeTopics(Collection<String> topicNames) {\n     }\n \n     /**\n-     * Descripe an individual topic in the cluster.\n+     * Describe some topics in the cluster.\n      *\n      * Note that if auto.create.topics.enable is true on the brokers,\n-     * AdminClient#describeTopic(topicName) may create a topic named topicName.\n+     * describeTopics(topicName, ...) may create a topic named topicName.\n      * There are two workarounds: either use AdminClient#listTopics and ensure\n      * that the topic is present before describing, or disable\n      * auto.create.topics.enable.",
      "parent_sha": "0a1689a749392d8de9dfbdc200ad9edce75024a1"
    }
  },
  {
    "oid": "abde0e0878557aa62c14c4532187e13496687957",
    "message": "MINOR: fix typo and comment (#14650)\n\nReviewers: hudeqi <1217150961@qq.com>, Ziming Deng <dengziming1993@gmail.com>.",
    "date": "2023-10-28T04:10:53Z",
    "url": "https://github.com/apache/kafka/commit/abde0e0878557aa62c14c4532187e13496687957",
    "details": {
      "sha": "9115358f52d7ac084182517ef9a01db189e50d3b",
      "filename": "raft/src/main/java/org/apache/kafka/raft/QuorumState.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/abde0e0878557aa62c14c4532187e13496687957/raft%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fraft%2FQuorumState.java",
      "raw_url": "https://github.com/apache/kafka/raw/abde0e0878557aa62c14c4532187e13496687957/raft%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fraft%2FQuorumState.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/raft%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fraft%2FQuorumState.java?ref=abde0e0878557aa62c14c4532187e13496687957",
      "patch": "@@ -291,8 +291,8 @@ public void transitionToResigned(List<Integer> preferredSuccessors) {\n     }\n \n     /**\n-     * Transition to the \"unattached\" state. This means we have found an epoch greater than\n-     * or equal to the current epoch, but wo do not yet know of the elected leader.\n+     * Transition to the \"unattached\" state. This means we have found an epoch greater than the current epoch,\n+     * but we do not yet know of the elected leader.\n      */\n     public void transitionToUnattached(int epoch) {\n         int currentEpoch = state.epoch();",
      "parent_sha": "37715862d701d9b1e340b8dd96d29b5591187d64"
    }
  },
  {
    "oid": "ed8748b7d17448a215ebbf68749d311aae30d7f7",
    "message": "KAFKA-2578; Client Metadata internal state should be synchronized\n\nAuthor: Edward Ribeiro <edward.ribeiro@gmail.com>\n\nReviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>, Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #659 from ijuma/KAFKA-2578",
    "date": "2015-12-10T18:01:58Z",
    "url": "https://github.com/apache/kafka/commit/ed8748b7d17448a215ebbf68749d311aae30d7f7",
    "details": {
      "sha": "73a9f333cc7a4c6205e0a003f16b575624edd97d",
      "filename": "clients/src/main/java/org/apache/kafka/clients/Metadata.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/apache/kafka/blob/ed8748b7d17448a215ebbf68749d311aae30d7f7/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FMetadata.java",
      "raw_url": "https://github.com/apache/kafka/raw/ed8748b7d17448a215ebbf68749d311aae30d7f7/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FMetadata.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FMetadata.java?ref=ed8748b7d17448a215ebbf68749d311aae30d7f7",
      "patch": "@@ -205,28 +205,28 @@ public long refreshBackoff() {\n      * Set state to indicate if metadata for all topics in Kafka cluster is required or not.\n      * @param needMetadaForAllTopics boolean indicating need for metadata of all topics in cluster.\n      */\n-    public void needMetadataForAllTopics(boolean needMetadaForAllTopics) {\n+    public synchronized void needMetadataForAllTopics(boolean needMetadaForAllTopics) {\n         this.needMetadataForAllTopics = needMetadaForAllTopics;\n     }\n \n     /**\n      * Get whether metadata for all topics is needed or not\n      */\n-    public boolean needMetadataForAllTopics() {\n+    public synchronized boolean needMetadataForAllTopics() {\n         return this.needMetadataForAllTopics;\n     }\n \n     /**\n      * Add a Metadata listener that gets notified of metadata updates\n      */\n-    public void addListener(Listener listener) {\n+    public synchronized void addListener(Listener listener) {\n         this.listeners.add(listener);\n     }\n \n     /**\n      * Stop notifying the listener of metadata updates\n      */\n-    public void removeListener(Listener listener) {\n+    public synchronized void removeListener(Listener listener) {\n         this.listeners.remove(listener);\n     }\n ",
      "parent_sha": "3b350cdff795ec08dc77e60f127f2790149d8d52"
    }
  },
  {
    "oid": "0bb8e66184931e2f7830cb713d9260cc0f3383a9",
    "message": "KAFKA-6024; Move arg validation in KafkaConsumer ahead of `acquireAndEnsureOpen`  (#4617)",
    "date": "2018-03-13T06:03:32Z",
    "url": "https://github.com/apache/kafka/commit/0bb8e66184931e2f7830cb713d9260cc0f3383a9",
    "details": {
      "sha": "81137f3c8dd74cac7a3126b47e1928c4e897a721",
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
      "status": "modified",
      "additions": 12,
      "deletions": 13,
      "changes": 25,
      "blob_url": "https://github.com/apache/kafka/blob/0bb8e66184931e2f7830cb713d9260cc0f3383a9/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FKafkaConsumer.java",
      "raw_url": "https://github.com/apache/kafka/raw/0bb8e66184931e2f7830cb713d9260cc0f3383a9/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FKafkaConsumer.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FKafkaConsumer.java?ref=0bb8e66184931e2f7830cb713d9260cc0f3383a9",
      "patch": "@@ -966,13 +966,12 @@ public void subscribe(Collection<String> topics) {\n      */\n     @Override\n     public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) {\n+        if (pattern == null)\n+            throw new IllegalArgumentException(\"Topic pattern to subscribe to cannot be null\");\n+\n         acquireAndEnsureOpen();\n         try {\n-            if (pattern == null)\n-                throw new IllegalArgumentException(\"Topic pattern to subscribe to cannot be null\");\n-\n             throwIfNoAssignorsConfigured();\n-\n             log.debug(\"Subscribed to pattern: {}\", pattern);\n             this.subscriptions.subscribe(pattern, listener);\n             this.metadata.needMetadataForAllTopics(true);\n@@ -1337,11 +1336,11 @@ public void commitAsync(final Map<TopicPartition, OffsetAndMetadata> offsets, Of\n      */\n     @Override\n     public void seek(TopicPartition partition, long offset) {\n+        if (offset < 0)\n+            throw new IllegalArgumentException(\"seek offset must not be a negative number\");\n+\n         acquireAndEnsureOpen();\n         try {\n-            if (offset < 0)\n-                throw new IllegalArgumentException(\"seek offset must not be a negative number\");\n-\n             log.debug(\"Seeking to offset {} for partition {}\", offset, partition);\n             this.subscriptions.seek(partition, offset);\n         } finally {\n@@ -1357,11 +1356,11 @@ public void seek(TopicPartition partition, long offset) {\n      * @throws IllegalArgumentException if {@code partitions} is {@code null} or the provided TopicPartition is not assigned to this consumer\n      */\n     public void seekToBeginning(Collection<TopicPartition> partitions) {\n+        if (partitions == null)\n+            throw new IllegalArgumentException(\"Partitions collection cannot be null\");\n+\n         acquireAndEnsureOpen();\n         try {\n-            if (partitions == null) {\n-                throw new IllegalArgumentException(\"Partitions collection cannot be null\");\n-            }\n             Collection<TopicPartition> parts = partitions.size() == 0 ? this.subscriptions.assignedPartitions() : partitions;\n             for (TopicPartition tp : parts) {\n                 log.debug(\"Seeking to beginning of partition {}\", tp);\n@@ -1383,11 +1382,11 @@ public void seekToBeginning(Collection<TopicPartition> partitions) {\n      * @throws IllegalArgumentException if {@code partitions} is {@code null} or the provided TopicPartition is not assigned to this consumer\n      */\n     public void seekToEnd(Collection<TopicPartition> partitions) {\n+        if (partitions == null)\n+            throw new IllegalArgumentException(\"Partitions collection cannot be null\");\n+\n         acquireAndEnsureOpen();\n         try {\n-            if (partitions == null) {\n-                throw new IllegalArgumentException(\"Partitions collection cannot be null\");\n-            }\n             Collection<TopicPartition> parts = partitions.size() == 0 ? this.subscriptions.assignedPartitions() : partitions;\n             for (TopicPartition tp : parts) {\n                 log.debug(\"Seeking to end of partition {}\", tp);",
      "parent_sha": "1ea07b993d75ed68f4c04282eb177bf84156e0b2"
    }
  },
  {
    "oid": "b526528cafe4142b73df8c930473b0cddc84ca9d",
    "message": "KAFKA-9701: Add more debug log on client to reproduce the issue (#8272)\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>",
    "date": "2020-03-11T22:52:53Z",
    "url": "https://github.com/apache/kafka/commit/b526528cafe4142b73df8c930473b0cddc84ca9d",
    "details": {
      "sha": "2d93766ac90f0bd972cebeb03170c03106f2814f",
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java",
      "status": "modified",
      "additions": 3,
      "deletions": 3,
      "changes": 6,
      "blob_url": "https://github.com/apache/kafka/blob/b526528cafe4142b73df8c930473b0cddc84ca9d/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2Finternals%2FAbstractCoordinator.java",
      "raw_url": "https://github.com/apache/kafka/raw/b526528cafe4142b73df8c930473b0cddc84ca9d/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2Finternals%2FAbstractCoordinator.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2Finternals%2FAbstractCoordinator.java?ref=b526528cafe4142b73df8c930473b0cddc84ca9d",
      "patch": "@@ -576,7 +576,7 @@ public void handle(JoinGroupResponse joinResponse, RequestFuture<ByteBuffer> fut\n             Errors error = joinResponse.error();\n             if (error == Errors.NONE) {\n                 if (isProtocolTypeInconsistent(joinResponse.data().protocolType())) {\n-                    log.debug(\"JoinGroup failed due to inconsistent Protocol Type, received {} but expected {}\",\n+                    log.error(\"JoinGroup failed due to inconsistent Protocol Type, received {} but expected {}\",\n                         joinResponse.data().protocolType(), protocolType());\n                     future.raise(Errors.INCONSISTENT_GROUP_PROTOCOL);\n                 } else {\n@@ -717,11 +717,11 @@ public void handle(SyncGroupResponse syncResponse,\n             Errors error = syncResponse.error();\n             if (error == Errors.NONE) {\n                 if (isProtocolTypeInconsistent(syncResponse.data.protocolType())) {\n-                    log.debug(\"SyncGroup failed due to inconsistent Protocol Type, received {} but expected {}\",\n+                    log.error(\"SyncGroup failed due to inconsistent Protocol Type, received {} but expected {}\",\n                         syncResponse.data.protocolType(), protocolType());\n                     future.raise(Errors.INCONSISTENT_GROUP_PROTOCOL);\n                 } else if (isProtocolNameInconsistent(syncResponse.data.protocolName())) {\n-                    log.debug(\"SyncGroup failed due to inconsistent Protocol Name, received {} but expected {}\",\n+                    log.error(\"SyncGroup failed due to inconsistent Protocol Name, received {} but expected {}\",\n                         syncResponse.data.protocolName(), generation().protocolName);\n                     future.raise(Errors.INCONSISTENT_GROUP_PROTOCOL);\n                 } else {",
      "parent_sha": "524182d7f4f5ce274da6cf6cdd2b648095bfe5e9"
    }
  },
  {
    "oid": "055c9c7bd6780436d201f5101df464814ba940e2",
    "message": "KAFKA 8311: better handle timeout exception on Stream thread (#6662)\n\nThe goals for this small diff are:\r\n\r\n1. Give user guidance if they want to relax commit timeout threshold\r\n2. Indicate the code path where timeout exception was caught\r\n\r\nReviewers: John Roesler <john@confluent.io>, Guozhang Wang <guozhang@confluent.io>",
    "date": "2019-06-04T19:14:34Z",
    "url": "https://github.com/apache/kafka/commit/055c9c7bd6780436d201f5101df464814ba940e2",
    "details": {
      "sha": "5be065d5f26216baa727cee3cd50db7b9c1bc360",
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/kafka/blob/055c9c7bd6780436d201f5101df464814ba940e2/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FKafkaConsumer.java",
      "raw_url": "https://github.com/apache/kafka/raw/055c9c7bd6780436d201f5101df464814ba940e2/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FKafkaConsumer.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FKafkaConsumer.java?ref=055c9c7bd6780436d201f5101df464814ba940e2",
      "patch": "@@ -1770,7 +1770,8 @@ public OffsetAndMetadata committed(TopicPartition partition, final Duration time\n                     Collections.singleton(partition), time.timer(timeout));\n             if (offsets == null) {\n                 throw new TimeoutException(\"Timeout of \" + timeout.toMillis() + \"ms expired before the last \" +\n-                        \"committed offset for partition \" + partition + \" could be determined\");\n+                        \"committed offset for partition \" + partition + \" could be determined. Try tuning default.api.timeout.ms \" +\n+                        \"larger to relax the threshold.\");\n             } else {\n                 offsets.forEach(this::updateLastSeenEpochIfNewer);\n                 return offsets.get(partition);",
      "parent_sha": "ba3dc494371145e8ad35d6b85f45b8fe1e44c21f"
    }
  },
  {
    "oid": "5a6f19b2a1ff72c52ad627230ffdf464456104ee",
    "message": "KAFKA-13246: StoreQueryIntegrationTest#shouldQueryStoresAfterAddingAndRemovingStreamThread now waits for the client state to go to REBALANCING/RUNNING after adding/removing a thread and waits for state RUNNING before querying the state store. (#11334)\n\nKAFKA-13246: StoreQueryIntegrationTest#shouldQueryStoresAfterAddingAndRemovingStreamThread does not gate on stream state well\r\n\r\nThe test now waits for the client to transition to REBALANCING/RUNNING after adding/removing a thread as well as to transition to RUNNING before querying the state store.\r\n\r\nReviewers: singingMan <@3schwartz>, Walker Carlson <wcarlson@confluent.io>, John Roesler <vvcephei@apache.org>",
    "date": "2021-09-21T16:18:15Z",
    "url": "https://github.com/apache/kafka/commit/5a6f19b2a1ff72c52ad627230ffdf464456104ee",
    "details": {
      "sha": "fb2d0a181768e7e4e00367df32daaf60d50dd82f",
      "filename": "streams/src/test/java/org/apache/kafka/streams/integration/StoreQueryIntegrationTest.java",
      "status": "modified",
      "additions": 4,
      "deletions": 2,
      "changes": 6,
      "blob_url": "https://github.com/apache/kafka/blob/5a6f19b2a1ff72c52ad627230ffdf464456104ee/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FStoreQueryIntegrationTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/5a6f19b2a1ff72c52ad627230ffdf464456104ee/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FStoreQueryIntegrationTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FStoreQueryIntegrationTest.java?ref=5a6f19b2a1ff72c52ad627230ffdf464456104ee",
      "patch": "@@ -477,6 +477,7 @@ public void shouldQueryStoresAfterAddingAndRemovingStreamThread() throws Excepti\n         //Add thread\n         final Optional<String> streamThread = kafkaStreams1.addStreamThread();\n         assertThat(streamThread.isPresent(), is(true));\n+        until(() -> kafkaStreams1.state().isRunningOrRebalancing());\n \n         produceValueRange(key, 0, batch1NumMessages);\n         produceValueRange(key2, 0, batch1NumMessages);\n@@ -485,6 +486,7 @@ public void shouldQueryStoresAfterAddingAndRemovingStreamThread() throws Excepti\n         // Assert that all messages in the batches were processed in a timely manner\n         assertThat(semaphore.tryAcquire(3 * batch1NumMessages, 60, TimeUnit.SECONDS), is(equalTo(true)));\n \n+        until(() -> KafkaStreams.State.RUNNING.equals(kafkaStreams1.state()));\n         until(() -> {\n             final QueryableStoreType<ReadOnlyKeyValueStore<Integer, Integer>> queryableStoreType = keyValueStore();\n             final ReadOnlyKeyValueStore<Integer, Integer> store1 = getStore(TABLE_NAME, kafkaStreams1, queryableStoreType);\n@@ -503,9 +505,9 @@ public void shouldQueryStoresAfterAddingAndRemovingStreamThread() throws Excepti\n \n         final Optional<String> removedThreadName = kafkaStreams1.removeStreamThread();\n         assertThat(removedThreadName.isPresent(), is(true));\n+        until(() -> kafkaStreams1.state().isRunningOrRebalancing());\n \n-        until(() -> kafkaStreams1.state().equals(KafkaStreams.State.RUNNING));\n-\n+        until(() -> KafkaStreams.State.RUNNING.equals(kafkaStreams1.state()));\n         until(() -> {\n             final QueryableStoreType<ReadOnlyKeyValueStore<Integer, Integer>> queryableStoreType = keyValueStore();\n             final ReadOnlyKeyValueStore<Integer, Integer> store1 = getStore(TABLE_NAME, kafkaStreams1, queryableStoreType);",
      "parent_sha": "f650a14d56c0cc33263c29d8d242760406943c5b"
    }
  },
  {
    "oid": "71875ec58eb922773163c61335bcdb9980d4bf34",
    "message": "KAFKA-18845: Fix flaky QuorumControllerTest#testUncleanShutdownBrokerElrEnabled (#19240)\n\nThere're two root causes:\n1. When we unclean shutdown `brokerToBeTheLeader`, we didn't wait for\nthe result. That means when we send heartbeat to unfence broker, it has\nchance to use stale broker epoch to send the request. [0]\n2. We use different replica directory to unclean shutdown broker. Even\nif broker is unfenced, it cannot get an online directory, so the\n`brokerToBeTheLeader` cannot be elected as a new leader. [1]\n\n\n[0]\nhttps://github.com/apache/kafka/blob/a5325e029e2493f22925af99482ad9fa1eb06947/metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java#L484-L497\n\n[1]\nhttps://github.com/apache/kafka/blob/a5325e029e2493f22925af99482ad9fa1eb06947/metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java#L2470-L2477\n\nReviewers: David Arthur <mumrah@gmail.com>",
    "date": "2025-03-20T17:37:09Z",
    "url": "https://github.com/apache/kafka/commit/71875ec58eb922773163c61335bcdb9980d4bf34",
    "details": {
      "sha": "908a850652c9572d0182f395dada94ebc3ad908d",
      "filename": "metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java",
      "status": "modified",
      "additions": 14,
      "deletions": 4,
      "changes": 18,
      "blob_url": "https://github.com/apache/kafka/blob/71875ec58eb922773163c61335bcdb9980d4bf34/metadata%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcontroller%2FQuorumControllerTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/71875ec58eb922773163c61335bcdb9980d4bf34/metadata%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcontroller%2FQuorumControllerTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcontroller%2FQuorumControllerTest.java?ref=71875ec58eb922773163c61335bcdb9980d4bf34",
      "patch": "@@ -380,6 +380,9 @@ public  void testElrEnabledByDefault() throws Throwable {\n     @Test\n     public void testUncleanShutdownBrokerElrEnabled() throws Throwable {\n         List<Integer> allBrokers = List.of(1, 2, 3);\n+        Map<Integer, Uuid> brokerLogDirs = allBrokers.stream().collect(\n+            Collectors.toMap(identity(), brokerId -> Uuid.randomUuid())\n+        );\n         short replicationFactor = (short) allBrokers.size();\n         long sessionTimeoutMillis = 500;\n \n@@ -406,7 +409,7 @@ public void testUncleanShutdownBrokerElrEnabled() throws Throwable {\n                         setClusterId(active.clusterId()).\n                         setFeatures(features).\n                         setIncarnationId(Uuid.randomUuid()).\n-                        setLogDirs(List.of(Uuid.randomUuid())).\n+                        setLogDirs(List.of(brokerLogDirs.get(brokerId))).\n                         setListeners(listeners));\n                 brokerEpochs.put(brokerId, reply.get().epoch());\n             }\n@@ -474,24 +477,31 @@ public void testUncleanShutdownBrokerElrEnabled() throws Throwable {\n                     setClusterId(active.clusterId()).\n                     setFeatures(features).\n                     setIncarnationId(Uuid.randomUuid()).\n-                    setLogDirs(List.of(Uuid.randomUuid())).\n+                    setLogDirs(List.of(brokerLogDirs.get(brokerToUncleanShutdown))).\n                     setListeners(listeners));\n             brokerEpochs.put(brokerToUncleanShutdown, reply.get().epoch());\n             partition = active.replicationControl().getPartition(topicIdFoo, 0);\n             assertArrayEquals(new int[]{brokerToBeTheLeader}, partition.elr, partition.toString());\n             assertArrayEquals(lastKnownElr, partition.lastKnownElr, partition.toString());\n \n             // Unclean shutdown should not remove the last known ELR members.\n-            active.registerBroker(\n+            CompletableFuture<BrokerRegistrationReply> replyLeader = active.registerBroker(\n                 anonymousContextFor(ApiKeys.BROKER_REGISTRATION),\n                 new BrokerRegistrationRequestData().\n                     setBrokerId(brokerToBeTheLeader).\n                     setClusterId(active.clusterId()).\n                     setFeatures(features).\n                     setIncarnationId(Uuid.randomUuid()).\n                     setPreviousBrokerEpoch(brokerEpochs.get(brokerToBeTheLeader)).\n-                    setLogDirs(List.of(Uuid.randomUuid())).\n+                    setLogDirs(List.of(brokerLogDirs.get(brokerToBeTheLeader))).\n                     setListeners(listeners));\n+            brokerEpochs.put(brokerToBeTheLeader, replyLeader.get().epoch());\n+            partition = active.replicationControl().getPartition(topicIdFoo, 0);\n+            int[] expectedIsr = {brokerToBeTheLeader};\n+            assertArrayEquals(expectedIsr, partition.elr, \"The ELR for topic partition foo-0 was \" + Arrays.toString(partition.elr) +\n+                \". It is expected to be \" + Arrays.toString(expectedIsr));\n+            assertArrayEquals(lastKnownElr, partition.lastKnownElr, \"The last known ELR for topic partition foo-0 was \" + Arrays.toString(partition.lastKnownElr) +\n+                \". It is expected to be \" + Arrays.toString(lastKnownElr));\n \n             // Unfence the last one in the ELR, it should be elected.\n             sendBrokerHeartbeatToUnfenceBrokers(active, List.of(brokerToBeTheLeader), brokerEpochs);",
      "parent_sha": "f24945b519005c0bc7a28db2db7aae6cec158927"
    }
  },
  {
    "oid": "d60e2ea581289a02efcef3b692bbb043fbdf3199",
    "message": "MINOR: Fix logging message in `NetworkClient.poll` not to mention `producer`\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Jun Rao <junrao@gmail.com>\n\nCloses #507 from ijuma/fix-error-message-in-network-client-poll",
    "date": "2015-11-12T15:20:10Z",
    "url": "https://github.com/apache/kafka/commit/d60e2ea581289a02efcef3b692bbb043fbdf3199",
    "details": {
      "sha": "52db61ad27e9307a6b3dd96fb9267ecdaed5b2ff",
      "filename": "clients/src/main/java/org/apache/kafka/clients/NetworkClient.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/d60e2ea581289a02efcef3b692bbb043fbdf3199/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FNetworkClient.java",
      "raw_url": "https://github.com/apache/kafka/raw/d60e2ea581289a02efcef3b692bbb043fbdf3199/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FNetworkClient.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FNetworkClient.java?ref=d60e2ea581289a02efcef3b692bbb043fbdf3199",
      "patch": "@@ -269,7 +269,7 @@ public List<ClientResponse> poll(long timeout, long now) {\n         try {\n             this.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs));\n         } catch (IOException e) {\n-            log.error(\"Unexpected error during I/O in producer network thread\", e);\n+            log.error(\"Unexpected error during I/O\", e);\n         }\n \n         // process completed actions",
      "parent_sha": "b5401f291a16977992c50f0a66d9f02aea07bbe7"
    }
  },
  {
    "oid": "4ad439c56db551697ea366dc4986eee5fc288706",
    "message": "MINOR: Change the log output information in the KafkaConsumer assign method (#12026)\n\nReviewers: Luke Chen <showuon@gmail.com>",
    "date": "2022-04-11T02:06:56Z",
    "url": "https://github.com/apache/kafka/commit/4ad439c56db551697ea366dc4986eee5fc288706",
    "details": {
      "sha": "a49c89560f2c361d6774d9950c13d1dedf2b072c",
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/4ad439c56db551697ea366dc4986eee5fc288706/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FKafkaConsumer.java",
      "raw_url": "https://github.com/apache/kafka/raw/4ad439c56db551697ea366dc4986eee5fc288706/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FKafkaConsumer.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FKafkaConsumer.java?ref=4ad439c56db551697ea366dc4986eee5fc288706",
      "patch": "@@ -1120,7 +1120,7 @@ public void assign(Collection<TopicPartition> partitions) {\n                 if (coordinator != null)\n                     this.coordinator.maybeAutoCommitOffsetsAsync(time.milliseconds());\n \n-                log.info(\"Subscribed to partition(s): {}\", Utils.join(partitions, \", \"));\n+                log.info(\"Assigned to partition(s): {}\", Utils.join(partitions, \", \"));\n                 if (this.subscriptions.assignFromUser(new HashSet<>(partitions)))\n                     metadata.requestUpdateForNewTopics();\n             }",
      "parent_sha": "6e9cd0c7f58a24b5ecef9746881ebcccd7633a7e"
    }
  },
  {
    "oid": "cbdd8218c16b56d952267c162dc1dfc93191571e",
    "message": "KAFKA-2756: Use request version Id instead of latest version Id to parse the corresponding response.\n\nAuthor: Guozhang Wang <wangguoz@gmail.com>\n\nReviewers: Guozhang Wang\n\nCloses #438 from guozhangwang/K2756",
    "date": "2015-11-05T23:51:42Z",
    "url": "https://github.com/apache/kafka/commit/cbdd8218c16b56d952267c162dc1dfc93191571e",
    "details": {
      "sha": "6c8853d2e50e25b574d8b4ec54b620d6c922367e",
      "filename": "clients/src/main/java/org/apache/kafka/clients/NetworkClient.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/cbdd8218c16b56d952267c162dc1dfc93191571e/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FNetworkClient.java",
      "raw_url": "https://github.com/apache/kafka/raw/cbdd8218c16b56d952267c162dc1dfc93191571e/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FNetworkClient.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FNetworkClient.java?ref=cbdd8218c16b56d952267c162dc1dfc93191571e",
      "patch": "@@ -458,8 +458,10 @@ private void handleCompletedReceives(List<ClientResponse> responses, long now) {\n             String source = receive.source();\n             ClientRequest req = inFlightRequests.completeNext(source);\n             ResponseHeader header = ResponseHeader.parse(receive.payload());\n+            // Always expect the response version id to be the same as the request version id\n             short apiKey = req.request().header().apiKey();\n-            Struct body = (Struct) ProtoUtils.currentResponseSchema(apiKey).read(receive.payload());\n+            short apiVer = req.request().header().apiVersion();\n+            Struct body = (Struct) ProtoUtils.responseSchema(apiKey, apiVer).read(receive.payload());\n             correlate(req.request().header(), header);\n             if (!metadataUpdater.maybeHandleCompletedReceive(req, now, body))\n                 responses.add(new ClientResponse(req, now, false, body));",
      "parent_sha": "0273c4379f12d7c3daedc89b0838485270e16bf4"
    }
  },
  {
    "oid": "91b4e3c00d84bf05387d55869062a2f988a8a4eb",
    "message": "KAFKA-4013; Include exception cause in SaslServerCallbackHandler\n\nAuthor: Bryan Baugher <bryan.baugher@cerner.com>\n\nReviewers: Sriharsha Chintalapani <harsha@hortonworks.com>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1695 from bbaugher/KAFKA-4013",
    "date": "2016-08-09T01:12:22Z",
    "url": "https://github.com/apache/kafka/commit/91b4e3c00d84bf05387d55869062a2f988a8a4eb",
    "details": {
      "sha": "c01f01d6a2606e759f75e7b01120ff41b4b30f8f",
      "filename": "clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerCallbackHandler.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/91b4e3c00d84bf05387d55869062a2f988a8a4eb/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fsecurity%2Fauthenticator%2FSaslServerCallbackHandler.java",
      "raw_url": "https://github.com/apache/kafka/raw/91b4e3c00d84bf05387d55869062a2f988a8a4eb/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fsecurity%2Fauthenticator%2FSaslServerCallbackHandler.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fsecurity%2Fauthenticator%2FSaslServerCallbackHandler.java?ref=91b4e3c00d84bf05387d55869062a2f988a8a4eb",
      "patch": "@@ -89,7 +89,7 @@ private void handleAuthorizeCallback(AuthorizeCallback ac) {\n             LOG.info(\"Setting authorizedID: {}\", userName);\n             ac.setAuthorizedID(userName);\n         } catch (IOException e) {\n-            LOG.error(\"Failed to set name based on Kerberos authentication rules.\");\n+            LOG.error(\"Failed to set name for '{}' based on Kerberos authentication rules.\", kerberosName, e);\n         }\n     }\n ",
      "parent_sha": "7b7f57df6229c02482e5176f2f265e7a890de2a3"
    }
  },
  {
    "oid": "d334f60944fa51622f0035039fa36d6d98405c59",
    "message": "MINOR: KStreamRepartitionIntegrationTest  bug  (#17963)\n\nThe KStreamRepartitionIntegrationTest.shouldThrowAnExceptionWhenNumberOfPartitionsOfRepartitionOperationDoNotMatchSourceTopicWhenJoining test was taking two minutes due not reaching an expected condition. By updating to have the StreamsUncaughtExceptionHandler return a response of SHUTDOWN_CLIENT the expected ERROR state is now reached. The root cause was using the Thread.UncaughtExceptionHandler to handle the exception.\r\n\r\nWithout this fix, the test takes 2 minutes to run, and now it's 1 second.\r\n\r\nReviewers: Matthias Sax <mjsax@apache.org>",
    "date": "2024-11-27T21:08:05Z",
    "url": "https://github.com/apache/kafka/commit/d334f60944fa51622f0035039fa36d6d98405c59",
    "details": {
      "sha": "d9c7c91bb5c72e3c286f9869df82a1e52fada247",
      "filename": "streams/integration-tests/src/test/java/org/apache/kafka/streams/integration/KStreamRepartitionIntegrationTest.java",
      "status": "modified",
      "additions": 19,
      "deletions": 39,
      "changes": 58,
      "blob_url": "https://github.com/apache/kafka/blob/d334f60944fa51622f0035039fa36d6d98405c59/streams%2Fintegration-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FKStreamRepartitionIntegrationTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/d334f60944fa51622f0035039fa36d6d98405c59/streams%2Fintegration-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FKStreamRepartitionIntegrationTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fintegration-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FKStreamRepartitionIntegrationTest.java?ref=d334f60944fa51622f0035039fa36d6d98405c59",
      "patch": "@@ -29,7 +29,6 @@\n import org.apache.kafka.common.serialization.StringSerializer;\n import org.apache.kafka.common.utils.Utils;\n import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.KafkaStreams.State;\n import org.apache.kafka.streams.KeyValue;\n import org.apache.kafka.streams.StreamsBuilder;\n import org.apache.kafka.streams.StreamsConfig;\n@@ -76,6 +75,7 @@\n import static org.apache.kafka.streams.KafkaStreams.State.ERROR;\n import static org.apache.kafka.streams.KafkaStreams.State.REBALANCING;\n import static org.apache.kafka.streams.KafkaStreams.State.RUNNING;\n+import static org.apache.kafka.streams.errors.StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_CLIENT;\n import static org.apache.kafka.streams.utils.TestUtils.safeUniqueTestName;\n import static org.junit.jupiter.api.Assertions.assertEquals;\n import static org.junit.jupiter.api.Assertions.assertNotNull;\n@@ -168,17 +168,21 @@ public void shouldThrowAnExceptionWhenNumberOfPartitionsOfRepartitionOperationDo\n                .to(outputTopic);\n \n         final Properties streamsConfiguration = createStreamsConfig(topologyOptimization);\n-        builder.build(streamsConfiguration);\n-\n-        startStreams(builder, REBALANCING, ERROR, streamsConfiguration, (t, e) -> expectedThrowable.set(e));\n-\n-        final String expectedMsg = String.format(\"Number of partitions [%s] of repartition topic [%s] \" +\n-                                                 \"doesn't match number of partitions [%s] of the source topic.\",\n-                                                 inputTopicRepartitionedNumOfPartitions,\n-                                                 toRepartitionTopicName(inputTopicRepartitionName),\n-                                                 topicBNumberOfPartitions);\n-        assertNotNull(expectedThrowable.get());\n-        assertTrue(expectedThrowable.get().getMessage().contains(expectedMsg));\n+        try (final KafkaStreams ks = new KafkaStreams(builder.build(streamsConfiguration), streamsConfiguration)) {\n+            ks.setUncaughtExceptionHandler(exception -> {\n+                expectedThrowable.set(exception);\n+                return SHUTDOWN_CLIENT;\n+            });\n+            ks.start();\n+            TestUtils.waitForCondition(() -> ks.state() == ERROR, 30_000, \"Kafka Streams never went into error state\");\n+            final String expectedMsg = String.format(\"Number of partitions [%s] of repartition topic [%s] \" +\n+                            \"doesn't match number of partitions [%s] of the source topic.\",\n+                    inputTopicRepartitionedNumOfPartitions,\n+                    toRepartitionTopicName(inputTopicRepartitionName),\n+                    topicBNumberOfPartitions);\n+            assertNotNull(expectedThrowable.get());\n+            assertTrue(expectedThrowable.get().getMessage().contains(expectedMsg));\n+        }\n     }\n \n     @ParameterizedTest\n@@ -723,7 +727,7 @@ public void shouldGoThroughRebalancingCorrectly(final String topologyOptimizatio\n             )\n         );\n \n-        kafkaStreamsToClose.close();\n+        kafkaStreamsToClose.close(Duration.ofSeconds(5));\n \n         sendEvents(\n             timestamp,\n@@ -814,36 +818,12 @@ private void sendEvents(final String topic,\n     }\n \n     private KafkaStreams startStreams(final StreamsBuilder builder, final Properties streamsConfiguration) throws InterruptedException {\n-        return startStreams(builder, REBALANCING, RUNNING, streamsConfiguration, null);\n-    }\n-\n-    private KafkaStreams startStreams(final StreamsBuilder builder,\n-                                      final State expectedOldState,\n-                                      final State expectedNewState,\n-                                      final Properties streamsConfiguration,\n-                                      final Thread.UncaughtExceptionHandler uncaughtExceptionHandler) throws InterruptedException {\n         final CountDownLatch latch;\n         final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(streamsConfiguration), streamsConfiguration);\n-\n-        if (uncaughtExceptionHandler == null) {\n-            latch = new CountDownLatch(1);\n-        } else {\n-            latch = new CountDownLatch(2);\n-            kafkaStreams.setUncaughtExceptionHandler(e -> {\n-                uncaughtExceptionHandler.uncaughtException(Thread.currentThread(), e);\n-                latch.countDown();\n-                if (e instanceof RuntimeException) {\n-                    throw (RuntimeException) e;\n-                } else if (e instanceof Error) {\n-                    throw (Error) e;\n-                } else {\n-                    throw new RuntimeException(\"Unexpected checked exception caught in the uncaught exception handler\", e);\n-                }\n-            });\n-        }\n+        latch = new CountDownLatch(1);\n \n         kafkaStreams.setStateListener((newState, oldState) -> {\n-            if (expectedOldState == oldState && expectedNewState == newState) {\n+            if (REBALANCING == oldState && RUNNING == newState) {\n                 latch.countDown();\n             }\n         });",
      "parent_sha": "cdf3aab661fb975064bb1bffb61be5bbff138b62"
    }
  },
  {
    "oid": "73a9fcaa4db3f758dd8a16eff5651555ded1b8b7",
    "message": "KAFKA-7799; Fix flaky test RestServerTest.testCORSEnabled (#6106)\n\nThe test always fails if testOptionsDoesNotIncludeWadlOutput is executed before testCORSEnabled. It seems the problem is the use of the system property. Perhaps there is some static caching somewhere.\r\n\r\nReviewers: Randall Hauch <rhauch@gmail.com>, Guozhang Wang <wangguoz@gmail.com>",
    "date": "2019-01-09T07:47:28Z",
    "url": "https://github.com/apache/kafka/commit/73a9fcaa4db3f758dd8a16eff5651555ded1b8b7",
    "details": {
      "sha": "39e35cb01df76322b040c3405b2b9ff7f3f19235",
      "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/rest/RestServerTest.java",
      "status": "modified",
      "additions": 7,
      "deletions": 3,
      "changes": 10,
      "blob_url": "https://github.com/apache/kafka/blob/73a9fcaa4db3f758dd8a16eff5651555ded1b8b7/connect%2Fruntime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Frest%2FRestServerTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/73a9fcaa4db3f758dd8a16eff5651555ded1b8b7/connect%2Fruntime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Frest%2FRestServerTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fruntime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Frest%2FRestServerTest.java?ref=73a9fcaa4db3f758dd8a16eff5651555ded1b8b7",
      "patch": "@@ -27,6 +27,7 @@\n import org.easymock.EasyMock;\n import org.junit.After;\n import org.junit.Assert;\n+import org.junit.Before;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.powermock.api.easymock.PowerMock;\n@@ -58,6 +59,12 @@ public class RestServerTest {\n     private Plugins plugins;\n     private RestServer server;\n \n+    @Before\n+    public void setUp() {\n+        // To be able to set the Origin, we need to toggle this flag\n+        System.setProperty(\"sun.net.http.allowRestrictedHeaders\", \"true\");\n+    }\n+\n     @After\n     public void tearDown() {\n         server.stop();\n@@ -185,13 +192,10 @@ public void testOptionsDoesNotIncludeWadlOutput() {\n     }\n \n     public void checkCORSRequest(String corsDomain, String origin, String expectedHeader, String method) {\n-        // To be able to set the Origin, we need to toggle this flag\n-\n         Map<String, String> workerProps = baseWorkerProps();\n         workerProps.put(WorkerConfig.ACCESS_CONTROL_ALLOW_ORIGIN_CONFIG, corsDomain);\n         workerProps.put(WorkerConfig.ACCESS_CONTROL_ALLOW_METHODS_CONFIG, method);\n         WorkerConfig workerConfig = new DistributedConfig(workerProps);\n-        System.setProperty(\"sun.net.http.allowRestrictedHeaders\", \"true\");\n \n         EasyMock.expect(herder.plugins()).andStubReturn(plugins);\n         EasyMock.expect(plugins.newPlugins(Collections.emptyList(),",
      "parent_sha": "e32567699451e2fb0ccd63dd8a3df582cfd18d61"
    }
  },
  {
    "oid": "b9eda22d71aa9896cac90798c7e888a04f14549d",
    "message": "HOTFIX: Fix checkstyle failure in KStreams by providing fully qualified class names.\n\nAuthor: Ewen Cheslack-Postava <me@ewencp.org>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #1000 from ewencp/hotfix-kstreams-checkstyle-javadocs",
    "date": "2016-03-03T07:42:17Z",
    "url": "https://github.com/apache/kafka/commit/b9eda22d71aa9896cac90798c7e888a04f14549d",
    "details": {
      "sha": "eed5fe1f40222634f74656db252e347f1a186705",
      "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/Windowed.java",
      "status": "modified",
      "additions": 3,
      "deletions": 4,
      "changes": 7,
      "blob_url": "https://github.com/apache/kafka/blob/b9eda22d71aa9896cac90798c7e888a04f14549d/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fkstream%2FWindowed.java",
      "raw_url": "https://github.com/apache/kafka/raw/b9eda22d71aa9896cac90798c7e888a04f14549d/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fkstream%2FWindowed.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fkstream%2FWindowed.java?ref=b9eda22d71aa9896cac90798c7e888a04f14549d",
      "patch": "@@ -17,12 +17,11 @@\n \n package org.apache.kafka.streams.kstream;\n \n-import org.apache.kafka.common.serialization.Deserializer;\n-import org.apache.kafka.common.serialization.Serializer;\n-\n /**\n  * The windowed key interface used in {@link KTable}, used for representing a windowed table result from windowed stream aggregations,\n- * i.e. {@link KStream#aggregateByKey(Initializer, Aggregator, Windows, Serializer, Serializer, Deserializer, Deserializer)}\n+ * i.e. {@link KStream#aggregateByKey(Initializer, Aggregator, Windows, org.apache.kafka.common.serialization.Serializer,\n+ * org.apache.kafka.common.serialization.Serializer, org.apache.kafka.common.serialization.Deserializer,\n+ * org.apache.kafka.common.serialization.Deserializer)}\n  *\n  * @param <T> Type of the key\n  */",
      "parent_sha": "00a58f8e1e0c82c2948a8fdfacf812ec4865b339"
    }
  },
  {
    "oid": "38c65a9a39081068e05e3222ccb0dbc3c9f56942",
    "message": "HOTFIX: Fix unstable Streams application reset integration test\n\nAuthor: Matthias J. Sax <matthias@confluent.io>\n\nReviewers: Eno Thereska <eno.thereska@gmail.com>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1673 from mjsax/hotfix\n\n(cherry picked from commit ad1dab9c3d3ae14746ee5d94434ef98ef4889023)\nSigned-off-by: Ismael Juma <ismael@juma.me.uk>",
    "date": "2016-07-28T23:01:48Z",
    "url": "https://github.com/apache/kafka/commit/38c65a9a39081068e05e3222ccb0dbc3c9f56942",
    "details": {
      "sha": "85aff269e6aaee2d9cc9e532e08addb4f6d1cb65",
      "filename": "streams/src/test/java/org/apache/kafka/streams/integration/ResetIntegrationTest.java",
      "status": "modified",
      "additions": 9,
      "deletions": 6,
      "changes": 15,
      "blob_url": "https://github.com/apache/kafka/blob/38c65a9a39081068e05e3222ccb0dbc3c9f56942/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FResetIntegrationTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/38c65a9a39081068e05e3222ccb0dbc3c9f56942/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FResetIntegrationTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FResetIntegrationTest.java?ref=38c65a9a39081068e05e3222ccb0dbc3c9f56942",
      "patch": "@@ -64,6 +64,7 @@ public class ResetIntegrationTest {\n     private static final String INPUT_TOPIC = \"inputTopic\";\n     private static final String OUTPUT_TOPIC = \"outputTopic\";\n     private static final String OUTPUT_TOPIC_2 = \"outputTopic2\";\n+    private static final String OUTPUT_TOPIC_2_RERUN = \"outputTopic2_rerun\";\n     private static final String INTERMEDIATE_USER_TOPIC = \"userTopic\";\n \n     private static final long STREAMS_CONSUMER_TIMEOUT = 2000L;\n@@ -74,16 +75,17 @@ public static void startKafkaCluster() throws Exception {\n         CLUSTER.createTopic(INPUT_TOPIC);\n         CLUSTER.createTopic(OUTPUT_TOPIC);\n         CLUSTER.createTopic(OUTPUT_TOPIC_2);\n+        CLUSTER.createTopic(OUTPUT_TOPIC_2_RERUN);\n         CLUSTER.createTopic(INTERMEDIATE_USER_TOPIC);\n     }\n \n     @Test\n-    public void testReprocessingFromScratchAfterCleanUp() throws Exception {\n+    public void testReprocessingFromScratchAfterReset() throws Exception {\n         final Properties streamsConfiguration = prepareTest();\n         final Properties resultTopicConsumerConfig = prepareResultConsumer();\n \n         prepareInputData();\n-        final KStreamBuilder builder = setupTopology();\n+        final KStreamBuilder builder = setupTopology(OUTPUT_TOPIC_2);\n \n         // RUN\n         KafkaStreams streams = new KafkaStreams(builder, streamsConfiguration);\n@@ -103,10 +105,10 @@ public void testReprocessingFromScratchAfterCleanUp() throws Exception {\n         Utils.sleep(CLEANUP_CONSUMER_TIMEOUT);\n \n         // RE-RUN\n-        streams = new KafkaStreams(setupTopology(), streamsConfiguration);\n+        streams = new KafkaStreams(setupTopology(OUTPUT_TOPIC_2_RERUN), streamsConfiguration);\n         streams.start();\n         final List<KeyValue<Long, Long>> resultRerun = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultTopicConsumerConfig, OUTPUT_TOPIC, 10);\n-        final KeyValue<Object, Object> resultRerun2 = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultTopicConsumerConfig, OUTPUT_TOPIC_2, 1).get(0);\n+        final KeyValue<Object, Object> resultRerun2 = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(resultTopicConsumerConfig, OUTPUT_TOPIC_2_RERUN, 1).get(0);\n         streams.close();\n \n         assertThat(resultRerun, equalTo(result));\n@@ -163,7 +165,7 @@ private void prepareInputData() throws Exception {\n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(INPUT_TOPIC, Collections.singleton(new KeyValue<>(1L, \"jjj\")), producerConfig, 64L);\n     }\n \n-    private KStreamBuilder setupTopology() {\n+    private KStreamBuilder setupTopology(final String outputTopic2) {\n         final KStreamBuilder builder = new KStreamBuilder();\n \n         final KStream<Long, String> input = builder.stream(INPUT_TOPIC);\n@@ -201,7 +203,7 @@ public KeyValue<Long, Long> apply(final Windowed<Long> key, final Long value) {\n                     return new KeyValue<>(key.window().start() + key.window().end(), value);\n                 }\n             });\n-        windowedCounts.to(Serdes.Long(), Serdes.Long(), OUTPUT_TOPIC_2);\n+        windowedCounts.to(Serdes.Long(), Serdes.Long(), outputTopic2);\n \n         return builder;\n     }\n@@ -229,6 +231,7 @@ private void assertInternalTopicsGotDeleted() {\n         expectedRemainingTopicsAfterCleanup.add(INTERMEDIATE_USER_TOPIC);\n         expectedRemainingTopicsAfterCleanup.add(OUTPUT_TOPIC);\n         expectedRemainingTopicsAfterCleanup.add(OUTPUT_TOPIC_2);\n+        expectedRemainingTopicsAfterCleanup.add(OUTPUT_TOPIC_2_RERUN);\n         expectedRemainingTopicsAfterCleanup.add(\"__consumer_offsets\");\n \n         Set<String> allTopics;",
      "parent_sha": "071b76cc50b4f79c839306f40eb84ee2b9724ab2"
    }
  },
  {
    "oid": "7cb0a1ef4f4a7c08a56cb9b84d482e60ed5f20f3",
    "message": "MINOR: Reinstate info-level log for dynamic update of SSL keystores (#6925)\n\nReviewers: Manikumar Reddy <manikumar.reddy@gmail.com>",
    "date": "2019-06-23T08:30:28Z",
    "url": "https://github.com/apache/kafka/commit/7cb0a1ef4f4a7c08a56cb9b84d482e60ed5f20f3",
    "details": {
      "sha": "c00941284200e49eb01736d2fddf48f1e82ad662",
      "filename": "clients/src/main/java/org/apache/kafka/common/security/ssl/SslFactory.java",
      "status": "modified",
      "additions": 6,
      "deletions": 1,
      "changes": 7,
      "blob_url": "https://github.com/apache/kafka/blob/7cb0a1ef4f4a7c08a56cb9b84d482e60ed5f20f3/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fsecurity%2Fssl%2FSslFactory.java",
      "raw_url": "https://github.com/apache/kafka/raw/7cb0a1ef4f4a7c08a56cb9b84d482e60ed5f20f3/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fsecurity%2Fssl%2FSslFactory.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fsecurity%2Fssl%2FSslFactory.java?ref=7cb0a1ef4f4a7c08a56cb9b84d482e60ed5f20f3",
      "patch": "@@ -114,7 +114,12 @@ public void validateReconfiguration(Map<String, ?> newConfigs) {\n \n     @Override\n     public void reconfigure(Map<String, ?> newConfigs) throws KafkaException {\n-        this.sslEngineBuilder = createNewSslEngineBuilder(newConfigs);\n+        SslEngineBuilder newSslEngineBuilder = createNewSslEngineBuilder(newConfigs);\n+        if (newSslEngineBuilder != this.sslEngineBuilder) {\n+            this.sslEngineBuilder = newSslEngineBuilder;\n+            log.info(\"Created new {} SSL engine builder with keystore {} truststore {}\", mode,\n+                    newSslEngineBuilder.keystore(), newSslEngineBuilder.truststore());\n+        }\n     }\n \n     private SslEngineBuilder createNewSslEngineBuilder(Map<String, ?> newConfigs) {",
      "parent_sha": "fc27cbe4159eaf9195017bc0fa929b0b84216d61"
    }
  },
  {
    "oid": "26e238c6f5a242459f52bfefa97a6b0c247b2d5e",
    "message": "KAFKA-10147 MockAdminClient#describeConfigs(Collection<ConfigResource>) is unable to handle broker resource (#8853)\n\nAuthor: Chia-Ping Tsai <chia7712@gmail.com>\r\nReviewers: Boyang Chen <boyang@confluent.io>, Randall Hauch <rhauch@gmail.com>",
    "date": "2020-06-17T14:56:07Z",
    "url": "https://github.com/apache/kafka/commit/26e238c6f5a242459f52bfefa97a6b0c247b2d5e",
    "details": {
      "sha": "7c6a9559b8eed7042aaea14dca6ecc195e38af05",
      "filename": "clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java",
      "status": "modified",
      "additions": 23,
      "deletions": 49,
      "changes": 72,
      "blob_url": "https://github.com/apache/kafka/blob/26e238c6f5a242459f52bfefa97a6b0c247b2d5e/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FMockAdminClient.java",
      "raw_url": "https://github.com/apache/kafka/raw/26e238c6f5a242459f52bfefa97a6b0c247b2d5e/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FMockAdminClient.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FMockAdminClient.java?ref=26e238c6f5a242459f52bfefa97a6b0c247b2d5e",
      "patch": "@@ -51,6 +51,7 @@\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.stream.Collectors;\n \n public class MockAdminClient extends AdminClient {\n     public static final String DEFAULT_CLUSTER_ID = \"I4ZmrWqfT2e-upky_4fdPA\";\n@@ -365,51 +366,6 @@ synchronized public DescribeTopicsResult describeTopics(Collection<String> topic\n         return new DescribeTopicsResult(topicDescriptions);\n     }\n \n-    @Override\n-    public DescribeConfigsResult describeConfigs(Collection<ConfigResource> resources) {\n-        Map<ConfigResource, KafkaFuture<Config>> topicConfigs = new HashMap<>();\n-\n-        if (timeoutNextRequests > 0) {\n-            for (ConfigResource requestedResource : resources) {\n-                KafkaFutureImpl<Config> future = new KafkaFutureImpl<>();\n-                future.completeExceptionally(new TimeoutException());\n-                topicConfigs.put(requestedResource, future);\n-            }\n-\n-            --timeoutNextRequests;\n-            return new DescribeConfigsResult(topicConfigs);\n-        }\n-\n-        for (ConfigResource requestedResource : resources) {\n-            if (requestedResource.type() != ConfigResource.Type.TOPIC) {\n-                continue;\n-            }\n-            for (Map.Entry<String, TopicMetadata> topicDescription : allTopics.entrySet()) {\n-                String topicName = topicDescription.getKey();\n-                if (topicName.equals(requestedResource.name()) && !topicDescription.getValue().markedForDeletion) {\n-                    if (topicDescription.getValue().fetchesRemainingUntilVisible > 0) {\n-                        topicDescription.getValue().fetchesRemainingUntilVisible--;\n-                    } else {\n-                        TopicMetadata topicMetadata = topicDescription.getValue();\n-                        KafkaFutureImpl<Config> future = new KafkaFutureImpl<>();\n-                        Collection<ConfigEntry> entries = new ArrayList<>();\n-                        topicMetadata.configs.forEach((k, v) -> entries.add(new ConfigEntry(k, v)));\n-                        future.complete(new Config(entries));\n-                        topicConfigs.put(requestedResource, future);\n-                        break;\n-                    }\n-                }\n-            }\n-            if (!topicConfigs.containsKey(requestedResource)) {\n-                KafkaFutureImpl<Config> future = new KafkaFutureImpl<>();\n-                future.completeExceptionally(new UnknownTopicOrPartitionException(\"Resource \" + requestedResource + \" not found.\"));\n-                topicConfigs.put(requestedResource, future);\n-            }\n-        }\n-\n-        return new DescribeConfigsResult(topicConfigs);\n-    }\n-\n     @Override\n     synchronized public DeleteTopicsResult deleteTopics(Collection<String> topicsToDelete, DeleteTopicsOptions options) {\n         Map<String, KafkaFuture<Void>> deleteTopicsResult = new HashMap<>();\n@@ -535,6 +491,19 @@ synchronized public DeleteAclsResult deleteAcls(Collection<AclBindingFilter> fil\n \n     @Override\n     synchronized public DescribeConfigsResult describeConfigs(Collection<ConfigResource> resources, DescribeConfigsOptions options) {\n+\n+        if (timeoutNextRequests > 0) {\n+            Map<ConfigResource, KafkaFuture<Config>> configs = new HashMap<>();\n+            for (ConfigResource requestedResource : resources) {\n+                KafkaFutureImpl<Config> future = new KafkaFutureImpl<>();\n+                future.completeExceptionally(new TimeoutException());\n+                configs.put(requestedResource, future);\n+            }\n+\n+            --timeoutNextRequests;\n+            return new DescribeConfigsResult(configs);\n+        }\n+\n         Map<ConfigResource, KafkaFuture<Config>> results = new HashMap<>();\n         for (ConfigResource resource : resources) {\n             KafkaFutureImpl<Config> future = new KafkaFutureImpl<>();\n@@ -551,7 +520,7 @@ synchronized public DescribeConfigsResult describeConfigs(Collection<ConfigResou\n     synchronized private Config getResourceDescription(ConfigResource resource) {\n         switch (resource.type()) {\n             case BROKER: {\n-                int brokerId = Integer.valueOf(resource.name());\n+                int brokerId = Integer.parseInt(resource.name());\n                 if (brokerId >= brokerConfigs.size()) {\n                     throw new InvalidRequestException(\"Broker \" + resource.name() +\n                         \" not found.\");\n@@ -560,10 +529,15 @@ synchronized private Config getResourceDescription(ConfigResource resource) {\n             }\n             case TOPIC: {\n                 TopicMetadata topicMetadata = allTopics.get(resource.name());\n-                if (topicMetadata == null) {\n-                    throw new UnknownTopicOrPartitionException();\n+                if (topicMetadata != null && !topicMetadata.markedForDeletion) {\n+                    if (topicMetadata.fetchesRemainingUntilVisible > 0)\n+                        topicMetadata.fetchesRemainingUntilVisible = Math.max(0, topicMetadata.fetchesRemainingUntilVisible - 1);\n+                    else return new Config(topicMetadata.configs.entrySet().stream()\n+                                .map(entry -> new ConfigEntry(entry.getKey(), entry.getValue()))\n+                                .collect(Collectors.toList()));\n+\n                 }\n-                return toConfigObject(topicMetadata.configs);\n+                throw new UnknownTopicOrPartitionException(\"Resource \" + resource + \" not found.\");\n             }\n             default:\n                 throw new UnsupportedOperationException(\"Not implemented yet\");",
      "parent_sha": "147ffb9a968d62ef78ac6b330a20023ed49ddbb8"
    }
  },
  {
    "oid": "29383d6d6a3d42d30e815fbbb084275d449928c8",
    "message": "KAFKA-7604; Fix flaky unit test `testRebalanceAfterTopicUnavailableWithPatternSubscribe` (#5889)\n\nThe problem is the concurrent metadata updates in the foreground and in the heartbeat thread. Changed the code to use ConsumerNetworkClient.poll, which enforces mutual exclusion when accessing the underlying client.",
    "date": "2018-11-08T13:37:05Z",
    "url": "https://github.com/apache/kafka/commit/29383d6d6a3d42d30e815fbbb084275d449928c8",
    "details": {
      "sha": "b430078f726c95967cb971597596b1737b700c83",
      "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinatorTest.java",
      "status": "modified",
      "additions": 5,
      "deletions": 5,
      "changes": 10,
      "blob_url": "https://github.com/apache/kafka/blob/29383d6d6a3d42d30e815fbbb084275d449928c8/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2Finternals%2FConsumerCoordinatorTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/29383d6d6a3d42d30e815fbbb084275d449928c8/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2Finternals%2FConsumerCoordinatorTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2Finternals%2FConsumerCoordinatorTest.java?ref=29383d6d6a3d42d30e815fbbb084275d449928c8",
      "patch": "@@ -558,7 +558,7 @@ public boolean matches(AbstractRequest body) {\n         // Refresh the metadata again. Since there have been no changes since the last refresh, it won't trigger\n         // rebalance again.\n         metadata.requestUpdate();\n-        client.poll(Long.MAX_VALUE, time.milliseconds());\n+        consumerClient.poll(time.timer(Long.MAX_VALUE));\n         assertFalse(coordinator.rejoinNeededOrPending());\n     }\n \n@@ -1010,13 +1010,13 @@ private void unavailableTopicTest(boolean patternSubscribe, Set<String> unavaila\n         coordinator.ensureCoordinatorReady(time.timer(Long.MAX_VALUE));\n \n         Map<String, List<String>> memberSubscriptions = singletonMap(consumerId, singletonList(topic1));\n-        partitionAssignor.prepare(Collections.<String, List<TopicPartition>>emptyMap());\n+        partitionAssignor.prepare(Collections.emptyMap());\n \n         client.prepareResponse(joinGroupLeaderResponse(1, consumerId, memberSubscriptions, Errors.NONE));\n-        client.prepareResponse(syncGroupResponse(Collections.<TopicPartition>emptyList(), Errors.NONE));\n+        client.prepareResponse(syncGroupResponse(Collections.emptyList(), Errors.NONE));\n         coordinator.poll(time.timer(Long.MAX_VALUE));\n         assertFalse(coordinator.rejoinNeededOrPending());\n-        assertEquals(Collections.<TopicPartition>emptySet(), rebalanceListener.assigned);\n+        assertEquals(Collections.emptySet(), rebalanceListener.assigned);\n         assertTrue(\"Metadata refresh not requested for unavailable partitions\", metadata.updateRequested());\n \n         Map<String, Errors> topicErrors = new HashMap<>();\n@@ -1026,7 +1026,7 @@ private void unavailableTopicTest(boolean patternSubscribe, Set<String> unavaila\n         client.prepareMetadataUpdate(TestUtils.metadataUpdateWith(\"kafka-cluster\", 1,\n                 topicErrors, singletonMap(topic1, 1)));\n \n-        client.poll(0, time.milliseconds());\n+        consumerClient.poll(time.timer(0));\n         client.prepareResponse(joinGroupLeaderResponse(2, consumerId, memberSubscriptions, Errors.NONE));\n         client.prepareResponse(syncGroupResponse(singletonList(t1p), Errors.NONE));\n         coordinator.poll(time.timer(Long.MAX_VALUE));",
      "parent_sha": "8abbf33b5978b93354ae35c5e21c474816c9cc76"
    }
  },
  {
    "oid": "3f834781a4311fb26fec740ee68bddb70a6f0701",
    "message": "KAFKA-12844: clean up TaskId (#17904)\n\nRename topicGroupId as subtopology.\r\n\r\nReviewers: Matthias J. Sax <matthias@confluent.io>",
    "date": "2024-11-27T01:06:36Z",
    "url": "https://github.com/apache/kafka/commit/3f834781a4311fb26fec740ee68bddb70a6f0701",
    "details": {
      "sha": "6b909ec72747f6e3e215f8405da4aefc67fcbbc3",
      "filename": "streams/src/main/java/org/apache/kafka/streams/processor/TaskId.java",
      "status": "modified",
      "additions": 12,
      "deletions": 12,
      "changes": 24,
      "blob_url": "https://github.com/apache/kafka/blob/3f834781a4311fb26fec740ee68bddb70a6f0701/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2FTaskId.java",
      "raw_url": "https://github.com/apache/kafka/raw/3f834781a4311fb26fec740ee68bddb70a6f0701/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2FTaskId.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2FTaskId.java?ref=3f834781a4311fb26fec740ee68bddb70a6f0701",
      "patch": "@@ -24,28 +24,28 @@\n import java.util.Objects;\n \n /**\n- * The task ID representation composed as subtopology (aka topicGroupId) plus the assigned partition ID.\n+ * The task ID representation composed as subtopology plus the assigned partition ID.\n  */\n public class TaskId implements Comparable<TaskId> {\n \n     private static final Logger LOG = LoggerFactory.getLogger(TaskId.class);\n \n     public static final String NAMED_TOPOLOGY_DELIMITER = \"__\";\n \n-    /** The ID of the subtopology, aka topicGroupId. */\n-    private final int topicGroupId;\n+    /** The ID of the subtopology. */\n+    private final int subtopology;\n     /** The ID of the partition. */\n     private final int partition;\n \n     /** The namedTopology that this task belongs to, or null if it does not belong to one */\n     private final String topologyName;\n \n-    public TaskId(final int topicGroupId, final int partition) {\n-        this(topicGroupId, partition, null);\n+    public TaskId(final int subtopology, final int partition) {\n+        this(subtopology, partition, null);\n     }\n \n-    public TaskId(final int topicGroupId, final int partition, final String topologyName) {\n-        this.topicGroupId = topicGroupId;\n+    public TaskId(final int subtopology, final int partition, final String topologyName) {\n+        this.subtopology = subtopology;\n         this.partition = partition;\n         if (topologyName != null && topologyName.length() == 0) {\n             LOG.warn(\"Empty string passed in for task's namedTopology, since NamedTopology name cannot be empty, we \"\n@@ -57,7 +57,7 @@ public TaskId(final int topicGroupId, final int partition, final String topology\n     }\n \n     public int subtopology() {\n-        return topicGroupId;\n+        return subtopology;\n     }\n \n     public int partition() {\n@@ -73,7 +73,7 @@ public String topologyName() {\n \n     @Override\n     public String toString() {\n-        return topologyName != null ? topologyName + NAMED_TOPOLOGY_DELIMITER + topicGroupId + \"_\" + partition : topicGroupId + \"_\" + partition;\n+        return topologyName != null ? topologyName + NAMED_TOPOLOGY_DELIMITER + subtopology + \"_\" + partition : subtopology + \"_\" + partition;\n     }\n \n     /**\n@@ -115,7 +115,7 @@ public boolean equals(final Object o) {\n         }\n         final TaskId taskId = (TaskId) o;\n \n-        if (topicGroupId != taskId.topicGroupId || partition != taskId.partition) {\n+        if (subtopology != taskId.subtopology || partition != taskId.partition) {\n             return false;\n         }\n \n@@ -128,7 +128,7 @@ public boolean equals(final Object o) {\n \n     @Override\n     public int hashCode() {\n-        return Objects.hash(topicGroupId, partition, topologyName);\n+        return Objects.hash(subtopology, partition, topologyName);\n     }\n \n     @Override\n@@ -142,7 +142,7 @@ public int compareTo(final TaskId other) {\n             LOG.error(\"Tried to compare this = {} with other = {}, but only one had a valid named topology\", this, other);\n             throw new IllegalStateException(\"Can't compare a TaskId with a namedTopology to one without\");\n         }\n-        final int comparingTopicGroupId = Integer.compare(this.topicGroupId, other.topicGroupId);\n+        final int comparingTopicGroupId = Integer.compare(this.subtopology, other.subtopology);\n         return comparingTopicGroupId != 0 ? comparingTopicGroupId : Integer.compare(this.partition, other.partition);\n     }\n }",
      "parent_sha": "2b2b3cd355c2bb226a869988a6b1a59a7f2db055"
    }
  },
  {
    "oid": "3dffd5df2f7d4a8a3e512be5af908328c9d5e92e",
    "message": "MINOR: Add log entry for KafkaException in StreamThread#runLoop (#6144)\n\nI've observed several reports of sudden unexpected streamthread shutdown with the log entry like:\r\n\r\nState transition from PENDING_SHUTDOWN to DEAD\r\n\r\nbut there is no related error logs before this line at all. I suspect this is because we intentionally do not log for KafkaException and there's some edge cases where we miss internally and hence caused this. I'm adding the ERROR level log entry here in order to reveal more information in case I saw this again in the future.\r\n\r\nReviewers: Matthias J. Sax <matthias@confluent.io>",
    "date": "2019-01-15T18:19:03Z",
    "url": "https://github.com/apache/kafka/commit/3dffd5df2f7d4a8a3e512be5af908328c9d5e92e",
    "details": {
      "sha": "851187319e64c6ff1410a16bb9fc487fa4d94fb6",
      "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/kafka/blob/3dffd5df2f7d4a8a3e512be5af908328c9d5e92e/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStreamThread.java",
      "raw_url": "https://github.com/apache/kafka/raw/3dffd5df2f7d4a8a3e512be5af908328c9d5e92e/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStreamThread.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStreamThread.java?ref=3dffd5df2f7d4a8a3e512be5af908328c9d5e92e",
      "patch": "@@ -754,7 +754,8 @@ public void run() {\n             runLoop();\n             cleanRun = true;\n         } catch (final KafkaException e) {\n-            // just re-throw the exception as it should be logged already\n+            log.error(\"Encountered the following unexpected Kafka exception during processing, \" +\n+                \"this usually indicate Streams internal errors:\", e);\n             throw e;\n         } catch (final Exception e) {\n             // we have caught all Kafka related exceptions, and other runtime exceptions",
      "parent_sha": "af634a4a98eaa2457752e3f2841720020e0e9ad0"
    }
  },
  {
    "oid": "de67ac6a9a156c35fc2ca36061c6dcbf57983421",
    "message": "MINOR: Update `getOrMaybeCreateClassicGroup` to only throw GroupIdNotFoundException (#35) (#16919)\n\nThis patch updates getOrMaybeCreateClassicGroup to only throw GroupIdNotFoundException as we did for other internal methods. The callers are responsible for translating the error to the appropriate one depending on the context. There is only one case.\r\n\r\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2024-08-20T11:36:42Z",
    "url": "https://github.com/apache/kafka/commit/de67ac6a9a156c35fc2ca36061c6dcbf57983421",
    "details": {
      "sha": "506203b9b83a53059076928bb4a727e87c68ff95",
      "filename": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java",
      "status": "modified",
      "additions": 10,
      "deletions": 10,
      "changes": 20,
      "blob_url": "https://github.com/apache/kafka/blob/de67ac6a9a156c35fc2ca36061c6dcbf57983421/group-coordinator%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcoordinator%2Fgroup%2FGroupMetadataManager.java",
      "raw_url": "https://github.com/apache/kafka/raw/de67ac6a9a156c35fc2ca36061c6dcbf57983421/group-coordinator%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcoordinator%2Fgroup%2FGroupMetadataManager.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/group-coordinator%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcoordinator%2Fgroup%2FGroupMetadataManager.java?ref=de67ac6a9a156c35fc2ca36061c6dcbf57983421",
      "patch": "@@ -880,19 +880,19 @@ ConsumerGroup getOrMaybeCreatePersistedConsumerGroup(\n      *                          created if it does not exist.\n      *\n      * @return A ClassicGroup.\n-     * @throws UnknownMemberIdException if the group does not exist and createIfNotExists is false.\n-     * @throws GroupIdNotFoundException if the group is not a classic group.\n+     * @throws GroupIdNotFoundException if the group does not exist and createIfNotExists is false or\n+     *                                  if the group is not a classic group.\n      *\n      * Package private for testing.\n      */\n     ClassicGroup getOrMaybeCreateClassicGroup(\n         String groupId,\n         boolean createIfNotExists\n-    ) throws UnknownMemberIdException, GroupIdNotFoundException {\n+    ) throws GroupIdNotFoundException {\n         Group group = groups.get(groupId);\n \n         if (group == null && !createIfNotExists) {\n-            throw new UnknownMemberIdException(String.format(\"Classic group %s not found.\", groupId));\n+            throw new GroupIdNotFoundException(String.format(\"Classic group %s not found.\", groupId));\n         }\n \n         if (group == null) {\n@@ -3900,10 +3900,10 @@ CoordinatorResult<Void, CoordinatorRecord> classicGroupJoinToClassicGroup(\n         boolean isNewGroup = !groups.containsKey(groupId);\n         try {\n             group = getOrMaybeCreateClassicGroup(groupId, isUnknownMember);\n-        } catch (Throwable t) {\n+        } catch (GroupIdNotFoundException t) {\n             responseFuture.complete(new JoinGroupResponseData()\n                 .setMemberId(memberId)\n-                .setErrorCode(Errors.forException(t).code())\n+                .setErrorCode(Errors.UNKNOWN_MEMBER_ID.code())\n             );\n             return EMPTY_RESULT;\n         }\n@@ -4289,7 +4289,7 @@ private CoordinatorResult<Void, CoordinatorRecord> completeClassicGroupJoin(Stri\n         ClassicGroup group;\n         try {\n             group = getOrMaybeCreateClassicGroup(groupId, false);\n-        } catch (UnknownMemberIdException | GroupIdNotFoundException exception) {\n+        } catch (GroupIdNotFoundException exception) {\n             log.debug(\"Cannot find the group, skipping rebalance stage.\", exception);\n             return EMPTY_RESULT;\n         }\n@@ -4433,7 +4433,7 @@ private CoordinatorResult<Void, CoordinatorRecord> expireClassicGroupMemberHeart\n         ClassicGroup group;\n         try {\n             group = getOrMaybeCreateClassicGroup(groupId, false);\n-        } catch (UnknownMemberIdException | GroupIdNotFoundException exception) {\n+        } catch (GroupIdNotFoundException exception) {\n             log.debug(\"Received notification of heartbeat expiration for member {} after group {} \" +\n                 \"had already been deleted or upgraded.\", memberId, groupId);\n             return EMPTY_RESULT;\n@@ -4707,7 +4707,7 @@ private CoordinatorResult<Void, CoordinatorRecord> tryCompleteInitialRebalanceEl\n         ClassicGroup group;\n         try {\n             group = getOrMaybeCreateClassicGroup(groupId, false);\n-        } catch (UnknownMemberIdException | GroupIdNotFoundException exception) {\n+        } catch (GroupIdNotFoundException exception) {\n             log.debug(\"Cannot find the group, skipping the initial rebalance stage.\", exception);\n             return EMPTY_RESULT;\n         }\n@@ -4864,7 +4864,7 @@ private CoordinatorResult<Void, CoordinatorRecord> expirePendingSync(\n         ClassicGroup group;\n         try {\n             group = getOrMaybeCreateClassicGroup(groupId, false);\n-        } catch (UnknownMemberIdException | GroupIdNotFoundException exception) {\n+        } catch (GroupIdNotFoundException exception) {\n             log.debug(\"Received notification of sync expiration for an unknown classic group {}.\", groupId);\n             return EMPTY_RESULT;\n         }",
      "parent_sha": "932e84096a96e199c0772c81bddf3ea3789377ba"
    }
  },
  {
    "oid": "8d6ae33ac51e8878e441e83256da313dc9fb4227",
    "message": "KAFKA-10192: Increase max time to wait for worker to start in some integration tests (#10118)\n\nAuthor: Luke Chen <showuon@gmail.com>\r\nReviewers: Chris Egerton <chrise@confluent.io>, Randall Hauch <rhauch@gmail.com>",
    "date": "2021-03-09T17:03:41Z",
    "url": "https://github.com/apache/kafka/commit/8d6ae33ac51e8878e441e83256da313dc9fb4227",
    "details": {
      "sha": "8268de2194499e2ba0f3f6af67671164e7e8ce6b",
      "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/BlockingConnectorTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/8d6ae33ac51e8878e441e83256da313dc9fb4227/connect%2Fruntime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fintegration%2FBlockingConnectorTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/8d6ae33ac51e8878e441e83256da313dc9fb4227/connect%2Fruntime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fintegration%2FBlockingConnectorTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fruntime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fintegration%2FBlockingConnectorTest.java?ref=8d6ae33ac51e8878e441e83256da313dc9fb4227",
      "patch": "@@ -77,7 +77,7 @@ public class BlockingConnectorTest {\n     private static final String NORMAL_CONNECTOR_NAME = \"normal-connector\";\n     private static final String TEST_TOPIC = \"normal-topic\";\n     private static final int NUM_RECORDS_PRODUCED = 100;\n-    private static final long CONNECT_WORKER_STARTUP_TIMEOUT = TimeUnit.SECONDS.toMillis(30);\n+    private static final long CONNECT_WORKER_STARTUP_TIMEOUT = TimeUnit.SECONDS.toMillis(60);\n     private static final long RECORD_TRANSFER_DURATION_MS = TimeUnit.SECONDS.toMillis(30);\n     private static final long REST_REQUEST_TIMEOUT = Worker.CONNECTOR_GRACEFUL_SHUTDOWN_TIMEOUT_MS * 2;\n ",
      "parent_sha": "0e8a84e5d7203765240894873fd2896906dfbc2b"
    }
  },
  {
    "oid": "cc4157d8773dc7f8b4fd771211bb24cc645e1341",
    "message": "KAFKA-7023: Move prepareForBulkLoad() call after customized RocksDBConfigSetter (#5166)\n\n*Summary\r\noptions.prepareForBulkLoad() and then use the configs from the customized customized RocksDBConfigSetter. This may overwrite the configs set in prepareBulkLoad call. The fix is to move prepareBulkLoad call after applying configs customized RocksDBConfigSetter.\r\n\r\nReviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Bill Bejeck <bill@confluent.io>",
    "date": "2018-06-11T20:32:03Z",
    "url": "https://github.com/apache/kafka/commit/cc4157d8773dc7f8b4fd771211bb24cc645e1341",
    "details": {
      "sha": "6084ecbf1e0ea5dde2b126d66d14934452a68648",
      "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java",
      "status": "modified",
      "additions": 5,
      "deletions": 4,
      "changes": 9,
      "blob_url": "https://github.com/apache/kafka/blob/cc4157d8773dc7f8b4fd771211bb24cc645e1341/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fstate%2Finternals%2FRocksDBStore.java",
      "raw_url": "https://github.com/apache/kafka/raw/cc4157d8773dc7f8b4fd771211bb24cc645e1341/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fstate%2Finternals%2FRocksDBStore.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fstate%2Finternals%2FRocksDBStore.java?ref=cc4157d8773dc7f8b4fd771211bb24cc645e1341",
      "patch": "@@ -130,10 +130,6 @@ public void openDB(final ProcessorContext context) {\n         // (this could be a bug in the RocksDB code and their devs have been contacted).\n         options.setIncreaseParallelism(Math.max(Runtime.getRuntime().availableProcessors(), 2));\n \n-        if (prepareForBulkload) {\n-            options.prepareForBulkLoad();\n-        }\n-\n         wOptions = new WriteOptions();\n         wOptions.setDisableWAL(true);\n \n@@ -148,6 +144,11 @@ public void openDB(final ProcessorContext context) {\n             final RocksDBConfigSetter configSetter = Utils.newInstance(configSetterClass);\n             configSetter.setConfig(name, options, configs);\n         }\n+\n+        if (prepareForBulkload) {\n+            options.prepareForBulkLoad();\n+        }\n+\n         this.dbDir = new File(new File(context.stateDir(), parentDir), this.name);\n \n         try {",
      "parent_sha": "49db5a63c043b50c10c2dfd0648f8d74ee917b6a"
    }
  },
  {
    "oid": "517503db2616531b08ee4d08d39c0e1c0bd19e97",
    "message": "kafka-1797; (delta follow-up patch) add the serializer/deserializer api to the new java client; patched by Jun Rao; reviewed by Neha Narkhede",
    "date": "2015-01-06T20:10:04Z",
    "url": "https://github.com/apache/kafka/commit/517503db2616531b08ee4d08d39c0e1c0bd19e97",
    "details": {
      "sha": "a61c56c01c6c6e229bda2a5bc73bdcbab84cb145",
      "filename": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
      "status": "modified",
      "additions": 6,
      "deletions": 2,
      "changes": 8,
      "blob_url": "https://github.com/apache/kafka/blob/517503db2616531b08ee4d08d39c0e1c0bd19e97/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java",
      "raw_url": "https://github.com/apache/kafka/raw/517503db2616531b08ee4d08d39c0e1c0bd19e97/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java?ref=517503db2616531b08ee4d08d39c0e1c0bd19e97",
      "patch": "@@ -76,6 +76,7 @@ public class KafkaProducer<K,V> implements Producer<K,V> {\n     private final Time time;\n     private final Serializer<K> keySerializer;\n     private final Serializer<V> valueSerializer;\n+    private final ProducerConfig producerConfig;\n \n     /**\n      * A producer is instantiated by providing a set of key-value pairs as configuration. Valid configuration strings\n@@ -152,6 +153,7 @@ private static Properties addSerializerToConfig(Properties properties,\n \n     private KafkaProducer(ProducerConfig config, Serializer<K> keySerializer, Serializer<V> valueSerializer) {\n         log.trace(\"Starting the Kafka producer\");\n+        this.producerConfig = config;\n         this.time = new SystemTime();\n         MetricConfig metricConfig = new MetricConfig().samples(config.getInt(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG))\n                                                       .timeWindow(config.getLong(ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG),\n@@ -307,14 +309,16 @@ public Future<RecordMetadata> send(ProducerRecord<K,V> record, Callback callback\n                 serializedKey = keySerializer.serialize(record.topic(), record.key());\n             } catch (ClassCastException cce) {\n                 throw new SerializationException(\"Can't convert key of class \" + record.key().getClass().getName() +\n-                        \" to the one specified in key.serializer\");\n+                        \" to class \" + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +\n+                        \" specified in key.serializer\");\n             }\n             byte[] serializedValue;\n             try {\n                 serializedValue = valueSerializer.serialize(record.topic(), record.value());\n             } catch (ClassCastException cce) {\n                 throw new SerializationException(\"Can't convert value of class \" + record.value().getClass().getName() +\n-                        \" to the one specified in value.serializer\");\n+                        \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +\n+                        \" specified in value.serializer\");\n             }\n             ProducerRecord serializedRecord = new ProducerRecord<byte[], byte[]>(record.topic(), record.partition(), serializedKey, serializedValue);\n             int partition = partitioner.partition(serializedRecord, metadata.fetch());",
      "parent_sha": "50b734690a93e58030f69cede8d0a84d1e3f5461"
    }
  },
  {
    "oid": "861fe68cee5ae052531a34a7c40196d2ffdda055",
    "message": "TRIVIAL: fix typo",
    "date": "2024-03-08T21:52:33Z",
    "url": "https://github.com/apache/kafka/commit/861fe68cee5ae052531a34a7c40196d2ffdda055",
    "details": {
      "sha": "9616ed81aac32ef244cbd22dc3eec5f6f8dee5e0",
      "filename": "streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/861fe68cee5ae052531a34a7c40196d2ffdda055/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FStreamsConfig.java",
      "raw_url": "https://github.com/apache/kafka/raw/861fe68cee5ae052531a34a7c40196d2ffdda055/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FStreamsConfig.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FStreamsConfig.java?ref=861fe68cee5ae052531a34a7c40196d2ffdda055",
      "patch": "@@ -519,7 +519,7 @@ public class StreamsConfig extends AbstractConfig {\n     /** {@code enable.metrics.push} */\n     @SuppressWarnings(\"WeakerAccess\")\n     public static  final String ENABLE_METRICS_PUSH_CONFIG = CommonClientConfigs.ENABLE_METRICS_PUSH_CONFIG;\n-    public static final String ENABLE_METRICS_PUSH_DOC = \"Whether to enable pushing of internal [main-|restore-|global]consumer, producer, and admin client metrics to the cluster, if the cluster has a client metrics subscription which matches a client.\";\n+    public static final String ENABLE_METRICS_PUSH_DOC = \"Whether to enable pushing of internal [main-|restore-|global-]consumer, producer, and admin client metrics to the cluster, if the cluster has a client metrics subscription which matches a client.\";\n \n     /** {@code commit.interval.ms} */\n     @SuppressWarnings(\"WeakerAccess\")",
      "parent_sha": "414365979e960d0705f955579836faff6881f46a"
    }
  },
  {
    "oid": "bc47e9d6ca976ba3c15249500b2bb6f6355816bc",
    "message": "KAFKA-5491; Enable transactions in ProducerPerformance Tool\n\nWith this patch, the `ProducePerfomance` tool can create transactions of differing durations.\n\nThis patch was used to to collect the initial set of benchmarks for transaction performance, documented here: https://docs.google.com/spreadsheets/d/1dHY6M7qCiX-NFvsgvaE0YoVdNq26uA8608XIh_DUpI4/edit#gid=282787170\n\nAuthor: Apurva Mehta <apurva@confluent.io>\n\nReviewers: Jun Rao <junrao@gmail.com>\n\nCloses #3400 from apurvam/MINOR-add-transaction-size-to-producre-perf",
    "date": "2017-06-21T21:41:51Z",
    "url": "https://github.com/apache/kafka/commit/bc47e9d6ca976ba3c15249500b2bb6f6355816bc",
    "details": {
      "sha": "0436d67080dec80bf1bc8f30c4f5a7ab14cb315c",
      "filename": "tools/src/main/java/org/apache/kafka/tools/ProducerPerformance.java",
      "status": "modified",
      "additions": 47,
      "deletions": 1,
      "changes": 48,
      "blob_url": "https://github.com/apache/kafka/blob/bc47e9d6ca976ba3c15249500b2bb6f6355816bc/tools%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Ftools%2FProducerPerformance.java",
      "raw_url": "https://github.com/apache/kafka/raw/bc47e9d6ca976ba3c15249500b2bb6f6355816bc/tools%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Ftools%2FProducerPerformance.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/tools%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Ftools%2FProducerPerformance.java?ref=bc47e9d6ca976ba3c15249500b2bb6f6355816bc",
      "patch": "@@ -59,7 +59,10 @@ public static void main(String[] args) throws Exception {\n             List<String> producerProps = res.getList(\"producerConfig\");\n             String producerConfig = res.getString(\"producerConfigFile\");\n             String payloadFilePath = res.getString(\"payloadFile\");\n+            String transactionalId = res.getString(\"transactionalId\");\n             boolean shouldPrintMetrics = res.getBoolean(\"printMetrics\");\n+            long transactionDurationMs = res.getLong(\"transactionDurationMs\");\n+            boolean transactionsEnabled =  0 < transactionDurationMs;\n \n             // since default value gets printed with the help text, we are escaping \\n there and replacing it with correct value here.\n             String payloadDelimiter = res.getString(\"payloadDelimiter\").equals(\"\\\\n\") ? \"\\n\" : res.getString(\"payloadDelimiter\");\n@@ -99,7 +102,13 @@ public static void main(String[] args) throws Exception {\n \n             props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n             props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.ByteArraySerializer\");\n-            KafkaProducer<byte[], byte[]> producer = new KafkaProducer<byte[], byte[]>(props);\n+            if (transactionsEnabled)\n+                props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);\n+\n+            KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(props);\n+\n+            if (transactionsEnabled)\n+                producer.initTransactions();\n \n             /* setup perf test */\n             byte[] payload = null;\n@@ -114,7 +123,16 @@ public static void main(String[] args) throws Exception {\n             long startMs = System.currentTimeMillis();\n \n             ThroughputThrottler throttler = new ThroughputThrottler(throughput, startMs);\n+\n+            int currentTransactionSize = 0;\n+            long transactionStartTime = 0;\n             for (int i = 0; i < numRecords; i++) {\n+                if (transactionsEnabled && currentTransactionSize == 0) {\n+                    producer.beginTransaction();\n+                    transactionStartTime = System.currentTimeMillis();\n+                }\n+\n+\n                 if (payloadFilePath != null) {\n                     payload = payloadByteList.get(random.nextInt(payloadByteList.size()));\n                 }\n@@ -124,11 +142,20 @@ record = new ProducerRecord<>(topicName, payload);\n                 Callback cb = stats.nextCompletion(sendStartMs, payload.length, stats);\n                 producer.send(record, cb);\n \n+                currentTransactionSize++;\n+                if (transactionsEnabled && transactionDurationMs <= (sendStartMs - transactionStartTime)) {\n+                    producer.commitTransaction();\n+                    currentTransactionSize = 0;\n+                }\n+\n                 if (throttler.shouldThrottle(i, sendStartMs)) {\n                     throttler.throttle();\n                 }\n             }\n \n+            if (transactionsEnabled && currentTransactionSize != 0)\n+                producer.commitTransaction();\n+\n             if (!shouldPrintMetrics) {\n                 producer.close();\n \n@@ -246,6 +273,25 @@ private static ArgumentParser argParser() {\n                 .dest(\"printMetrics\")\n                 .help(\"print out metrics at the end of the test.\");\n \n+        parser.addArgument(\"--transactional-id\")\n+               .action(store())\n+               .required(false)\n+               .type(String.class)\n+               .metavar(\"TRANSACTIONAL-ID\")\n+               .dest(\"transactionalId\")\n+               .setDefault(\"performance-producer-default-transactional-id\")\n+               .help(\"The transactionalId to use if transaction-duration-ms is > 0. Useful when testing the performance of concurrent transactions.\");\n+\n+        parser.addArgument(\"--transaction-duration-ms\")\n+               .action(store())\n+               .required(false)\n+               .type(Long.class)\n+               .metavar(\"TRANSACTION-DURATION\")\n+               .dest(\"transactionDurationMs\")\n+               .setDefault(0L)\n+               .help(\"The max age of each transaction. The commitTransaction will be called after this this time has elapsed. Transactions are only enabled if this value is positive.\");\n+\n+\n         return parser;\n     }\n ",
      "parent_sha": "96587f4b1ffd372d3e4f9a1fba6fc1d2f84a191d"
    }
  },
  {
    "oid": "732bffcae6ad049d894e4dffe1907e8ceeb74a60",
    "message": "KAFKA-15569: test and add test cases in IQv2StoreIntegrationTest (#14523)\n\nReviewers: Matthias J. Sax <matthias@confluent.io>",
    "date": "2023-10-17T00:30:05Z",
    "url": "https://github.com/apache/kafka/commit/732bffcae6ad049d894e4dffe1907e8ceeb74a60",
    "details": {
      "sha": "26d868962966db758cf6a7eba756fc32c609269b",
      "filename": "streams/src/test/java/org/apache/kafka/streams/integration/IQv2StoreIntegrationTest.java",
      "status": "modified",
      "additions": 576,
      "deletions": 75,
      "changes": 651,
      "blob_url": "https://github.com/apache/kafka/blob/732bffcae6ad049d894e4dffe1907e8ceeb74a60/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FIQv2StoreIntegrationTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/732bffcae6ad049d894e4dffe1907e8ceeb74a60/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FIQv2StoreIntegrationTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FIQv2StoreIntegrationTest.java?ref=732bffcae6ad049d894e4dffe1907e8ceeb74a60",
      "patch": "@@ -140,7 +140,7 @@ public class IQv2StoreIntegrationTest {\n \n     public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(NUM_BROKERS);\n     private static final Position POSITION_0 =\n-        Position.fromMap(mkMap(mkEntry(INPUT_TOPIC_NAME, mkMap(mkEntry(0, 1L)))));\n+        Position.fromMap(mkMap(mkEntry(INPUT_TOPIC_NAME, mkMap(mkEntry(0, 5L)))));\n \n     public static class UnknownQuery implements Query<Void> { }\n \n@@ -408,13 +408,15 @@ public static void before()\n \n         final List<Future<RecordMetadata>> futures = new LinkedList<>();\n         try (final Producer<Integer, Integer> producer = new KafkaProducer<>(producerProps)) {\n-            for (int i = 0; i < 4; i++) {\n+            for (int i = 0; i < 10; i++) {\n+                final int key = i / 2;\n+                final int partition = key % partitions;\n                 final Future<RecordMetadata> send = producer.send(\n                     new ProducerRecord<>(\n                         INPUT_TOPIC_NAME,\n-                        i % partitions,\n-                        RECORD_TIME,\n-                        i,\n+                        partition,\n+                        WINDOW_START + Duration.ofMinutes(2).toMillis() * i,\n+                        key,\n                         i,\n                         null\n                     )\n@@ -438,8 +440,8 @@ public static void before()\n         assertThat(INPUT_POSITION, equalTo(\n             Position\n                 .emptyPosition()\n-                .withComponent(INPUT_TOPIC_NAME, 0, 1L)\n-                .withComponent(INPUT_TOPIC_NAME, 1, 1L)\n+                .withComponent(INPUT_TOPIC_NAME, 0, 5L)\n+                .withComponent(INPUT_TOPIC_NAME, 1, 3L)\n         ));\n     }\n \n@@ -650,12 +652,13 @@ private void setUpWindowPAPITopology(final WindowBytesStoreSupplier supplier,\n                 public void process(final Record<Integer, Integer> record) {\n                     final TimestampedWindowStore<Integer, Integer> stateStore =\n                         context().getStateStore(windowStoreStoreBuilder.name());\n+                    // We don't re-implement the DSL logic (which implements sum) but instead just keep the lasted value per window\n                     stateStore.put(\n                         record.key(),\n                         ValueAndTimestamp.make(\n                             record.value(), record.timestamp()\n                         ),\n-                        WINDOW_START\n+                        (record.timestamp() / WINDOW_SIZE.toMillis()) * WINDOW_SIZE.toMillis()\n                     );\n                 }\n             };\n@@ -671,7 +674,8 @@ public void process(final Record<Integer, Integer> record) {\n                     public void process(final Record<Integer, Integer> record) {\n                         final WindowStore<Integer, Integer> stateStore =\n                             context().getStateStore(windowStoreStoreBuilder.name());\n-                        stateStore.put(record.key(), record.value(), WINDOW_START);\n+                        // We don't re-implement the DSL logic (which implements sum) but instead just keep the lasted value per window\n+                        stateStore.put(record.key(), record.value(), (record.timestamp() / WINDOW_SIZE.toMillis()) * WINDOW_SIZE.toMillis());\n                     }\n                 };\n         }\n@@ -716,7 +720,9 @@ public void process(final Record<Integer, Integer> record) {\n                 final SessionStore<Integer, Integer> stateStore =\n                     context().getStateStore(sessionStoreStoreBuilder.name());\n                 stateStore.put(\n-                    new Windowed<>(record.key(), new SessionWindow(WINDOW_START, WINDOW_START)),\n+                    // we do not re-implement the actual session-window logic from the DSL here to keep the test simple,\n+                    // but instead just put each record into it's own session\n+                    new Windowed<>(record.key(), new SessionWindow(record.timestamp(), record.timestamp())),\n                     record.value()\n                 );\n             }\n@@ -772,37 +778,61 @@ public void verifyStore() {\n                 shouldRejectUnknownQuery();\n                 shouldCollectExecutionInfo();\n                 shouldCollectExecutionInfoUnderFailure();\n-\n+                final String kind = this.kind;\n                 if (storeToTest.keyValue()) {\n                     if (storeToTest.timestamped()) {\n                         final Function<ValueAndTimestamp<Integer>, Integer> valueExtractor =\n                             ValueAndTimestamp::value;\n-                        shouldHandleKeyQuery(2, valueExtractor, 2);\n-                        shouldHandleRangeQueries(valueExtractor);\n+                        if (kind.equals(\"DSL\")) {\n+                            shouldHandleKeyQuery(2, valueExtractor, 5);\n+                            shouldHandleRangeDSLQueries(valueExtractor);\n+                        } else {\n+                            shouldHandleKeyQuery(2, valueExtractor, 5);\n+                            shouldHandleRangePAPIQueries(valueExtractor);\n+                        }\n                     } else {\n                         final Function<Integer, Integer> valueExtractor = Function.identity();\n-                        shouldHandleKeyQuery(2, valueExtractor, 2);\n-                        shouldHandleRangeQueries(valueExtractor);\n+                        if (kind.equals(\"DSL\")) {\n+                            shouldHandleKeyQuery(2, valueExtractor, 5);\n+                            shouldHandleRangeDSLQueries(valueExtractor);\n+                        } else {\n+                            shouldHandleKeyQuery(2, valueExtractor, 5);\n+                            shouldHandleRangePAPIQueries(valueExtractor);\n+                        }\n                     }\n                 }\n \n                 if (storeToTest.isWindowed()) {\n                     if (storeToTest.timestamped()) {\n                         final Function<ValueAndTimestamp<Integer>, Integer> valueExtractor =\n                             ValueAndTimestamp::value;\n-                        shouldHandleWindowKeyQueries(valueExtractor);\n-                        shouldHandleWindowRangeQueries(valueExtractor);\n+                        if (kind.equals(\"DSL\")) {\n+                            shouldHandleWindowKeyDSLQueries(valueExtractor);\n+                            shouldHandleWindowRangeDSLQueries(valueExtractor);\n+                        } else {\n+                            shouldHandleWindowKeyPAPIQueries(valueExtractor);\n+                            shouldHandleWindowRangePAPIQueries(valueExtractor);\n+                        }\n                     } else {\n                         final Function<Integer, Integer> valueExtractor = Function.identity();\n-                        shouldHandleWindowKeyQueries(valueExtractor);\n-                        shouldHandleWindowRangeQueries(valueExtractor);\n+                        if (kind.equals(\"DSL\")) {\n+                            shouldHandleWindowKeyDSLQueries(valueExtractor);\n+                            shouldHandleWindowRangeDSLQueries(valueExtractor);\n+                        } else {\n+                            shouldHandleWindowKeyPAPIQueries(valueExtractor);\n+                            shouldHandleWindowRangePAPIQueries(valueExtractor);\n+                        }\n                     }\n                 }\n \n                 if (storeToTest.isSession()) {\n                     // Note there's no \"timestamped\" differentiation here.\n                     // Idiosyncratically, SessionStores are _never_ timestamped.\n-                    shouldHandleSessionKeyQueries();\n+                    if (kind.equals(\"DSL\")) {\n+                        shouldHandleSessionKeyDSLQueries();\n+                    } else {\n+                        shouldHandleSessionKeyPAPIQueries();\n+                    }\n                 }\n             }\n         } catch (final AssertionError e) {\n@@ -812,57 +842,211 @@ public void verifyStore() {\n     }\n \n \n-    private <T> void shouldHandleRangeQueries(final Function<T, Integer> extractor) {\n+    private <T> void shouldHandleRangeDSLQueries(final Function<T, Integer> extractor) {\n+        shouldHandleRangeQuery(\n+            Optional.of(0),\n+            Optional.of(4),\n+            extractor,\n+            mkSet(1, 3, 5, 7, 9)\n+        );\n+\n         shouldHandleRangeQuery(\n             Optional.of(1),\n             Optional.of(3),\n             extractor,\n-            mkSet(1, 2, 3)\n-\n+            mkSet(3, 5, 7)\n         );\n+\n         shouldHandleRangeQuery(\n-            Optional.of(1),\n+            Optional.of(3),\n             Optional.empty(),\n             extractor,\n-            mkSet(1, 2, 3)\n+            mkSet(7, 9)\n+        );\n \n+        shouldHandleRangeQuery(\n+            Optional.empty(),\n+            Optional.of(3),\n+            extractor,\n+            mkSet(1, 3, 5, 7)\n         );\n+\n         shouldHandleRangeQuery(\n             Optional.empty(),\n-            Optional.of(1),\n+            Optional.empty(),\n             extractor,\n-            mkSet(0, 1)\n+            mkSet(1, 3, 5, 7, 9)\n+        );\n+    }\n+\n+    private <T> void shouldHandleRangePAPIQueries(final Function<T, Integer> extractor) {\n+        shouldHandleRangeQuery(\n+            Optional.of(0),\n+            Optional.of(4),\n+            extractor,\n+            mkSet(1, 3, 5, 7, 9)\n+        );\n \n+        shouldHandleRangeQuery(\n+            Optional.of(1),\n+            Optional.of(3),\n+            extractor,\n+            mkSet(3, 5, 7)\n         );\n+\n         shouldHandleRangeQuery(\n+            Optional.of(3),\n             Optional.empty(),\n+            extractor,\n+            mkSet(7, 9)\n+        );\n+\n+        shouldHandleRangeQuery(\n             Optional.empty(),\n+            Optional.of(3),\n             extractor,\n-            mkSet(0, 1, 2, 3)\n+            mkSet(1, 3, 5, 7)\n+        );\n \n+        shouldHandleRangeQuery(\n+            Optional.empty(),\n+            Optional.empty(),\n+            extractor,\n+            mkSet(1, 3, 5, 7, 9)\n         );\n     }\n \n-    private <T> void shouldHandleWindowKeyQueries(final Function<T, Integer> extractor) {\n+    private <T> void shouldHandleWindowKeyDSLQueries(final Function<T, Integer> extractor) {\n \n         // tightest possible start range\n+        shouldHandleWindowKeyQuery(\n+            0,\n+            Instant.ofEpochMilli(WINDOW_START),\n+            Instant.ofEpochMilli(WINDOW_START),\n+            extractor,\n+            mkSet(1)\n+        );\n+\n+        // miss the window start range\n+        shouldHandleWindowKeyQuery(\n+            0,\n+            Instant.ofEpochMilli(WINDOW_START - 1),\n+            Instant.ofEpochMilli(WINDOW_START - 1),\n+            extractor,\n+            mkSet()\n+        );\n+\n+        // do the window key query at the first window and the key of record which we want to query is 2\n         shouldHandleWindowKeyQuery(\n             2,\n             Instant.ofEpochMilli(WINDOW_START),\n             Instant.ofEpochMilli(WINDOW_START),\n             extractor,\n+            mkSet()\n+        );\n+\n+        // miss the key\n+        shouldHandleWindowKeyQuery(\n+            999,\n+            Instant.ofEpochMilli(WINDOW_START),\n+            Instant.ofEpochMilli(WINDOW_START),\n+            extractor,\n+            mkSet()\n+        );\n+\n+        // miss both\n+        shouldHandleWindowKeyQuery(\n+            999,\n+            Instant.ofEpochMilli(WINDOW_START - 1),\n+            Instant.ofEpochMilli(WINDOW_START - 1),\n+            extractor,\n+            mkSet()\n+        );\n+\n+        // do the window key query at the first and the second windows and the key of record which we want to query is 0\n+        shouldHandleWindowKeyQuery(\n+            0,\n+            Instant.ofEpochMilli(WINDOW_START),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(5).toMillis()),\n+            extractor,\n+            mkSet(1)\n+        );\n+\n+        // do the window key query at the first window and the key of record which we want to query is 1\n+        shouldHandleWindowKeyQuery(\n+            1,\n+            Instant.ofEpochMilli(WINDOW_START),\n+            Instant.ofEpochMilli(WINDOW_START),\n+            extractor,\n             mkSet(2)\n         );\n \n-        // miss the window start range\n+        // do the window key query at the second and the third windows and the key of record which we want to query is 2\n         shouldHandleWindowKeyQuery(\n             2,\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(5).toMillis()),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(10).toMillis()),\n+            extractor,\n+            mkSet(4, 5)\n+        );\n+\n+        // do the window key query at the second and the third windows and the key of record which we want to query is 3\n+        shouldHandleWindowKeyQuery(\n+            3,\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(5).toMillis()),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(10).toMillis()),\n+            extractor,\n+            mkSet(13)\n+        );\n+\n+        // do the window key query at the fourth and the fifth windows and the key of record which we want to query is 4\n+        shouldHandleWindowKeyQuery(\n+            4,\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(15).toMillis()),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(20).toMillis()),\n+            extractor,\n+            mkSet(17)\n+        );\n+\n+        // do the window key query at the fifth window and the key of record which we want to query is 4\n+        shouldHandleWindowKeyQuery(\n+            4,\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(20).toMillis()),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(24).toMillis()),\n+            extractor,\n+            mkSet()\n+        );\n+    }\n+\n+    private <T> void shouldHandleWindowKeyPAPIQueries(final Function<T, Integer> extractor) {\n+\n+        // tightest possible start range\n+        shouldHandleWindowKeyQuery(\n+            0,\n+            Instant.ofEpochMilli(WINDOW_START),\n+            Instant.ofEpochMilli(WINDOW_START),\n+            extractor,\n+            mkSet(1)\n+        );\n+\n+        // miss the window start range\n+        shouldHandleWindowKeyQuery(\n+            0,\n             Instant.ofEpochMilli(WINDOW_START - 1),\n             Instant.ofEpochMilli(WINDOW_START - 1),\n             extractor,\n             mkSet()\n         );\n \n+        // do the window key query at the first window and the key of record which we want to query is 2\n+        shouldHandleWindowKeyQuery(\n+            2,\n+            Instant.ofEpochMilli(WINDOW_START),\n+            Instant.ofEpochMilli(WINDOW_START),\n+            extractor,\n+            mkSet()\n+        );\n+\n         // miss the key\n         shouldHandleWindowKeyQuery(\n             999,\n@@ -880,19 +1064,179 @@ private <T> void shouldHandleWindowKeyQueries(final Function<T, Integer> extract\n             extractor,\n             mkSet()\n         );\n+\n+        // do the window key query at the first and the second windows and the key of record which we want to query is 0\n+        shouldHandleWindowKeyQuery(\n+            0,\n+            Instant.ofEpochMilli(WINDOW_START),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(5).toMillis()),\n+            extractor,\n+            mkSet(1)\n+        );\n+\n+        // do the window key query at the first window and the key of record which we want to query is 1\n+        shouldHandleWindowKeyQuery(\n+            1,\n+            Instant.ofEpochMilli(WINDOW_START),\n+            Instant.ofEpochMilli(WINDOW_START),\n+            extractor,\n+            mkSet(2)\n+        );\n+\n+        // do the window key query at the second and the third windows and the key of record which we want to query is 2\n+        shouldHandleWindowKeyQuery(\n+            2,\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(5).toMillis()),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(10).toMillis()),\n+            extractor,\n+            mkSet(4, 5)\n+        );\n+\n+        // do the window key query at the second and the third windows and the key of record which we want to query is 3\n+        shouldHandleWindowKeyQuery(\n+            3,\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(5).toMillis()),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(10).toMillis()),\n+            extractor,\n+            mkSet(7)\n+        );\n+\n+        // do the window key query at the fourth and the fifth windows and the key of record which we want to query is 4\n+        shouldHandleWindowKeyQuery(\n+            4,\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(15).toMillis()),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(20).toMillis()),\n+            extractor,\n+            mkSet(9)\n+        );\n+\n+        // do the window key query at the fifth window and the key of record which we want to query is 4\n+        shouldHandleWindowKeyQuery(\n+            4,\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(20).toMillis()),\n+            Instant.ofEpochMilli(WINDOW_START + Duration.ofMinutes(24).toMillis()),\n+            extractor,\n+            mkSet()\n+        );\n     }\n \n-    private <T> void shouldHandleWindowRangeQueries(final Function<T, Integer> extractor) {\n+    private <T> void shouldHandleWindowRangeDSLQueries(final Function<T, Integer> extractor) {\n         final long windowSize = WINDOW_SIZE.toMillis();\n         final long windowStart = (RECORD_TIME / windowSize) * windowSize;\n \n+        // miss the window start\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart - 1),\n+            Instant.ofEpochMilli(windowStart - 1),\n+            extractor,\n+            mkSet()\n+        );\n+\n+        // do the query at the first window\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart),\n+            Instant.ofEpochMilli(windowStart),\n+            extractor,\n+            mkSet(1, 2)\n+        );\n+\n+        // do the query at the first and the second windows\n         shouldHandleWindowRangeQuery(\n             Instant.ofEpochMilli(windowStart),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(5).toMillis()),\n+            extractor,\n+            mkSet(1, 2, 3, 4)\n+        );\n+\n+        // do the query at the second and the third windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(5).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(10).toMillis()),\n+            extractor,\n+            mkSet(3, 4, 5, 13)\n+        );\n+\n+        // do the query at the third and the fourth windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(10).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(15).toMillis()),\n+            extractor,\n+            mkSet(17, 5, 13)\n+        );\n+\n+        // do the query at the fourth and the fifth windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(15).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(20).toMillis()),\n+            extractor,\n+            mkSet(17)\n+        );\n+\n+        //do the query at the fifth and the sixth windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(20).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(25).toMillis()),\n+            extractor,\n+            mkSet()\n+        );\n+\n+        // do the query from the second to the fourth windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(5).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(15).toMillis()),\n+            extractor,\n+            mkSet(17, 3, 4, 5, 13)\n+        );\n+\n+        // do the query from the first to the fourth windows\n+        shouldHandleWindowRangeQuery(\n             Instant.ofEpochMilli(windowStart),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(15).toMillis()),\n             extractor,\n-            mkSet(0, 1, 2, 3)\n+            mkSet(1, 17, 2, 3, 4, 5, 13)\n         );\n \n+        // Should fail to execute this query on a WindowStore.\n+        final WindowRangeQuery<Integer, T> query = WindowRangeQuery.withKey(2);\n+\n+        final StateQueryRequest<KeyValueIterator<Windowed<Integer>, T>> request =\n+            inStore(STORE_NAME)\n+                .withQuery(query)\n+                .withPartitions(mkSet(0, 1))\n+                .withPositionBound(PositionBound.at(INPUT_POSITION));\n+\n+        final StateQueryResult<KeyValueIterator<Windowed<Integer>, T>> result =\n+            IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n+\n+        if (result.getGlobalResult() != null) {\n+            fail(\"global tables aren't implemented\");\n+        } else {\n+            final Map<Integer, QueryResult<KeyValueIterator<Windowed<Integer>, T>>> queryResult =\n+                result.getPartitionResults();\n+            for (final int partition : queryResult.keySet()) {\n+                final QueryResult<KeyValueIterator<Windowed<Integer>, T>> partitionResult =\n+                    queryResult.get(partition);\n+                final boolean failure = partitionResult.isFailure();\n+                if (!failure) {\n+                    throw new AssertionError(queryResult.toString());\n+                }\n+                assertThat(partitionResult.getFailureReason(), is(FailureReason.UNKNOWN_QUERY_TYPE));\n+                assertThat(partitionResult.getFailureMessage(), matchesPattern(\n+                    \"This store\"\n+                        + \" \\\\(class org.apache.kafka.streams.state.internals.Metered.*WindowStore\\\\)\"\n+                        + \" doesn't know how to execute the given query\"\n+                        + \" \\\\(WindowRangeQuery\\\\{key=Optional\\\\[2], timeFrom=Optional.empty, timeTo=Optional.empty}\\\\)\"\n+                        + \" because WindowStores only supports WindowRangeQuery.withWindowStartRange\\\\.\"\n+                        + \" Contact the store maintainer if you need support for a new query type\\\\.\"\n+                ));\n+            }\n+        }\n+    }\n+\n+    private <T> void shouldHandleWindowRangePAPIQueries(final Function<T, Integer> extractor) {\n+        final long windowSize = WINDOW_SIZE.toMillis();\n+        final long windowStart = (RECORD_TIME / windowSize) * windowSize;\n+\n         // miss the window start\n         shouldHandleWindowRangeQuery(\n             Instant.ofEpochMilli(windowStart - 1),\n@@ -901,17 +1245,81 @@ private <T> void shouldHandleWindowRangeQueries(final Function<T, Integer> extra\n             mkSet()\n         );\n \n+        // do the query at the first window\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart),\n+            Instant.ofEpochMilli(windowStart),\n+            extractor,\n+            mkSet(1, 2)\n+        );\n+\n+        // do the query at the first and the second windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(5).toMillis()),\n+            extractor,\n+            mkSet(1, 2, 3, 4)\n+        );\n+\n+        // do the query at the second and the third windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(5).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(10).toMillis()),\n+            extractor,\n+            mkSet(3, 4, 5, 7)\n+        );\n+\n+        // do the query at the third and the fourth windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(10).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(15).toMillis()),\n+            extractor,\n+            mkSet(5, 7, 9)\n+        );\n+\n+        // do the query at the fourth and the fifth windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(15).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(20).toMillis()),\n+            extractor,\n+            mkSet(9)\n+        );\n+\n+        //do the query at the fifth and the sixth windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(20).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(25).toMillis()),\n+            extractor,\n+            mkSet()\n+        );\n+\n+        // do the query from the second to the fourth windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(5).toMillis()),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(15).toMillis()),\n+            extractor,\n+            mkSet(3, 4, 5, 7, 9)\n+        );\n+\n+        // do the query from the first to the fourth windows\n+        shouldHandleWindowRangeQuery(\n+            Instant.ofEpochMilli(windowStart),\n+            Instant.ofEpochMilli(windowStart + Duration.ofMinutes(15).toMillis()),\n+            extractor,\n+            mkSet(1, 2, 3, 4, 5, 7, 9)\n+        );\n+\n         // Should fail to execute this query on a WindowStore.\n         final WindowRangeQuery<Integer, T> query = WindowRangeQuery.withKey(2);\n \n         final StateQueryRequest<KeyValueIterator<Windowed<Integer>, T>> request =\n-                inStore(STORE_NAME)\n-                        .withQuery(query)\n-                        .withPartitions(mkSet(0, 1))\n-                        .withPositionBound(PositionBound.at(INPUT_POSITION));\n+            inStore(STORE_NAME)\n+                .withQuery(query)\n+                .withPartitions(mkSet(0, 1))\n+                .withPositionBound(PositionBound.at(INPUT_POSITION));\n \n         final StateQueryResult<KeyValueIterator<Windowed<Integer>, T>> result =\n-                IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n+            IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n \n         if (result.getGlobalResult() != null) {\n             fail(\"global tables aren't implemented\");\n@@ -938,10 +1346,103 @@ private <T> void shouldHandleWindowRangeQueries(final Function<T, Integer> extra\n         }\n     }\n \n-    private <T> void shouldHandleSessionKeyQueries() {\n+    private <T> void shouldHandleSessionKeyDSLQueries() {\n+        shouldHandleSessionRangeQuery(\n+            0,\n+            mkSet(1)\n+        );\n+\n+        shouldHandleSessionRangeQuery(\n+            1,\n+            mkSet(5)\n+        );\n+\n         shouldHandleSessionRangeQuery(\n             2,\n-            mkSet(2)\n+            mkSet(9)\n+        );\n+\n+        shouldHandleSessionRangeQuery(\n+            3,\n+            mkSet(13)\n+        );\n+\n+        shouldHandleSessionRangeQuery(\n+            4,\n+            mkSet(17)\n+        );\n+\n+        // not preset, so empty result iter\n+        shouldHandleSessionRangeQuery(\n+            999,\n+            mkSet()\n+        );\n+\n+        // Should fail to execute this query on a SessionStore.\n+        final WindowRangeQuery<Integer, T> query =\n+            WindowRangeQuery.withWindowStartRange(\n+                Instant.ofEpochMilli(0L),\n+                Instant.ofEpochMilli(0L)\n+            );\n+\n+        final StateQueryRequest<KeyValueIterator<Windowed<Integer>, T>> request =\n+            inStore(STORE_NAME)\n+                .withQuery(query)\n+                .withPartitions(mkSet(0, 1))\n+                .withPositionBound(PositionBound.at(INPUT_POSITION));\n+\n+        final StateQueryResult<KeyValueIterator<Windowed<Integer>, T>> result =\n+            IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n+\n+        if (result.getGlobalResult() != null) {\n+            fail(\"global tables aren't implemented\");\n+        } else {\n+            final Map<Integer, QueryResult<KeyValueIterator<Windowed<Integer>, T>>> queryResult =\n+                result.getPartitionResults();\n+            for (final int partition : queryResult.keySet()) {\n+                final QueryResult<KeyValueIterator<Windowed<Integer>, T>> partitionResult =\n+                    queryResult.get(partition);\n+                final boolean failure = partitionResult.isFailure();\n+                if (!failure) {\n+                    throw new AssertionError(queryResult.toString());\n+                }\n+                assertThat(partitionResult.getFailureReason(), is(FailureReason.UNKNOWN_QUERY_TYPE));\n+                assertThat(partitionResult.getFailureMessage(), is(\n+                    \"This store\"\n+                        + \" (class org.apache.kafka.streams.state.internals.MeteredSessionStore)\"\n+                        + \" doesn't know how to execute the given query\"\n+                        + \" (WindowRangeQuery{key=Optional.empty, timeFrom=Optional[1970-01-01T00:00:00Z], timeTo=Optional[1970-01-01T00:00:00Z]})\"\n+                        + \" because SessionStores only support WindowRangeQuery.withKey.\"\n+                        + \" Contact the store maintainer if you need support for a new query type.\"\n+                ));\n+            }\n+        }\n+    }\n+\n+    private <T> void shouldHandleSessionKeyPAPIQueries() {\n+        shouldHandleSessionRangeQuery(\n+            0,\n+            mkSet(0, 1)\n+        );\n+\n+        shouldHandleSessionRangeQuery(\n+            1,\n+            mkSet(2, 3)\n+        );\n+\n+        shouldHandleSessionRangeQuery(\n+            2,\n+            mkSet(4, 5)\n+        );\n+\n+        shouldHandleSessionRangeQuery(\n+            3,\n+            mkSet(6, 7)\n+        );\n+\n+        shouldHandleSessionRangeQuery(\n+            4,\n+            mkSet(8, 9)\n         );\n \n         // not preset, so empty result iter\n@@ -1008,7 +1509,7 @@ private void globalShouldRejectAllQueries() {\n         assertThat(\n             result.getGlobalResult().getFailureMessage(),\n             is(\"Global stores do not yet support the KafkaStreams#query API.\"\n-                   + \" Use KafkaStreams#store instead.\")\n+                + \" Use KafkaStreams#store instead.\")\n         );\n     }\n \n@@ -1048,19 +1549,19 @@ public void shouldRejectUnknownQuery() {\n     }\n \n     public <V> void shouldHandleKeyQuery(\n-            final Integer key,\n-            final Function<V, Integer> valueExtactor,\n-            final Integer expectedValue) {\n+        final Integer key,\n+        final Function<V, Integer> valueExtactor,\n+        final Integer expectedValue) {\n \n         final KeyQuery<Integer, V> query = KeyQuery.withKey(key);\n         final StateQueryRequest<V> request =\n-                inStore(STORE_NAME)\n-                        .withQuery(query)\n-                        .withPartitions(mkSet(0, 1))\n-                        .withPositionBound(PositionBound.at(INPUT_POSITION));\n+            inStore(STORE_NAME)\n+                .withQuery(query)\n+                .withPartitions(mkSet(0, 1))\n+                .withPositionBound(PositionBound.at(INPUT_POSITION));\n \n         final StateQueryResult<V> result =\n-                IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n+            IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n \n         final QueryResult<V> queryResult = result.getOnlyPartitionResult();\n         final boolean failure = queryResult.isFailure();\n@@ -1071,16 +1572,16 @@ public <V> void shouldHandleKeyQuery(\n \n         assertThrows(IllegalArgumentException.class, queryResult::getFailureReason);\n         assertThrows(\n-                IllegalArgumentException.class,\n-                queryResult::getFailureMessage\n+            IllegalArgumentException.class,\n+            queryResult::getFailureMessage\n         );\n \n         final V result1 = queryResult.getResult();\n         final Integer integer = valueExtactor.apply(result1);\n         assertThat(integer, is(expectedValue));\n         assertThat(queryResult.getExecutionInfo(), is(empty()));\n         assertThat(queryResult.getPosition(), is(POSITION_0));\n-    }   \n+    }\n \n     public <V> void shouldHandleRangeQuery(\n         final Optional<Integer> lower,\n@@ -1189,21 +1690,21 @@ public <V> void shouldHandleWindowKeyQuery(\n     }\n \n     public <V> void shouldHandleWindowRangeQuery(\n-            final Instant timeFrom,\n-            final Instant timeTo,\n-            final Function<V, Integer> valueExtactor,\n-            final Set<Integer> expectedValue) {\n+        final Instant timeFrom,\n+        final Instant timeTo,\n+        final Function<V, Integer> valueExtactor,\n+        final Set<Integer> expectedValue) {\n \n         final WindowRangeQuery<Integer, V> query = WindowRangeQuery.withWindowStartRange(timeFrom, timeTo);\n \n         final StateQueryRequest<KeyValueIterator<Windowed<Integer>, V>> request =\n-                inStore(STORE_NAME)\n-                        .withQuery(query)\n-                        .withPartitions(mkSet(0, 1))\n-                        .withPositionBound(PositionBound.at(INPUT_POSITION));\n+            inStore(STORE_NAME)\n+                .withQuery(query)\n+                .withPartitions(mkSet(0, 1))\n+                .withPositionBound(PositionBound.at(INPUT_POSITION));\n \n         final StateQueryResult<KeyValueIterator<Windowed<Integer>, V>> result =\n-                IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n+            IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n \n         if (result.getGlobalResult() != null) {\n             fail(\"global tables aren't implemented\");\n@@ -1218,12 +1719,12 @@ public <V> void shouldHandleWindowRangeQuery(\n                 assertThat(queryResult.get(partition).isSuccess(), is(true));\n \n                 assertThrows(\n-                        IllegalArgumentException.class,\n-                        queryResult.get(partition)::getFailureReason\n+                    IllegalArgumentException.class,\n+                    queryResult.get(partition)::getFailureReason\n                 );\n                 assertThrows(\n-                        IllegalArgumentException.class,\n-                        queryResult.get(partition)::getFailureMessage\n+                    IllegalArgumentException.class,\n+                    queryResult.get(partition)::getFailureMessage\n                 );\n \n                 try (final KeyValueIterator<Windowed<Integer>, V> iterator = queryResult.get(partition).getResult()) {\n@@ -1239,18 +1740,18 @@ public <V> void shouldHandleWindowRangeQuery(\n     }\n \n     public <V> void shouldHandleSessionRangeQuery(\n-            final Integer key,\n-            final Set<Integer> expectedValue) {\n+        final Integer key,\n+        final Set<Integer> expectedValue) {\n \n         final WindowRangeQuery<Integer, V> query = WindowRangeQuery.withKey(key);\n \n         final StateQueryRequest<KeyValueIterator<Windowed<Integer>, V>> request =\n-                inStore(STORE_NAME)\n-                        .withQuery(query)\n-                        .withPartitions(mkSet(0, 1))\n-                        .withPositionBound(PositionBound.at(INPUT_POSITION));\n+            inStore(STORE_NAME)\n+                .withQuery(query)\n+                .withPartitions(mkSet(0, 1))\n+                .withPositionBound(PositionBound.at(INPUT_POSITION));\n         final StateQueryResult<KeyValueIterator<Windowed<Integer>, V>> result =\n-                IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n+            IntegrationTestUtils.iqv2WaitForResult(kafkaStreams, request);\n \n         if (result.getGlobalResult() != null) {\n             fail(\"global tables aren't implemented\");\n@@ -1265,12 +1766,12 @@ public <V> void shouldHandleSessionRangeQuery(\n                 assertThat(queryResult.get(partition).isSuccess(), is(true));\n \n                 assertThrows(\n-                        IllegalArgumentException.class,\n-                        queryResult.get(partition)::getFailureReason\n+                    IllegalArgumentException.class,\n+                    queryResult.get(partition)::getFailureReason\n                 );\n                 assertThrows(\n-                        IllegalArgumentException.class,\n-                        queryResult.get(partition)::getFailureMessage\n+                    IllegalArgumentException.class,\n+                    queryResult.get(partition)::getFailureMessage\n                 );\n \n                 try (final KeyValueIterator<Windowed<Integer>, V> iterator = queryResult.get(partition).getResult()) {",
      "parent_sha": "da314ee48c31f85e99301c37f26710f67383e8de"
    }
  },
  {
    "oid": "ba774a09f45e1c97eeae2f0839fae9cd0303312c",
    "message": "KAFKA-8862: Improve Producer error message for failed metadata update (#18587)\n\nWe should provide the same informative error message for both timeout\r\ncases.\r\n\r\nReviewers: Kirk True <ktrue@confluent.io>, Andrew Schofield <aschofield@confluent.io>, Ismael Juma <ismael@juma.me.uk>",
    "date": "2025-01-21T16:37:45Z",
    "url": "https://github.com/apache/kafka/commit/ba774a09f45e1c97eeae2f0839fae9cd0303312c",
    "details": {
      "sha": "648de3ab4b90ada6d4fa44da3fde71a0ff27a86e",
      "filename": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
      "status": "modified",
      "additions": 9,
      "deletions": 7,
      "changes": 16,
      "blob_url": "https://github.com/apache/kafka/blob/ba774a09f45e1c97eeae2f0839fae9cd0303312c/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java",
      "raw_url": "https://github.com/apache/kafka/raw/ba774a09f45e1c97eeae2f0839fae9cd0303312c/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java?ref=ba774a09f45e1c97eeae2f0839fae9cd0303312c",
      "patch": "@@ -1104,8 +1104,7 @@ private ClusterAndWaitTime waitOnMetadata(String topic, Integer partition, long\n                 metadata.awaitUpdate(version, remainingWaitMs);\n             } catch (TimeoutException ex) {\n                 // Rethrow with original maxWaitMs to prevent logging exception with remainingWaitMs\n-                final String errorMessage = String.format(\"Topic %s not present in metadata after %d ms.\",\n-                        topic, maxWaitMs);\n+                final String errorMessage = getErrorMessage(partitionsCount, topic, partition, maxWaitMs);\n                 if (metadata.getError(topic) != null) {\n                     throw new TimeoutException(errorMessage, metadata.getError(topic).exception());\n                 }\n@@ -1114,11 +1113,7 @@ private ClusterAndWaitTime waitOnMetadata(String topic, Integer partition, long\n             cluster = metadata.fetch();\n             elapsed = time.milliseconds() - nowMs;\n             if (elapsed >= maxWaitMs) {\n-                final String errorMessage = partitionsCount == null ?\n-                        String.format(\"Topic %s not present in metadata after %d ms.\",\n-                                topic, maxWaitMs) :\n-                        String.format(\"Partition %d of topic %s with partition count %d is not present in metadata after %d ms.\",\n-                                partition, topic, partitionsCount, maxWaitMs);\n+                final String errorMessage = getErrorMessage(partitionsCount, topic, partition, maxWaitMs);\n                 if (metadata.getError(topic) != null && metadata.getError(topic).exception() instanceof RetriableException) {\n                     throw new TimeoutException(errorMessage, metadata.getError(topic).exception());\n                 }\n@@ -1134,6 +1129,13 @@ private ClusterAndWaitTime waitOnMetadata(String topic, Integer partition, long\n         return new ClusterAndWaitTime(cluster, elapsed);\n     }\n \n+    private String getErrorMessage(Integer partitionsCount, String topic, Integer partition, long maxWaitMs) {\n+        return partitionsCount == null ?\n+            String.format(\"Topic %s not present in metadata after %d ms.\",\n+                topic, maxWaitMs) :\n+            String.format(\"Partition %d of topic %s with partition count %d is not present in metadata after %d ms.\",\n+                partition, topic, partitionsCount, maxWaitMs);\n+    }\n     /**\n      * Validate that the record size isn't too large\n      */",
      "parent_sha": "faff2de6a5d10b79e1992d1e4357ae9c80c0b56d"
    }
  },
  {
    "oid": "8bb89c4beb6c21558bff6207dfa5f68729ac51bc",
    "message": "MINOR: Fix compilation issue in FileStreamSourceConnector (#12938)\n\nFix compilation failure introduced in https://github.com/apache/kafka/pull/12355.\r\n\r\nReviewers: Mickael Maison <mickael.maison@gmail.com>",
    "date": "2022-12-01T21:01:12Z",
    "url": "https://github.com/apache/kafka/commit/8bb89c4beb6c21558bff6207dfa5f68729ac51bc",
    "details": {
      "sha": "55090d889c99dd64b8865ee7b3b803816a9ffbc3",
      "filename": "connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceConnector.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/8bb89c4beb6c21558bff6207dfa5f68729ac51bc/connect%2Ffile%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Ffile%2FFileStreamSourceConnector.java",
      "raw_url": "https://github.com/apache/kafka/raw/8bb89c4beb6c21558bff6207dfa5f68729ac51bc/connect%2Ffile%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Ffile%2FFileStreamSourceConnector.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Ffile%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Ffile%2FFileStreamSourceConnector.java?ref=8bb89c4beb6c21558bff6207dfa5f68729ac51bc",
      "patch": "@@ -91,7 +91,7 @@ public ConfigDef config() {\n     @Override\n     public ExactlyOnceSupport exactlyOnceSupport(Map<String, String> props) {\n         AbstractConfig parsedConfig = new AbstractConfig(CONFIG_DEF, props);\n-        filename = parsedConfig.getString(FILE_CONFIG);\n+        String filename = parsedConfig.getString(FILE_CONFIG);\n         // We can provide exactly-once guarantees if reading from a \"real\" file\n         // (as long as the file is only appended to over the lifetime of the connector)\n         // If we're reading from stdin, we can't provide exactly-once guarantees",
      "parent_sha": "f5305fb38d8009bc90c5665a2d58830b6d3ddf59"
    }
  },
  {
    "oid": "efec315d0a79ef281f337d328cb51667814fa5b3",
    "message": "KAFKA-9018: Throw clearer exceptions on serialisation errors (#7496)\n\nImproved the exception messages that are thrown to indicate whether it was a key or value conversion problem.\r\n\r\nAuthor: Mario Molina <mmolimar@gmail.com>\r\nReviewer: Randall Hauch <rhauch@gmail.com>",
    "date": "2020-07-01T19:03:11Z",
    "url": "https://github.com/apache/kafka/commit/efec315d0a79ef281f337d328cb51667814fa5b3",
    "details": {
      "sha": "53dc254f737f6b0e0625a52323344ceeba605dfc",
      "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java",
      "status": "modified",
      "additions": 22,
      "deletions": 2,
      "changes": 24,
      "blob_url": "https://github.com/apache/kafka/blob/efec315d0a79ef281f337d328cb51667814fa5b3/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2FWorkerSinkTask.java",
      "raw_url": "https://github.com/apache/kafka/raw/efec315d0a79ef281f337d328cb51667814fa5b3/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2FWorkerSinkTask.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2FWorkerSinkTask.java?ref=efec315d0a79ef281f337d328cb51667814fa5b3",
      "patch": "@@ -489,10 +489,10 @@ private void convertMessages(ConsumerRecords<byte[], byte[]> msgs) {\n     }\n \n     private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]> msg) {\n-        SchemaAndValue keyAndSchema = retryWithToleranceOperator.execute(() -> keyConverter.toConnectData(msg.topic(), msg.headers(), msg.key()),\n+        SchemaAndValue keyAndSchema = retryWithToleranceOperator.execute(() -> convertKey(msg),\n                 Stage.KEY_CONVERTER, keyConverter.getClass());\n \n-        SchemaAndValue valueAndSchema = retryWithToleranceOperator.execute(() -> valueConverter.toConnectData(msg.topic(), msg.headers(), msg.value()),\n+        SchemaAndValue valueAndSchema = retryWithToleranceOperator.execute(() -> convertValue(msg),\n                 Stage.VALUE_CONVERTER, valueConverter.getClass());\n \n         Headers headers = retryWithToleranceOperator.execute(() -> convertHeadersFor(msg), Stage.HEADER_CONVERTER, headerConverter.getClass());\n@@ -524,6 +524,26 @@ private SinkRecord convertAndTransformRecord(final ConsumerRecord<byte[], byte[]\n         return new InternalSinkRecord(msg, transformedRecord);\n     }\n \n+    private SchemaAndValue convertKey(ConsumerRecord<byte[], byte[]> msg) {\n+        try {\n+            return keyConverter.toConnectData(msg.topic(), msg.headers(), msg.key());\n+        } catch (Exception e) {\n+            log.error(\"{} Error converting message key in topic '{}' partition {} at offset {} and timestamp {}: {}\",\n+                    this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp(), e.getMessage(), e);\n+            throw e;\n+        }\n+    }\n+\n+    private SchemaAndValue convertValue(ConsumerRecord<byte[], byte[]> msg) {\n+        try {\n+            return valueConverter.toConnectData(msg.topic(), msg.headers(), msg.value());\n+        } catch (Exception e) {\n+            log.error(\"{} Error converting message value in topic '{}' partition {} at offset {} and timestamp {}: {}\",\n+                    this, msg.topic(), msg.partition(), msg.offset(), msg.timestamp(), e.getMessage(), e);\n+            throw e;\n+        }\n+    }\n+\n     private Headers convertHeadersFor(ConsumerRecord<byte[], byte[]> record) {\n         Headers result = new ConnectHeaders();\n         org.apache.kafka.common.header.Headers recordHeaders = record.headers();",
      "parent_sha": "d1133bf7bff5a6f8c880953b0aff863020703887"
    }
  },
  {
    "oid": "d5366471d2c1a611f2241362041debc0118e5422",
    "message": "MINOR: Fix wrong comments\n\nAuthor: Yukun Guo <gyk.net@gmail.com>\n\nReviewers: Gwen Shapira\n\nCloses #1198 from gyk/fix-comment",
    "date": "2016-05-26T17:23:24Z",
    "url": "https://github.com/apache/kafka/commit/d5366471d2c1a611f2241362041debc0118e5422",
    "details": {
      "sha": "91697c12ca8b571343d575108efb4f8294b52419",
      "filename": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
      "status": "modified",
      "additions": 1,
      "deletions": 2,
      "changes": 3,
      "blob_url": "https://github.com/apache/kafka/blob/d5366471d2c1a611f2241362041debc0118e5422/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java",
      "raw_url": "https://github.com/apache/kafka/raw/d5366471d2c1a611f2241362041debc0118e5422/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java?ref=d5366471d2c1a611f2241362041debc0118e5422",
      "patch": "@@ -431,8 +431,7 @@ public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callbac\n     }\n \n     /**\n-     * Implementation of asynchronously send a record to a topic. Equivalent to <code>send(record, null)</code>.\n-     * See {@link #send(ProducerRecord, Callback)} for details.\n+     * Implementation of asynchronously send a record to a topic.\n      */\n     private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {\n         TopicPartition tp = null;",
      "parent_sha": "c699b1a914260b7d6fd84596f78128369d2bf2bf"
    }
  },
  {
    "oid": "20028e24cca91422b8f02fdbf45d2cd9ef24c901",
    "message": "KAFKA-14054: Handle TimeoutException gracefully (#13534)\n\nWe incorrectly assumed, that `consumer.position()` should always be\r\nserved by the consumer locally set position.\r\n\r\nHowever, within `commitNeeded()` we check if first `if(commitNeeded)`\r\nand thus go into the else only if we have not processed data (otherwise,\r\n`commitNeeded` would be true). For this reason, we actually don't know\r\nif the consumer has a valid position or not.\r\n\r\nWe should just swallow a timeout if the consumer cannot get the position\r\nfrom the broker, and try the next partition. If any position advances, we\r\ncan return true, and if we timeout for all partitions we can return\r\nfalse.\r\n\r\nReviewers: Michal Cabak (@miccab), John Roesler <john@confluent.io>, Guozhang Wang <guozhand@confluent.io>",
    "date": "2023-04-14T16:43:53Z",
    "url": "https://github.com/apache/kafka/commit/20028e24cca91422b8f02fdbf45d2cd9ef24c901",
    "details": {
      "sha": "22edf797b54fa06191e48d2e2d04f36ca0af6c0e",
      "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java",
      "status": "modified",
      "additions": 5,
      "deletions": 7,
      "changes": 12,
      "blob_url": "https://github.com/apache/kafka/blob/20028e24cca91422b8f02fdbf45d2cd9ef24c901/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStreamTask.java",
      "raw_url": "https://github.com/apache/kafka/raw/20028e24cca91422b8f02fdbf45d2cd9ef24c901/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStreamTask.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStreamTask.java?ref=20028e24cca91422b8f02fdbf45d2cd9ef24c901",
      "patch": "@@ -1195,13 +1195,11 @@ public boolean commitNeeded() {\n                         commitNeeded = true;\n                         entry.setValue(offset - 1);\n                     }\n-                } catch (final TimeoutException error) {\n-                    // the `consumer.position()` call should never block, because we know that we did process data\n-                    // for the requested partition and thus the consumer should have a valid local position\n-                    // that it can return immediately\n-\n-                    // hence, a `TimeoutException` indicates a bug and thus we rethrow it as fatal `IllegalStateException`\n-                    throw new IllegalStateException(error);\n+                } catch (final TimeoutException swallow) {\n+                    log.debug(\n+                        String.format(\"Could not get consumer position for partition %s\", partition),\n+                        swallow\n+                    );\n                 } catch (final KafkaException fatal) {\n                     throw new StreamsException(fatal);\n                 }",
      "parent_sha": "cbf360b37dc422e1b6db6b315cb099bf85115c98"
    }
  },
  {
    "oid": "2e2fad747d273f98edfb9b294ef1f1321f2f5505",
    "message": "MINOR: Remove redundant semicolon (#10358)",
    "date": "2021-03-21T16:43:03Z",
    "url": "https://github.com/apache/kafka/commit/2e2fad747d273f98edfb9b294ef1f1321f2f5505",
    "details": {
      "sha": "a7586028db32b4d90e0c49a030a8359553b40830",
      "filename": "clients/src/test/java/org/apache/kafka/clients/ClusterConnectionStatesTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/2e2fad747d273f98edfb9b294ef1f1321f2f5505/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FClusterConnectionStatesTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/2e2fad747d273f98edfb9b294ef1f1321f2f5505/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FClusterConnectionStatesTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2FClusterConnectionStatesTest.java?ref=2e2fad747d273f98edfb9b294ef1f1321f2f5505",
      "patch": "@@ -79,7 +79,7 @@ public class ClusterConnectionStatesTest {\n \n     // For testing nodes with multiple IP addresses, mock DNS resolution to get consistent results\n     private AddressChangeHostResolver multipleIPHostResolver = new AddressChangeHostResolver(\n-            initialAddresses.toArray(new InetAddress[0]), newAddresses.toArray(new InetAddress[0]));;\n+            initialAddresses.toArray(new InetAddress[0]), newAddresses.toArray(new InetAddress[0]));\n \n     @BeforeEach\n     public void setup() {",
      "parent_sha": "a290c8e1df371a5605d1eb2ed527f6021f2766b6"
    }
  },
  {
    "oid": "81e609802187ac2bcbd0ac169fa10e8c02c237f5",
    "message": "KAFKA-16797 A bit cleanup of FeatureControlManager (#15997)\n\nReviewers: Luke Chen <showuon@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2024-05-20T09:19:01Z",
    "url": "https://github.com/apache/kafka/commit/81e609802187ac2bcbd0ac169fa10e8c02c237f5",
    "details": {
      "sha": "ebf1f9067bef0853a215d40ee0ccc03000d2658c",
      "filename": "metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/81e609802187ac2bcbd0ac169fa10e8c02c237f5/metadata%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcontroller%2FFeatureControlManager.java",
      "raw_url": "https://github.com/apache/kafka/raw/81e609802187ac2bcbd0ac169fa10e8c02c237f5/metadata%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcontroller%2FFeatureControlManager.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcontroller%2FFeatureControlManager.java?ref=81e609802187ac2bcbd0ac169fa10e8c02c237f5",
      "patch": "@@ -59,12 +59,12 @@ public static class Builder {\n         private ClusterFeatureSupportDescriber clusterSupportDescriber = new ClusterFeatureSupportDescriber() {\n             @Override\n             public Iterator<Entry<Integer, Map<String, VersionRange>>> brokerSupported() {\n-                return Collections.<Integer, Map<String, VersionRange>>emptyMap().entrySet().iterator();\n+                return Collections.emptyIterator();\n             }\n \n             @Override\n             public Iterator<Entry<Integer, Map<String, VersionRange>>> controllerSupported() {\n-                return Collections.<Integer, Map<String, VersionRange>>emptyMap().entrySet().iterator();\n+                return Collections.emptyIterator();\n             }\n         };\n ",
      "parent_sha": "a0ca025f9d0c2b96f758724509e4438b9c2aaf75"
    }
  },
  {
    "oid": "93bf96589471acadfb90e57ebfecbd91f679f77b",
    "message": "KAFKA-8559: Allocate ArrayList with correct size in PartitionStates (#6964)\n\nReviewers: Ismael Juma <ismael@juma.me.uk>",
    "date": "2019-06-19T04:28:04Z",
    "url": "https://github.com/apache/kafka/commit/93bf96589471acadfb90e57ebfecbd91f679f77b",
    "details": {
      "sha": "daad3550738b0678a9125a4d3eb049ce9ec4a71b",
      "filename": "clients/src/main/java/org/apache/kafka/common/internals/PartitionStates.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/93bf96589471acadfb90e57ebfecbd91f679f77b/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Finternals%2FPartitionStates.java",
      "raw_url": "https://github.com/apache/kafka/raw/93bf96589471acadfb90e57ebfecbd91f679f77b/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Finternals%2FPartitionStates.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Finternals%2FPartitionStates.java?ref=93bf96589471acadfb90e57ebfecbd91f679f77b",
      "patch": "@@ -89,7 +89,7 @@ public boolean contains(TopicPartition topicPartition) {\n      * Returns the partition states in order.\n      */\n     public List<PartitionState<S>> partitionStates() {\n-        List<PartitionState<S>> result = new ArrayList<>();\n+        List<PartitionState<S>> result = new ArrayList<>(map.size());\n         for (Map.Entry<TopicPartition, S> entry : map.entrySet()) {\n             result.add(new PartitionState<>(entry.getKey(), entry.getValue()));\n         }",
      "parent_sha": "2c9a15045eaac519c7a8dd20208e95a2bfccf76a"
    }
  },
  {
    "oid": "a7342a2e62fa0c5753ce103d0ba1268ae1e923d1",
    "message": "MINOR: fix flaky RemoteLogManagerTest#testStopPartitionsWithDeletion (#18474)\n\nThe test has become flakier recently and it's easy to reproduce by running the single test (vs\nrunning the the class test suite).\n\nThe root cause is that following functions call `RemoteLogMetadataManager#listRemoteLogSegments`.\nIt returns iterator. If one of function goes through iterator first, another can't get expected result.\nI changed `thenReturn` to `thenAnswer` to avoid the issue.\n\nThe race is between:\n* RLMExpirationTask#cleanupExpiredRemoteLogSegments\n* RemoteLogManager#deleteRemoteLogPartition\n\nReviewers: Ismael Juma <ismael@juma.me.uk>\n\nSigned-off-by: PoAn Yang <payang@apache.org>",
    "date": "2025-01-10T14:33:10Z",
    "url": "https://github.com/apache/kafka/commit/a7342a2e62fa0c5753ce103d0ba1268ae1e923d1",
    "details": {
      "sha": "166e58f92175da0353405ec99d1721eea626b93e",
      "filename": "core/src/test/java/kafka/log/remote/RemoteLogManagerTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/a7342a2e62fa0c5753ce103d0ba1268ae1e923d1/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Flog%2Fremote%2FRemoteLogManagerTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/a7342a2e62fa0c5753ce103d0ba1268ae1e923d1/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Flog%2Fremote%2FRemoteLogManagerTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Flog%2Fremote%2FRemoteLogManagerTest.java?ref=a7342a2e62fa0c5753ce103d0ba1268ae1e923d1",
      "patch": "@@ -2150,9 +2150,9 @@ public void testStopPartitionsWithDeletion() throws RemoteStorageException {\n         partitions.add(new StopPartition(followerTopicIdPartition.topicPartition(), true, true, true));\n \n         when(remoteLogMetadataManager.listRemoteLogSegments(eq(leaderTopicIdPartition)))\n-                .thenReturn(listRemoteLogSegmentMetadata(leaderTopicIdPartition, 5, 100, 1024, RemoteLogSegmentState.DELETE_SEGMENT_FINISHED).iterator());\n+            .thenAnswer(invocation -> listRemoteLogSegmentMetadata(leaderTopicIdPartition, 5, 100, 1024, RemoteLogSegmentState.DELETE_SEGMENT_FINISHED).iterator());\n         when(remoteLogMetadataManager.listRemoteLogSegments(eq(followerTopicIdPartition)))\n-                .thenReturn(listRemoteLogSegmentMetadata(followerTopicIdPartition, 3, 100, 1024, RemoteLogSegmentState.DELETE_SEGMENT_FINISHED).iterator());\n+            .thenAnswer(invocation -> listRemoteLogSegmentMetadata(followerTopicIdPartition, 3, 100, 1024, RemoteLogSegmentState.DELETE_SEGMENT_FINISHED).iterator());\n         CompletableFuture<Void> dummyFuture = new CompletableFuture<>();\n         dummyFuture.complete(null);\n         when(remoteLogMetadataManager.updateRemoteLogSegmentMetadata(any()))",
      "parent_sha": "b9a952df6c69f6fcadb5b7e68b48aba796804480"
    }
  },
  {
    "oid": "7748fc2fc6cab504403276696df9ae8cf193601f",
    "message": "KAFKA-9477 Document RoundRobinAssignor as an option for partition.assignment.strategy (#8007)\n\nReviewers: Colin P. McCabe <cmccabe@apache.org>",
    "date": "2020-02-04T17:44:12Z",
    "url": "https://github.com/apache/kafka/commit/7748fc2fc6cab504403276696df9ae8cf193601f",
    "details": {
      "sha": "e02cd84beb15936f999b9ada150d0b28f74790e3",
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java",
      "status": "modified",
      "additions": 12,
      "deletions": 1,
      "changes": 13,
      "blob_url": "https://github.com/apache/kafka/blob/7748fc2fc6cab504403276696df9ae8cf193601f/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FConsumerConfig.java",
      "raw_url": "https://github.com/apache/kafka/raw/7748fc2fc6cab504403276696df9ae8cf193601f/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FConsumerConfig.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FConsumerConfig.java?ref=7748fc2fc6cab504403276696df9ae8cf193601f",
      "patch": "@@ -103,7 +103,18 @@ public class ConsumerConfig extends AbstractConfig {\n      * <code>partition.assignment.strategy</code>\n      */\n     public static final String PARTITION_ASSIGNMENT_STRATEGY_CONFIG = \"partition.assignment.strategy\";\n-    private static final String PARTITION_ASSIGNMENT_STRATEGY_DOC = \"A list of class names or class types, ordered by preference, of supported assignors responsible for the partition assignment strategy that the client will use to distribute partition ownership amongst consumer instances when group management is used. Implementing the <code>org.apache.kafka.clients.consumer.ConsumerPartitionAssignor</code> interface allows you to plug in a custom assignment strategy.\";\n+    private static final String PARTITION_ASSIGNMENT_STRATEGY_DOC = \"A list of class names or class types, \"+\n+            \"ordered by preference, of supported partition assignment \"+\n+            \"strategies that the client will use to distribute partition \"+\n+            \"ownership amongst consumer instances when group management is \"+\n+            \"used.<p>In addition to the default class specified below, \"+\n+            \"you can use the \" + \n+            \"<code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>\"+\n+            \"class for round robin assignments of partitions to consumers. \"+\n+            \"</p><p>Implementing the \" +\n+            \"<code>org.apache.kafka.clients.consumer.ConsumerPartitionAssignor\"+\n+            \"</code> interface allows you to plug in a custom assignment\"+\n+            \"strategy.\";\n \n     /**\n      * <code>auto.offset.reset</code>",
      "parent_sha": "dd7a314591c54af67d769f78d85c10be063cb909"
    }
  },
  {
    "oid": "c85d67796f6fae028f1976b889155a124617a364",
    "message": "MINOR: fix comments on deleteTopics method (#10966)\n\nReviewers: Luke Chen <showuon@gmail.com>, David Jacot <djacot@confluent.io>",
    "date": "2021-07-05T07:25:17Z",
    "url": "https://github.com/apache/kafka/commit/c85d67796f6fae028f1976b889155a124617a364",
    "details": {
      "sha": "eb9ec2e2b9e0fa94cf799f1bbc37700a17543933",
      "filename": "clients/src/main/java/org/apache/kafka/clients/admin/Admin.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/c85d67796f6fae028f1976b889155a124617a364/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FAdmin.java",
      "raw_url": "https://github.com/apache/kafka/raw/c85d67796f6fae028f1976b889155a124617a364/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FAdmin.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FAdmin.java?ref=c85d67796f6fae028f1976b889155a124617a364",
      "patch": "@@ -230,7 +230,7 @@ default DeleteTopicsResult deleteTopics(Collection<String> topics, DeleteTopicsO\n      * This is a convenience method for {@link #deleteTopics(TopicCollection, DeleteTopicsOptions)}\n      * with default options. See the overload for more details.\n      * <p>\n-     * When using topic IDs, this operation is supported by brokers with version 3.0.0 or higher.\n+     * When using topic IDs, this operation is supported by brokers with inter-broker protocol 2.8 or higher.\n      * When using topic names, this operation is supported by brokers with version 0.10.1.0 or higher.\n      *\n      * @param topics The topics to delete.\n@@ -254,7 +254,7 @@ default DeleteTopicsResult deleteTopics(TopicCollection topics) {\n      * the topics for deletion, but not actually delete them. The futures will\n      * return successfully in this case.\n      * <p>\n-     * When using topic IDs, this operation is supported by brokers with version 3.0.0 or higher.\n+     * When using topic IDs, this operation is supported by brokers with inter-broker protocol 2.8 or higher.\n      * When using topic names, this operation is supported by brokers with version 0.10.1.0 or higher.\n      *\n      * @param topics  The topics to delete.",
      "parent_sha": "855011f92a20c07ae5ee5a93f29f11a356e30ba3"
    }
  },
  {
    "oid": "3ba688e75f29b2089317b3cf7331192ce81ea3f0",
    "message": "KAFKA-9203: Check for buggy LZ4 libraries and remove corresponding workarounds (#10196)\n\n* Remove the workarounds that were added back in https://github.com/apache/kafka/pull/7769\r\n* Add a check to detect buggy LZ4 library versions\r\n\r\nThis check allows us to safely remove the workarounds for buggy\r\nLZ4 versions without users encountering cryptic errors if they\r\naccidentally have an older LZ4 library on the classpath, as\r\ndescribed in KAFKA-9203.\r\n\r\nWith this change the use will get a clear error message indicating\r\nwhat the problem might be if they encounter this situation.\r\n\r\nNote: This now instantiates a compressor in the decompression code.\r\nThis should be safe with respect to JNI libraries, since we always use\r\n`LZ4Factory.fastestInstance()` which takes care of falling back to a pure\r\nJava implementation if JNI libraries are not present.\r\n\r\nThis was tested with lz4 1.3.0 to make sure it triggers the exception when running\r\n`KafkaLZ4Test`.\r\n\r\nReviewers: Manikumar Reddy <manikumar.reddy@gmail.com>, Ismael Juma <ismael@juma.me.uk>\r\n\r\nCo-authored-by: Ismael Juma <ismael@juma.me.uk>",
    "date": "2021-07-23T21:32:41Z",
    "url": "https://github.com/apache/kafka/commit/3ba688e75f29b2089317b3cf7331192ce81ea3f0",
    "details": {
      "sha": "e2fbd5ac04d989e041dfeeaf94dd9029d2885ce2",
      "filename": "clients/src/main/java/org/apache/kafka/common/compress/KafkaLZ4BlockInputStream.java",
      "status": "modified",
      "additions": 56,
      "deletions": 29,
      "changes": 85,
      "blob_url": "https://github.com/apache/kafka/blob/3ba688e75f29b2089317b3cf7331192ce81ea3f0/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fcompress%2FKafkaLZ4BlockInputStream.java",
      "raw_url": "https://github.com/apache/kafka/raw/3ba688e75f29b2089317b3cf7331192ce81ea3f0/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fcompress%2FKafkaLZ4BlockInputStream.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fcompress%2FKafkaLZ4BlockInputStream.java?ref=3ba688e75f29b2089317b3cf7331192ce81ea3f0",
      "patch": "@@ -16,6 +16,7 @@\n  */\n package org.apache.kafka.common.compress;\n \n+import net.jpountz.lz4.LZ4Compressor;\n import net.jpountz.lz4.LZ4Exception;\n import net.jpountz.lz4.LZ4Factory;\n import net.jpountz.lz4.LZ4SafeDecompressor;\n@@ -51,6 +52,19 @@ public final class KafkaLZ4BlockInputStream extends InputStream {\n     private static final LZ4SafeDecompressor DECOMPRESSOR = LZ4Factory.fastestInstance().safeDecompressor();\n     private static final XXHash32 CHECKSUM = XXHashFactory.fastestInstance().hash32();\n \n+    private static final RuntimeException BROKEN_LZ4_EXCEPTION;\n+    // https://issues.apache.org/jira/browse/KAFKA-9203\n+    // detect buggy lz4 libraries on the classpath\n+    static {\n+        RuntimeException exception = null;\n+        try {\n+            detectBrokenLz4Version();\n+        } catch (RuntimeException e) {\n+            exception = e;\n+        }\n+        BROKEN_LZ4_EXCEPTION = exception;\n+    }\n+\n     private final ByteBuffer in;\n     private final boolean ignoreFlagDescriptorChecksum;\n     private final BufferSupplier bufferSupplier;\n@@ -73,16 +87,14 @@ public final class KafkaLZ4BlockInputStream extends InputStream {\n      * @throws IOException\n      */\n     public KafkaLZ4BlockInputStream(ByteBuffer in, BufferSupplier bufferSupplier, boolean ignoreFlagDescriptorChecksum) throws IOException {\n+        if (BROKEN_LZ4_EXCEPTION != null) {\n+            throw BROKEN_LZ4_EXCEPTION;\n+        }\n         this.ignoreFlagDescriptorChecksum = ignoreFlagDescriptorChecksum;\n         this.in = in.duplicate().order(ByteOrder.LITTLE_ENDIAN);\n         this.bufferSupplier = bufferSupplier;\n         readHeader();\n         decompressionBuffer = bufferSupplier.get(maxBlockSize);\n-        if (!decompressionBuffer.hasArray() || decompressionBuffer.arrayOffset() != 0) {\n-            // require array backed decompression buffer with zero offset\n-            // to simplify workaround for https://github.com/lz4/lz4-java/pull/65\n-            throw new RuntimeException(\"decompression buffer must have backing array with zero array offset\");\n-        }\n         finished = false;\n     }\n \n@@ -132,10 +144,7 @@ private void readHeader() throws IOException {\n \n         int len = in.position() - in.reset().position();\n \n-        int hash = in.hasArray() ?\n-                       // workaround for https://github.com/lz4/lz4-java/pull/65\n-                       CHECKSUM.hash(in.array(), in.arrayOffset() + in.position(), len, 0) :\n-                       CHECKSUM.hash(in, in.position(), len, 0);\n+        int hash = CHECKSUM.hash(in, in.position(), len, 0);\n         in.position(in.position() + len);\n         if (in.get() != (byte) ((hash >> 8) & 0xFF)) {\n             throw new IOException(DESCRIPTOR_HASH_MISMATCH);\n@@ -173,22 +182,8 @@ private void readBlock() throws IOException {\n \n         if (compressed) {\n             try {\n-                // workaround for https://github.com/lz4/lz4-java/pull/65\n-                final int bufferSize;\n-                if (in.hasArray()) {\n-                    bufferSize = DECOMPRESSOR.decompress(\n-                        in.array(),\n-                        in.position() + in.arrayOffset(),\n-                        blockSize,\n-                        decompressionBuffer.array(),\n-                        0,\n-                        maxBlockSize\n-                    );\n-                } else {\n-                    // decompressionBuffer has zero arrayOffset, so we don't need to worry about\n-                    // https://github.com/lz4/lz4-java/pull/65\n-                    bufferSize = DECOMPRESSOR.decompress(in, in.position(), blockSize, decompressionBuffer, 0, maxBlockSize);\n-                }\n+                final int bufferSize = DECOMPRESSOR.decompress(in, in.position(), blockSize, decompressionBuffer, 0,\n+                    maxBlockSize);\n                 decompressionBuffer.position(0);\n                 decompressionBuffer.limit(bufferSize);\n                 decompressedBuffer = decompressionBuffer;\n@@ -202,10 +197,7 @@ private void readBlock() throws IOException {\n \n         // verify checksum\n         if (flg.isBlockChecksumSet()) {\n-            // workaround for https://github.com/lz4/lz4-java/pull/65\n-            int hash = in.hasArray() ?\n-                       CHECKSUM.hash(in.array(), in.arrayOffset() + in.position(), blockSize, 0) :\n-                       CHECKSUM.hash(in, in.position(), blockSize, 0);\n+            int hash = CHECKSUM.hash(in, in.position(), blockSize, 0);\n             in.position(in.position() + blockSize);\n             if (hash != in.getInt()) {\n                 throw new IOException(BLOCK_HASH_MISMATCH);\n@@ -288,4 +280,39 @@ public void reset() {\n     public boolean markSupported() {\n         return false;\n     }\n+\n+    /**\n+     * Checks whether the version of lz4 on the classpath has the fix for reading from ByteBuffers with\n+     * non-zero array offsets (see https://github.com/lz4/lz4-java/pull/65)\n+     */\n+    static void detectBrokenLz4Version() {\n+        byte[] source = new byte[]{1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3};\n+        final LZ4Compressor compressor = LZ4Factory.fastestInstance().fastCompressor();\n+\n+        final byte[] compressed = new byte[compressor.maxCompressedLength(source.length)];\n+        final int compressedLength = compressor.compress(source, 0, source.length, compressed, 0,\n+                                                         compressed.length);\n+\n+        // allocate an array-backed ByteBuffer with non-zero array-offset containing the compressed data\n+        // a buggy decompressor will read the data from the beginning of the underlying array instead of\n+        // the beginning of the ByteBuffer, failing to decompress the invalid data.\n+        final byte[] zeroes = {0, 0, 0, 0, 0};\n+        ByteBuffer nonZeroOffsetBuffer = ByteBuffer\n+            .allocate(zeroes.length + compressed.length) // allocates the backing array with extra space to offset the data\n+            .put(zeroes) // prepend invalid bytes (zeros) before the compressed data in the array\n+            .slice() // create a new ByteBuffer sharing the underlying array, offset to start on the compressed data\n+            .put(compressed); // write the compressed data at the beginning of this new buffer\n+\n+        ByteBuffer dest = ByteBuffer.allocate(source.length);\n+        try {\n+            DECOMPRESSOR.decompress(nonZeroOffsetBuffer, 0, compressedLength, dest, 0, source.length);\n+        } catch (Exception e) {\n+            throw new RuntimeException(\"Kafka has detected detected a buggy lz4-java library (< 1.4.x) on the classpath.\"\n+                                       + \" If you are using Kafka client libraries, make sure your application does not\"\n+                                       + \" accidentally override the version provided by Kafka or include multiple versions\"\n+                                       + \" of the library on the classpath. The lz4-java version on the classpath should\"\n+                                       + \" match the version the Kafka client libraries depend on. Adding -verbose:class\"\n+                                       + \" to your JVM arguments may help understand which lz4-java version is getting loaded.\", e);\n+        }\n+    }\n }",
      "parent_sha": "f34bb28ab6c69945d944abc1283cacecdaf4ef8e"
    }
  },
  {
    "oid": "f08c9c7de6f7b85bddc6c83c0d852b5b3b1f430a",
    "message": "HOTFIX: fix flaky StateDirectoryTest.shouldReturnEmptyArrayIfListFilesReturnsNull (#8310)\n\nStateDirectoryTest.shouldReturnEmptyArrayIfListFilesReturnsNull always moves the stage dir to /tmp/state-renamed so it always fails if there is already a folder (for example, the stuff leaved by previous test).\r\n\r\nReviewers: Boyang Chen <boyang@confluent.io>, A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <wangguoz@gmail.com>",
    "date": "2020-03-18T05:13:13Z",
    "url": "https://github.com/apache/kafka/commit/f08c9c7de6f7b85bddc6c83c0d852b5b3b1f430a",
    "details": {
      "sha": "245675555bc7285a79d3974d7e0042d5c1dfa487",
      "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StateDirectoryTest.java",
      "status": "modified",
      "additions": 7,
      "deletions": 4,
      "changes": 11,
      "blob_url": "https://github.com/apache/kafka/blob/f08c9c7de6f7b85bddc6c83c0d852b5b3b1f430a/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStateDirectoryTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/f08c9c7de6f7b85bddc6c83c0d852b5b3b1f430a/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStateDirectoryTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStateDirectoryTest.java?ref=f08c9c7de6f7b85bddc6c83c0d852b5b3b1f430a",
      "patch": "@@ -316,7 +316,7 @@ public void shouldReturnEmptyArrayIfStateDirDoesntExist() throws IOException {\n     }\n \n     @Test\n-    public void shouldReturnEmptyArrayIfListFilesReturnsNull() {\n+    public void shouldReturnEmptyArrayIfListFilesReturnsNull() throws IOException {\n         stateDir = new File(TestUtils.IO_TMP_DIR, \"kafka-\" + TestUtils.randomString(5));\n         directory = new StateDirectory(\n             new StreamsConfig(new Properties() {\n@@ -329,9 +329,12 @@ public void shouldReturnEmptyArrayIfListFilesReturnsNull() {\n             time, true);\n         appDir = new File(stateDir, applicationId);\n \n-        assertTrue(stateDir.renameTo(new File(TestUtils.IO_TMP_DIR, \"state-renamed\")));\n-\n-        assertTrue(Arrays.asList(directory.listAllTaskDirectories()).isEmpty());\n+        // make sure the File#listFiles returns null and StateDirectory#listAllTaskDirectories is able to handle null\n+        Utils.delete(appDir);\n+        assertTrue(appDir.createNewFile());\n+        assertTrue(appDir.exists());\n+        assertNull(appDir.listFiles());\n+        assertEquals(0, directory.listAllTaskDirectories().length);\n     }\n \n     @Test",
      "parent_sha": "6a88d32b9fe3cb320088dec72fce30992b89ef28"
    }
  },
  {
    "oid": "a0da6785a27b13be173762965402b55860ac5635",
    "message": "MINOR: Fix log message for transition from standby to active (#8872)\n\nReviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Guozhang Wang <guozhang@confluent.io>",
    "date": "2020-06-15T17:56:40Z",
    "url": "https://github.com/apache/kafka/commit/a0da6785a27b13be173762965402b55860ac5635",
    "details": {
      "sha": "8d8090184f93f80bbf346f8385fd8f8f960184e2",
      "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java",
      "status": "modified",
      "additions": 2,
      "deletions": 1,
      "changes": 3,
      "blob_url": "https://github.com/apache/kafka/blob/a0da6785a27b13be173762965402b55860ac5635/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FProcessorStateManager.java",
      "raw_url": "https://github.com/apache/kafka/raw/a0da6785a27b13be173762965402b55860ac5635/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FProcessorStateManager.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FProcessorStateManager.java?ref=a0da6785a27b13be173762965402b55860ac5635",
      "patch": "@@ -510,11 +510,12 @@ void transitionTaskType(final TaskType newType, final LogContext logContext) {\n             throw new IllegalStateException(\"Tried to recycle state for task type conversion but new type was the same.\");\n         }\n \n+        TaskType oldType = taskType;\n         taskType = newType;\n         log = logContext.logger(ProcessorStateManager.class);\n         logPrefix = logContext.logPrefix();\n \n-        log.debug(\"Transitioning state manager for {} task {} to {}\", taskType, taskId, newType);\n+        log.debug(\"Transitioning state manager for {} task {} to {}\", oldType, taskId, newType);\n     }\n \n     @Override",
      "parent_sha": "a3e8f495cfa4624202eb327512785d66e7e730a9"
    }
  },
  {
    "oid": "ee4f8f8c429dfcf0b22fa8917ed0171f3aaf16fb",
    "message": "KAFKA-18541: fix flaky KafkaStreamsTelemetryIntegrationTest (#18569)\n\nReviewers: Bill Bejeck <bill@confluent.io>",
    "date": "2025-01-27T17:43:25Z",
    "url": "https://github.com/apache/kafka/commit/ee4f8f8c429dfcf0b22fa8917ed0171f3aaf16fb",
    "details": {
      "sha": "750b2381a1a490939cf6d971799132cd6feb13d7",
      "filename": "streams/integration-tests/src/test/java/org/apache/kafka/streams/integration/KafkaStreamsTelemetryIntegrationTest.java",
      "status": "modified",
      "additions": 24,
      "deletions": 12,
      "changes": 36,
      "blob_url": "https://github.com/apache/kafka/blob/ee4f8f8c429dfcf0b22fa8917ed0171f3aaf16fb/streams%2Fintegration-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FKafkaStreamsTelemetryIntegrationTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/ee4f8f8c429dfcf0b22fa8917ed0171f3aaf16fb/streams%2Fintegration-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FKafkaStreamsTelemetryIntegrationTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fintegration-tests%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FKafkaStreamsTelemetryIntegrationTest.java?ref=ee4f8f8c429dfcf0b22fa8917ed0171f3aaf16fb",
      "patch": "@@ -70,6 +70,7 @@\n import java.time.Duration;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.Locale;\n import java.util.Map;\n@@ -172,9 +173,11 @@ public void shouldPushMetricsToBroker(final String recordingLevel) throws Except\n                     .map(metric -> metric.metricName().tags().get(\"process-id\"))\n                     .findFirst().orElseThrow();\n \n-            TestUtils.waitForCondition(() -> !TelemetryPlugin.SUBSCRIBED_METRICS.get(mainConsumerInstanceId).isEmpty(),\n-                    30_000,\n-                    \"Never received subscribed metrics\");\n+            TestUtils.waitForCondition(\n+                () -> !TelemetryPlugin.SUBSCRIBED_METRICS.getOrDefault(mainConsumerInstanceId, Collections.emptyList()).isEmpty(),\n+                30_000,\n+                \"Never received subscribed metrics\"\n+            );\n             final List<String> expectedMetrics = streams.metrics().values().stream().map(Metric::metricName)\n                     .filter(metricName -> metricName.tags().containsKey(\"thread-id\")).map(mn -> {\n                         final String name = mn.name().replace('-', '.');\n@@ -185,9 +188,11 @@ public void shouldPushMetricsToBroker(final String recordingLevel) throws Except\n             final List<String> actualMetrics = new ArrayList<>(TelemetryPlugin.SUBSCRIBED_METRICS.get(mainConsumerInstanceId));\n             assertEquals(expectedMetrics, actualMetrics);\n \n-            TestUtils.waitForCondition(() -> !TelemetryPlugin.SUBSCRIBED_METRICS.get(adminInstanceId).isEmpty(),\n-                    30_000,\n-                    \"Never received subscribed metrics\");\n+            TestUtils.waitForCondition(\n+                () -> !TelemetryPlugin.SUBSCRIBED_METRICS.getOrDefault(adminInstanceId, Collections.emptyList()).isEmpty(),\n+                30_000,\n+                \"Never received subscribed metrics\"\n+            );\n             final List<String> actualInstanceMetrics = TelemetryPlugin.SUBSCRIBED_METRICS.get(adminInstanceId);\n             final List<String> expectedInstanceMetrics = Arrays.asList(\n                 \"org.apache.kafka.stream.alive.stream.threads\",\n@@ -267,18 +272,25 @@ public void shouldPassCorrectMetricsDynamicInstances(final boolean stateUpdaterE\n             assertEquals(streamsTaskMetricNames.size(), consumerPassedStreamTaskMetricNames.size());\n             assertEquals(consumerPassedTaskMetricCount, streamsTaskMetricNames.size());\n \n-\n             try (final KafkaStreams streamsTwo = new KafkaStreams(topology, streamsSecondApplicationProperties)) {\n                 streamsTwo.start();\n-                waitForCondition(() -> KafkaStreams.State.RUNNING == streamsTwo.state() && KafkaStreams.State.RUNNING == streamsOne.state(),\n-                        IntegrationTestUtils.DEFAULT_TIMEOUT,\n-                        () -> \"Kafka Streams one or two never transitioned to a RUNNING state.\");\n \n                 /*\n                   Now with 2 instances, the tasks will get split amongst both Kafka Streams applications\n                  */\n-                final List<String> streamOneTaskIds = getTaskIdsAsStrings(streamsOne);\n-                final List<String> streamTwoTasksIds = getTaskIdsAsStrings(streamsTwo);\n+                final List<String> streamOneTaskIds = new ArrayList<>();\n+                final List<String> streamTwoTasksIds = new ArrayList<>();\n+                waitForCondition(() -> {\n+                        streamOneTaskIds.clear();\n+                        streamTwoTasksIds.clear();\n+\n+                        streamOneTaskIds.addAll(getTaskIdsAsStrings(streamsOne));\n+                        streamTwoTasksIds.addAll(getTaskIdsAsStrings(streamsTwo));\n+\n+                        return streamOneTaskIds.size() == 2 && streamTwoTasksIds.size() == 2;\n+                    },\n+                    \"Task assignment did not complete.\"\n+                );\n \n                 final List<MetricName> streamsOneTaskMetrics = streamsOne.metrics().values().stream().map(Metric::metricName)\n                         .filter(metricName -> metricName.tags().containsKey(\"task-id\")).collect(Collectors.toList());",
      "parent_sha": "d001b47093c8ca45c53bd8220d3c735e77f57999"
    }
  },
  {
    "oid": "7692643f593ec6574f6b5c49302d11c8d4315335",
    "message": "KAFKA-13967: Document guarantees for producer callbacks on transaction commit (#12264)\n\nClarify in producer docs that `send` callbacks are invoked prior to transaction completion.\r\n\r\nReviewers: Tom Bentley <tbentley@redhat.com>, Jason Gustafson <jason@confluent.io>",
    "date": "2022-06-10T16:51:36Z",
    "url": "https://github.com/apache/kafka/commit/7692643f593ec6574f6b5c49302d11c8d4315335",
    "details": {
      "sha": "e85d9eb8a9ec8a497a59d90a484445a9d3808bce",
      "filename": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java",
      "status": "modified",
      "additions": 6,
      "deletions": 2,
      "changes": 8,
      "blob_url": "https://github.com/apache/kafka/blob/7692643f593ec6574f6b5c49302d11c8d4315335/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java",
      "raw_url": "https://github.com/apache/kafka/raw/7692643f593ec6574f6b5c49302d11c8d4315335/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducer.java?ref=7692643f593ec6574f6b5c49302d11c8d4315335",
      "patch": "@@ -754,11 +754,15 @@ public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offs\n \n     /**\n      * Commits the ongoing transaction. This method will flush any unsent records before actually committing the transaction.\n-     *\n+     * <p>\n      * Further, if any of the {@link #send(ProducerRecord)} calls which were part of the transaction hit irrecoverable\n      * errors, this method will throw the last received exception immediately and the transaction will not be committed.\n      * So all {@link #send(ProducerRecord)} calls in a transaction must succeed in order for this method to succeed.\n-     *\n+     * <p>\n+     * If the transaction is committed successfully and this method returns without throwing an exception, it is guaranteed\n+     * that all {@link Callback callbacks} for records in the transaction will have been invoked and completed.\n+     * Note that exceptions thrown by callbacks are ignored; the producer proceeds to commit the transaction in any case.\n+     * <p>\n      * Note that this method will raise {@link TimeoutException} if the transaction cannot be committed before expiration\n      * of {@code max.block.ms}. Additionally, it will raise {@link InterruptException} if interrupted.\n      * It is safe to retry in either case, but it is not possible to attempt a different operation (such as abortTransaction)",
      "parent_sha": "3d5b41e05f0011dcf607f7d9cb269e8bb4558c57"
    }
  },
  {
    "oid": "c9c3adca43fb066de0e16c65d381981c313cfb0c",
    "message": "MINOR: move \"Added/Removed sensor\" log messages to TRACE (#7502)\n\nReviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bill@confluent.io>",
    "date": "2019-10-14T19:38:33Z",
    "url": "https://github.com/apache/kafka/commit/c9c3adca43fb066de0e16c65d381981c313cfb0c",
    "details": {
      "sha": "3fc73e131e284ec1aa9fd7dc4995a39d6416c421",
      "filename": "clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/c9c3adca43fb066de0e16c65d381981c313cfb0c/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fmetrics%2FMetrics.java",
      "raw_url": "https://github.com/apache/kafka/raw/c9c3adca43fb066de0e16c65d381981c313cfb0c/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fmetrics%2FMetrics.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fmetrics%2FMetrics.java?ref=c9c3adca43fb066de0e16c65d381981c313cfb0c",
      "patch": "@@ -413,7 +413,7 @@ public synchronized Sensor sensor(String name, MetricConfig config, long inactiv\n                     children.add(s);\n                 }\n             }\n-            log.debug(\"Added sensor with name {}\", name);\n+            log.trace(\"Added sensor with name {}\", name);\n         }\n         return s;\n     }\n@@ -446,7 +446,7 @@ public void removeSensor(String name) {\n                     if (sensors.remove(name, sensor)) {\n                         for (KafkaMetric metric : sensor.metrics())\n                             removeMetric(metric.metricName());\n-                        log.debug(\"Removed sensor with name {}\", name);\n+                        log.trace(\"Removed sensor with name {}\", name);\n                         childSensors = childrenSensors.remove(sensor);\n                         for (final Sensor parent : sensor.parents()) {\n                             childrenSensors.getOrDefault(parent, emptyList()).remove(sensor);",
      "parent_sha": "11a401d51d835c32d365f6a089016bdb871e42e0"
    }
  },
  {
    "oid": "eb39c95080b994398c40bcf5d54181e713ed6faa",
    "message": "MINOR: StoreChangelogReaderTest fails with log-level DEBUG (#14300)\n\nA mocked method is executed unexpectedly when we enable DEBUG\r\nlog level, leading to confusing test failures during debugging.\r\nSince the log message itself seems useful, we adapt the test\r\nto take the additional mocked method call into account).\r\n\r\nReviewer: Bruno Cadonna <cadonna@apache.org>",
    "date": "2023-09-06T12:49:48Z",
    "url": "https://github.com/apache/kafka/commit/eb39c95080b994398c40bcf5d54181e713ed6faa",
    "details": {
      "sha": "2848f0dcd5ea046e42178289c6d93b9e8ca579dd",
      "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java",
      "status": "modified",
      "additions": 5,
      "deletions": 1,
      "changes": 6,
      "blob_url": "https://github.com/apache/kafka/blob/eb39c95080b994398c40bcf5d54181e713ed6faa/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStoreChangelogReaderTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/eb39c95080b994398c40bcf5d54181e713ed6faa/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStoreChangelogReaderTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fprocessor%2Finternals%2FStoreChangelogReaderTest.java?ref=eb39c95080b994398c40bcf5d54181e713ed6faa",
      "patch": "@@ -520,7 +520,11 @@ public void shouldRestoreFromPositionAndCheckForCompletion() {\n     public void shouldRestoreFromBeginningAndCheckCompletion() {\n         final TaskId taskId = new TaskId(0, 0);\n \n-        EasyMock.expect(storeMetadata.offset()).andReturn(null).andReturn(9L).anyTimes();\n+        if (type == STANDBY && logContext.logger(StoreChangelogReader.class).isDebugEnabled()) {\n+            EasyMock.expect(storeMetadata.offset()).andReturn(null).andReturn(null).andReturn(9L).anyTimes();\n+        } else {\n+            EasyMock.expect(storeMetadata.offset()).andReturn(null).andReturn(9L).anyTimes();\n+        }\n         EasyMock.expect(stateManager.changelogOffsets()).andReturn(singletonMap(tp, 5L));\n         EasyMock.expect(stateManager.taskId()).andReturn(taskId).anyTimes();\n         EasyMock.replay(stateManager, storeMetadata, store);",
      "parent_sha": "cc289d04c701a59f571683b908f778e0b236d72f"
    }
  },
  {
    "oid": "91c23a3b9383cdaff32e97a7dcca07e13532e032",
    "message": "MINOR: Cleanup NetworkReceive constructors (#12511)\n\nThere was unnecessary duplication and one of the overloads\r\ndid not set the size field for no good reason.\r\n\r\nReviewers: Luke Chen <showuon@gmail.com>",
    "date": "2022-08-14T19:54:20Z",
    "url": "https://github.com/apache/kafka/commit/91c23a3b9383cdaff32e97a7dcca07e13532e032",
    "details": {
      "sha": "b1c71abd699214fca1ab865ec199fd33e68985d1",
      "filename": "clients/src/main/java/org/apache/kafka/common/network/NetworkReceive.java",
      "status": "modified",
      "additions": 5,
      "deletions": 16,
      "changes": 21,
      "blob_url": "https://github.com/apache/kafka/blob/91c23a3b9383cdaff32e97a7dcca07e13532e032/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FNetworkReceive.java",
      "raw_url": "https://github.com/apache/kafka/raw/91c23a3b9383cdaff32e97a7dcca07e13532e032/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FNetworkReceive.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FNetworkReceive.java?ref=91c23a3b9383cdaff32e97a7dcca07e13532e032",
      "patch": "@@ -30,8 +30,8 @@\n  */\n public class NetworkReceive implements Receive {\n \n-    public final static String UNKNOWN_SOURCE = \"\";\n-    public final static int UNLIMITED = -1;\n+    public static final String UNKNOWN_SOURCE = \"\";\n+    public static final int UNLIMITED = -1;\n     private static final Logger log = LoggerFactory.getLogger(NetworkReceive.class);\n     private static final ByteBuffer EMPTY_BUFFER = ByteBuffer.allocate(0);\n \n@@ -44,27 +44,16 @@ public class NetworkReceive implements Receive {\n \n \n     public NetworkReceive(String source, ByteBuffer buffer) {\n-        this.source = source;\n+        this(UNLIMITED, source);\n         this.buffer = buffer;\n-        this.size = null;\n-        this.maxSize = UNLIMITED;\n-        this.memoryPool = MemoryPool.NONE;\n     }\n \n     public NetworkReceive(String source) {\n-        this.source = source;\n-        this.size = ByteBuffer.allocate(4);\n-        this.buffer = null;\n-        this.maxSize = UNLIMITED;\n-        this.memoryPool = MemoryPool.NONE;\n+        this(UNLIMITED, source);\n     }\n \n     public NetworkReceive(int maxSize, String source) {\n-        this.source = source;\n-        this.size = ByteBuffer.allocate(4);\n-        this.buffer = null;\n-        this.maxSize = maxSize;\n-        this.memoryPool = MemoryPool.NONE;\n+        this(maxSize, source, MemoryPool.NONE);\n     }\n \n     public NetworkReceive(int maxSize, String source, MemoryPool memoryPool) {",
      "parent_sha": "aecd47b3bc691c3158463dab2c698a3ba9eb2f98"
    }
  },
  {
    "oid": "4c0660bf3da9879cb405a0f85cf1524511e091e8",
    "message": "MINOR: Fix typo and tweak wording in `RecordAccumulator` comments\n\nThis was recently introduced in:\nhttps://github.com/apache/kafka/commit/1fbe445dde71df0023a978c5e54dd229d3d23e1b\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Jun Rao <junrao@gmail.com>\n\nCloses #1152 from ijuma/fix-typos-in-record-accumulator",
    "date": "2016-03-28T16:00:03Z",
    "url": "https://github.com/apache/kafka/commit/4c0660bf3da9879cb405a0f85cf1524511e091e8",
    "details": {
      "sha": "7f5b16f244eca65599b051c77dd8646fe90e4b1b",
      "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java",
      "status": "modified",
      "additions": 5,
      "deletions": 5,
      "changes": 10,
      "blob_url": "https://github.com/apache/kafka/blob/4c0660bf3da9879cb405a0f85cf1524511e091e8/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2Finternals%2FRecordAccumulator.java",
      "raw_url": "https://github.com/apache/kafka/raw/4c0660bf3da9879cb405a0f85cf1524511e091e8/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2Finternals%2FRecordAccumulator.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2Finternals%2FRecordAccumulator.java?ref=4c0660bf3da9879cb405a0f85cf1524511e091e8",
      "patch": "@@ -218,14 +218,14 @@ public List<RecordBatch> abortExpiredBatches(int requestTimeout, Cluster cluster\n         int count = 0;\n         for (Map.Entry<TopicPartition, Deque<RecordBatch>> entry : this.batches.entrySet()) {\n             Deque<RecordBatch> dq = entry.getValue();\n-            // We only check if the batch should be expired if the partition does not have a batch in flight.\n-            // This is to avoid the later batches get expired when an earlier batch is still in progress.\n-            // This protection only takes effect when user sets max.in.flight.request.per.connection=1.\n-            // Otherwise the expiration order is not guranteed.\n             TopicPartition tp = entry.getKey();\n+            // We only check if the batch should be expired if the partition does not have a batch in flight.\n+            // This is to prevent later batches from being expired while an earlier batch is still in progress.\n+            // Note that `muted` is only ever populated if `max.in.flight.request.per.connection=1` so this protection\n+            // is only active in this case. Otherwise the expiration order is not guaranteed.\n             if (!muted.contains(tp)) {\n                 synchronized (dq) {\n-                    // iterate over the batches and expire them if they have stayed in accumulator for more than requestTimeOut\n+                    // iterate over the batches and expire them if they have been in the accumulator for more than requestTimeOut\n                     RecordBatch lastBatch = dq.peekLast();\n                     Iterator<RecordBatch> batchIterator = dq.iterator();\n                     while (batchIterator.hasNext()) {",
      "parent_sha": "1fbe445dde71df0023a978c5e54dd229d3d23e1b"
    }
  },
  {
    "oid": "f0e0db1aad913142e3cead2e63af65b8b7725bd9",
    "message": "MINOR: Wait all brokers to get ready (#16537)\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com",
    "date": "2024-07-07T16:32:21Z",
    "url": "https://github.com/apache/kafka/commit/f0e0db1aad913142e3cead2e63af65b8b7725bd9",
    "details": {
      "sha": "e582cefaeccdf6595b7d149164f5948bfc343536",
      "filename": "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/f0e0db1aad913142e3cead2e63af65b8b7725bd9/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Ftest%2Fjunit%2FRaftClusterInvocationContext.java",
      "raw_url": "https://github.com/apache/kafka/raw/f0e0db1aad913142e3cead2e63af65b8b7725bd9/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Ftest%2Fjunit%2FRaftClusterInvocationContext.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Ftest%2Fjunit%2FRaftClusterInvocationContext.java?ref=f0e0db1aad913142e3cead2e63af65b8b7725bd9",
      "patch": "@@ -189,7 +189,9 @@ public void start() {\n                 if (started.compareAndSet(false, true)) {\n                     clusterTestKit.startup();\n                     kafka.utils.TestUtils.waitUntilTrue(\n-                            () -> this.clusterTestKit.brokers().get(0).brokerState() == BrokerState.RUNNING,\n+                            () -> this.clusterTestKit.brokers().values().stream().allMatch(\n+                                    brokers -> brokers.brokerState() == BrokerState.RUNNING\n+                            ),\n                             () -> \"Broker never made it to RUNNING state.\",\n                             org.apache.kafka.test.TestUtils.DEFAULT_MAX_WAIT_MS,\n                             100L);",
      "parent_sha": "932759bd70ce646ced5298a2ad8db02c0cea3643"
    }
  },
  {
    "oid": "ea77352dfc0e8adf860c0f31dc4b53bf36f95b4b",
    "message": "Rename the variable to reflect its purpose (#18525)\n\nReviewers: Andrew Schofield <aschofield@confluent.io>",
    "date": "2025-01-14T18:00:27Z",
    "url": "https://github.com/apache/kafka/commit/ea77352dfc0e8adf860c0f31dc4b53bf36f95b4b",
    "details": {
      "sha": "080a0fdb73f195a53e7f3c9ccc52be602b6cc19b",
      "filename": "clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java",
      "status": "modified",
      "additions": 8,
      "deletions": 8,
      "changes": 16,
      "blob_url": "https://github.com/apache/kafka/blob/ea77352dfc0e8adf860c0f31dc4b53bf36f95b4b/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducerTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/ea77352dfc0e8adf860c0f31dc4b53bf36f95b4b/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducerTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2FKafkaProducerTest.java?ref=ea77352dfc0e8adf860c0f31dc4b53bf36f95b4b",
      "patch": "@@ -421,45 +421,45 @@ public void testInflightRequestsAndIdempotenceForIdempotentProducers() {\n             config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION),\n             \"max.in.flight.requests.per.connection should be overwritten\");\n \n-        Properties validProps2 = new Properties() {{\n+        Properties invalidProps1 = new Properties() {{\n                 putAll(baseProps);\n                 setProperty(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"6\");\n             }};\n \n-        ConfigException configException = assertThrows(ConfigException.class, () -> new ProducerConfig(validProps2));\n+        ConfigException configException = assertThrows(ConfigException.class, () -> new ProducerConfig(invalidProps1));\n         assertEquals(\"To use the idempotent producer, \" + ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION +\n                      \" must be set to at most 5. Current value is 6.\", configException.getMessage());\n \n-        Properties invalidProps = new Properties() {{\n+        Properties invalidProps2 = new Properties() {{\n                 putAll(baseProps);\n                 setProperty(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"5\");\n                 setProperty(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"false\");\n                 setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"transactionalId\");\n             }};\n         assertThrows(\n             ConfigException.class,\n-            () -> new ProducerConfig(invalidProps),\n+            () -> new ProducerConfig(invalidProps2),\n             \"Cannot set a transactional.id without also enabling idempotence\");\n \n-        Properties invalidProps2 = new Properties() {{\n+        Properties invalidProps3 = new Properties() {{\n                 putAll(baseProps);\n                 setProperty(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"6\");\n                 // explicitly enabling idempotence should still throw exception\n                 setProperty(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, \"true\");\n             }};\n         assertThrows(\n             ConfigException.class,\n-            () -> new ProducerConfig(invalidProps2),\n+            () -> new ProducerConfig(invalidProps3),\n             \"Must set max.in.flight.requests.per.connection to at most 5 when using the idempotent producer.\");\n \n-        Properties invalidProps3 = new Properties() {{\n+        Properties invalidProps4 = new Properties() {{\n                 putAll(baseProps);\n                 setProperty(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \"6\");\n                 setProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"transactionalId\");\n             }};\n         assertThrows(\n             ConfigException.class,\n-            () -> new ProducerConfig(invalidProps3),\n+            () -> new ProducerConfig(invalidProps4),\n             \"Must set retries to non-zero when using the idempotent producer.\");\n     }\n ",
      "parent_sha": "00b22b001fb35e611d85abbfd4511b32058e0048"
    }
  },
  {
    "oid": "346d0ca5382800f4bbc670b80660408705742392",
    "message": "MINOR: Fix needless GC + Result time unit in JMH\n\nFixes two issues with the JMH benchmark example:\n* Trivial: The output should be in `ops/ms` for readability\nreasons (it's in the millions of operations per second)\n* Important: The benchmark is not actually measuring the\nLRU Cache performance as most of the time in each run is\nwasted on concatenating `key + counter` as well as\n`value + counter`. Fixed by pre-generating 10k K-V pairs\n(100x the cache capacity) and iterating over them. This\nbrings the performance up by a factor of more than 5 on\na standard 4 core i7 (`~6k/ms` before goes to `~35k/ms`).\n\nAuthor: Armin Braun <me@obrown.io>\n\nReviewers: Bill Bejeck <bbejeck@gmail.com>, Guozhang Wang <wangguoz@gmail.com>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #2903 from original-brownbear/fix-jmh-example",
    "date": "2017-09-18T09:52:54Z",
    "url": "https://github.com/apache/kafka/commit/346d0ca5382800f4bbc670b80660408705742392",
    "details": {
      "sha": "7d6597993bf03bddb320dbf2c415793061f744ce",
      "filename": "jmh-benchmarks/src/main/java/org/apache/kafka/jmh/cache/LRUCacheBenchmark.java",
      "status": "modified",
      "additions": 22,
      "deletions": 6,
      "changes": 28,
      "blob_url": "https://github.com/apache/kafka/blob/346d0ca5382800f4bbc670b80660408705742392/jmh-benchmarks%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fjmh%2Fcache%2FLRUCacheBenchmark.java",
      "raw_url": "https://github.com/apache/kafka/raw/346d0ca5382800f4bbc670b80660408705742392/jmh-benchmarks%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fjmh%2Fcache%2FLRUCacheBenchmark.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/jmh-benchmarks%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fjmh%2Fcache%2FLRUCacheBenchmark.java?ref=346d0ca5382800f4bbc670b80660408705742392",
      "patch": "@@ -17,9 +17,11 @@\n \n package org.apache.kafka.jmh.cache;\n \n+import java.util.concurrent.TimeUnit;\n import org.apache.kafka.common.cache.LRUCache;\n import org.openjdk.jmh.annotations.Benchmark;\n import org.openjdk.jmh.annotations.Level;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n import org.openjdk.jmh.annotations.Scope;\n import org.openjdk.jmh.annotations.Setup;\n import org.openjdk.jmh.annotations.State;\n@@ -35,25 +37,39 @@\n  * http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/\n  */\n @State(Scope.Thread)\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n public class LRUCacheBenchmark {\n \n+    private static final int DISTINCT_KEYS = 10_000;\n+\n+    private static final String KEY = \"the_key_to_use\";\n+\n+    private static final String VALUE = \"the quick brown fox jumped over the lazy dog the olympics are about to start\";\n+\n+    private final String[] keys = new String[DISTINCT_KEYS];\n+\n+    private final String[] values = new String[DISTINCT_KEYS];\n+\n     private LRUCache<String, String> lruCache;\n \n-    private final String key = \"the_key_to_use\";\n-    private final String value = \"the quick brown fox jumped over the lazy dog the olympics are about to start\";\n     int counter;\n \n-\n     @Setup(Level.Trial)\n-    public void setUpCaches() {\n+    public void setUp() {\n+        for (int i = 0; i < DISTINCT_KEYS; ++i) {\n+            keys[i] = KEY + i;\n+            values[i] = VALUE + i;\n+        }\n         lruCache = new LRUCache<>(100);\n     }\n \n     @Benchmark\n     public String testCachePerformance() {\n         counter++;\n-        lruCache.put(key + counter, value + counter);\n-        return lruCache.get(key + counter);\n+        int index = counter % DISTINCT_KEYS;\n+        String hashkey = keys[index];\n+        lruCache.put(hashkey, values[index]);\n+        return lruCache.get(hashkey);\n     }\n \n     public static void main(String[] args) throws RunnerException {",
      "parent_sha": "a3f068e22da6915cbc473be8da0cfbb229817436"
    }
  },
  {
    "oid": "f99bb0466e9c8845963b350f3ad9b353db72fd5a",
    "message": "MINOR: Start correlation id at 0 in SaslClientAuthenticator (#7432)\n\nI tried to add a test for this, but it's actually pretty hard to verify what we want to\r\nverify. I could add a test that checks the correlation field after the connection\r\nhas been established, but it would not catch this kind of bug where the issue is not\r\nthe value we store, but the value we create the request header with.\r\n\r\nI have another PR that avoids intermediate structs during serialization/deserialization,\r\nwhich has a test that fails without this change. So we'll get coverage that way.\r\n\r\nReviewers: Rajini Sivaram <rajinisivaram@googlemail.com>",
    "date": "2019-10-03T04:02:33Z",
    "url": "https://github.com/apache/kafka/commit/f99bb0466e9c8845963b350f3ad9b353db72fd5a",
    "details": {
      "sha": "266759000b12a539bfef12e466a43ff2b62a70ce",
      "filename": "clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslClientAuthenticator.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/f99bb0466e9c8845963b350f3ad9b353db72fd5a/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fsecurity%2Fauthenticator%2FSaslClientAuthenticator.java",
      "raw_url": "https://github.com/apache/kafka/raw/f99bb0466e9c8845963b350f3ad9b353db72fd5a/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fsecurity%2Fauthenticator%2FSaslClientAuthenticator.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fsecurity%2Fauthenticator%2FSaslClientAuthenticator.java?ref=f99bb0466e9c8845963b350f3ad9b353db72fd5a",
      "patch": "@@ -150,7 +150,7 @@ public SaslClientAuthenticator(Map<String, ?> configs,\n         this.host = host;\n         this.servicePrincipal = servicePrincipal;\n         this.mechanism = mechanism;\n-        this.correlationId = -1;\n+        this.correlationId = 0;\n         this.transportLayer = transportLayer;\n         this.configs = configs;\n         this.saslAuthenticateVersion = DISABLE_KAFKA_SASL_AUTHENTICATE_HEADER;",
      "parent_sha": "6925775e63fd33e6a44bbda671b2de7db41d150e"
    }
  },
  {
    "oid": "f54cfff1dcaaa43139417fddf38a7009cd26e710",
    "message": "MINOR: simplify producer TX abort error handling (#18486)\n\nReviewers: Justine Olshan <jolshan@confluent.io>, Jason Gustafson <jason@responsive.dev>",
    "date": "2025-01-11T01:54:40Z",
    "url": "https://github.com/apache/kafka/commit/f54cfff1dcaaa43139417fddf38a7009cd26e710",
    "details": {
      "sha": "9190281a660809f861b47598682bc9f09be793a4",
      "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java",
      "status": "modified",
      "additions": 4,
      "deletions": 11,
      "changes": 15,
      "blob_url": "https://github.com/apache/kafka/blob/f54cfff1dcaaa43139417fddf38a7009cd26e710/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2Finternals%2FSender.java",
      "raw_url": "https://github.com/apache/kafka/raw/f54cfff1dcaaa43139417fddf38a7009cd26e710/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2Finternals%2FSender.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fproducer%2Finternals%2FSender.java?ref=f54cfff1dcaaa43139417fddf38a7009cd26e710",
      "patch": "@@ -462,17 +462,10 @@ private boolean maybeSendAndPollTransactionalRequest() {\n             return true;\n         }\n \n-        if (transactionManager.hasAbortableError() || transactionManager.isAborting()) {\n-            if (accumulator.hasIncomplete()) {\n-                // Attempt to get the last error that caused this abort.\n-                RuntimeException exception = transactionManager.lastError();\n-                // If there was no error, but we are still aborting,\n-                // then this is most likely a case where there was no fatal error.\n-                if (exception == null) {\n-                    exception = new TransactionAbortedException();\n-                }\n-                accumulator.abortUndrainedBatches(exception);\n-            }\n+        if (transactionManager.hasAbortableError()) {\n+            accumulator.abortUndrainedBatches(transactionManager.lastError());\n+        } else if (transactionManager.isAborting()) {\n+            accumulator.abortUndrainedBatches(new TransactionAbortedException());\n         }\n \n         TransactionManager.TxnRequestHandler nextRequestHandler = transactionManager.nextRequest(accumulator.hasIncomplete());",
      "parent_sha": "32dbbe6a1f3ef39318c796bdc0a3b8da2c7060ad"
    }
  },
  {
    "oid": "b8c095074d6710f4cde4b1f0b568b0c66f6d53d8",
    "message": "MINOR: Rename RemoteLogStorageManager variable to RemoteStorageManager (#19401)\n\nThis patch renames the KIP-405 Plugin variable from\n`remoteLogStorageManager` to `remoteStorageManager`. After [writing\nabout\n\nit](https://aiven.io/blog/apache-kafka-tiered-storage-in-depth-how-writes-and-metadata-flow),\nI realized I got swayed by the code and called the component incorrectly\n- the official name doesn't have `Log` in it. I thought i'd go ahead and\nchange the code so it's consistent with the naming too\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2025-04-07T16:02:38Z",
    "url": "https://github.com/apache/kafka/commit/b8c095074d6710f4cde4b1f0b568b0c66f6d53d8",
    "details": {
      "sha": "036d817066c7d8589b1e968ea018e65f0387d97e",
      "filename": "core/src/main/java/kafka/log/remote/RemoteLogManager.java",
      "status": "modified",
      "additions": 11,
      "deletions": 11,
      "changes": 22,
      "blob_url": "https://github.com/apache/kafka/blob/b8c095074d6710f4cde4b1f0b568b0c66f6d53d8/core%2Fsrc%2Fmain%2Fjava%2Fkafka%2Flog%2Fremote%2FRemoteLogManager.java",
      "raw_url": "https://github.com/apache/kafka/raw/b8c095074d6710f4cde4b1f0b568b0c66f6d53d8/core%2Fsrc%2Fmain%2Fjava%2Fkafka%2Flog%2Fremote%2FRemoteLogManager.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/core%2Fsrc%2Fmain%2Fjava%2Fkafka%2Flog%2Fremote%2FRemoteLogManager.java?ref=b8c095074d6710f4cde4b1f0b568b0c66f6d53d8",
      "patch": "@@ -168,7 +168,7 @@ public class RemoteLogManager implements Closeable, AsyncOffsetReader {\n     private final BrokerTopicStats brokerTopicStats;\n     private final Metrics metrics;\n \n-    private final RemoteStorageManager remoteLogStorageManager;\n+    private final RemoteStorageManager remoteStorageManager;\n \n     private final RemoteLogMetadataManager remoteLogMetadataManager;\n \n@@ -238,7 +238,7 @@ public RemoteLogManager(RemoteLogManagerConfig rlmConfig,\n         this.brokerTopicStats = brokerTopicStats;\n         this.metrics = metrics;\n \n-        remoteLogStorageManager = createRemoteStorageManager();\n+        remoteStorageManager = createRemoteStorageManager();\n         remoteLogMetadataManager = createRemoteLogMetadataManager();\n         rlmCopyQuotaManager = createRLMCopyQuotaManager();\n         rlmFetchQuotaManager = createRLMFetchQuotaManager();\n@@ -248,7 +248,7 @@ public RemoteLogManager(RemoteLogManagerConfig rlmConfig,\n         copyThrottleTimeSensor = new RLMQuotaMetrics(metrics, \"remote-copy-throttle-time\", RemoteLogManager.class.getSimpleName(),\n             \"The %s time in millis remote copies was throttled by a broker\", INACTIVE_SENSOR_EXPIRATION_TIME_SECONDS).sensor();\n \n-        indexCache = new RemoteIndexCache(rlmConfig.remoteLogIndexFileCacheTotalSizeBytes(), remoteLogStorageManager, logDir);\n+        indexCache = new RemoteIndexCache(rlmConfig.remoteLogIndexFileCacheTotalSizeBytes(), remoteStorageManager, logDir);\n         delayInMs = rlmConfig.remoteLogManagerTaskIntervalMs();\n         rlmCopyThreadPool = new RLMScheduledThreadPool(rlmConfig.remoteLogManagerCopierThreadPoolSize(),\n             \"RLMCopyThreadPool\", \"kafka-rlm-copy-thread-pool-%d\");\n@@ -374,7 +374,7 @@ RemoteStorageManager createRemoteStorageManager() {\n     private void configureRSM() {\n         final Map<String, Object> rsmProps = new HashMap<>(rlmConfig.remoteStorageManagerProps());\n         rsmProps.put(ServerConfigs.BROKER_ID_CONFIG, brokerId);\n-        remoteLogStorageManager.configure(rsmProps);\n+        remoteStorageManager.configure(rsmProps);\n     }\n \n     RemoteLogMetadataManager createRemoteLogMetadataManager() {\n@@ -423,7 +423,7 @@ private boolean isRemoteLogManagerConfigured() {\n     }\n \n     public RemoteStorageManager storageManager() {\n-        return remoteLogStorageManager;\n+        return remoteStorageManager;\n     }\n \n     private Stream<Partition> filterPartitions(Set<Partition> partitions) {\n@@ -575,7 +575,7 @@ private void deleteRemoteLogPartition(TopicIdPartition partition) throws RemoteS\n         Collection<Uuid> deletedSegmentIds = new ArrayList<>();\n         for (RemoteLogSegmentMetadata metadata: metadataList) {\n             deletedSegmentIds.add(metadata.remoteLogSegmentId().id());\n-            remoteLogStorageManager.deleteLogSegmentData(metadata);\n+            remoteStorageManager.deleteLogSegmentData(metadata);\n         }\n         indexCache.removeAll(deletedSegmentIds);\n \n@@ -632,7 +632,7 @@ Optional<FileRecords.TimestampAndOffset> lookupTimestamp(RemoteLogSegmentMetadat\n         InputStream remoteSegInputStream = null;\n         try {\n             // Search forward for the position of the last offset that is greater than or equal to the startingOffset\n-            remoteSegInputStream = remoteLogStorageManager.fetchLogSegment(rlsMetadata, startPos);\n+            remoteSegInputStream = remoteStorageManager.fetchLogSegment(rlsMetadata, startPos);\n             RemoteLogInputStream remoteLogInputStream = new RemoteLogInputStream(remoteSegInputStream);\n \n             while (true) {\n@@ -1027,7 +1027,7 @@ private void copyLogSegment(UnifiedLog log, LogSegment segment, RemoteLogSegment\n             Optional<CustomMetadata> customMetadata;\n             \n             try {\n-                customMetadata = remoteLogStorageManager.copyLogSegmentData(copySegmentStartedRlsm, segmentData);\n+                customMetadata = remoteStorageManager.copyLogSegmentData(copySegmentStartedRlsm, segmentData);\n             } catch (RemoteStorageException e) {\n                 logger.info(\"Copy failed, cleaning segment {}\", copySegmentStartedRlsm.remoteLogSegmentId());\n                 try {\n@@ -1492,7 +1492,7 @@ private boolean deleteRemoteLogSegment(\n \n             // Delete the segment in remote storage.\n             try {\n-                remoteLogStorageManager.deleteLogSegmentData(segmentMetadata);\n+                remoteStorageManager.deleteLogSegmentData(segmentMetadata);\n             } catch (RemoteStorageException e) {\n                 brokerTopicStats.topicStats(topic).failedRemoteDeleteRequestRate().mark();\n                 brokerTopicStats.allTopicsStats().failedRemoteDeleteRequestRate().mark();\n@@ -1696,7 +1696,7 @@ public FetchDataInfo read(RemoteStorageFetchInfo remoteStorageFetchInfo) throws\n                 remoteLogSegmentMetadata = rlsMetadataOptional.get();\n                 // Search forward for the position of the last offset that is greater than or equal to the target offset\n                 startPos = lookupPositionForOffset(remoteLogSegmentMetadata, offset);\n-                remoteSegInputStream = remoteLogStorageManager.fetchLogSegment(remoteLogSegmentMetadata, startPos);\n+                remoteSegInputStream = remoteStorageManager.fetchLogSegment(remoteLogSegmentMetadata, startPos);\n                 RemoteLogInputStream remoteLogInputStream = getRemoteLogInputStream(remoteSegInputStream);\n                 enrichedRecordBatch = findFirstBatch(remoteLogInputStream, offset);\n                 if (enrichedRecordBatch.batch == null) {\n@@ -2045,7 +2045,7 @@ public void close() {\n                 leaderCopyRLMTasks.values().forEach(RLMTaskWithFuture::cancel);\n                 leaderExpirationRLMTasks.values().forEach(RLMTaskWithFuture::cancel);\n                 followerRLMTasks.values().forEach(RLMTaskWithFuture::cancel);\n-                Utils.closeQuietly(remoteLogStorageManager, \"RemoteLogStorageManager\");\n+                Utils.closeQuietly(remoteStorageManager, \"RemoteStorageManager\");\n                 Utils.closeQuietly(remoteLogMetadataManager, \"RemoteLogMetadataManager\");\n                 Utils.closeQuietly(indexCache, \"RemoteIndexCache\");\n ",
      "parent_sha": "2ae4ffb5e008c6654905d4c524be3dd48ad7f390"
    }
  },
  {
    "oid": "b5da5f8bec2b87c13f82d57be8e38a397c61366e",
    "message": "MINOR: Optimize KTable-KTable join value getter supplier (#4458)\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>, Damian Guy <damian.guy@gmail.com>, Matthias J. Sax <mjsax@apache.org>",
    "date": "2018-01-29T04:17:48Z",
    "url": "https://github.com/apache/kafka/commit/b5da5f8bec2b87c13f82d57be8e38a397c61366e",
    "details": {
      "sha": "d36920ad7f0280f3f42b39611dd1e3711d83db0f",
      "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableKTableAbstractJoinValueGetterSupplier.java",
      "status": "modified",
      "additions": 3,
      "deletions": 6,
      "changes": 9,
      "blob_url": "https://github.com/apache/kafka/blob/b5da5f8bec2b87c13f82d57be8e38a397c61366e/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fkstream%2Finternals%2FKTableKTableAbstractJoinValueGetterSupplier.java",
      "raw_url": "https://github.com/apache/kafka/raw/b5da5f8bec2b87c13f82d57be8e38a397c61366e/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fkstream%2Finternals%2FKTableKTableAbstractJoinValueGetterSupplier.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fkstream%2Finternals%2FKTableKTableAbstractJoinValueGetterSupplier.java?ref=b5da5f8bec2b87c13f82d57be8e38a397c61366e",
      "patch": "@@ -17,6 +17,7 @@\n package org.apache.kafka.streams.kstream.internals;\n \n import java.util.ArrayList;\n+import java.util.Collections;\n \n public abstract class KTableKTableAbstractJoinValueGetterSupplier<K, R, V1, V2> implements KTableValueGetterSupplier<K, R> {\n     final KTableValueGetterSupplier<K, V1> valueGetterSupplier1;\n@@ -33,12 +34,8 @@ public String[] storeNames() {\n         final String[] storeNames1 = valueGetterSupplier1.storeNames();\n         final String[] storeNames2 = valueGetterSupplier2.storeNames();\n         final ArrayList<String> stores = new ArrayList<>(storeNames1.length + storeNames2.length);\n-        for (final String storeName : storeNames1) {\n-            stores.add(storeName);\n-        }\n-        for (final String storeName : storeNames2) {\n-            stores.add(storeName);\n-        }\n+        Collections.addAll(stores, storeNames1);\n+        Collections.addAll(stores, storeNames2);\n         return stores.toArray(new String[stores.size()]);\n     }\n ",
      "parent_sha": "2f267871eac86623950e8e3a67a98687e42a0b3e"
    }
  },
  {
    "oid": "47252f431e3d558e0f94ca282b21ed28d30888de",
    "message": "KAFKA-14216: Remove ZK reference from org.apache.kafka.server.quota.ClientQuotaCallback javadoc (#12617)\n\nReviewers: Luke Chen <showuon@gmail.com>",
    "date": "2022-09-12T15:34:46Z",
    "url": "https://github.com/apache/kafka/commit/47252f431e3d558e0f94ca282b21ed28d30888de",
    "details": {
      "sha": "bb1a5deafe02a03181d95a98cf5a1393b5a6714e",
      "filename": "clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaCallback.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/47252f431e3d558e0f94ca282b21ed28d30888de/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fserver%2Fquota%2FClientQuotaCallback.java",
      "raw_url": "https://github.com/apache/kafka/raw/47252f431e3d558e0f94ca282b21ed28d30888de/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fserver%2Fquota%2FClientQuotaCallback.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fserver%2Fquota%2FClientQuotaCallback.java?ref=47252f431e3d558e0f94ca282b21ed28d30888de",
      "patch": "@@ -55,7 +55,7 @@ public interface ClientQuotaCallback extends Configurable {\n \n     /**\n      * Quota configuration update callback that is invoked when quota configuration for an entity is\n-     * updated in ZooKeeper. This is useful to track configured quotas if built-in quota configuration\n+     * updated in the quorum. This is useful to track configured quotas if built-in quota configuration\n      * tools are used for quota management.\n      *\n      * @param quotaType   Type of quota being updated\n@@ -66,7 +66,7 @@ public interface ClientQuotaCallback extends Configurable {\n \n     /**\n      * Quota configuration removal callback that is invoked when quota configuration for an entity is\n-     * removed in ZooKeeper. This is useful to track configured quotas if built-in quota configuration\n+     * removed in the quorum. This is useful to track configured quotas if built-in quota configuration\n      * tools are used for quota management.\n      *\n      * @param quotaType   Type of quota being updated",
      "parent_sha": "26e3b813af88e29776c81bd065ded46b2daad720"
    }
  },
  {
    "oid": "b4100d4b2898df070450b7326f32303f834f8e09",
    "message": "KAFKA-10644; Fix VotedToUnattached test error (#9503)\n\nThis patch fixes a test a test case in `QuorumStateTest`. The method name is \"testVotedToUnattachedHigherEpoch,\" but the code initialized in the unattached state instead of the voted state.\r\n\r\nReviewers: Jason Gustafson <jason@confluent.io>",
    "date": "2020-10-27T23:41:00Z",
    "url": "https://github.com/apache/kafka/commit/b4100d4b2898df070450b7326f32303f834f8e09",
    "details": {
      "sha": "5db5395d157ca555e1d8b964761c92ed8a462782",
      "filename": "raft/src/test/java/org/apache/kafka/raft/QuorumStateTest.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/b4100d4b2898df070450b7326f32303f834f8e09/raft%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fraft%2FQuorumStateTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/b4100d4b2898df070450b7326f32303f834f8e09/raft%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fraft%2FQuorumStateTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/raft%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fraft%2FQuorumStateTest.java?ref=b4100d4b2898df070450b7326f32303f834f8e09",
      "patch": "@@ -658,9 +658,9 @@ public void testVotedToUnattachedHigherEpoch() throws IOException {\n         Set<Integer> voters = Utils.mkSet(localId, otherNodeId);\n         QuorumState state = initializeEmptyState(voters);\n         state.initialize(new OffsetAndEpoch(0L, logEndEpoch));\n-        state.transitionToUnattached(5);\n+        state.transitionToVoted(5, otherNodeId);\n \n-        long remainingElectionTimeMs = state.unattachedStateOrThrow().remainingElectionTimeMs(time.milliseconds());\n+        long remainingElectionTimeMs = state.votedStateOrThrow().remainingElectionTimeMs(time.milliseconds());\n         time.sleep(1000);\n \n         state.transitionToUnattached(6);",
      "parent_sha": "d5c77695a2a26f0103853e2401ae93018a1accf8"
    }
  },
  {
    "oid": "9f954ac614cd5dd7efbcabe34799207128f16e63",
    "message": "MINOR: Safe string conversion to avoid NPEs\n\nShould be ported back to 2.0\n\nAuthor: Cyrus Vafadari <cyrus@confluent.io>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #6004 from cyrusv/cyrus-npe",
    "date": "2018-12-05T21:23:52Z",
    "url": "https://github.com/apache/kafka/commit/9f954ac614cd5dd7efbcabe34799207128f16e63",
    "details": {
      "sha": "b181209163926978714f7d8d0d313c22da12aa77",
      "filename": "connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectRecord.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/9f954ac614cd5dd7efbcabe34799207128f16e63/connect%2Fapi%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fconnector%2FConnectRecord.java",
      "raw_url": "https://github.com/apache/kafka/raw/9f954ac614cd5dd7efbcabe34799207128f16e63/connect%2Fapi%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fconnector%2FConnectRecord.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fapi%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fconnector%2FConnectRecord.java?ref=9f954ac614cd5dd7efbcabe34799207128f16e63",
      "patch": "@@ -140,9 +140,9 @@ public String toString() {\n                 \"topic='\" + topic + '\\'' +\n                 \", kafkaPartition=\" + kafkaPartition +\n                 \", key=\" + key +\n-                \", keySchema=\" + keySchema.toString() +\n+                \", keySchema=\" + keySchema +\n                 \", value=\" + value +\n-                \", valueSchema=\" + valueSchema.toString() +\n+                \", valueSchema=\" + valueSchema +\n                 \", timestamp=\" + timestamp +\n                 \", headers=\" + headers +\n                 '}';",
      "parent_sha": "b616f913c8a57609476e5cf99b1802ef1dfe1c98"
    }
  },
  {
    "oid": "80aea23beb2654b2a53e0248387f2b4dbc0f0dc1",
    "message": "KAFKA-9295: increase startup timeout for flaky test in KTableKTableForeignKeyInnerJoinMultiIntegrationTest (#10635)\n\nTry to address the extreme flakiness of shouldInnerJoinMultiPartitionQueryable since the recent test cleanup. Since we need to wait for 3 streams reach RUNNING state, it makes sense to increase the waiting time to make the test more reliable.\r\n\r\nReviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>",
    "date": "2021-05-06T04:58:27Z",
    "url": "https://github.com/apache/kafka/commit/80aea23beb2654b2a53e0248387f2b4dbc0f0dc1",
    "details": {
      "sha": "92520fb0186a728adfc887f4d782c7b151c2b9ec",
      "filename": "streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyInnerJoinMultiIntegrationTest.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/80aea23beb2654b2a53e0248387f2b4dbc0f0dc1/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FKTableKTableForeignKeyInnerJoinMultiIntegrationTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/80aea23beb2654b2a53e0248387f2b4dbc0f0dc1/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FKTableKTableForeignKeyInnerJoinMultiIntegrationTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fintegration%2FKTableKTableForeignKeyInnerJoinMultiIntegrationTest.java?ref=80aea23beb2654b2a53e0248387f2b4dbc0f0dc1",
      "patch": "@@ -194,7 +194,7 @@ private void verifyKTableKTableJoin(final Set<KeyValue<Integer, String>> expecte\n         streamsThree = prepareTopology(queryableName, queryableNameTwo, streamsConfigThree);\n \n         final List<KafkaStreams> kafkaStreamsList = asList(streams, streamsTwo, streamsThree);\n-        startApplicationAndWaitUntilRunning(kafkaStreamsList, ofSeconds(60));\n+        startApplicationAndWaitUntilRunning(kafkaStreamsList, ofSeconds(120));\n \n         final Set<KeyValue<Integer, String>> result = new HashSet<>(IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(\n             CONSUMER_CONFIG,",
      "parent_sha": "79225504ed920e63b2a31a968b7d50f88af5ada2"
    }
  },
  {
    "oid": "102de21355e465771ddd0ba1464cac389abb0e01",
    "message": "KAFKA-17379: Fix inexpected state transition from ERROR to PENDING_SHUTDOWN (#18765)\n\nThe exception stack trace shown in the the ticket can happen when we are\r\nconcurrently closing the producer because of an error and doing a\r\nregular close. This is not a bug in the test, but a real race condition\r\nthat can happen.\r\n\r\nThe sequence is this:\r\n\r\nThread1: Enter PENDING_ERROR\r\nThread2: Check if state is already ERROR\r\nThread1: Transition to ERROR\r\nThread2: Check if state is already PENDING_ERROR\r\nThread2: Transition to PENDING_SHUTDOWN\r\n\r\nOne idea to fix this would be to synchronize the sequence performed by\r\nThread1 using the state lock. However, this would require more changes,\r\nsince we cannot use the normal state transition method `setState` while\r\nowning the lock, as it calls user-defined callbacks, which may create\r\ndeadlocks. Do avoid adding more synchronization, we can also fix it by\r\nfirst attempting to transition to PENDING_SHUTDOWN, and _then_ checking\r\nwhether another thread is already attempting to shut down (states\r\nPENDING_SHUTDOWN, PENDING_ERROR, ERROR, NOT_RUNNING). Since we never\r\ntransition from a shutdown state back to a non-shutdown state.\r\n\r\nReviewers: Matthias J. Sax <matthias@confluent.io>",
    "date": "2025-02-05T16:09:14Z",
    "url": "https://github.com/apache/kafka/commit/102de21355e465771ddd0ba1464cac389abb0e01",
    "details": {
      "sha": "4ee22f045564e3a773086d2b978556e39dba05d3",
      "filename": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
      "status": "modified",
      "additions": 33,
      "deletions": 30,
      "changes": 63,
      "blob_url": "https://github.com/apache/kafka/blob/102de21355e465771ddd0ba1464cac389abb0e01/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FKafkaStreams.java",
      "raw_url": "https://github.com/apache/kafka/raw/102de21355e465771ddd0ba1464cac389abb0e01/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FKafkaStreams.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FKafkaStreams.java?ref=102de21355e465771ddd0ba1464cac389abb0e01",
      "patch": "@@ -109,6 +109,7 @@\n import java.util.function.BiConsumer;\n import java.util.function.Consumer;\n import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n \n import static org.apache.kafka.streams.StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG;\n import static org.apache.kafka.streams.errors.StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_CLIENT;\n@@ -293,13 +294,14 @@ public boolean isValidTransition(final State newState) {\n     private final Object stateLock = new Object();\n     protected volatile State state = State.CREATED;\n \n-    private boolean waitOnState(final State targetState, final long waitMs) {\n+    private boolean waitOnStates(final long waitMs, final State... targetStates) {\n+        final Set<State> targetStateSet = Set.of(targetStates);\n         final long begin = time.milliseconds();\n         synchronized (stateLock) {\n             boolean interrupted = false;\n             long elapsedMs = 0L;\n             try {\n-                while (state != targetState) {\n+                while (!targetStateSet.contains(state)) {\n                     if (waitMs > elapsedMs) {\n                         final long remainingMs = waitMs - elapsedMs;\n                         try {\n@@ -308,7 +310,11 @@ private boolean waitOnState(final State targetState, final long waitMs) {\n                             interrupted = true;\n                         }\n                     } else {\n-                        log.debug(\"Cannot transit to {} within {}ms\", targetState, waitMs);\n+                        log.debug(\n+                            \"Cannot transit to {} within {}ms\",\n+                            Arrays.stream(targetStates).map(State::toString).collect(Collectors.joining(\" or \")),\n+                            waitMs\n+                        );\n                         return false;\n                     }\n                     elapsedMs = time.milliseconds() - begin;\n@@ -347,8 +353,9 @@ private boolean setState(final State newState) {\n             } else if (state == State.REBALANCING && newState == State.REBALANCING) {\n                 // when the state is already in REBALANCING, it should not transit to REBALANCING again\n                 return false;\n-            } else if (state == State.ERROR && (newState == State.PENDING_ERROR || newState == State.ERROR)) {\n-                // when the state is already in ERROR, its transition to PENDING_ERROR or ERROR (due to consecutive close calls)\n+            } else if (state == State.ERROR && (newState == State.PENDING_ERROR || newState == State.ERROR || newState == State.PENDING_SHUTDOWN)) {\n+                // when the state is already in ERROR, its transition attempts to PENDING_SHUTDOWN, PENDING_ERROR or ERROR will\n+                // not throw an exception.\n                 return false;\n             } else if (state == State.PENDING_ERROR && newState != State.ERROR) {\n                 // when the state is already in PENDING_ERROR, all other transitions than ERROR (due to thread dying) will be\n@@ -1543,38 +1550,34 @@ private boolean close(final Optional<Long> timeout, final boolean leaveGroup) {\n             timeoutMs = Long.MAX_VALUE;\n         }\n \n-        if (state.hasCompletedShutdown()) {\n-            log.info(\"Streams client is already in the terminal {} state, all resources are closed and the client has stopped.\", state);\n-            return true;\n-        }\n-        if (state.isShuttingDown()) {\n-            log.info(\"Streams client is in {}, all resources are being closed and the client will be stopped.\", state);\n-            if (state == State.PENDING_ERROR && waitOnState(State.ERROR, timeoutMs)) {\n-                log.info(\"Streams client stopped to ERROR completely\");\n+        if (!setState(State.PENDING_SHUTDOWN)) {\n+            // Copy the state so that we can atomically check if we are shut down and act on it (log it)\n+            final State immutableStateCopy = state;\n+            if (immutableStateCopy.isShuttingDown()) {\n+                log.info(\"Skipping shutdown since Streams client is already in {}, waiting for a terminal state\", immutableStateCopy);\n+                if (!waitOnStates(timeoutMs, State.ERROR, State.NOT_RUNNING)) {\n+                    log.warn(\"Streams client did transition to a terminal state (ERROR or NOT_RUNNING) within the {}ms timeout\", timeoutMs);\n+                    return false;\n+                }\n+                log.info(\"Streams client stopped completely and transitioned to the terminal {} state\", state);\n                 return true;\n-            } else if (state == State.PENDING_SHUTDOWN && waitOnState(State.NOT_RUNNING, timeoutMs)) {\n-                log.info(\"Streams client stopped to NOT_RUNNING completely\");\n+            }\n+\n+            if (state.hasCompletedShutdown()) {\n+                log.info(\"Skipping shutdown since Streams client is already in the terminal {} state\", state);\n                 return true;\n-            } else {\n-                log.warn(\"Streams client cannot transition to {} completely within the timeout\",\n-                         state == State.PENDING_SHUTDOWN ? State.NOT_RUNNING : State.ERROR);\n-                return false;\n             }\n-        }\n \n-        if (!setState(State.PENDING_SHUTDOWN)) {\n-            // if we can't transition to PENDING_SHUTDOWN but not because we're already shutting down, then it must be fatal\n-            log.error(\"Failed to transition to PENDING_SHUTDOWN, current state is {}\", state);\n-            throw new StreamsException(\"Failed to shut down while in state \" + state);\n-        } else {\n+            throw new IllegalStateException(\"If transitioning to PENDING_SHUTDOWN fails, the state should be either in \"\n+                + \"PENDING_SHUTDOWN, PENDING_ERROR, ERROR, or NOT_RUNNING\");\n+        }\n \n-            final Thread shutdownThread = shutdownHelper(false, timeoutMs, leaveGroup);\n+        final Thread shutdownThread = shutdownHelper(false, timeoutMs, leaveGroup);\n \n-            shutdownThread.setDaemon(true);\n-            shutdownThread.start();\n-        }\n+        shutdownThread.setDaemon(true);\n+        shutdownThread.start();\n \n-        if (waitOnState(State.NOT_RUNNING, timeoutMs)) {\n+        if (waitOnStates(timeoutMs, State.NOT_RUNNING)) {\n             log.info(\"Streams client stopped completely\");\n             return true;\n         } else {",
      "parent_sha": "01587d09d82861144f57ba87067c01e72329a127"
    }
  },
  {
    "oid": "d1db3d8e14022e1d5f60b161aff1344112bc3d0d",
    "message": "KAFKA-18805: add synchronized block for Consumer Heartbeat close (#18920)\n\nadd synchronized block for Consumer Heartbeat close.\r\n\r\nReviewers: Luke Chen <showuon@gmail.com>",
    "date": "2025-02-17T06:38:20Z",
    "url": "https://github.com/apache/kafka/commit/d1db3d8e14022e1d5f60b161aff1344112bc3d0d",
    "details": {
      "sha": "b530ca562b981d08ef1f96987354bbd986fcd945",
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java",
      "status": "modified",
      "additions": 3,
      "deletions": 1,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/d1db3d8e14022e1d5f60b161aff1344112bc3d0d/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2Finternals%2FAbstractCoordinator.java",
      "raw_url": "https://github.com/apache/kafka/raw/d1db3d8e14022e1d5f60b161aff1344112bc3d0d/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2Finternals%2FAbstractCoordinator.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2Finternals%2FAbstractCoordinator.java?ref=d1db3d8e14022e1d5f60b161aff1344112bc3d0d",
      "patch": "@@ -1577,7 +1577,9 @@ public void onFailure(RuntimeException e) {\n                     this.failed.set(new RuntimeException(e));\n             } finally {\n                 log.debug(\"Heartbeat thread has closed\");\n-                this.closed = true;\n+                synchronized (AbstractCoordinator.this) {\n+                    this.closed = true;\n+                }\n             }\n         }\n ",
      "parent_sha": "85c337af440e53ceadac90a517eee43a127699f0"
    }
  },
  {
    "oid": "ed5076923483bba36df6d4b70575ee0ffd6a046c",
    "message": "MINOR: Check for null timestamp rather than value in hashcode\n\nAuthor: Andrew Stevenson <andrew@datamountaineer.com>\n\nReviewers: Shikhar Bhushan <shikhar@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #2055 from andrewstevenson/kafka-4334",
    "date": "2016-10-24T05:32:03Z",
    "url": "https://github.com/apache/kafka/commit/ed5076923483bba36df6d4b70575ee0ffd6a046c",
    "details": {
      "sha": "d6319a171198c4ef2e3965a254ed64273ddf0a85",
      "filename": "connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectRecord.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/ed5076923483bba36df6d4b70575ee0ffd6a046c/connect%2Fapi%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fconnector%2FConnectRecord.java",
      "raw_url": "https://github.com/apache/kafka/raw/ed5076923483bba36df6d4b70575ee0ffd6a046c/connect%2Fapi%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fconnector%2FConnectRecord.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fapi%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fconnector%2FConnectRecord.java?ref=ed5076923483bba36df6d4b70575ee0ffd6a046c",
      "patch": "@@ -125,7 +125,7 @@ public int hashCode() {\n         result = 31 * result + (key != null ? key.hashCode() : 0);\n         result = 31 * result + (valueSchema != null ? valueSchema.hashCode() : 0);\n         result = 31 * result + (value != null ? value.hashCode() : 0);\n-        result = 31 * result + (value != null ? timestamp.hashCode() : 0);\n+        result = 31 * result + (timestamp != null ? timestamp.hashCode() : 0);\n         return result;\n     }\n }",
      "parent_sha": "63da487213765cb543a4255a83acb876a57d3634"
    }
  },
  {
    "oid": "bd262ac708062e502406e8d775f4c9432a5364e7",
    "message": "KAFKA-842 Mirror maker can lose some messages during shutdown; reviewed by Jun Rao",
    "date": "2013-04-03T20:43:50Z",
    "url": "https://github.com/apache/kafka/commit/bd262ac708062e502406e8d775f4c9432a5364e7",
    "details": {
      "sha": "3c18286182f3aef527e7727e16128c4fd3b2e67e",
      "filename": "core/src/main/scala/kafka/tools/KafkaMigrationTool.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/apache/kafka/blob/bd262ac708062e502406e8d775f4c9432a5364e7/core%2Fsrc%2Fmain%2Fscala%2Fkafka%2Ftools%2FKafkaMigrationTool.java",
      "raw_url": "https://github.com/apache/kafka/raw/bd262ac708062e502406e8d775f4c9432a5364e7/core%2Fsrc%2Fmain%2Fscala%2Fkafka%2Ftools%2FKafkaMigrationTool.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/core%2Fsrc%2Fmain%2Fscala%2Fkafka%2Ftools%2FKafkaMigrationTool.java?ref=bd262ac708062e502406e8d775f4c9432a5364e7",
      "patch": "@@ -385,8 +385,10 @@ public void run() {\n       try{\n         while(true) {\n           KeyedMessage<byte[], byte[]> data = producerDataChannel.receiveRequest();\n-          if(!data.equals(shutdownMessage))\n+          if(!data.equals(shutdownMessage)) {\n             producer.send(data);\n+            if(logger.isDebugEnabled()) logger.debug(\"Sending message %s\".format(new String(data.message())));\n+          }\n           else\n             break;\n         }\n@@ -410,6 +412,7 @@ public void shutdown() {\n     public void awaitShutdown() {\n       try {\n         shutdownComplete.await();\n+        producer.close();\n         logger.info(\"Producer thread \" + threadName + \" shutdown complete\");\n       } catch(InterruptedException ie) {\n         logger.warn(\"Interrupt during shutdown of ProducerThread\", ie);",
      "parent_sha": "3c27988ca4036985f4c7bef62b9bbae3f95f0fb9"
    }
  },
  {
    "oid": "83081652abe3a7c836efe21cdfa52f2c0063953f",
    "message": "MINOR: Fix ConnectWorkerIntegrationTest::testBrokerCoordinator (#16585)\n\nReviewers: Greg Harris <greg.harris@aiven.io>",
    "date": "2024-07-13T07:48:19Z",
    "url": "https://github.com/apache/kafka/commit/83081652abe3a7c836efe21cdfa52f2c0063953f",
    "details": {
      "sha": "6402669f677d7506bb31a88aca35ee3adec7341d",
      "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ConnectWorkerIntegrationTest.java",
      "status": "modified",
      "additions": 4,
      "deletions": 4,
      "changes": 8,
      "blob_url": "https://github.com/apache/kafka/blob/83081652abe3a7c836efe21cdfa52f2c0063953f/connect%2Fruntime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fintegration%2FConnectWorkerIntegrationTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/83081652abe3a7c836efe21cdfa52f2c0063953f/connect%2Fruntime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fintegration%2FConnectWorkerIntegrationTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fruntime%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fintegration%2FConnectWorkerIntegrationTest.java?ref=83081652abe3a7c836efe21cdfa52f2c0063953f",
      "patch": "@@ -260,6 +260,10 @@ public void testBrokerCoordinator() throws Exception {\n \n         connect.kafka().stopOnlyKafka();\n \n+        // Allow for the workers to discover that the coordinator is unavailable, wait is\n+        // heartbeat timeout * 2 + 4sec\n+        Thread.sleep(TimeUnit.SECONDS.toMillis(10));\n+\n         connect.requestTimeout(1000);\n         assertFalse(\n                 connect.anyWorkersHealthy(),\n@@ -279,10 +283,6 @@ public void testBrokerCoordinator() throws Exception {\n         });\n         connect.resetRequestTimeout();\n \n-        // Allow for the workers to discover that the coordinator is unavailable, wait is\n-        // heartbeat timeout * 2 + 4sec\n-        Thread.sleep(TimeUnit.SECONDS.toMillis(10));\n-\n         // Wait for the connector to be stopped\n         assertTrue(stopLatch.await(CONNECTOR_SETUP_DURATION_MS, TimeUnit.MILLISECONDS),\n                 \"Failed to stop connector and tasks after coordinator failure within \"",
      "parent_sha": "7495e70365ebeba4806fc4da23c2ca75fa191649"
    }
  },
  {
    "oid": "5f02ef952e500fda5272609cec90fe1eb30da6fa",
    "message": "KAFKA-17340 correct the docs of allow.auto.create.topics (#16880)\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2024-08-18T19:56:25Z",
    "url": "https://github.com/apache/kafka/commit/5f02ef952e500fda5272609cec90fe1eb30da6fa",
    "details": {
      "sha": "e32d42861387ee2dc9d8cf8b839c7ca80696b289",
      "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/ConsumerConfig.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/5f02ef952e500fda5272609cec90fe1eb30da6fa/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FConsumerConfig.java",
      "raw_url": "https://github.com/apache/kafka/raw/5f02ef952e500fda5272609cec90fe1eb30da6fa/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FConsumerConfig.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fconsumer%2FConsumerConfig.java?ref=5f02ef952e500fda5272609cec90fe1eb30da6fa",
      "patch": "@@ -374,7 +374,7 @@ public class ConsumerConfig extends AbstractConfig {\n     private static final String ALLOW_AUTO_CREATE_TOPICS_DOC = \"Allow automatic topic creation on the broker when\" +\n             \" subscribing to or assigning a topic. A topic being subscribed to will be automatically created only if the\" +\n             \" broker allows for it using `auto.create.topics.enable` broker configuration. This configuration must\" +\n-            \" be set to `false` when using brokers older than 0.11.0\";\n+            \" be set to `true` when using brokers older than 0.11.0\";\n     public static final boolean DEFAULT_ALLOW_AUTO_CREATE_TOPICS = true;\n \n     /**",
      "parent_sha": "4b4107116538f99ded780fd8efef094576c3d151"
    }
  },
  {
    "oid": "437382cceb8549f33948398c92040e6fc16e0ed7",
    "message": "KAFKA-8348: Fix KafkaStreams JavaDocs (#6707)",
    "date": "2019-05-10T09:54:09Z",
    "url": "https://github.com/apache/kafka/commit/437382cceb8549f33948398c92040e6fc16e0ed7",
    "details": {
      "sha": "02586f99d15f3aee6e1d85643025854a85c6225b",
      "filename": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/437382cceb8549f33948398c92040e6fc16e0ed7/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FKafkaStreams.java",
      "raw_url": "https://github.com/apache/kafka/raw/437382cceb8549f33948398c92040e6fc16e0ed7/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FKafkaStreams.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FKafkaStreams.java?ref=437382cceb8549f33948398c92040e6fc16e0ed7",
      "patch": "@@ -106,14 +106,14 @@\n  * <p>\n  * A simple example might look like this:\n  * <pre>{@code\n- * Map<String, Object> props = new HashMap<>();\n+ * Properties props = new Properties();\n  * props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"my-stream-processing-application\");\n  * props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n  * props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n  * props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());\n  *\n  * StreamsBuilder builder = new StreamsBuilder();\n- * builder.<String, String>stream(\"my-input-topic\").mapValues(value -> value.length().toString()).to(\"my-output-topic\");\n+ * builder.<String, String>stream(\"my-input-topic\").mapValues(value -> String.valueOf(value.length())).to(\"my-output-topic\");\n  *\n  * KafkaStreams streams = new KafkaStreams(builder.build(), props);\n  * streams.start();",
      "parent_sha": "31859a452b68ddd98281ff341a6b75c579f3c050"
    }
  },
  {
    "oid": "5c00191ea957fef425bf5dbbe47d70e41249e2d6",
    "message": "MINOR: Removed accidental double negation in error message. (#7834)",
    "date": "2020-01-15T20:25:43Z",
    "url": "https://github.com/apache/kafka/commit/5c00191ea957fef425bf5dbbe47d70e41249e2d6",
    "details": {
      "sha": "11a8c70a0075e07654264ec4602022680f27c739",
      "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/5c00191ea957fef425bf5dbbe47d70e41249e2d6/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fdistributed%2FDistributedHerder.java",
      "raw_url": "https://github.com/apache/kafka/raw/5c00191ea957fef425bf5dbbe47d70e41249e2d6/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fdistributed%2FDistributedHerder.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fdistributed%2FDistributedHerder.java?ref=5c00191ea957fef425bf5dbbe47d70e41249e2d6",
      "patch": "@@ -1620,7 +1620,7 @@ public void onRevoked(String leader, Collection<String> connectors, Collection<C\n                 statusBackingStore.flush();\n                 log.info(\"Finished flushing status backing store in preparation for rebalance\");\n             } else {\n-                log.info(\"Wasn't unable to resume work after last rebalance, can skip stopping connectors and tasks\");\n+                log.info(\"Wasn't able to resume work after last rebalance, can skip stopping connectors and tasks\");\n             }\n         }\n     }",
      "parent_sha": "0c76fbbbedb5d342080dcaf0207602c158d3116d"
    }
  },
  {
    "oid": "c1e840050c11d2f1d20118273c572e4c8491866d",
    "message": "KAFKA-4290: Fix timeout overflow in WorkerCoordinator.poll\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #2009 from hachikuji/KAFKA-4290",
    "date": "2016-10-11T06:03:06Z",
    "url": "https://github.com/apache/kafka/commit/c1e840050c11d2f1d20118273c572e4c8491866d",
    "details": {
      "sha": "8a065f1dab6defdfd4bf46c8f476d78123670457",
      "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerCoordinator.java",
      "status": "modified",
      "additions": 12,
      "deletions": 6,
      "changes": 18,
      "blob_url": "https://github.com/apache/kafka/blob/c1e840050c11d2f1d20118273c572e4c8491866d/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fdistributed%2FWorkerCoordinator.java",
      "raw_url": "https://github.com/apache/kafka/raw/c1e840050c11d2f1d20118273c572e4c8491866d/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fdistributed%2FWorkerCoordinator.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fdistributed%2FWorkerCoordinator.java?ref=c1e840050c11d2f1d20118273c572e4c8491866d",
      "patch": "@@ -102,10 +102,11 @@ public String protocolType() {\n \n     public void poll(long timeout) {\n         // poll for io until the timeout expires\n-        long now = time.milliseconds();\n-        long deadline = now + timeout;\n+        final long start = time.milliseconds();\n+        long now = start;\n+        long remaining;\n \n-        while (now <= deadline) {\n+        do {\n             if (coordinatorUnknown()) {\n                 ensureCoordinatorReady();\n                 now = time.milliseconds();\n@@ -118,12 +119,17 @@ public void poll(long timeout) {\n \n             pollHeartbeat(now);\n \n+            long elapsed = now - start;\n+            remaining = timeout - elapsed;\n+\n             // Note that because the network client is shared with the background heartbeat thread,\n             // we do not want to block in poll longer than the time to the next heartbeat.\n-            long remaining = Math.max(0, deadline - now);\n-            client.poll(Math.min(remaining, timeToNextHeartbeat(now)));\n+            client.poll(Math.min(Math.max(0, remaining), timeToNextHeartbeat(now)));\n+\n             now = time.milliseconds();\n-        }\n+            elapsed = now - start;\n+            remaining = timeout - elapsed;\n+        } while (remaining > 0);\n     }\n \n     @Override",
      "parent_sha": "72d5675a7cfc64a5547b921672856ea4afc4f4c2"
    }
  },
  {
    "oid": "0dfc4017b85a4e908662672c412bafcf1d13ad52",
    "message": "KAFKA-18441: Fix flaky KafkaAdminClientTest#testAdminClientApisAuthenticationFailure (#18735)\n\nReviewers: Lianet Magrans <lmagrans@confluent.io>, Chia-Ping Tsai <chia7712@gmail.com>, Andrew Schofield <aschofield@confluent.io>",
    "date": "2025-01-30T08:01:20Z",
    "url": "https://github.com/apache/kafka/commit/0dfc4017b85a4e908662672c412bafcf1d13ad52",
    "details": {
      "sha": "07b4b55e268f203f3eb6d250211d5e94d32bb7a9",
      "filename": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/apache/kafka/blob/0dfc4017b85a4e908662672c412bafcf1d13ad52/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FKafkaAdminClientTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/0dfc4017b85a4e908662672c412bafcf1d13ad52/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FKafkaAdminClientTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fclients%2Fadmin%2FKafkaAdminClientTest.java?ref=0dfc4017b85a4e908662672c412bafcf1d13ad52",
      "patch": "@@ -1788,7 +1788,10 @@ public void testDescribeTopicsWithDescribeTopicPartitionsApiErrorHandling() thro\n     public void testAdminClientApisAuthenticationFailure() {\n         Cluster cluster = mockBootstrapCluster();\n         try (final AdminClientUnitTestEnv env = new AdminClientUnitTestEnv(Time.SYSTEM, cluster,\n-                newStrMap(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, \"1000\"))) {\n+                newStrMap(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, \"1000\",\n+                    // Default \"metadata.recovery.strategy\" is rebootstrap. If it meets \"retry.backoff.ms\" (default is 100L),\n+                    // following assertion will fail. Set it to none to avoid authentication error cleanup.\n+                    AdminClientConfig.METADATA_RECOVERY_STRATEGY_CONFIG, \"none\"))) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n             env.kafkaClient().createPendingAuthenticationError(cluster.nodes().get(0),\n                     TimeUnit.DAYS.toMillis(1));",
      "parent_sha": "617196c68eb64951274a24fd978785e3caa786b5"
    }
  },
  {
    "oid": "4b4102884c8d317fe362523a9f5af20a27a4ee52",
    "message": "HOTFIX: Handle Connector version returning 'null' during plugin loading.\n\nAuthor: Konstantine Karantasis <konstantine@confluent.io>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #3321 from kkonstantine/HOTFIX-Handle-null-version-returned-from-Connector-interface-during-plugin-loading",
    "date": "2017-06-13T21:40:07Z",
    "url": "https://github.com/apache/kafka/commit/4b4102884c8d317fe362523a9f5af20a27a4ee52",
    "details": {
      "sha": "904537aa8da510fa6e435d3e3da6d229fec6bf10",
      "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginDesc.java",
      "status": "modified",
      "additions": 2,
      "deletions": 2,
      "changes": 4,
      "blob_url": "https://github.com/apache/kafka/blob/4b4102884c8d317fe362523a9f5af20a27a4ee52/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fisolation%2FPluginDesc.java",
      "raw_url": "https://github.com/apache/kafka/raw/4b4102884c8d317fe362523a9f5af20a27a4ee52/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fisolation%2FPluginDesc.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect%2Fruntime%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fconnect%2Fruntime%2Fisolation%2FPluginDesc.java?ref=4b4102884c8d317fe362523a9f5af20a27a4ee52",
      "patch": "@@ -33,8 +33,8 @@ public class PluginDesc<T> implements Comparable<PluginDesc<T>> {\n     public PluginDesc(Class<? extends T> klass, String version, ClassLoader loader) {\n         this.klass = klass;\n         this.name = klass.getName();\n-        this.version = version;\n-        this.encodedVersion = new DefaultArtifactVersion(version);\n+        this.version = version != null ? version : \"null\";\n+        this.encodedVersion = new DefaultArtifactVersion(this.version);\n         this.type = PluginType.from(klass);\n         this.typeName = type.toString();\n         this.location = loader instanceof PluginClassLoader",
      "parent_sha": "5c9557857602a4c5a57afd06d98447d02cc325c8"
    }
  },
  {
    "oid": "620f1d88d80fdf8150bd0b75be307bc4a2d3a0ea",
    "message": "Polish Javadoc for EpochState (#11897)\n\nPolish Javadoc for EpochState\r\n\r\nReviewers: Bill Bejeck <bbejeck@apache.org>",
    "date": "2022-03-15T23:58:47Z",
    "url": "https://github.com/apache/kafka/commit/620f1d88d80fdf8150bd0b75be307bc4a2d3a0ea",
    "details": {
      "sha": "9cf231c42131bfda7d1d6e41e6e7018ff5b0b8a7",
      "filename": "raft/src/main/java/org/apache/kafka/raft/EpochState.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/620f1d88d80fdf8150bd0b75be307bc4a2d3a0ea/raft%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fraft%2FEpochState.java",
      "raw_url": "https://github.com/apache/kafka/raw/620f1d88d80fdf8150bd0b75be307bc4a2d3a0ea/raft%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fraft%2FEpochState.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/raft%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fraft%2FEpochState.java?ref=620f1d88d80fdf8150bd0b75be307bc4a2d3a0ea",
      "patch": "@@ -27,7 +27,7 @@ default Optional<LogOffsetMetadata> highWatermark() {\n \n     /**\n      * Decide whether to grant a vote to a candidate, it is the responsibility of the caller to invoke\n-     * {@link QuorumState##transitionToVoted(int, int)} if vote is granted.\n+     * {@link QuorumState#transitionToVoted(int, int)} if vote is granted.\n      *\n      * @param candidateId The ID of the voter who attempt to become leader\n      * @param isLogUpToDate Whether the candidate\u2019s log is at least as up-to-date as receiver\u2019s log, it",
      "parent_sha": "9e8ace080990a6471ed058edffaafa74984649e4"
    }
  },
  {
    "oid": "89286668eb2f7c6629e288dc5925429fbb42fbd4",
    "message": "MINOR: add serde configs to properly set serdes in failing StreamsStaticMembershipTest (#11093)\n\nAfter changing the default serde to be null, some system tests started failing. This test didn't explicitly pass in a serde and didn't set the default config so when the test was trying to setup the source node it wasn't able to find any config to use and threw a config exception.\r\n\r\n Reviewers: Anna Sophie Blee-Goldman <ableegoldman@apache.org>, Bruno Cadonna <cadonna@confluent.io>",
    "date": "2021-07-21T19:54:59Z",
    "url": "https://github.com/apache/kafka/commit/89286668eb2f7c6629e288dc5925429fbb42fbd4",
    "details": {
      "sha": "e4b96fe1053b24f55094659f5408a17db5ce6c07",
      "filename": "streams/src/test/java/org/apache/kafka/streams/tests/StaticMemberTestClient.java",
      "status": "modified",
      "additions": 4,
      "deletions": 1,
      "changes": 5,
      "blob_url": "https://github.com/apache/kafka/blob/89286668eb2f7c6629e288dc5925429fbb42fbd4/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Ftests%2FStaticMemberTestClient.java",
      "raw_url": "https://github.com/apache/kafka/raw/89286668eb2f7c6629e288dc5925429fbb42fbd4/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Ftests%2FStaticMemberTestClient.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Ftests%2FStaticMemberTestClient.java?ref=89286668eb2f7c6629e288dc5925429fbb42fbd4",
      "patch": "@@ -17,6 +17,7 @@\n package org.apache.kafka.streams.tests;\n \n import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.common.utils.Exit;\n import org.apache.kafka.common.utils.Utils;\n import org.apache.kafka.streams.KafkaStreams;\n@@ -52,12 +53,14 @@ public static void main(final String[] args) throws Exception {\n         final StreamsBuilder builder = new StreamsBuilder();\n         final String inputTopic = (String) (Objects.requireNonNull(streamsProperties.remove(\"input.topic\")));\n \n-        final KStream dataStream = builder.stream(inputTopic);\n+        final KStream<String, String> dataStream = builder.stream(inputTopic);\n         dataStream.peek((k, v) ->  System.out.println(String.format(\"PROCESSED key=%s value=%s\", k, v)));\n \n         final Properties config = new Properties();\n         config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, testName);\n         config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000L);\n+        config.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class);\n+        config.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class);\n \n         config.putAll(streamsProperties);\n ",
      "parent_sha": "6dd425e276497f7e22109d94c0d0a57e7fc65ce5"
    }
  },
  {
    "oid": "f7d2a8cd52ca2e88072e177f971c3bc85d842903",
    "message": "MINOR: Cleanup GroupCoordinatorRecordHelpers (#17718)\n\nReviewers: Jeff Kim <jeff.kim@confluent.io>, Mickael Maison <mickael.maison@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2024-11-08T16:00:03Z",
    "url": "https://github.com/apache/kafka/commit/f7d2a8cd52ca2e88072e177f971c3bc85d842903",
    "details": {
      "sha": "b3aa3b9db77fb21450d3f59217d7f29e4f1979b6",
      "filename": "group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupCoordinatorRecordHelpers.java",
      "status": "modified",
      "additions": 3,
      "deletions": 32,
      "changes": 35,
      "blob_url": "https://github.com/apache/kafka/blob/f7d2a8cd52ca2e88072e177f971c3bc85d842903/group-coordinator%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcoordinator%2Fgroup%2FGroupCoordinatorRecordHelpers.java",
      "raw_url": "https://github.com/apache/kafka/raw/f7d2a8cd52ca2e88072e177f971c3bc85d842903/group-coordinator%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcoordinator%2Fgroup%2FGroupCoordinatorRecordHelpers.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/group-coordinator%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcoordinator%2Fgroup%2FGroupCoordinatorRecordHelpers.java?ref=f7d2a8cd52ca2e88072e177f971c3bc85d842903",
      "patch": "@@ -360,35 +360,6 @@ public static CoordinatorRecord newConsumerGroupCurrentAssignmentRecord(\n         );\n     }\n \n-    /**\n-     * Creates a ConsumerGroupCurrentMemberAssignment record.\n-     *\n-     * @param groupId   The consumer group id.\n-     * @param member    The share group member.\n-     * @return The record.\n-     */\n-    public static CoordinatorRecord newConsumerGroupCurrentAssignmentRecord(\n-        String groupId,\n-        ShareGroupMember member\n-    ) {\n-        return new CoordinatorRecord(\n-            new ApiMessageAndVersion(\n-                new ConsumerGroupCurrentMemberAssignmentKey()\n-                    .setGroupId(groupId)\n-                    .setMemberId(member.memberId()),\n-                (short) 8\n-            ),\n-            new ApiMessageAndVersion(\n-                new ConsumerGroupCurrentMemberAssignmentValue()\n-                    .setMemberEpoch(member.memberEpoch())\n-                    .setPreviousMemberEpoch(member.previousMemberEpoch())\n-                    .setState(member.state().value())\n-                    .setAssignedPartitions(toTopicPartitions(member.assignedPartitions())),\n-                (short) 0\n-            )\n-        );\n-    }\n-\n     /**\n      * Creates a ConsumerGroupCurrentMemberAssignment tombstone.\n      *\n@@ -731,7 +702,7 @@ public static CoordinatorRecord newShareGroupSubscriptionMetadataRecord(\n      * @return The record.\n      */\n     public static CoordinatorRecord newShareGroupSubscriptionMetadataTombstoneRecord(\n-            String groupId\n+        String groupId\n     ) {\n         return new CoordinatorRecord(\n             new ApiMessageAndVersion(\n@@ -929,8 +900,8 @@ public static CoordinatorRecord newShareGroupCurrentAssignmentRecord(\n      * @return The record.\n      */\n     public static CoordinatorRecord newShareGroupCurrentAssignmentTombstoneRecord(\n-            String groupId,\n-            String memberId\n+        String groupId,\n+        String memberId\n     ) {\n         return new CoordinatorRecord(\n             new ApiMessageAndVersion(",
      "parent_sha": "02b2fa0e3c6e52ba01a88dc4a949a85cb87177ec"
    }
  },
  {
    "oid": "0b75cf7c0bf71688a2ddcb5c2bf2610836da6943",
    "message": "KAFKA-16705 the flag \"started\" of RaftClusterInstance is false even though the cluster is started (#15946)\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2024-05-29T14:38:00Z",
    "url": "https://github.com/apache/kafka/commit/0b75cf7c0bf71688a2ddcb5c2bf2610836da6943",
    "details": {
      "sha": "9857d4c92cd398daee9000e7b2dfdc22ba78e363",
      "filename": "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
      "status": "modified",
      "additions": 62,
      "deletions": 62,
      "changes": 124,
      "blob_url": "https://github.com/apache/kafka/blob/0b75cf7c0bf71688a2ddcb5c2bf2610836da6943/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Ftest%2Fjunit%2FRaftClusterInvocationContext.java",
      "raw_url": "https://github.com/apache/kafka/raw/0b75cf7c0bf71688a2ddcb5c2bf2610836da6943/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Ftest%2Fjunit%2FRaftClusterInvocationContext.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/core%2Fsrc%2Ftest%2Fjava%2Fkafka%2Ftest%2Fjunit%2FRaftClusterInvocationContext.java?ref=0b75cf7c0bf71688a2ddcb5c2bf2610836da6943",
      "patch": "@@ -48,7 +48,6 @@\n import java.util.concurrent.ConcurrentLinkedQueue;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.atomic.AtomicReference;\n import java.util.stream.Collectors;\n import java.util.stream.Stream;\n \n@@ -67,15 +66,11 @@ public class RaftClusterInvocationContext implements TestTemplateInvocationConte\n \n     private final String baseDisplayName;\n     private final ClusterConfig clusterConfig;\n-    private final AtomicReference<KafkaClusterTestKit> clusterReference;\n-    private final AtomicReference<EmbeddedZookeeper> zkReference;\n     private final boolean isCombined;\n \n     public RaftClusterInvocationContext(String baseDisplayName, ClusterConfig clusterConfig, boolean isCombined) {\n         this.baseDisplayName = baseDisplayName;\n         this.clusterConfig = clusterConfig;\n-        this.clusterReference = new AtomicReference<>();\n-        this.zkReference = new AtomicReference<>();\n         this.isCombined = isCombined;\n     }\n \n@@ -86,67 +81,43 @@ public String getDisplayName(int invocationIndex) {\n \n     @Override\n     public List<Extension> getAdditionalExtensions() {\n-        RaftClusterInstance clusterInstance = new RaftClusterInstance(clusterReference, zkReference, clusterConfig, isCombined);\n+        RaftClusterInstance clusterInstance = new RaftClusterInstance(clusterConfig, isCombined);\n         return Arrays.asList(\n-            (BeforeTestExecutionCallback) context -> {\n-                TestKitNodes nodes = new TestKitNodes.Builder().\n-                        setBootstrapMetadataVersion(clusterConfig.metadataVersion()).\n-                        setCombined(isCombined).\n-                        setNumBrokerNodes(clusterConfig.numBrokers()).\n-                        setPerServerProperties(clusterConfig.perServerOverrideProperties()).\n-                        setNumDisksPerBroker(clusterConfig.numDisksPerBroker()).\n-                        setNumControllerNodes(clusterConfig.numControllers()).build();\n-                KafkaClusterTestKit.Builder builder = new KafkaClusterTestKit.Builder(nodes);\n-\n-                if (Boolean.parseBoolean(clusterConfig.serverProperties().getOrDefault(\"zookeeper.metadata.migration.enable\", \"false\"))) {\n-                    zkReference.set(new EmbeddedZookeeper());\n-                    builder.setConfigProp(\"zookeeper.connect\", String.format(\"localhost:%d\", zkReference.get().port()));\n-                }\n-                // Copy properties into the TestKit builder\n-                clusterConfig.serverProperties().forEach(builder::setConfigProp);\n-                // KAFKA-12512 need to pass security protocol and listener name here\n-                KafkaClusterTestKit cluster = builder.build();\n-                clusterReference.set(cluster);\n-                cluster.format();\n-                if (clusterConfig.isAutoStart()) {\n-                    cluster.startup();\n-                    kafka.utils.TestUtils.waitUntilTrue(\n-                        () -> cluster.brokers().get(0).brokerState() == BrokerState.RUNNING,\n-                        () -> \"Broker never made it to RUNNING state.\",\n-                        org.apache.kafka.test.TestUtils.DEFAULT_MAX_WAIT_MS,\n-                        100L);\n-                }\n-            },\n-            (AfterTestExecutionCallback) context -> clusterInstance.stop(),\n-            new ClusterInstanceParameterResolver(clusterInstance)\n+                (BeforeTestExecutionCallback) context -> {\n+                    clusterInstance.format();\n+                    if (clusterConfig.isAutoStart()) {\n+                        clusterInstance.start();\n+                    }\n+                },\n+                (AfterTestExecutionCallback) context -> clusterInstance.stop(),\n+                new ClusterInstanceParameterResolver(clusterInstance)\n         );\n     }\n \n     public static class RaftClusterInstance implements ClusterInstance {\n \n-        private final AtomicReference<KafkaClusterTestKit> clusterReference;\n-        private final AtomicReference<EmbeddedZookeeper> zkReference;\n         private final ClusterConfig clusterConfig;\n         final AtomicBoolean started = new AtomicBoolean(false);\n         final AtomicBoolean stopped = new AtomicBoolean(false);\n+        final AtomicBoolean formated = new AtomicBoolean(false);\n         private final ConcurrentLinkedQueue<Admin> admins = new ConcurrentLinkedQueue<>();\n+        private EmbeddedZookeeper embeddedZookeeper;\n+        private KafkaClusterTestKit clusterTestKit;\n         private final boolean isCombined;\n \n-        RaftClusterInstance(AtomicReference<KafkaClusterTestKit> clusterReference, AtomicReference<EmbeddedZookeeper> zkReference, ClusterConfig clusterConfig, boolean isCombined) {\n-            this.clusterReference = clusterReference;\n-            this.zkReference = zkReference;\n+        RaftClusterInstance(ClusterConfig clusterConfig, boolean isCombined) {\n             this.clusterConfig = clusterConfig;\n             this.isCombined = isCombined;\n         }\n \n         @Override\n         public String bootstrapServers() {\n-            return clusterReference.get().bootstrapServers();\n+            return clusterTestKit.bootstrapServers();\n         }\n \n         @Override\n         public String bootstrapControllers() {\n-            return clusterReference.get().bootstrapControllers();\n+            return clusterTestKit.bootstrapControllers();\n         }\n \n         @Override\n@@ -193,25 +164,30 @@ public Set<Integer> controllerIds() {\n \n         @Override\n         public KafkaClusterTestKit getUnderlying() {\n-            return clusterReference.get();\n+            return clusterTestKit;\n         }\n \n         @Override\n         public Admin createAdminClient(Properties configOverrides) {\n-            Admin admin = Admin.create(clusterReference.get().\n-                newClientPropertiesBuilder(configOverrides).build());\n+            Admin admin = Admin.create(clusterTestKit.newClientPropertiesBuilder(configOverrides).build());\n             admins.add(admin);\n             return admin;\n         }\n \n         @Override\n         public void start() {\n-            if (started.compareAndSet(false, true)) {\n-                try {\n-                    clusterReference.get().startup();\n-                } catch (Exception e) {\n-                    throw new RuntimeException(\"Failed to start Raft server\", e);\n+            try {\n+                format();\n+                if (started.compareAndSet(false, true)) {\n+                    clusterTestKit.startup();\n+                    kafka.utils.TestUtils.waitUntilTrue(\n+                            () -> this.clusterTestKit.brokers().get(0).brokerState() == BrokerState.RUNNING,\n+                            () -> \"Broker never made it to RUNNING state.\",\n+                            org.apache.kafka.test.TestUtils.DEFAULT_MAX_WAIT_MS,\n+                            100L);\n                 }\n+            } catch (Exception e) {\n+                throw new RuntimeException(\"Failed to start Raft server\", e);\n             }\n         }\n \n@@ -220,9 +196,9 @@ public void stop() {\n             if (stopped.compareAndSet(false, true)) {\n                 admins.forEach(admin -> Utils.closeQuietly(admin, \"admin\"));\n                 admins.clear();\n-                Utils.closeQuietly(clusterReference.get(), \"cluster\");\n-                if (zkReference.get() != null) {\n-                    Utils.closeQuietly(zkReference.get(), \"zk\");\n+                Utils.closeQuietly(clusterTestKit, \"cluster\");\n+                if (embeddedZookeeper != null) {\n+                    Utils.closeQuietly(embeddedZookeeper, \"zk\");\n                 }\n             }\n         }\n@@ -240,27 +216,51 @@ public void startBroker(int brokerId) {\n         @Override\n         public void waitForReadyBrokers() throws InterruptedException {\n             try {\n-                clusterReference.get().waitForReadyBrokers();\n+                clusterTestKit.waitForReadyBrokers();\n             } catch (ExecutionException e) {\n                 throw new AssertionError(\"Failed while waiting for brokers to become ready\", e);\n             }\n         }\n \n-        private BrokerServer findBrokerOrThrow(int brokerId) {\n-            return Optional.ofNullable(clusterReference.get().brokers().get(brokerId))\n-                .orElseThrow(() -> new IllegalArgumentException(\"Unknown brokerId \" + brokerId));\n-        }\n \n         @Override\n         public Map<Integer, KafkaBroker> brokers() {\n-            return clusterReference.get().brokers().entrySet()\n+            return clusterTestKit.brokers().entrySet()\n                     .stream()\n                     .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n         }\n \n         @Override\n         public Map<Integer, ControllerServer> controllers() {\n-            return Collections.unmodifiableMap(clusterReference.get().controllers());\n+            return Collections.unmodifiableMap(clusterTestKit.controllers());\n+        }\n+\n+        public void format() throws Exception {\n+            if (formated.compareAndSet(false, true)) {\n+                TestKitNodes nodes = new TestKitNodes.Builder()\n+                        .setBootstrapMetadataVersion(clusterConfig.metadataVersion())\n+                        .setCombined(isCombined)\n+                        .setNumBrokerNodes(clusterConfig.numBrokers())\n+                        .setNumDisksPerBroker(clusterConfig.numDisksPerBroker())\n+                        .setPerServerProperties(clusterConfig.perServerOverrideProperties())\n+                        .setNumControllerNodes(clusterConfig.numControllers()).build();\n+                KafkaClusterTestKit.Builder builder = new KafkaClusterTestKit.Builder(nodes);\n+                if (Boolean.parseBoolean(clusterConfig.serverProperties()\n+                        .getOrDefault(\"zookeeper.metadata.migration.enable\", \"false\"))) {\n+                    this.embeddedZookeeper = new EmbeddedZookeeper();\n+                    builder.setConfigProp(\"zookeeper.connect\", String.format(\"localhost:%d\", embeddedZookeeper.port()));\n+                }\n+                // Copy properties into the TestKit builder\n+                clusterConfig.serverProperties().forEach(builder::setConfigProp);\n+                // KAFKA-12512 need to pass security protocol and listener name here\n+                this.clusterTestKit = builder.build();\n+                this.clusterTestKit.format();\n+            }\n+        }\n+\n+        private BrokerServer findBrokerOrThrow(int brokerId) {\n+            return Optional.ofNullable(clusterTestKit.brokers().get(brokerId))\n+                    .orElseThrow(() -> new IllegalArgumentException(\"Unknown brokerId \" + brokerId));\n         }\n \n     }",
      "parent_sha": "8d11d9579503426edfaeae791ec4bb212da37ad2"
    }
  },
  {
    "oid": "f8078f380381604f1c7b07204b98e9c08d5e0af1",
    "message": "KAFKA-8736: Track size in InMemoryKeyValueStore (#7177)\n\nInMemoryKeyValueStore uses ConcurrentSkipListMap#size which takes linear time as it iterates over the entire map. We should just track size ourselves for approximateNumEntries\r\n\r\nReviewers: Guozhang Wang <wangguoz@gmail.com>, Matthias J. Sax <mjsax@apache.org>",
    "date": "2019-08-13T21:54:58Z",
    "url": "https://github.com/apache/kafka/commit/f8078f380381604f1c7b07204b98e9c08d5e0af1",
    "details": {
      "sha": "2d68214371ab1eee7abfb0f68e313d350bd2698f",
      "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java",
      "status": "modified",
      "additions": 10,
      "deletions": 13,
      "changes": 23,
      "blob_url": "https://github.com/apache/kafka/blob/f8078f380381604f1c7b07204b98e9c08d5e0af1/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fstate%2Finternals%2FInMemoryKeyValueStore.java",
      "raw_url": "https://github.com/apache/kafka/raw/f8078f380381604f1c7b07204b98e9c08d5e0af1/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fstate%2Finternals%2FInMemoryKeyValueStore.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2Fstate%2Finternals%2FInMemoryKeyValueStore.java?ref=f8078f380381604f1c7b07204b98e9c08d5e0af1",
      "patch": "@@ -35,6 +35,7 @@ public class InMemoryKeyValueStore implements KeyValueStore<Bytes, byte[]> {\n     private final String name;\n     private final ConcurrentNavigableMap<Bytes, byte[]> map = new ConcurrentSkipListMap<>();\n     private volatile boolean open = false;\n+    private long size = 0L; // SkipListMap#size is O(N) so we just do our best to track it\n \n     private static final Logger LOG = LoggerFactory.getLogger(InMemoryKeyValueStore.class);\n \n@@ -50,17 +51,10 @@ public String name() {\n     @Override\n     public void init(final ProcessorContext context,\n                      final StateStore root) {\n-\n+        size = 0;\n         if (root != null) {\n             // register the store\n-            context.register(root, (key, value) -> {\n-                // this is a delete\n-                if (value == null) {\n-                    delete(Bytes.wrap(key));\n-                } else {\n-                    put(Bytes.wrap(key), value);\n-                }\n-            });\n+            context.register(root, (key, value) -> put(Bytes.wrap(key), value));\n         }\n \n         open = true;\n@@ -84,9 +78,9 @@ public byte[] get(final Bytes key) {\n     @Override\n     public void put(final Bytes key, final byte[] value) {\n         if (value == null) {\n-            map.remove(key);\n+            size -= map.remove(key) == null ? 0 : 1;\n         } else {\n-            map.put(key, value);\n+            size += map.put(key, value) == null ? 1 : 0;\n         }\n     }\n \n@@ -108,7 +102,9 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n \n     @Override\n     public byte[] delete(final Bytes key) {\n-        return map.remove(key);\n+        final byte[] oldValue = map.remove(key);\n+        size -= oldValue == null ? 0 : 1;\n+        return oldValue;\n     }\n \n     @Override\n@@ -135,7 +131,7 @@ public KeyValueIterator<Bytes, byte[]> all() {\n \n     @Override\n     public long approximateNumEntries() {\n-        return map.size();\n+        return size;\n     }\n \n     @Override\n@@ -146,6 +142,7 @@ public void flush() {\n     @Override\n     public void close() {\n         map.clear();\n+        size = 0;\n         open = false;\n     }\n ",
      "parent_sha": "ff9e95cb09907739c17b4f4681b11c525515b995"
    }
  },
  {
    "oid": "134f6c07a48219d2b54a6fed38ecb576af2f7cf3",
    "message": "KAFKA-15427: Fix resource leak in integration tests for tiered storage (#14319)\n\nCo-authored-by: Nikhil Ramakrishnan <nikrmk@amazon.com>\r\n\r\nReviewers: Satish Duggana <satishd@apache.org>, Luke Chen <showuon@gmail.com>",
    "date": "2023-09-01T17:42:57Z",
    "url": "https://github.com/apache/kafka/commit/134f6c07a48219d2b54a6fed38ecb576af2f7cf3",
    "details": {
      "sha": "70ce9190d81c21d32102eb559b63d8c51b81e03a",
      "filename": "storage/src/test/java/org/apache/kafka/server/log/remote/metadata/storage/TopicBasedRemoteLogMetadataManagerWrapperWithHarness.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/134f6c07a48219d2b54a6fed38ecb576af2f7cf3/storage%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fserver%2Flog%2Fremote%2Fmetadata%2Fstorage%2FTopicBasedRemoteLogMetadataManagerWrapperWithHarness.java",
      "raw_url": "https://github.com/apache/kafka/raw/134f6c07a48219d2b54a6fed38ecb576af2f7cf3/storage%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fserver%2Flog%2Fremote%2Fmetadata%2Fstorage%2FTopicBasedRemoteLogMetadataManagerWrapperWithHarness.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/storage%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fserver%2Flog%2Fremote%2Fmetadata%2Fstorage%2FTopicBasedRemoteLogMetadataManagerWrapperWithHarness.java?ref=134f6c07a48219d2b54a6fed38ecb576af2f7cf3",
      "patch": "@@ -93,7 +93,7 @@ public long remoteLogSize(TopicIdPartition topicIdPartition, int leaderEpoch) th\n \n     @Override\n     public void close() throws IOException {\n-        remoteLogMetadataManagerHarness.remoteLogMetadataManager().close();\n+        remoteLogMetadataManagerHarness.close();\n     }\n \n     @Override",
      "parent_sha": "6391c6603582a4e4b5bb670233c345e53f82a77b"
    }
  },
  {
    "oid": "9f42739dd32e48aa583780f67e34284cc8cc9505",
    "message": "KAFKA-13558: NioEchoServer fails to close resources (#11618)\n\nDue to resource leaks in the NioEchoServer, at times it won't start\r\nproperly to accept clients and will throw an exception in the\r\nServerSocketChannel.accept() call. Previous to this change, the error\r\nwas not being logged. The logged error was that there were too many open\r\nfiles. Using the UnixOperatingSystemMXBean, I was able to detect that\r\nuse of the NioEchoServer creates several FDs but does not close them.\r\nThis then caused the client to never be able to connect to the server,\r\nso the waitForCondition failed intermittently.\r\n\r\nThis change closes the internal Selector and the AcceptorThread's\r\nselector so that the file descriptors are reclaimed.\r\n\r\nReviewers: Ismael Juma <ismael@juma.me.uk>",
    "date": "2022-02-02T14:11:56Z",
    "url": "https://github.com/apache/kafka/commit/9f42739dd32e48aa583780f67e34284cc8cc9505",
    "details": {
      "sha": "15bfa559dcebb722c56a62298e4e738916359807",
      "filename": "clients/src/test/java/org/apache/kafka/common/network/NioEchoServer.java",
      "status": "modified",
      "additions": 13,
      "deletions": 3,
      "changes": 16,
      "blob_url": "https://github.com/apache/kafka/blob/9f42739dd32e48aa583780f67e34284cc8cc9505/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FNioEchoServer.java",
      "raw_url": "https://github.com/apache/kafka/raw/9f42739dd32e48aa583780f67e34284cc8cc9505/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FNioEchoServer.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Fnetwork%2FNioEchoServer.java?ref=9f42739dd32e48aa583780f67e34284cc8cc9505",
      "patch": "@@ -31,6 +31,7 @@\n import org.apache.kafka.common.utils.LogContext;\n import org.apache.kafka.common.utils.MockTime;\n import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.common.utils.Utils;\n import org.apache.kafka.test.TestUtils;\n \n import java.io.IOException;\n@@ -50,6 +51,8 @@\n import java.util.Locale;\n import java.util.Map;\n import java.util.Set;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import static org.junit.jupiter.api.Assertions.assertEquals;\n \n@@ -59,6 +62,8 @@\n  *\n  */\n public class NioEchoServer extends Thread {\n+    private final static Logger LOG = LoggerFactory.getLogger(NioEchoServer.class);\n+\n     public enum MetricType {\n         TOTAL, RATE, AVG, MAX;\n \n@@ -243,7 +248,7 @@ public void run() {\n                 }\n             }\n         } catch (IOException e) {\n-            // ignore\n+            LOG.warn(e.getMessage(), e);\n         }\n     }\n \n@@ -346,6 +351,7 @@ public void closeSocketChannels() throws IOException {\n     public void close() throws IOException, InterruptedException {\n         this.serverSocketChannel.close();\n         closeSocketChannels();\n+        Utils.closeQuietly(selector, \"selector\");\n         acceptorThread.interrupt();\n         acceptorThread.join();\n         interrupt();\n@@ -358,8 +364,10 @@ public AcceptorThread() {\n         }\n         @Override\n         public void run() {\n+            java.nio.channels.Selector acceptSelector = null;\n+\n             try {\n-                java.nio.channels.Selector acceptSelector = java.nio.channels.Selector.open();\n+                acceptSelector = java.nio.channels.Selector.open();\n                 serverSocketChannel.register(acceptSelector, SelectionKey.OP_ACCEPT);\n                 while (serverSocketChannel.isOpen()) {\n                     if (acceptSelector.select(1000) > 0) {\n@@ -377,7 +385,9 @@ public void run() {\n                     }\n                 }\n             } catch (IOException e) {\n-                // ignore\n+                LOG.warn(e.getMessage(), e);\n+            } finally {\n+                Utils.closeQuietly(acceptSelector, \"acceptSelector\");\n             }\n         }\n     }",
      "parent_sha": "756fa1c7f3666da881a7138acae0af7fc37f926e"
    }
  },
  {
    "oid": "2736a2e50adb87ef94c977acfeeef1cf5294183f",
    "message": "KAFKA-15689: Logging skipped event when expected migration state is wrong (#14646)\n\nAs described in ticket KAFKA-15689, this PR fixes the logging of a migration event when the expected migration state is wrong.\r\n\r\nSigned-off-by: Paolo Patierno <ppatierno@live.com>\r\n\r\nReviewers: Luke Chen <showuon@gmail.com>",
    "date": "2023-10-30T09:59:11Z",
    "url": "https://github.com/apache/kafka/commit/2736a2e50adb87ef94c977acfeeef1cf5294183f",
    "details": {
      "sha": "25a1cf5ba8be777e107bffa3925850e95e6c4f87",
      "filename": "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
      "status": "modified",
      "additions": 8,
      "deletions": 8,
      "changes": 16,
      "blob_url": "https://github.com/apache/kafka/blob/2736a2e50adb87ef94c977acfeeef1cf5294183f/metadata%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fmetadata%2Fmigration%2FKRaftMigrationDriver.java",
      "raw_url": "https://github.com/apache/kafka/raw/2736a2e50adb87ef94c977acfeeef1cf5294183f/metadata%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fmetadata%2Fmigration%2FKRaftMigrationDriver.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/metadata%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fmetadata%2Fmigration%2FKRaftMigrationDriver.java?ref=2736a2e50adb87ef94c977acfeeef1cf5294183f",
      "patch": "@@ -298,12 +298,12 @@ private boolean isValidStateChange(MigrationDriverState newState) {\n         }\n     }\n \n-    private boolean checkDriverState(MigrationDriverState expectedState) {\n+    private boolean checkDriverState(MigrationDriverState expectedState, MigrationEvent migrationEvent) {\n         if (migrationState.equals(expectedState)) {\n             return true;\n         } else {\n             log.info(\"Expected driver state {} but found {}. Not running this event {}.\",\n-                expectedState, migrationState, this.getClass().getSimpleName());\n+                expectedState, migrationState, migrationEvent.getClass().getSimpleName());\n             return false;\n         }\n     }\n@@ -572,7 +572,7 @@ class WaitForControllerQuorumEvent extends MigrationEvent {\n \n         @Override\n         public void run() throws Exception {\n-            if (checkDriverState(MigrationDriverState.WAIT_FOR_CONTROLLER_QUORUM)) {\n+            if (checkDriverState(MigrationDriverState.WAIT_FOR_CONTROLLER_QUORUM, this)) {\n                 if (!firstPublish) {\n                     log.trace(\"Waiting until we have received metadata before proceeding with migration\");\n                     return;\n@@ -618,7 +618,7 @@ public void run() throws Exception {\n     class WaitForZkBrokersEvent extends MigrationEvent {\n         @Override\n         public void run() throws Exception {\n-            if (checkDriverState(MigrationDriverState.WAIT_FOR_BROKERS)) {\n+            if (checkDriverState(MigrationDriverState.WAIT_FOR_BROKERS, this)) {\n                 if (areZkBrokersReadyForMigration()) {\n                     log.info(\"Zk brokers are registered and ready for migration\");\n                     transitionTo(MigrationDriverState.BECOME_CONTROLLER);\n@@ -630,7 +630,7 @@ public void run() throws Exception {\n     class BecomeZkControllerEvent extends MigrationEvent {\n         @Override\n         public void run() throws Exception {\n-            if (checkDriverState(MigrationDriverState.BECOME_CONTROLLER)) {\n+            if (checkDriverState(MigrationDriverState.BECOME_CONTROLLER, this)) {\n                 applyMigrationOperation(\"Claiming ZK controller leadership\", zkMigrationClient::claimControllerLeadership);\n                 if (migrationLeadershipState.zkControllerEpochZkVersion() == -1) {\n                     log.info(\"Unable to claim leadership, will retry until we learn of a different KRaft leader\");\n@@ -648,7 +648,7 @@ public void run() throws Exception {\n     class MigrateMetadataEvent extends MigrationEvent {\n         @Override\n         public void run() throws Exception {\n-            if (!checkDriverState(MigrationDriverState.ZK_MIGRATION)) {\n+            if (!checkDriverState(MigrationDriverState.ZK_MIGRATION, this)) {\n                 return;\n             }\n             Set<Integer> brokersInMetadata = new HashSet<>();\n@@ -715,7 +715,7 @@ public void run() throws Exception {\n     class SyncKRaftMetadataEvent extends MigrationEvent {\n         @Override\n         public void run() throws Exception {\n-            if (checkDriverState(MigrationDriverState.SYNC_KRAFT_TO_ZK)) {\n+            if (checkDriverState(MigrationDriverState.SYNC_KRAFT_TO_ZK, this)) {\n                 // The migration offset will be non-negative at this point, so we just need to check that the image\n                 // we have actually includes the migration metadata.\n                 if (image.highestOffsetAndEpoch().compareTo(migrationLeadershipState.offsetAndEpoch()) < 0) {\n@@ -741,7 +741,7 @@ class SendRPCsToBrokersEvent extends MigrationEvent {\n         @Override\n         public void run() throws Exception {\n             // Ignore sending RPCs to the brokers since we're no longer in the state.\n-            if (checkDriverState(MigrationDriverState.KRAFT_CONTROLLER_TO_BROKER_COMM)) {\n+            if (checkDriverState(MigrationDriverState.KRAFT_CONTROLLER_TO_BROKER_COMM, this)) {\n                 if (image.highestOffsetAndEpoch().compareTo(migrationLeadershipState.offsetAndEpoch()) >= 0) {\n                     log.info(\"Sending RPCs to broker before moving to dual-write mode using \" +\n                             \"at offset and epoch {}\", image.highestOffsetAndEpoch());",
      "parent_sha": "0c7d1fca925943eea49d14ec0ea60917d4ae426b"
    }
  },
  {
    "oid": "35f03ec5c657fc4d2404692e6e1cc8631bb5458a",
    "message": "MINOR: Fix typo in Utils#toPositive (#9943)\n\nReviewers: Luke Chen <showuon@gmail.com>, Chia-Ping Tsai <chia7712@gmail.com>",
    "date": "2021-01-25T07:54:12Z",
    "url": "https://github.com/apache/kafka/commit/35f03ec5c657fc4d2404692e6e1cc8631bb5458a",
    "details": {
      "sha": "198b5b1ba01e1e36b46c351d4c2a4cf2d4addd66",
      "filename": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java",
      "status": "modified",
      "additions": 1,
      "deletions": 1,
      "changes": 2,
      "blob_url": "https://github.com/apache/kafka/blob/35f03ec5c657fc4d2404692e6e1cc8631bb5458a/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Futils%2FUtils.java",
      "raw_url": "https://github.com/apache/kafka/raw/35f03ec5c657fc4d2404692e6e1cc8631bb5458a/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Futils%2FUtils.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fkafka%2Fcommon%2Futils%2FUtils.java?ref=35f03ec5c657fc4d2404692e6e1cc8631bb5458a",
      "patch": "@@ -982,7 +982,7 @@ public static void closeAllQuietly(AtomicReference<Throwable> firstException, St\n     /**\n      * A cheap way to deterministically convert a number to a positive value. When the input is\n      * positive, the original value is returned. When the input number is negative, the returned\n-     * positive value is the original value bit AND against 0x7fffffff which is not its absolutely\n+     * positive value is the original value bit AND against 0x7fffffff which is not its absolute\n      * value.\n      *\n      * Note: changing this method in the future will possibly cause partition selection not to be",
      "parent_sha": "cd0b9280d4ba39f6adf450c70b8c011374c27f12"
    }
  },
  {
    "oid": "9d81a6700916aa5960fea5e98b951411febb5896",
    "message": "MINOR: update flaky KafkaStreamsTest (#16756)\n\ntestStateGlobalThreadClose() does fail sometimes, with unclear root\r\ncause. This PR is an attempt to fix it, by cleaning up and improving the\r\ntest code across the board.\r\n\r\nReviewers: Lucas Brutschy <lbrutschy@confluent.io>",
    "date": "2024-08-21T21:23:27Z",
    "url": "https://github.com/apache/kafka/commit/9d81a6700916aa5960fea5e98b951411febb5896",
    "details": {
      "sha": "6ad4e4afc6ead75e0de8ce59048cd4dbbdeb2a73",
      "filename": "streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java",
      "status": "modified",
      "additions": 49,
      "deletions": 43,
      "changes": 92,
      "blob_url": "https://github.com/apache/kafka/blob/9d81a6700916aa5960fea5e98b951411febb5896/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FKafkaStreamsTest.java",
      "raw_url": "https://github.com/apache/kafka/raw/9d81a6700916aa5960fea5e98b951411febb5896/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FKafkaStreamsTest.java",
      "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fkafka%2Fstreams%2FKafkaStreamsTest.java?ref=9d81a6700916aa5960fea5e98b951411febb5896",
      "patch": "@@ -30,6 +30,7 @@\n import org.apache.kafka.common.metrics.MetricsReporter;\n import org.apache.kafka.common.metrics.Sensor.RecordingLevel;\n import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n import org.apache.kafka.common.serialization.StringSerializer;\n import org.apache.kafka.common.utils.LogCaptureAppender;\n import org.apache.kafka.common.utils.MockTime;\n@@ -138,7 +139,7 @@\n public class KafkaStreamsTest {\n \n     private static final int NUM_THREADS = 2;\n-    private static final String APPLICATION_ID = \"appId\";\n+    private static final String APPLICATION_ID = \"appId-\";\n     private static final String CLIENT_ID = \"test-client\";\n     private static final Duration DEFAULT_DURATION = Duration.ofSeconds(30);\n \n@@ -180,14 +181,14 @@ public void onChange(final KafkaStreams.State newState,\n     }\n \n     @BeforeEach\n-    public void before() throws Exception {\n+    public void before(final TestInfo testInfo) throws Exception {\n         time = new MockTime();\n         supplier = new MockClientSupplier();\n         supplier.setCluster(Cluster.bootstrap(singletonList(new InetSocketAddress(\"localhost\", 9999))));\n         adminClient = (MockAdminClient) supplier.getAdmin(null);\n         streamsStateListener = new StateListenerStub();\n         props = new Properties();\n-        props.put(StreamsConfig.APPLICATION_ID_CONFIG, APPLICATION_ID);\n+        props.put(StreamsConfig.APPLICATION_ID_CONFIG, APPLICATION_ID + safeUniqueTestName(testInfo));\n         props.put(StreamsConfig.CLIENT_ID_CONFIG, CLIENT_ID);\n         props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:2018\");\n         props.put(StreamsConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());\n@@ -851,13 +852,14 @@ public void shouldAllowCleanupBeforeStartAndAfterClose() {\n         prepareStreams();\n         prepareStreamThread(streamThreadOne, 1);\n         prepareStreamThread(streamThreadTwo, 2);\n-        final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time);\n-        try {\n-            streams.cleanUp();\n-            streams.start();\n-        } finally {\n-            streams.close();\n-            streams.cleanUp();\n+        try (final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time)) {\n+            try {\n+                streams.cleanUp();\n+                streams.start();\n+            } finally {\n+                streams.close();\n+                streams.cleanUp();\n+            }\n         }\n     }\n \n@@ -913,19 +915,19 @@ public void shouldThrowOnCleanupWhileShuttingDown() throws Exception {\n         prepareThreadState(streamThreadOne, state1);\n         prepareThreadState(streamThreadTwo, state2);\n         prepareTerminableThread(streamThreadOne);\n-        final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time);\n-        streams.start();\n-        waitForCondition(\n-            () -> streams.state() == KafkaStreams.State.RUNNING,\n-            \"Streams never started.\");\n+        try (final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time)) {\n+            streams.start();\n+            waitForCondition(\n+                () -> streams.state() == KafkaStreams.State.RUNNING,\n+                \"Streams never started.\");\n \n-        streams.close(Duration.ZERO);\n-        assertThat(streams.state() == State.PENDING_SHUTDOWN, equalTo(true));\n-        assertThrows(IllegalStateException.class, streams::cleanUp);\n-        assertThat(streams.state() == State.PENDING_SHUTDOWN, equalTo(true));\n+            streams.close(Duration.ZERO);\n+            assertThat(streams.state() == State.PENDING_SHUTDOWN, equalTo(true));\n+            assertThrows(IllegalStateException.class, streams::cleanUp);\n+            assertThat(streams.state() == State.PENDING_SHUTDOWN, equalTo(true));\n+        }\n     }\n \n-    @SuppressWarnings(\"unchecked\")\n     @Test\n     public void shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeaveGroupFalse() throws Exception {\n         prepareStreams();\n@@ -963,19 +965,20 @@ public void shouldThrowOnCleanupWhileShuttingDownStreamClosedWithCloseOptionLeav\n         prepareThreadState(streamThreadOne, state1);\n         prepareThreadState(streamThreadTwo, state2);\n         prepareTerminableThread(streamThreadOne);\n-        final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time);\n-        streams.start();\n-        waitForCondition(\n+        try (final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time)) {\n+            streams.start();\n+            waitForCondition(\n                 () -> streams.state() == KafkaStreams.State.RUNNING,\n                 \"Streams never started.\");\n \n-        final KafkaStreams.CloseOptions closeOptions = new KafkaStreams.CloseOptions();\n-        closeOptions.timeout(Duration.ZERO);\n+            final KafkaStreams.CloseOptions closeOptions = new KafkaStreams.CloseOptions();\n+            closeOptions.timeout(Duration.ZERO);\n \n-        streams.close(closeOptions);\n-        assertThat(streams.state() == State.PENDING_SHUTDOWN, equalTo(true));\n-        assertThrows(IllegalStateException.class, streams::cleanUp);\n-        assertThat(streams.state() == State.PENDING_SHUTDOWN, equalTo(true));\n+            streams.close(closeOptions);\n+            assertThat(streams.state() == State.PENDING_SHUTDOWN, equalTo(true));\n+            assertThrows(IllegalStateException.class, streams::cleanUp);\n+            assertThat(streams.state() == State.PENDING_SHUTDOWN, equalTo(true));\n+        }\n     }\n \n     @Test\n@@ -1020,12 +1023,12 @@ public void shouldNotGetQueryMetadataWithSerializerWhenNotRunningOrRebalancing()\n         prepareThreadState(streamThreadOne, state1);\n         prepareThreadState(streamThreadTwo, state2);\n         try (final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time)) {\n-            assertThrows(StreamsNotStartedException.class, () -> streams.queryMetadataForKey(\"store\", \"key\", Serdes.String().serializer()));\n+            assertThrows(StreamsNotStartedException.class, () -> streams.queryMetadataForKey(\"store\", \"key\", new StringSerializer()));\n             streams.start();\n             waitForApplicationState(Collections.singletonList(streams), KafkaStreams.State.RUNNING, DEFAULT_DURATION);\n             streams.close();\n             waitForApplicationState(Collections.singletonList(streams), KafkaStreams.State.NOT_RUNNING, DEFAULT_DURATION);\n-            assertThrows(IllegalStateException.class, () -> streams.queryMetadataForKey(\"store\", \"key\", Serdes.String().serializer()));\n+            assertThrows(IllegalStateException.class, () -> streams.queryMetadataForKey(\"store\", \"key\", new StringSerializer()));\n         }\n     }\n \n@@ -1037,7 +1040,7 @@ public void shouldGetQueryMetadataWithSerializerWhenRunningOrRebalancing() {\n \n         try (final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time)) {\n             streams.start();\n-            assertEquals(KeyQueryMetadata.NOT_AVAILABLE, streams.queryMetadataForKey(\"store\", \"key\", Serdes.String().serializer()));\n+            assertEquals(KeyQueryMetadata.NOT_AVAILABLE, streams.queryMetadataForKey(\"store\", \"key\", new StringSerializer()));\n         }\n     }\n \n@@ -1191,7 +1194,6 @@ public void shouldNotBlockInCloseWithCloseOptionLeaveGroupFalseForZeroDuration()\n         }\n     }\n \n-    @SuppressWarnings(\"unchecked\")\n     @Test\n     public void shouldReturnFalseOnCloseWithCloseOptionWithLeaveGroupTrueWhenThreadsHaventTerminated() throws Exception {\n         prepareStreams();\n@@ -1229,7 +1231,6 @@ public void shouldThrowOnNegativeTimeoutForCloseWithCloseOptionLeaveGroupTrue()\n         }\n     }\n \n-    @SuppressWarnings(\"unchecked\")\n     @Test\n     public void shouldNotBlockInCloseWithCloseOptionLeaveGroupTrueForZeroDuration() throws Exception {\n         prepareStreams();\n@@ -1291,7 +1292,9 @@ public void shouldGetClientSupplierFromConfigForConstructor() throws Exception {\n         final StreamsConfig mockConfig = spy(config);\n         when(mockConfig.getKafkaClientSupplier()).thenReturn(supplier);\n \n-        new KafkaStreams(getBuilderWithSource().build(), mockConfig);\n+        try (final KafkaStreams ignored = new KafkaStreams(getBuilderWithSource().build(), mockConfig)) {\n+            // no-op\n+        }\n         // It's called once in above when mock\n         verify(mockConfig, times(2)).getKafkaClientSupplier();\n     }\n@@ -1308,7 +1311,9 @@ public void shouldGetClientSupplierFromConfigForConstructorWithTime() throws Exc\n         final StreamsConfig mockConfig = spy(config);\n         when(mockConfig.getKafkaClientSupplier()).thenReturn(supplier);\n \n-        new KafkaStreams(getBuilderWithSource().build(), mockConfig, time);\n+        try (final KafkaStreams ignored = new KafkaStreams(getBuilderWithSource().build(), mockConfig, time)) {\n+            // no-op\n+        }\n         // It's called once in above when mock\n         verify(mockConfig, times(2)).getKafkaClientSupplier();\n     }\n@@ -1324,7 +1329,9 @@ public void shouldUseProvidedClientSupplier() throws Exception {\n         final StreamsConfig config = new StreamsConfig(props);\n         final StreamsConfig mockConfig = spy(config);\n \n-        new KafkaStreams(getBuilderWithSource().build(), mockConfig, supplier);\n+        try (final KafkaStreams ignored = new KafkaStreams(getBuilderWithSource().build(), mockConfig, supplier)) {\n+            // no-op\n+        }\n         // It's called once in above when mock\n         verify(mockConfig, times(0)).getKafkaClientSupplier();\n     }\n@@ -1389,7 +1396,7 @@ public void statelessTopologyShouldNotCreateStateDirectory(final TestInfo testIn\n         final String inputTopic = safeTestName + \"-input\";\n         final String outputTopic = safeTestName + \"-output\";\n         final Topology topology = new Topology();\n-        topology.addSource(\"source\", Serdes.String().deserializer(), Serdes.String().deserializer(), inputTopic)\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), inputTopic)\n                 .addProcessor(\"process\", () -> new Processor<String, String, String, String>() {\n                     private ProcessorContext<String, String> context;\n \n@@ -1444,8 +1451,7 @@ public void statefulTopologyShouldCreateStateDirectory(final TestInfo testInfo)\n     @Test\n     public void shouldThrowTopologyExceptionOnEmptyTopology() {\n         prepareStreams();\n-        try {\n-            new KafkaStreams(new StreamsBuilder().build(), props, supplier, time);\n+        try (final KafkaStreams ignored = new KafkaStreams(new StreamsBuilder().build(), props, supplier, time)) {\n             fail(\"Should have thrown TopologyException\");\n         } catch (final TopologyException e) {\n             assertThat(\n@@ -1831,7 +1837,7 @@ private Topology getStatefulTopology(final String inputTopic,\n             Serdes.String(),\n             Serdes.Long());\n         final Topology topology = new Topology();\n-        topology.addSource(\"source\", Serdes.String().deserializer(), Serdes.String().deserializer(), inputTopic)\n+        topology.addSource(\"source\", new StringDeserializer(), new StringDeserializer(), inputTopic)\n             .addProcessor(\"process\", () -> new Processor<String, String, String, String>() {\n                 private ProcessorContext<String, String> context;\n \n@@ -1858,8 +1864,8 @@ public void process(final Record<String, String> record) {\n         topology.addGlobalStore(\n             globalStoreBuilder,\n             \"global\",\n-            Serdes.String().deserializer(),\n-            Serdes.String().deserializer(),\n+            new StringDeserializer(),\n+            new StringDeserializer(),\n             globalTopicName,\n             globalTopicName + \"-processor\",\n             new MockProcessorSupplier<>());",
      "parent_sha": "0bb2aee8385c98d38b306f77a8c71875f5d48c1a"
    }
  }
]